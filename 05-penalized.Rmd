\def\cD{\cal{D}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bzero{\mathbf{0}}
\def\bbeta{\boldsymbol \beta}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
```

# (PART) Regression Models {-}

# Ridge Regression

Ridge regression was proposed by @hoerl1970ridge, but is also a special case of Tikhonov regularization. The essential idea is very simple: Knowing the ordinary least squares (OLS) solution, a ridge regression is adding a ridge on the diagonal elements of $\bX^\T \bX$ so that it becomes invertible:

$$\widehat{\bbeta}^\text{ridge} = (\bX^\T \bX + n \lambda \bI)^{-1} \bX^\T \by,$$
It provides a solution of linear regression when multicollinearity happens, especially when the number of variables is larger than the sample size. Alternatively, this is also the solution of a regularized least square estimator. We add an $\ell_2$ penalty to the residual sum of squares, i.e., 

$$
\begin{align}
\widehat{\bbeta}^\text{ridge} =& \argmin_{\bbeta} (\by - \bX \bbeta)^\T (\by - \bX \bbeta) + n \lambda \lVert\bbeta\rVert^2\\
=& \argmin_{\bbeta} \frac{1}{n} \sum_{i=1}^n (y_i - x_i^\T \bbeta)^2 + \lambda \sum_{i=1}^n \beta_j^2,
\end{align}
$$

for some penalty $\lambda > 0$. Ridge regression is used extensively in genetic analysis to address such difficulties. We will start with a motivation example and then discuss the bias-variance trade-off issue. 

## Motivation: Correlated Variables and Convexity

Ridge regression has many advantages. Most notably, it can address highly correlated variables. From an optimization point of view, having highly correlated variables means that the objective function ($\ell_2$ loss) becomes "flat" along certain directions in the parameter domain. This can be seen from the following example, where the true parameters are both 1s while the estimated parameters concludes almost all effects to the first variable. You can change different seed to observe the variability of these parameter estimates and notice that they are quite large. Instead, if we fit a ridge regression, the parameter estimates are relatively stable.  

```{r}
  library(MASS)
  set.seed(2)
  n = 30
  
  # create highly correlated variables and a linear model
  X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
  y = rnorm(n, mean = X[,1] + X[,2])
  
  # compare parameter estimates
  summary(lm(y~X-1))$coef
  
  # note that the true parameters are all 1's
  lm.ridge(y~X-1, lambda=5)
```

The variance of both $\beta_1$ and $\beta_2$ are quite large. This is expected because we know from linear regression that the variance of $\widehat{\bbeta}$ is $\sigma^2 (\bX^\T \bX)^{-1}$. However, since the columns of $\bX$ are highly correlated, the smallest eigenvalue of $\bX^\T \bX$ is close to 0, making the largest eigenvalue of $(\bX^\T \bX)^{-1}$ very large. This can also be interpreted through an optimization point of view. The objective function for an OLS estimator is demonstrated in the following.

```{r}
  beta1 <- seq(0, 3, 0.005)
  beta2 <- seq(-1, 2, 0.005)
  allbeta <- data.matrix(expand.grid(beta1, beta2))
  rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y), 
                length(beta1), length(beta2))
  
  # quantile levels for drawing contour
  quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)
  
  # plot the contour
  contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
  box()
  
  # the truth
  points(1, 1, pch = 19, col = "red", cex = 2)
  
  # the data 
  betahat <- coef(lm(y~X-1))
  points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 2)
```

As an alternative, if we add a ridge regression penalty, the contour is forced to be more convex due to the added eigenvalues. Here is a plot of the Ridge $\ell_2$ penalty.

```{r echo = FALSE}
  pen <- matrix(apply(allbeta, 1, function(b) 3*b %*% b),
                length(beta1), length(beta2))
  
  contour(beta1, beta2, pen, levels = quantile(pen, quanlvl))
  points(1, 1, pch = 19, col = "red", cex = 2)
  box()
```

Hence, by adding this to the OLS objective function, the solution is more stable. This may be interpreted in several different ways such as: 1) the objective function is more convex; 2) the variance of the estimator is smaller. However, this causes some bias too. Choosing the tuning parameter is a balance of the bias-variance trade-off, which will be discussed in the following. 

```{r fig.dim = c(12, 6), out.width = '90%'}
    par(mfrow=c(1, 2))

    # adding a L2 penalty to the objective function
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + b %*% b, X, y),
                  length(beta1), length(beta2))
    
    # the ridge solution
    bh = solve(t(X) %*% X + diag(2)) %*% t(X) %*% y
    
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
    points(1, 1, pch = 19, col = "red", cex = 2)
    points(bh[1], bh[2], pch = 19, col = "blue", cex = 2)
    box()
    
    # adding a larger penalty
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + 10*b %*% b, X, y),
                  length(beta1), length(beta2))
    
    bh = solve(t(X) %*% X + 10*diag(2)) %*% t(X) %*% y
    
    # the ridge solution
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
    points(1, 1, pch = 19, col = "red", cex = 2)
    points(bh[1], bh[2], pch = 19, col = "blue", cex = 2)
    box()
```

## Bias and Variance of Ridge Regression

We can set a relationship between Ridge and OLS, assuming that the OLS estimator exist. 

\begin{align}
\widehat{\bbeta}^\text{ridge} =& (\bX^\T \bX + n\lambda \bI)^{-1} \bX^\T \by \\
=& (\bX^\T \bX + n\lambda \bI)^{-1} (\bX^\T \bX) \color{OrangeRed}{(\bX^\T \bX)^{-1} \bX^\T \by}\\
=& (\bX^\T \bX + n\lambda \bI)^{-1} (\bX^\T \bX) \color{OrangeRed}{\widehat{\bbeta}^\text{ols}}
\end{align}

This leads to a biased estimator (since the OLS estimator is unbiased) if we use any nonzero $\lambda$. 

  * As $\lambda \rightarrow 0$, the ridge solution is eventually the same as OLS
  * As $\lambda \rightarrow \infty$, $\widehat{\bbeta}^\text{ridge} \rightarrow 0$

It can be easier to analyze a case with $\bX^\T \bX = n \bI$, i.e, with standardized and orthogonal columns in $\bX$. Note that in this case, each $\beta_j^{\text{ols}}$ is just the projection of $\by$ onto $\bx_j$, the $j$th column of the design matrix. We also have 

\begin{align}
\widehat{\bbeta}^\text{ridge} =& (\bX^\T \bX + n\lambda \bI)^{-1} (\bX^\T \bX) \widehat{\bbeta}^\text{ols}\\
=& (\bI + \lambda \bI)^{-1}\widehat{\bbeta}^\text{ols}\\
=& (1 + \lambda)^{-1} \widehat{\bbeta}^\text{ols}\\

\Longrightarrow \beta_j^{\text{ridge}} =& \frac{1}{1 + \lambda} \beta_j^\text{ols}
\end{align}

Then in this case, the bias and variance of the ridge estimator can be explicitly expressed: 

  * $\text{Bias}(\beta_j^{\text{ridge}}) = \frac{-\lambda}{1 + \lambda} \beta_j^\text{ols}$ (not zero)
  * $\text{Var}(\beta_j^{\text{ridge}}) = \frac{1}{(1 + \lambda)^2} \text{Var}(\beta_j^\text{ols})$ (reduced from OLS)

Of course, we can ask the question: is it worth it? We could proceed with a simple analysis of the MSE of $\beta$ (dropping $j$):

\begin{align}
\text{MSE}(\beta) &= \E(\widehat{\beta} - \beta)^2 \\
&= \E[\widehat{\beta} - \E(\widehat{\beta})]^2 + \E[\widehat{\beta} - \beta]^2 \\
&= \E[\widehat{\beta} - \E(\widehat{\beta})]^2 + 0 + [\E(\widehat{\beta}) - \beta]^2 \\
&= \Var(\widehat{\beta}) + \text{Bias}^2.
\end{align}

This bias-variance breakdown formula will appear multiple times. Now, plug-in the results developed earlier based on the orthogonal design matrix, and investigate the derivative of the MSE of the Ridge estimator, we have 

\begin{align}
\frac{\partial \text{MSE}(\widehat{\beta}^\text{ridge})}{ \partial \beta} =& \frac{\partial}{\partial \beta} \left[ \frac{1}{(1+\lambda)^2} \Var(\widehat{\beta}^\text{ols}) + \frac{\lambda^2}{(1 + \lambda)^2} \beta^2 \right] \\
=& \frac{2}{(1+\lambda)^3} \left[ \lambda \beta^2 - \Var(\widehat{\beta}^\text{ols}) \right]
\end{align}

Note that when the derivative is negative, increasing $\lambda$ would decrease the MSE. This implies that we can reduce the MSE by choosing a small $\lambda$. Of course the situation is much more involving when the columns in $\bX$ are not orthogonal. However, the following analysis helps to understand a non-orthogonal case. It is essentially re-organizing the columns of $\bX$ into its principle components so that they are still orthogonal. 

Let's first take a singular value decomposition (SVD) of $\bX$, with $\bX = \bU \bD \bV^\T$, then the columns in $\bU$ form an orthonormal basis and columns in $\bU \bD$ are the __principal components__ and $\bV$ defines the principle directions. In addition, we have $n \widehat{\boldsymbol \Sigma} = \bX^\T \bX = \bV \bD^2 \bV^\T$. Assuming that $p < n$, and $\bX$ has full column ranks, then the Ridge estimator fitted $\by$ value can be decomposed as

\begin{align}
\widehat{\by}^\text{ridge} =& \bX \widehat{\beta}^\text{ridge} \\ 
=& \bX (\bX^\T \bX + n \lambda)^{-1} \bX^\T \by \\
=& \bU \bD \bV^\T ( \bV \bD^2 \bV^\T + n \lambda \bV \bV^\T)^{-1} \bV \bD \bU^\T \by \\
=& \bU \bD^2 (n \lambda + \bD^2)^{-1} \bU^\T \by \\
=& \sum_{j = 1}^p \bu_j \left( \frac{d_j^2}{n \lambda + d_j^2} \bu_j^\T \by \right),
\end{align}

where $d_j$ is the $j$th eigenvalue of the PCA. Hence, the Ridge regression fitted value can be understood as 

  * Perform PCA of $\bX$
  * Project $\by$ onto the PCs
  * Shrink the projection $\bu_j^\T \by$ by the factor $d_j^2 / (n \lambda + d_j^2)$
  * Reassemble the PCs using all the shrunken length

## Degrees of Freedom




