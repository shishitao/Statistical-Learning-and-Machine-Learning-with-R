<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Random Forests | Statistical Learning and Machine Learning with R</title>
  <meta name="description" content="A textbook for STAT 542 and 432 at UIUC" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Random Forests | Statistical Learning and Machine Learning with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://teazrq.github.io/SMLR/" />
  
  <meta property="og:description" content="A textbook for STAT 542 and 432 at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Random Forests | Statistical Learning and Machine Learning with R" />
  
  <meta name="twitter:description" content="A textbook for STAT 542 and 432 at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2021-12-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="classification-and-regression-trees.html"/>
<link rel="next" href="boosting.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.20/datatables.js"></script>
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Learning and Machine Learning with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#target-audience"><i class="fa fa-check"></i>Target Audience</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-covered"><i class="fa fa-check"></i>What’s Covered?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="part"><span><b>I Basics Knowledge</b></span></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-basic"><i class="fa fa-check"></i><b>1.2</b> Resources and Guides</a></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>1.3</b> Basic Mathematical Operations</a></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-objects"><i class="fa fa-check"></i><b>1.4</b> Data Objects</a></li>
<li class="chapter" data-level="1.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#readin-and-save-data"><i class="fa fa-check"></i><b>1.5</b> Readin and save data</a></li>
<li class="chapter" data-level="1.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-and-defining-functions"><i class="fa fa-check"></i><b>1.6</b> Using and defining functions</a></li>
<li class="chapter" data-level="1.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#distribution-and-random-numbers"><i class="fa fa-check"></i><b>1.7</b> Distribution and random numbers</a></li>
<li class="chapter" data-level="1.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-packages-and-other-resources"><i class="fa fa-check"></i><b>1.8</b> Using packages and other resources</a></li>
<li class="chapter" data-level="1.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#practice-questions"><i class="fa fa-check"></i><b>1.9</b> Practice questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>2</b> RMarkdown</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rmarkdown.html"><a href="rmarkdown.html#basics-and-resources"><i class="fa fa-check"></i><b>2.1</b> Basics and Resources</a></li>
<li class="chapter" data-level="2.2" data-path="rmarkdown.html"><a href="rmarkdown.html#formatting-text"><i class="fa fa-check"></i><b>2.2</b> Formatting Text</a></li>
<li class="chapter" data-level="2.3" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-r-code"><i class="fa fa-check"></i><b>2.3</b> Adding <code>R</code> Code</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rmarkdown.html"><a href="rmarkdown.html#r-chunks"><i class="fa fa-check"></i><b>2.3.1</b> <code>R</code> Chunks</a></li>
<li class="chapter" data-level="2.3.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-r"><i class="fa fa-check"></i><b>2.3.2</b> Inline <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rmarkdown.html"><a href="rmarkdown.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="rmarkdown.html"><a href="rmarkdown.html#working-directory"><i class="fa fa-check"></i><b>2.5</b> Working Directory</a></li>
<li class="chapter" data-level="2.6" data-path="rmarkdown.html"><a href="rmarkdown.html#plotting"><i class="fa fa-check"></i><b>2.6</b> Plotting</a></li>
<li class="chapter" data-level="2.7" data-path="rmarkdown.html"><a href="rmarkdown.html#chunk-options"><i class="fa fa-check"></i><b>2.7</b> Chunk Options</a></li>
<li class="chapter" data-level="2.8" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-math-with-latex"><i class="fa fa-check"></i><b>2.8</b> Adding Math with LaTeX</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="rmarkdown.html"><a href="rmarkdown.html#displaystyle-latex"><i class="fa fa-check"></i><b>2.8.1</b> Displaystyle LaTeX</a></li>
<li class="chapter" data-level="2.8.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-latex"><i class="fa fa-check"></i><b>2.8.2</b> Inline LaTex</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rmarkdown.html"><a href="rmarkdown.html#output-options"><i class="fa fa-check"></i><b>2.9</b> Output Options</a></li>
<li class="chapter" data-level="2.10" data-path="rmarkdown.html"><a href="rmarkdown.html#try-it"><i class="fa fa-check"></i><b>2.10</b> Try It!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html"><i class="fa fa-check"></i><b>3</b> Linear Algebra Basics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#definition"><i class="fa fa-check"></i><b>3.1</b> Definition</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="optimization-basics.html"><a href="optimization-basics.html"><i class="fa fa-check"></i><b>4</b> Optimization Basics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="optimization-basics.html"><a href="optimization-basics.html#basic-concept"><i class="fa fa-check"></i><b>4.1</b> Basic Concept</a></li>
<li class="chapter" data-level="4.2" data-path="optimization-basics.html"><a href="optimization-basics.html#global_local"><i class="fa fa-check"></i><b>4.2</b> Global vs. Local Optima</a></li>
<li class="chapter" data-level="4.3" data-path="optimization-basics.html"><a href="optimization-basics.html#example-linear-regression-using-optim"><i class="fa fa-check"></i><b>4.3</b> Example: Linear Regression using <code>optim()</code></a></li>
<li class="chapter" data-level="4.4" data-path="optimization-basics.html"><a href="optimization-basics.html#first-and-second-order-properties"><i class="fa fa-check"></i><b>4.4</b> First and Second Order Properties</a></li>
<li class="chapter" data-level="4.5" data-path="optimization-basics.html"><a href="optimization-basics.html#algorithm"><i class="fa fa-check"></i><b>4.5</b> Algorithm</a></li>
<li class="chapter" data-level="4.6" data-path="optimization-basics.html"><a href="optimization-basics.html#second-order-methods"><i class="fa fa-check"></i><b>4.6</b> Second-order Methods</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="optimization-basics.html"><a href="optimization-basics.html#newtons-method"><i class="fa fa-check"></i><b>4.6.1</b> Newton’s Method</a></li>
<li class="chapter" data-level="4.6.2" data-path="optimization-basics.html"><a href="optimization-basics.html#quasi-newton-methods"><i class="fa fa-check"></i><b>4.6.2</b> Quasi-Newton Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="optimization-basics.html"><a href="optimization-basics.html#first-order-methods"><i class="fa fa-check"></i><b>4.7</b> First-order Methods</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent"><i class="fa fa-check"></i><b>4.7.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="4.7.2" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent-example-linear-regression"><i class="fa fa-check"></i><b>4.7.2</b> Gradient Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate"><i class="fa fa-check"></i><b>4.8</b> Coordinate Descent</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent-example-linear-regression"><i class="fa fa-check"></i><b>4.8.1</b> Coordinate Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="optimization-basics.html"><a href="optimization-basics.html#stocastic-gradient-descent"><i class="fa fa-check"></i><b>4.9</b> Stocastic Gradient Descent</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="optimization-basics.html"><a href="optimization-basics.html#mini-batch-stocastic-gradient-descent"><i class="fa fa-check"></i><b>4.9.1</b> Mini-batch Stocastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="optimization-basics.html"><a href="optimization-basics.html#lagrangian-multiplier-for-constrained-problems"><i class="fa fa-check"></i><b>4.10</b> Lagrangian Multiplier for Constrained Problems</a></li>
</ul></li>
<li class="part"><span><b>II Linear and Penalized Linear Models</b></span></li>
<li class="chapter" data-level="5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html"><i class="fa fa-check"></i><b>5</b> Linear Regression and Model Selection</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#example-real-estate-data"><i class="fa fa-check"></i><b>5.1</b> Example: real estate data</a></li>
<li class="chapter" data-level="5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#notation-and-basic-properties"><i class="fa fa-check"></i><b>5.2</b> Notation and Basic Properties</a></li>
<li class="chapter" data-level="5.3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-the-lm-function"><i class="fa fa-check"></i><b>5.3</b> Using the <code>lm()</code> Function</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#adding-covariates"><i class="fa fa-check"></i><b>5.3.1</b> Adding Covariates</a></li>
<li class="chapter" data-level="5.3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#categorical-variables"><i class="fa fa-check"></i><b>5.3.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-criteria"><i class="fa fa-check"></i><b>5.4</b> Model Selection Criteria</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-marrows-c_p"><i class="fa fa-check"></i><b>5.4.1</b> Using Marrows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="5.4.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-aic-and-bic"><i class="fa fa-check"></i><b>5.4.2</b> Using AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-algorithms"><i class="fa fa-check"></i><b>5.5</b> Model Selection Algorithms</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps"><i class="fa fa-check"></i><b>5.5.1</b> Best Subset Selection with <code>leaps</code></a></li>
<li class="chapter" data-level="5.5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step"><i class="fa fa-check"></i><b>5.5.2</b> Step-wise regression using <code>step()</code></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#marrows-cp"><i class="fa fa-check"></i><b>5.6</b> Derivation of Marrows’ <span class="math inline">\(C_p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>6</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ridge-regression.html"><a href="ridge-regression.html#motivation-correlated-variables-and-convexity"><i class="fa fa-check"></i><b>6.1</b> Motivation: Correlated Variables and Convexity</a></li>
<li class="chapter" data-level="6.2" data-path="ridge-regression.html"><a href="ridge-regression.html#bias-and-variance-of-ridge-regression"><i class="fa fa-check"></i><b>6.2</b> Bias and Variance of Ridge Regression</a></li>
<li class="chapter" data-level="6.3" data-path="ridge-regression.html"><a href="ridge-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>6.3</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="6.4" data-path="ridge-regression.html"><a href="ridge-regression.html#using-the-lm.ridge-function"><i class="fa fa-check"></i><b>6.4</b> Using the <code>lm.ridge()</code> function</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue"><i class="fa fa-check"></i><b>6.4.1</b> Scaling Issue</a></li>
<li class="chapter" data-level="6.4.2" data-path="ridge-regression.html"><a href="ridge-regression.html#multiple-lambda-values"><i class="fa fa-check"></i><b>6.4.2</b> Multiple <span class="math inline">\(\lambda\)</span> values</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ridge-regression.html"><a href="ridge-regression.html#cross-validation"><i class="fa fa-check"></i><b>6.5</b> Cross-validation</a></li>
<li class="chapter" data-level="6.6" data-path="ridge-regression.html"><a href="ridge-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>6.6</b> Leave-one-out cross-validation</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="ridge-regression.html"><a href="ridge-regression.html#generalized-cross-validation"><i class="fa fa-check"></i><b>6.6.1</b> Generalized cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="ridge-regression.html"><a href="ridge-regression.html#the-glmnet-package"><i class="fa fa-check"></i><b>6.7</b> The <code>glmnet</code> package</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue-1"><i class="fa fa-check"></i><b>6.7.1</b> Scaling Issue</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>7</b> Lasso</a>
<ul>
<li class="chapter" data-level="7.1" data-path="lasso.html"><a href="lasso.html#one-variable-lasso-and-shrinkage"><i class="fa fa-check"></i><b>7.1</b> One-Variable Lasso and Shrinkage</a></li>
<li class="chapter" data-level="7.2" data-path="lasso.html"><a href="lasso.html#constrained-optimization-view"><i class="fa fa-check"></i><b>7.2</b> Constrained Optimization View</a></li>
<li class="chapter" data-level="7.3" data-path="lasso.html"><a href="lasso.html#the-solution-path"><i class="fa fa-check"></i><b>7.3</b> The Solution Path</a></li>
<li class="chapter" data-level="7.4" data-path="lasso.html"><a href="lasso.html#path-wise-coordinate-descent"><i class="fa fa-check"></i><b>7.4</b> Path-wise Coordinate Descent</a></li>
<li class="chapter" data-level="7.5" data-path="lasso.html"><a href="lasso.html#using-the-glmnet-package"><i class="fa fa-check"></i><b>7.5</b> Using the <code>glmnet</code> package</a></li>
<li class="chapter" data-level="7.6" data-path="lasso.html"><a href="lasso.html#elastic-net"><i class="fa fa-check"></i><b>7.6</b> Elastic-Net</a></li>
</ul></li>
<li class="part"><span><b>III Nonparametric Models</b></span></li>
<li class="chapter" data-level="8" data-path="spline.html"><a href="spline.html"><i class="fa fa-check"></i><b>8</b> Spline</a></li>
<li class="chapter" data-level="9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html"><i class="fa fa-check"></i><b>9</b> K-Neariest Neighber</a>
<ul>
<li class="chapter" data-level="9.1" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#definition-1"><i class="fa fa-check"></i><b>9.1</b> Definition</a></li>
<li class="chapter" data-level="9.2" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-k"><i class="fa fa-check"></i><b>9.2</b> Tuning <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="9.3" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>9.3</b> The Bias-variance Trade-off</a></li>
<li class="chapter" data-level="9.4" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#knn-for-classification"><i class="fa fa-check"></i><b>9.4</b> KNN for Classification</a></li>
<li class="chapter" data-level="9.5" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-1-an-artificial-data"><i class="fa fa-check"></i><b>9.5</b> Example 1: An artificial data</a></li>
<li class="chapter" data-level="9.6" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-with-the-caret-package"><i class="fa fa-check"></i><b>9.6</b> Tuning with the <code>caret</code> Package</a></li>
<li class="chapter" data-level="9.7" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#distance-measures"><i class="fa fa-check"></i><b>9.7</b> Distance Measures</a></li>
<li class="chapter" data-level="9.8" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#nn-error-bound"><i class="fa fa-check"></i><b>9.8</b> 1NN Error Bound</a></li>
<li class="chapter" data-level="9.9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-2-handwritten-digit-data"><i class="fa fa-check"></i><b>9.9</b> Example 2: Handwritten Digit Data</a></li>
<li class="chapter" data-level="9.10" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>9.10</b> Curse of Dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html"><i class="fa fa-check"></i><b>10</b> Kernel Smoothing</a>
<ul>
<li class="chapter" data-level="10.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#knn-vs.-kernel"><i class="fa fa-check"></i><b>10.1</b> KNN vs. Kernel</a></li>
<li class="chapter" data-level="10.2" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#kernel-density-estimations"><i class="fa fa-check"></i><b>10.2</b> Kernel Density Estimations</a></li>
<li class="chapter" data-level="10.3" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>10.3</b> Bias-variance trade-off</a></li>
<li class="chapter" data-level="10.4" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#gaussian-kernel-regression"><i class="fa fa-check"></i><b>10.4</b> Gaussian Kernel Regression</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off-1"><i class="fa fa-check"></i><b>10.4.1</b> Bias-variance Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#choice-of-kernel-functions"><i class="fa fa-check"></i><b>10.5</b> Choice of Kernel Functions</a></li>
<li class="chapter" data-level="10.6" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-linear-regression"><i class="fa fa-check"></i><b>10.6</b> Local Linear Regression</a></li>
<li class="chapter" data-level="10.7" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-polynomial-regression"><i class="fa fa-check"></i><b>10.7</b> Local Polynomial Regression</a></li>
<li class="chapter" data-level="10.8" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#r-implementations"><i class="fa fa-check"></i><b>10.8</b> R Implementations</a></li>
</ul></li>
<li class="part"><span><b>IV Classification Models</b></span></li>
<li class="chapter" data-level="11" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="logistic-regression.html"><a href="logistic-regression.html#modeling-binary-outcomes"><i class="fa fa-check"></i><b>11.1</b> Modeling Binary Outcomes</a></li>
<li class="chapter" data-level="11.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-cleveland-clinic-heart-disease-data"><i class="fa fa-check"></i><b>11.2</b> Example: Cleveland Clinic Heart Disease Data</a></li>
<li class="chapter" data-level="11.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-the-parameters"><i class="fa fa-check"></i><b>11.3</b> Interpretation of the Parameters</a></li>
<li class="chapter" data-level="11.4" data-path="logistic-regression.html"><a href="logistic-regression.html#solving-a-logistic-regression"><i class="fa fa-check"></i><b>11.4</b> Solving a Logistic Regression</a></li>
<li class="chapter" data-level="11.5" data-path="logistic-regression.html"><a href="logistic-regression.html#example-south-africa-heart-data"><i class="fa fa-check"></i><b>11.5</b> Example: South Africa Heart Data</a></li>
<li class="chapter" data-level="11.6" data-path="logistic-regression.html"><a href="logistic-regression.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>11.6</b> Penalized Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>12</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="12.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#bayes-rule"><i class="fa fa-check"></i><b>12.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="12.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>12.2</b> Example: Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="12.3" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>12.3</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="12.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>12.4</b> Example: Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="12.5" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>12.5</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="12.6" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-the-hand-written-digit-data"><i class="fa fa-check"></i><b>12.6</b> Example: the Hand Written Digit Data</a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning Algorithms</b></span></li>
<li class="chapter" data-level="13" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>13</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="13.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>13.1</b> Maximum-margin Classifier</a></li>
<li class="chapter" data-level="13.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-separable-svm"><i class="fa fa-check"></i><b>13.2</b> Linearly Separable SVM</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#from-primal-to-dual"><i class="fa fa-check"></i><b>13.2.1</b> From Primal to Dual</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-non-separable-svm-with-slack-variables"><i class="fa fa-check"></i><b>13.3</b> Linearly Non-separable SVM with Slack Variables</a></li>
<li class="chapter" data-level="13.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-saheart-data"><i class="fa fa-check"></i><b>13.4</b> Example: <code>SAheart</code> Data</a></li>
<li class="chapter" data-level="13.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-svm-via-kernel-trick"><i class="fa fa-check"></i><b>13.5</b> Nonlinear SVM via Kernel Trick</a></li>
<li class="chapter" data-level="13.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-mixture.example-data"><i class="fa fa-check"></i><b>13.6</b> Example: <code>mixture.example</code> Data</a></li>
<li class="chapter" data-level="13.7" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-as-a-penalized-model"><i class="fa fa-check"></i><b>13.7</b> SVM as a Penalized Model</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html"><i class="fa fa-check"></i><b>14</b> Classification and Regression Trees</a>
<ul>
<li class="chapter" data-level="14.1" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#example-classification-tree"><i class="fa fa-check"></i><b>14.1</b> Example: Classification Tree</a></li>
<li class="chapter" data-level="14.2" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#splitting-a-node"><i class="fa fa-check"></i><b>14.2</b> Splitting a Node</a></li>
<li class="chapter" data-level="14.3" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#regression-trees"><i class="fa fa-check"></i><b>14.3</b> Regression Trees</a></li>
<li class="chapter" data-level="14.4" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#predicting-a-target-point"><i class="fa fa-check"></i><b>14.4</b> Predicting a Target Point</a></li>
<li class="chapter" data-level="14.5" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#tuning-a-tree-model"><i class="fa fa-check"></i><b>14.5</b> Tuning a Tree Model</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>15</b> Random Forests</a>
<ul>
<li class="chapter" data-level="15.1" data-path="random-forests.html"><a href="random-forests.html#bagging-predictors"><i class="fa fa-check"></i><b>15.1</b> Bagging Predictors</a></li>
<li class="chapter" data-level="15.2" data-path="random-forests.html"><a href="random-forests.html#random-forests-1"><i class="fa fa-check"></i><b>15.2</b> Random Forests</a></li>
<li class="chapter" data-level="15.3" data-path="random-forests.html"><a href="random-forests.html#effect-of-mtry"><i class="fa fa-check"></i><b>15.3</b> Effect of <code>mtry</code></a></li>
<li class="chapter" data-level="15.4" data-path="random-forests.html"><a href="random-forests.html#effect-of-nodesize"><i class="fa fa-check"></i><b>15.4</b> Effect of <code>nodesize</code></a></li>
<li class="chapter" data-level="15.5" data-path="random-forests.html"><a href="random-forests.html#variable-importance"><i class="fa fa-check"></i><b>15.5</b> Variable Importance</a></li>
<li class="chapter" data-level="15.6" data-path="random-forests.html"><a href="random-forests.html#kernel-view-of-random-forets"><i class="fa fa-check"></i><b>15.6</b> Kernel view of Random Forets</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>16</b> Boosting</a></li>
<li class="part"><span><b>VI Unsupervised Learning</b></span></li>
<li class="chapter" data-level="17" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>17</b> K-Means</a>
<ul>
<li class="chapter" data-level="17.1" data-path="k-means.html"><a href="k-means.html#basic-concepts"><i class="fa fa-check"></i><b>17.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="17.2" data-path="k-means.html"><a href="k-means.html#example-1-iris-data"><i class="fa fa-check"></i><b>17.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="17.3" data-path="k-means.html"><a href="k-means.html#example-2-clustering-of-image-pixels"><i class="fa fa-check"></i><b>17.3</b> Example 2: clustering of image pixels</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>18</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="18.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#basic-concepts-1"><i class="fa fa-check"></i><b>18.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="18.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-1-iris-data-1"><i class="fa fa-check"></i><b>18.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="18.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-2-rna-expression-data"><i class="fa fa-check"></i><b>18.3</b> Example 2: RNA Expression Data</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>19</b> Principle Component Analysis</a>
<ul>
<li class="chapter" data-level="19.1" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html#basic-concepts-2"><i class="fa fa-check"></i><b>19.1</b> Basic Concepts</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html#note-scaling"><i class="fa fa-check"></i><b>19.1.1</b> Note: Scaling</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html#example-1-iris-data-2"><i class="fa fa-check"></i><b>19.2</b> Example 1: <code>iris</code> Data</a></li>
<li class="chapter" data-level="19.3" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html#example-2-handwritten-digits"><i class="fa fa-check"></i><b>19.3</b> Example 2: Handwritten Digits</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="self-organizing-map.html"><a href="self-organizing-map.html"><i class="fa fa-check"></i><b>20</b> Self-Organizing Map</a>
<ul>
<li class="chapter" data-level="20.1" data-path="self-organizing-map.html"><a href="self-organizing-map.html#basic-concepts-3"><i class="fa fa-check"></i><b>20.1</b> Basic Concepts</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="spectral-clustering.html"><a href="spectral-clustering.html"><i class="fa fa-check"></i><b>21</b> Spectral Clustering</a>
<ul>
<li class="chapter" data-level="21.1" data-path="spectral-clustering.html"><a href="spectral-clustering.html#basic-concepts-4"><i class="fa fa-check"></i><b>21.1</b> Basic Concepts</a></li>
</ul></li>
<li class="part"><span><b>VII Reference</b></span></li>
<li class="chapter" data-level="22" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>22</b> Reference</a></li>
<li class="divider"></li>
<li><a href="https://github.com/teazrq/SMLR" target="blank">&copy; 2021 Ruoqing Zhu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning and Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="random-forests" class="section level1" number="15">
<h1><span class="header-section-number">Chapter 15</span> Random Forests</h1>
<p>Roughly speaking, random forests <span class="citation">(<a href="#ref-breiman2001random" role="doc-biblioref">Breiman 2001</a>)</span> are parallelly fitted CART models with some randomness. There are several main components:</p>
<ul>
<li>Bootstrapping of data for each tree using the Bagging idea <span class="citation">(<a href="#ref-breiman1996bagging" role="doc-biblioref">Breiman 1996</a>)</span>, and use the averaged result (for regression) or majority voting (for classification) of all trees as the prediction.</li>
<li>At each internal node, we may not consider all variables. Instead, we consider a randomly selected <code>mtry</code> variables to search for the best split. This idea was inspired by by <span class="citation"><a href="#ref-ho1998random" role="doc-biblioref">Ho</a> (<a href="#ref-ho1998random" role="doc-biblioref">1998</a>)</span>.</li>
<li>For each tree, we will not perform pruning. Instead, we simply stop when the internal node contains no more than <code>nodesize</code> number of observations.</li>
</ul>
<p>Later on, there were various version of random forests that attempts to improve the performance, from both computational and theoretical prospective. We will introduce them later.</p>
<div id="bagging-predictors" class="section level2" number="15.1">
<h2><span class="header-section-number">15.1</span> Bagging Predictors</h2>
<p>CART models may be difficult when dealing with non-axis-aligned decision boundaries. This can be seen from the example below, in a two-dimensional case. The idea of Bagging is that we can fit many CART models, each from a Bootstrap sample, i.e., sample with replacement from the original <span class="math inline">\(n\)</span> observations. The reason that Breiman considered bootstrap samples is because it can approximate the original distribution that generates the data. But the end result is that since each tree may be slightly different from each other, when we stack them, the decision bound can be more “smooth.”</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="random-forests.html#cb137-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate some data </span></span>
<span id="cb137-2"><a href="random-forests.html#cb137-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb137-3"><a href="random-forests.html#cb137-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb137-4"><a href="random-forests.html#cb137-4" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb137-5"><a href="random-forests.html#cb137-5" aria-hidden="true" tabindex="-1"></a>  x2 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb137-6"><a href="random-forests.html#cb137-6" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">ifelse</span>((x1 <span class="sc">+</span> x2 <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">&amp;</span> (x1 <span class="sc">+</span> x2 <span class="sc">&lt;</span> <span class="fl">0.5</span>) , <span class="fl">0.8</span>, <span class="fl">0.2</span>))</span>
<span id="cb137-7"><a href="random-forests.html#cb137-7" aria-hidden="true" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="at">x2 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>))</span></code></pre></div>
<p>Let’s compare the decision rule of CART and Bagging. For CART, the decision line has to be aligned to axis. For Bagging, we use a total of 200 trees, specified by <code>nbagg</code> in the <code>ipred</code> package.</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="random-forests.html#cb138-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit CART</span></span>
<span id="cb138-2"><a href="random-forests.html#cb138-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(rpart)</span>
<span id="cb138-3"><a href="random-forests.html#cb138-3" aria-hidden="true" tabindex="-1"></a>  rpart.fit <span class="ot">=</span> <span class="fu">rpart</span>(<span class="fu">as.factor</span>(y)<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data =</span> <span class="fu">data.frame</span>(x1, x2, y))</span>
<span id="cb138-4"><a href="random-forests.html#cb138-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-5"><a href="random-forests.html#cb138-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># we could fit a different tree using a bootstrap sample</span></span>
<span id="cb138-6"><a href="random-forests.html#cb138-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># rpart.fit = rpart(as.factor(y)~x1+x2, data = data.frame(x1, x2, y)[sample(1:n, n, replace = TRUE), ])</span></span>
<span id="cb138-7"><a href="random-forests.html#cb138-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-8"><a href="random-forests.html#cb138-8" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">predict</span>(rpart.fit, xgrid, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>) <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">201</span>)</span>
<span id="cb138-9"><a href="random-forests.html#cb138-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), pred, <span class="at">levels=</span><span class="fl">0.5</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>,<span class="at">axes=</span><span class="cn">FALSE</span>)</span>
<span id="cb138-10"><a href="random-forests.html#cb138-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(x1, x2, <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb138-11"><a href="random-forests.html#cb138-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">pch=</span><span class="st">&quot;.&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>, <span class="at">col=</span><span class="fu">ifelse</span>(pred, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>))</span>
<span id="cb138-12"><a href="random-forests.html#cb138-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()    </span>
<span id="cb138-13"><a href="random-forests.html#cb138-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">title</span>(<span class="st">&quot;CART&quot;</span>)</span>
<span id="cb138-14"><a href="random-forests.html#cb138-14" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb138-15"><a href="random-forests.html#cb138-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit Bagging</span></span>
<span id="cb138-16"><a href="random-forests.html#cb138-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(ipred)</span>
<span id="cb138-17"><a href="random-forests.html#cb138-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: package &#39;ipred&#39; was built under R version 4.1.2</span></span>
<span id="cb138-18"><a href="random-forests.html#cb138-18" aria-hidden="true" tabindex="-1"></a>  bag.fit <span class="ot">=</span> <span class="fu">bagging</span>(<span class="fu">as.factor</span>(y)<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data =</span> <span class="fu">data.frame</span>(x1, x2, y), <span class="at">nbagg =</span> <span class="dv">200</span>, <span class="at">ns =</span> <span class="dv">400</span>)</span>
<span id="cb138-19"><a href="random-forests.html#cb138-19" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">predict</span>(<span class="fu">prune</span>(bag.fit), xgrid) <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">201</span>)</span>
<span id="cb138-20"><a href="random-forests.html#cb138-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), pred, <span class="at">levels=</span><span class="fl">0.5</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>,<span class="at">axes=</span><span class="cn">FALSE</span>)</span>
<span id="cb138-21"><a href="random-forests.html#cb138-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(x1, x2, <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb138-22"><a href="random-forests.html#cb138-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">pch=</span><span class="st">&quot;.&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>, <span class="at">col=</span><span class="fu">ifelse</span>(pred, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>))</span>
<span id="cb138-23"><a href="random-forests.html#cb138-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span>
<span id="cb138-24"><a href="random-forests.html#cb138-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">title</span>(<span class="st">&quot;Bagging&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-199-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="random-forests-1" class="section level2" number="15.2">
<h2><span class="header-section-number">15.2</span> Random Forests</h2>
<p>Random forests are equipped with this Bootstrapping strategy, but also with other things, which are mentioned previously. They are controlled by several key parameters:</p>
<ul>
<li><code>ntree</code>: number of trees</li>
<li><code>sampsize</code>: how many samples to use when fitting each tree</li>
<li><code>mtry</code>: number of randomly sampled variable to consider at each internal node</li>
<li><code>nodesize</code>: stop splitting when the node sample size is no larger than <code>nodesize</code></li>
</ul>
<p>Using the <code>randomForest</code> package, we can fit the model. It is difficult to visualize this when <code>p &gt; 2</code>. But we can look at the testing error.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="random-forests.html#cb139-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate some data with larger p</span></span>
<span id="cb139-2"><a href="random-forests.html#cb139-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb139-3"><a href="random-forests.html#cb139-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb139-4"><a href="random-forests.html#cb139-4" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb139-5"><a href="random-forests.html#cb139-5" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n<span class="sc">*</span>p, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), n, p)</span>
<span id="cb139-6"><a href="random-forests.html#cb139-6" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">=</span> X[, <span class="dv">1</span>]</span>
<span id="cb139-7"><a href="random-forests.html#cb139-7" aria-hidden="true" tabindex="-1"></a>  x2 <span class="ot">=</span> X[, <span class="dv">2</span>]</span>
<span id="cb139-8"><a href="random-forests.html#cb139-8" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">ifelse</span>((x1 <span class="sc">+</span> x2 <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">&amp;</span> (x1 <span class="sc">+</span> x2 <span class="sc">&lt;</span> <span class="fl">0.5</span>), <span class="fl">0.8</span>, <span class="fl">0.2</span>))</span>
<span id="cb139-9"><a href="random-forests.html#cb139-9" aria-hidden="true" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="at">x2 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>))</span>
<span id="cb139-10"><a href="random-forests.html#cb139-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-11"><a href="random-forests.html#cb139-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit random forests with a selected tuning</span></span>
<span id="cb139-12"><a href="random-forests.html#cb139-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(randomForest)</span>
<span id="cb139-13"><a href="random-forests.html#cb139-13" aria-hidden="true" tabindex="-1"></a><span class="do">## randomForest 4.6-14</span></span>
<span id="cb139-14"><a href="random-forests.html#cb139-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Type rfNews() to see new features/changes/bug fixes.</span></span>
<span id="cb139-15"><a href="random-forests.html#cb139-15" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb139-16"><a href="random-forests.html#cb139-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Attaching package: &#39;randomForest&#39;</span></span>
<span id="cb139-17"><a href="random-forests.html#cb139-17" aria-hidden="true" tabindex="-1"></a><span class="do">## The following object is masked from &#39;package:ggplot2&#39;:</span></span>
<span id="cb139-18"><a href="random-forests.html#cb139-18" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb139-19"><a href="random-forests.html#cb139-19" aria-hidden="true" tabindex="-1"></a><span class="do">##     margin</span></span>
<span id="cb139-20"><a href="random-forests.html#cb139-20" aria-hidden="true" tabindex="-1"></a>  rf.fit <span class="ot">=</span> <span class="fu">randomForest</span>(X, <span class="fu">as.factor</span>(y), <span class="at">ntree =</span> <span class="dv">1000</span>, </span>
<span id="cb139-21"><a href="random-forests.html#cb139-21" aria-hidden="true" tabindex="-1"></a>                        <span class="at">mtry =</span> <span class="dv">7</span>, <span class="at">nodesize =</span> <span class="dv">10</span>, <span class="at">sampsize =</span> <span class="dv">800</span>)</span></code></pre></div>
<p>Instead of generating a set of testing samples labels, let’s directly compare with the “true” decision rule, the Bayes rule.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="random-forests.html#cb140-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the testing data </span></span>
<span id="cb140-2"><a href="random-forests.html#cb140-2" aria-hidden="true" tabindex="-1"></a>  Xtest <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n<span class="sc">*</span>p, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), n, p)</span>
<span id="cb140-3"><a href="random-forests.html#cb140-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb140-4"><a href="random-forests.html#cb140-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the Bayes rule</span></span>
<span id="cb140-5"><a href="random-forests.html#cb140-5" aria-hidden="true" tabindex="-1"></a>  BayesRule <span class="ot">=</span> <span class="fu">ifelse</span>((Xtest[, <span class="dv">1</span>] <span class="sc">+</span> Xtest[, <span class="dv">2</span>] <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">&amp;</span> </span>
<span id="cb140-6"><a href="random-forests.html#cb140-6" aria-hidden="true" tabindex="-1"></a>                     (Xtest[, <span class="dv">1</span>] <span class="sc">+</span> Xtest[, <span class="dv">2</span>] <span class="sc">&lt;</span> <span class="fl">0.5</span>), <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb140-7"><a href="random-forests.html#cb140-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb140-8"><a href="random-forests.html#cb140-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>( (<span class="fu">predict</span>(rf.fit, Xtest) <span class="sc">==</span> <span class="st">&quot;1&quot;</span>) <span class="sc">==</span> BayesRule )</span>
<span id="cb140-9"><a href="random-forests.html#cb140-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.785</span></span></code></pre></div>
</div>
<div id="effect-of-mtry" class="section level2" number="15.3">
<h2><span class="header-section-number">15.3</span> Effect of <code>mtry</code></h2>
<p>In the two dimensional setting, we probably won’t see much difference by using random forests, since the only effective change is <code>mtry = 1</code>, which is not really different than <code>mtry = 2</code> (the CART choice). You can try this by yourself.
However, the difference would be significant in higher dimensional settings, in our case <span class="math inline">\(p=10\)</span>. This is again an issue of bias-variance trade-off. The intuition is that, when we use a small <code>mtry</code>, and when <span class="math inline">\(p\)</span> is large, we may by chance randomly select some irrelevant variables that has nothing to do with the outcome. Then this particular split would be wasted. Missing the true variable may cause larger bias. On the other hand, when we use a large <code>mtry</code>, we will be greedy for signals since we compare many different variables and pick the best one. But this is also as the risk of over-fitting. Hence, tuning is necessary.</p>
<p>Just as an example, let’s try a small <code>mtry</code>:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="random-forests.html#cb141-1" aria-hidden="true" tabindex="-1"></a>  rf.fit <span class="ot">=</span> <span class="fu">randomForest</span>(X, <span class="fu">as.factor</span>(y), <span class="at">ntree =</span> <span class="dv">1000</span>, </span>
<span id="cb141-2"><a href="random-forests.html#cb141-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">mtry =</span> <span class="dv">1</span>, <span class="at">nodesize =</span> <span class="dv">10</span>, <span class="at">sampsize =</span> <span class="dv">800</span>)</span>
<span id="cb141-3"><a href="random-forests.html#cb141-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-4"><a href="random-forests.html#cb141-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>( (<span class="fu">predict</span>(rf.fit, Xtest) <span class="sc">==</span> <span class="st">&quot;1&quot;</span>) <span class="sc">==</span> BayesRule )</span>
<span id="cb141-5"><a href="random-forests.html#cb141-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.634</span></span></code></pre></div>
</div>
<div id="effect-of-nodesize" class="section level2" number="15.4">
<h2><span class="header-section-number">15.4</span> Effect of <code>nodesize</code></h2>
<p>When we use a small <code>nodesize</code>, we are at the risk of over-fitting. This is similar to the 1NN example. When we use large <code>nodesize</code>, there could be under-fitting.</p>
</div>
<div id="variable-importance" class="section level2" number="15.5">
<h2><span class="header-section-number">15.5</span> Variable Importance</h2>
<p>Random forests model provides a way to evaluate the importance of each variable. This can be done by specifying the <code>importance</code> argument. We usually use the <code>MeanDecreaseAccuracy</code> or <code>MeanDecreaseGini</code> column as the summary of the importance of each variable.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="random-forests.html#cb142-1" aria-hidden="true" tabindex="-1"></a>  rf.fit <span class="ot">=</span> <span class="fu">randomForest</span>(X, <span class="fu">as.factor</span>(y), <span class="at">ntree =</span> <span class="dv">1000</span>, </span>
<span id="cb142-2"><a href="random-forests.html#cb142-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">mtry =</span> <span class="dv">7</span>, <span class="at">nodesize =</span> <span class="dv">10</span>, <span class="at">sampsize =</span> <span class="dv">800</span>,</span>
<span id="cb142-3"><a href="random-forests.html#cb142-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">importance=</span><span class="cn">TRUE</span>)</span>
<span id="cb142-4"><a href="random-forests.html#cb142-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-5"><a href="random-forests.html#cb142-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">importance</span>(rf.fit)</span>
<span id="cb142-6"><a href="random-forests.html#cb142-6" aria-hidden="true" tabindex="-1"></a><span class="do">##             0         1 MeanDecreaseAccuracy MeanDecreaseGini</span></span>
<span id="cb142-7"><a href="random-forests.html#cb142-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 1  39.1053057 39.823786           45.4232897         47.79065</span></span>
<span id="cb142-8"><a href="random-forests.html#cb142-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 2  38.1820764 40.119387           45.1964485         54.89580</span></span>
<span id="cb142-9"><a href="random-forests.html#cb142-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 3   3.2719270  1.461298            3.2274895         28.44828</span></span>
<span id="cb142-10"><a href="random-forests.html#cb142-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 4  -0.2777943 -6.287430           -4.8470758         22.09006</span></span>
<span id="cb142-11"><a href="random-forests.html#cb142-11" aria-hidden="true" tabindex="-1"></a><span class="do">## 5   2.0937973  1.654224            2.5256400         28.57575</span></span>
<span id="cb142-12"><a href="random-forests.html#cb142-12" aria-hidden="true" tabindex="-1"></a><span class="do">## 6   2.2354984 -2.435663           -0.1297796         25.29836</span></span>
<span id="cb142-13"><a href="random-forests.html#cb142-13" aria-hidden="true" tabindex="-1"></a><span class="do">## 7   0.2083020  2.724449            2.0679184         24.28751</span></span>
<span id="cb142-14"><a href="random-forests.html#cb142-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 8   0.2018946  3.350897            2.3962745         25.51630</span></span>
<span id="cb142-15"><a href="random-forests.html#cb142-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 9  -1.6159803  2.150674            0.3912234         23.41498</span></span>
<span id="cb142-16"><a href="random-forests.html#cb142-16" aria-hidden="true" tabindex="-1"></a><span class="do">## 10  2.6081961  4.417256            4.8004480         27.80399</span></span></code></pre></div>
</div>
<div id="kernel-view-of-random-forets" class="section level2" number="15.6">
<h2><span class="header-section-number">15.6</span> Kernel view of Random Forets</h2>
<p>I wrote a small function that will extract the kernel weights from a random forests for predicting a testing point <span class="math inline">\(x\)</span>. This is essentially the counts for how many times a training data falls into the same terminal node as <span class="math inline">\(x\)</span>. Since the prediction on <span class="math inline">\(x\)</span> are essentially the average of them in a weighted fashion, this is basically a kernel averaging approach. However, the kernel weights are adaptive to the true structure.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="random-forests.html#cb143-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate the 2 dimensional case</span></span>
<span id="cb143-2"><a href="random-forests.html#cb143-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb143-3"><a href="random-forests.html#cb143-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb143-4"><a href="random-forests.html#cb143-4" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb143-5"><a href="random-forests.html#cb143-5" aria-hidden="true" tabindex="-1"></a>  x2 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb143-6"><a href="random-forests.html#cb143-6" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">ifelse</span>((x1 <span class="sc">+</span> x2 <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">&amp;</span> (x1 <span class="sc">+</span> x2 <span class="sc">&lt;</span> <span class="fl">0.5</span>) , <span class="fl">0.8</span>, <span class="fl">0.2</span>))</span>
<span id="cb143-7"><a href="random-forests.html#cb143-7" aria-hidden="true" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="at">x2 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>))</span>
<span id="cb143-8"><a href="random-forests.html#cb143-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-9"><a href="random-forests.html#cb143-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit a random forest model</span></span>
<span id="cb143-10"><a href="random-forests.html#cb143-10" aria-hidden="true" tabindex="-1"></a>  rf.fit <span class="ot">=</span> <span class="fu">randomForest</span>(<span class="fu">cbind</span>(x1, x2), <span class="fu">as.factor</span>(y), <span class="at">ntree =</span> <span class="dv">300</span>, </span>
<span id="cb143-11"><a href="random-forests.html#cb143-11" aria-hidden="true" tabindex="-1"></a>                        <span class="at">mtry =</span> <span class="dv">1</span>, <span class="at">nodesize =</span> <span class="dv">20</span>, <span class="at">keep.inbag =</span> <span class="cn">TRUE</span>)</span>
<span id="cb143-12"><a href="random-forests.html#cb143-12" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">predict</span>(rf.fit, xgrid) <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">201</span>)</span>
<span id="cb143-13"><a href="random-forests.html#cb143-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb143-14"><a href="random-forests.html#cb143-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="at">mar=</span><span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">2</span>, <span class="fl">0.5</span>))</span>
<span id="cb143-15"><a href="random-forests.html#cb143-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-16"><a href="random-forests.html#cb143-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># check the kernel weight at different points</span></span>
<span id="cb143-17"><a href="random-forests.html#cb143-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plotRFKernel</span>(rf.fit, <span class="fu">data.frame</span>(<span class="fu">cbind</span>(x1, x2)), <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">0.4</span>))</span>
<span id="cb143-18"><a href="random-forests.html#cb143-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plotRFKernel</span>(rf.fit, <span class="fu">data.frame</span>(<span class="fu">cbind</span>(x1, x2)), <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.6</span>))</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-206-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>As contrast, here is the regular Gaussian kernel weights (after some tuning). This effect will play an important role when <span class="math inline">\(p\)</span> is large.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="random-forests.html#cb144-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Gaussian kernel weights</span></span>
<span id="cb144-2"><a href="random-forests.html#cb144-2" aria-hidden="true" tabindex="-1"></a>  onex <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">0.4</span>)</span>
<span id="cb144-3"><a href="random-forests.html#cb144-3" aria-hidden="true" tabindex="-1"></a>  h <span class="ot">=</span> <span class="fl">0.2</span></span>
<span id="cb144-4"><a href="random-forests.html#cb144-4" aria-hidden="true" tabindex="-1"></a>  wt <span class="ot">=</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">rowSums</span>(<span class="fu">sweep</span>(<span class="fu">cbind</span>(x1, x2), <span class="dv">2</span>, onex, <span class="at">FUN =</span> <span class="st">&quot;-&quot;</span>)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>h<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb144-5"><a href="random-forests.html#cb144-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), pred, </span>
<span id="cb144-6"><a href="random-forests.html#cb144-6" aria-hidden="true" tabindex="-1"></a>          <span class="at">levels=</span><span class="fl">0.5</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>,<span class="at">axes=</span><span class="cn">FALSE</span>)</span>
<span id="cb144-7"><a href="random-forests.html#cb144-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(x1, x2, <span class="at">cex =</span> <span class="dv">4</span><span class="sc">*</span>wt<span class="sc">^</span>(<span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>), <span class="at">pch =</span> <span class="dv">1</span>, <span class="at">cex.axis=</span><span class="fl">1.25</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb144-8"><a href="random-forests.html#cb144-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(x1, x2, <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb144-9"><a href="random-forests.html#cb144-9" aria-hidden="true" tabindex="-1"></a>         <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.75</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb144-10"><a href="random-forests.html#cb144-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">pch=</span><span class="st">&quot;.&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>, </span>
<span id="cb144-11"><a href="random-forests.html#cb144-11" aria-hidden="true" tabindex="-1"></a>         <span class="at">col=</span><span class="fu">ifelse</span>(pred, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>))</span>
<span id="cb144-12"><a href="random-forests.html#cb144-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(onex[<span class="dv">1</span>], onex[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">4</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span><span class="dv">4</span>, <span class="at">lwd =</span> <span class="dv">6</span>)</span>
<span id="cb144-13"><a href="random-forests.html#cb144-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-208-1.png" width="45%" style="display: block; margin: auto;" /></p>

</div>
</div>
<h3> Reference</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-breiman1996bagging" class="csl-entry">
Breiman, Leo. 1996. <span>“Bagging Predictors.”</span> <em>Machine Learning</em> 24 (2): 123–40.
</div>
<div id="ref-breiman2001random" class="csl-entry">
———. 2001. <span>“Random Forests.”</span> <em>Machine Learning</em> 45 (1): 5–32.
</div>
<div id="ref-ho1998random" class="csl-entry">
Ho, Tin Kam. 1998. <span>“The Random Subspace Method for Constructing Decision Forests.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 20 (8): 832–44.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-and-regression-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="boosting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "serif",
"size": 1
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
