<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Ridge Regression | Statistical Learning and Machine Learning with R</title>
  <meta name="description" content="A textbook for STAT 542 and 432 at UIUC" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Ridge Regression | Statistical Learning and Machine Learning with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://teazrq.github.io/SMLR/" />
  
  <meta property="og:description" content="A textbook for STAT 542 and 432 at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Ridge Regression | Statistical Learning and Machine Learning with R" />
  
  <meta name="twitter:description" content="A textbook for STAT 542 and 432 at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2021-09-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="optimization-basics.html"/>

<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Learning and Machine Learning with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#target-audience"><i class="fa fa-check"></i>Target Audience</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-covered"><i class="fa fa-check"></i>What’s Covered?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="part"><span><b>I Basics Knowledge</b></span></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-basic"><i class="fa fa-check"></i><b>1.2</b> Resources and Guides</a></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>1.3</b> Basic Mathematical Operations</a></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-objects"><i class="fa fa-check"></i><b>1.4</b> Data Objects</a></li>
<li class="chapter" data-level="1.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#readin-and-save-data"><i class="fa fa-check"></i><b>1.5</b> Readin and save data</a></li>
<li class="chapter" data-level="1.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-and-defining-functions"><i class="fa fa-check"></i><b>1.6</b> Using and defining functions</a></li>
<li class="chapter" data-level="1.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#distribution-and-random-numbers"><i class="fa fa-check"></i><b>1.7</b> Distribution and random numbers</a></li>
<li class="chapter" data-level="1.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-packages-and-other-resources"><i class="fa fa-check"></i><b>1.8</b> Using packages and other resources</a></li>
<li class="chapter" data-level="1.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#practice-questions"><i class="fa fa-check"></i><b>1.9</b> Practice questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>2</b> RMarkdown</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rmarkdown.html"><a href="rmarkdown.html#basics-and-resources"><i class="fa fa-check"></i><b>2.1</b> Basics and Resources</a></li>
<li class="chapter" data-level="2.2" data-path="rmarkdown.html"><a href="rmarkdown.html#formatting-text"><i class="fa fa-check"></i><b>2.2</b> Formatting Text</a></li>
<li class="chapter" data-level="2.3" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-r-code"><i class="fa fa-check"></i><b>2.3</b> Adding <code>R</code> Code</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rmarkdown.html"><a href="rmarkdown.html#r-chunks"><i class="fa fa-check"></i><b>2.3.1</b> <code>R</code> Chunks</a></li>
<li class="chapter" data-level="2.3.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-r"><i class="fa fa-check"></i><b>2.3.2</b> Inline <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rmarkdown.html"><a href="rmarkdown.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="rmarkdown.html"><a href="rmarkdown.html#working-directory"><i class="fa fa-check"></i><b>2.5</b> Working Directory</a></li>
<li class="chapter" data-level="2.6" data-path="rmarkdown.html"><a href="rmarkdown.html#plotting"><i class="fa fa-check"></i><b>2.6</b> Plotting</a></li>
<li class="chapter" data-level="2.7" data-path="rmarkdown.html"><a href="rmarkdown.html#chunk-options"><i class="fa fa-check"></i><b>2.7</b> Chunk Options</a></li>
<li class="chapter" data-level="2.8" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-math-with-latex"><i class="fa fa-check"></i><b>2.8</b> Adding Math with LaTeX</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="rmarkdown.html"><a href="rmarkdown.html#displaystyle-latex"><i class="fa fa-check"></i><b>2.8.1</b> Displaystyle LaTeX</a></li>
<li class="chapter" data-level="2.8.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-latex"><i class="fa fa-check"></i><b>2.8.2</b> Inline LaTex</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rmarkdown.html"><a href="rmarkdown.html#output-options"><i class="fa fa-check"></i><b>2.9</b> Output Options</a></li>
<li class="chapter" data-level="2.10" data-path="rmarkdown.html"><a href="rmarkdown.html#try-it"><i class="fa fa-check"></i><b>2.10</b> Try It!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html"><i class="fa fa-check"></i><b>3</b> Linear Regression and Model Selection</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#example-real-estate-data"><i class="fa fa-check"></i><b>3.1</b> Example: real estate data</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#notation-and-basic-properties"><i class="fa fa-check"></i><b>3.2</b> Notation and Basic Properties</a></li>
<li class="chapter" data-level="3.3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-the-lm-function"><i class="fa fa-check"></i><b>3.3</b> Using the <code>lm()</code> Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#adding-covariates"><i class="fa fa-check"></i><b>3.3.1</b> Adding Covariates</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#categorical-variables"><i class="fa fa-check"></i><b>3.3.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-criteria"><i class="fa fa-check"></i><b>3.4</b> Model Selection Criteria</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-marrows-c_p"><i class="fa fa-check"></i><b>3.4.1</b> Using Marrows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-aic-and-bic"><i class="fa fa-check"></i><b>3.4.2</b> Using AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-algorithms"><i class="fa fa-check"></i><b>3.5</b> Model Selection Algorithms</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps"><i class="fa fa-check"></i><b>3.5.1</b> Best Subset Selection with <code>leaps</code></a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step"><i class="fa fa-check"></i><b>3.5.2</b> Step-wise regression using <code>step()</code></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#marrows-cp"><i class="fa fa-check"></i><b>3.6</b> Derivation of Marrows’ <span class="math inline">\(C_p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="optimization-basics.html"><a href="optimization-basics.html"><i class="fa fa-check"></i><b>4</b> Optimization Basics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="optimization-basics.html"><a href="optimization-basics.html#basic-concept"><i class="fa fa-check"></i><b>4.1</b> Basic Concept</a></li>
<li class="chapter" data-level="4.2" data-path="optimization-basics.html"><a href="optimization-basics.html#global_local"><i class="fa fa-check"></i><b>4.2</b> Global vs. Local Optima</a></li>
<li class="chapter" data-level="4.3" data-path="optimization-basics.html"><a href="optimization-basics.html#example-linear-regression-using-optim"><i class="fa fa-check"></i><b>4.3</b> Example: Linear Regression using <code>optim()</code></a></li>
<li class="chapter" data-level="4.4" data-path="optimization-basics.html"><a href="optimization-basics.html#first-and-second-order-properties"><i class="fa fa-check"></i><b>4.4</b> First and Second Order Properties</a></li>
<li class="chapter" data-level="4.5" data-path="optimization-basics.html"><a href="optimization-basics.html#algorithm"><i class="fa fa-check"></i><b>4.5</b> Algorithm</a></li>
<li class="chapter" data-level="4.6" data-path="optimization-basics.html"><a href="optimization-basics.html#second-order-methods"><i class="fa fa-check"></i><b>4.6</b> Second-order Methods</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="optimization-basics.html"><a href="optimization-basics.html#newtons-method"><i class="fa fa-check"></i><b>4.6.1</b> Newton’s Method</a></li>
<li class="chapter" data-level="4.6.2" data-path="optimization-basics.html"><a href="optimization-basics.html#quasi-newton-methods"><i class="fa fa-check"></i><b>4.6.2</b> Quasi-Newton Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="optimization-basics.html"><a href="optimization-basics.html#first-order-methods"><i class="fa fa-check"></i><b>4.7</b> First-order Methods</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent"><i class="fa fa-check"></i><b>4.7.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="4.7.2" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent-example-linear-regression"><i class="fa fa-check"></i><b>4.7.2</b> Gradient Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent"><i class="fa fa-check"></i><b>4.8</b> Coordinate Descent</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent-example-linear-regression"><i class="fa fa-check"></i><b>4.8.1</b> Coordinate Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="optimization-basics.html"><a href="optimization-basics.html#stocastic-gradient-descent"><i class="fa fa-check"></i><b>4.9</b> Stocastic Gradient Descent</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="optimization-basics.html"><a href="optimization-basics.html#mini-batch-stocastic-gradient-descent"><i class="fa fa-check"></i><b>4.9.1</b> Mini-batch Stocastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="optimization-basics.html"><a href="optimization-basics.html#lagrangian-multiplier-for-constrained-problems"><i class="fa fa-check"></i><b>4.10</b> Lagrangian Multiplier for Constrained Problems</a></li>
</ul></li>
<li class="part"><span><b>II Regression Models</b></span></li>
<li class="chapter" data-level="5" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>5</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ridge-regression.html"><a href="ridge-regression.html#motivation-correlated-variables-and-convexity"><i class="fa fa-check"></i><b>5.1</b> Motivation: Correlated Variables and Convexity</a></li>
<li class="chapter" data-level="5.2" data-path="ridge-regression.html"><a href="ridge-regression.html#bias-and-variance-of-ridge-regression"><i class="fa fa-check"></i><b>5.2</b> Bias and Variance of Ridge Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ridge-regression.html"><a href="ridge-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>5.3</b> Degrees of Freedom</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/teazrq/SMLR" target="blank">&copy; 2021 Ruoqing Zhu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning and Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ridge-regression" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Ridge Regression</h1>
<p>Ridge regression was proposed by <span class="citation"><a href="#ref-hoerl1970ridge" role="doc-biblioref">Hoerl and Kennard</a> (<a href="#ref-hoerl1970ridge" role="doc-biblioref">1970</a>)</span>, but is also a special case of Tikhonov regularization. The essential idea is very simple: Knowing the ordinary least squares (OLS) solution, a ridge regression is adding a ridge on the diagonal elements of <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span> so that it becomes invertible:</p>
<p><span class="math display">\[\widehat{\boldsymbol \beta}^\text{ridge} = (\mathbf{X}^\text{T}\mathbf{X}+ n \lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y},\]</span>
It provides a solution of linear regression when multicollinearity happens, especially when the number of variables is larger than the sample size. Alternatively, this is also the solution of a regularized least square estimator. We add an <span class="math inline">\(\ell_2\)</span> penalty to the residual sum of squares, i.e.,</p>
<p><span class="math display">\[
\begin{align}
\widehat{\boldsymbol \beta}^\text{ridge} =&amp; \mathop{\mathrm{arg\,min}}_{\boldsymbol \beta} (\mathbf{y}- \mathbf{X}\boldsymbol \beta)^\text{T}(\mathbf{y}- \mathbf{X}\boldsymbol \beta) + n \lambda \lVert\boldsymbol \beta\rVert^2\\
=&amp; \mathop{\mathrm{arg\,min}}_{\boldsymbol \beta} \frac{1}{n} \sum_{i=1}^n (y_i - x_i^\text{T}\boldsymbol \beta)^2 + \lambda \sum_{i=1}^n \beta_j^2,
\end{align}
\]</span></p>
<p>for some penalty <span class="math inline">\(\lambda &gt; 0\)</span>. Ridge regression is used extensively in genetic analysis to address such difficulties. We will start with a motivation example and then discuss the bias-variance trade-off issue.</p>
<div id="motivation-correlated-variables-and-convexity" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Motivation: Correlated Variables and Convexity</h2>
<p>Ridge regression has many advantages. Most notably, it can address highly correlated variables. From an optimization point of view, having highly correlated variables means that the objective function (<span class="math inline">\(\ell_2\)</span> loss) becomes “flat” along certain directions in the parameter domain. This can be seen from the following example, where the true parameters are both 1s while the estimated parameters concludes almost all effects to the first variable. You can change different seed to observe the variability of these parameter estimates and notice that they are quite large. Instead, if we fit a ridge regression, the parameter estimates are relatively stable.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="ridge-regression.html#cb1-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(MASS)</span>
<span id="cb1-2"><a href="ridge-regression.html#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb1-3"><a href="ridge-regression.html#cb1-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">30</span></span>
<span id="cb1-4"><a href="ridge-regression.html#cb1-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-5"><a href="ridge-regression.html#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># create highly correlated variables and a linear model</span></span>
<span id="cb1-6"><a href="ridge-regression.html#cb1-6" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.99</span>, <span class="fl">0.99</span>, <span class="dv">1</span>), <span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb1-7"><a href="ridge-regression.html#cb1-7" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> X[,<span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>])</span>
<span id="cb1-8"><a href="ridge-regression.html#cb1-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-9"><a href="ridge-regression.html#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compare parameter estimates</span></span>
<span id="cb1-10"><a href="ridge-regression.html#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summary</span>(<span class="fu">lm</span>(y<span class="sc">~</span>X<span class="dv">-1</span>))<span class="sc">$</span>coef</span>
<span id="cb1-11"><a href="ridge-regression.html#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="do">##     Estimate Std. Error    t value  Pr(&gt;|t|)</span></span>
<span id="cb1-12"><a href="ridge-regression.html#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="do">## X1 1.8461255   1.294541 1.42608527 0.1648987</span></span>
<span id="cb1-13"><a href="ridge-regression.html#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="do">## X2 0.0990278   1.321283 0.07494822 0.9407888</span></span>
<span id="cb1-14"><a href="ridge-regression.html#cb1-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-15"><a href="ridge-regression.html#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># note that the true parameters are all 1&#39;s</span></span>
<span id="cb1-16"><a href="ridge-regression.html#cb1-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm.ridge</span>(y<span class="sc">~</span>X<span class="dv">-1</span>, <span class="at">lambda=</span><span class="dv">5</span>)</span>
<span id="cb1-17"><a href="ridge-regression.html#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="do">##        X1        X2 </span></span>
<span id="cb1-18"><a href="ridge-regression.html#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.9413221 0.8693253</span></span></code></pre></div>
<p>The variance of both <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are quite large. This is expected because we know from linear regression that the variance of <span class="math inline">\(\widehat{\boldsymbol \beta}\)</span> is <span class="math inline">\(\sigma^2 (\mathbf{X}^\text{T}\mathbf{X})^{-1}\)</span>. However, since the columns of <span class="math inline">\(\mathbf{X}\)</span> are highly correlated, the smallest eigenvalue of <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span> is close to 0, making the largest eigenvalue of <span class="math inline">\((\mathbf{X}^\text{T}\mathbf{X})^{-1}\)</span> very large. This can also be interpreted through an optimization point of view. The objective function for an OLS estimator is demonstrated in the following.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="ridge-regression.html#cb2-1" aria-hidden="true" tabindex="-1"></a>  beta1 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="fl">0.005</span>)</span>
<span id="cb2-2"><a href="ridge-regression.html#cb2-2" aria-hidden="true" tabindex="-1"></a>  beta2 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="fl">0.005</span>)</span>
<span id="cb2-3"><a href="ridge-regression.html#cb2-3" aria-hidden="true" tabindex="-1"></a>  allbeta <span class="ot">&lt;-</span> <span class="fu">data.matrix</span>(<span class="fu">expand.grid</span>(beta1, beta2))</span>
<span id="cb2-4"><a href="ridge-regression.html#cb2-4" aria-hidden="true" tabindex="-1"></a>  rss <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">apply</span>(allbeta, <span class="dv">1</span>, <span class="cf">function</span>(b, X, y) <span class="fu">sum</span>((y <span class="sc">-</span> X <span class="sc">%*%</span> b)<span class="sc">^</span><span class="dv">2</span>), X, y), </span>
<span id="cb2-5"><a href="ridge-regression.html#cb2-5" aria-hidden="true" tabindex="-1"></a>                <span class="fu">length</span>(beta1), <span class="fu">length</span>(beta2))</span>
<span id="cb2-6"><a href="ridge-regression.html#cb2-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-7"><a href="ridge-regression.html#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># quantile levels for drawing contour</span></span>
<span id="cb2-8"><a href="ridge-regression.html#cb2-8" aria-hidden="true" tabindex="-1"></a>  quanlvl <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.025</span>, <span class="fl">0.05</span>, <span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>)</span>
<span id="cb2-9"><a href="ridge-regression.html#cb2-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-10"><a href="ridge-regression.html#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the contour</span></span>
<span id="cb2-11"><a href="ridge-regression.html#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb2-12"><a href="ridge-regression.html#cb2-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span>
<span id="cb2-13"><a href="ridge-regression.html#cb2-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-14"><a href="ridge-regression.html#cb2-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the truth</span></span>
<span id="cb2-15"><a href="ridge-regression.html#cb2-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb2-16"><a href="ridge-regression.html#cb2-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-17"><a href="ridge-regression.html#cb2-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the data </span></span>
<span id="cb2-18"><a href="ridge-regression.html#cb2-18" aria-hidden="true" tabindex="-1"></a>  betahat <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm</span>(y<span class="sc">~</span>X<span class="dv">-1</span>))</span>
<span id="cb2-19"><a href="ridge-regression.html#cb2-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(betahat[<span class="dv">1</span>], betahat[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-4-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>As an alternative, if we add a ridge regression penalty, the contour is forced to be more convex due to the added eigenvalues. Here is a plot of the Ridge <span class="math inline">\(\ell_2\)</span> penalty.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-5-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Hence, by adding this to the OLS objective function, the solution is more stable. This may be interpreted in several different ways such as: 1) the objective function is more convex; 2) the variance of the estimator is smaller. However, this causes some bias too. Choosing the tuning parameter is a balance of the bias-variance trade-off, which will be discussed in the following.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="ridge-regression.html#cb3-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb3-2"><a href="ridge-regression.html#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="ridge-regression.html#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># adding a L2 penalty to the objective function</span></span>
<span id="cb3-4"><a href="ridge-regression.html#cb3-4" aria-hidden="true" tabindex="-1"></a>    rss <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">apply</span>(allbeta, <span class="dv">1</span>, <span class="cf">function</span>(b, X, y) <span class="fu">sum</span>((y <span class="sc">-</span> X <span class="sc">%*%</span> b)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> b <span class="sc">%*%</span> b, X, y),</span>
<span id="cb3-5"><a href="ridge-regression.html#cb3-5" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">length</span>(beta1), <span class="fu">length</span>(beta2))</span>
<span id="cb3-6"><a href="ridge-regression.html#cb3-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-7"><a href="ridge-regression.html#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the ridge solution</span></span>
<span id="cb3-8"><a href="ridge-regression.html#cb3-8" aria-hidden="true" tabindex="-1"></a>    bh <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> <span class="fu">diag</span>(<span class="dv">2</span>)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb3-9"><a href="ridge-regression.html#cb3-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-10"><a href="ridge-regression.html#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb3-11"><a href="ridge-regression.html#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb3-12"><a href="ridge-regression.html#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(bh[<span class="dv">1</span>], bh[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb3-13"><a href="ridge-regression.html#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">box</span>()</span>
<span id="cb3-14"><a href="ridge-regression.html#cb3-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-15"><a href="ridge-regression.html#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># adding a larger penalty</span></span>
<span id="cb3-16"><a href="ridge-regression.html#cb3-16" aria-hidden="true" tabindex="-1"></a>    rss <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">apply</span>(allbeta, <span class="dv">1</span>, <span class="cf">function</span>(b, X, y) <span class="fu">sum</span>((y <span class="sc">-</span> X <span class="sc">%*%</span> b)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="dv">10</span><span class="sc">*</span>b <span class="sc">%*%</span> b, X, y),</span>
<span id="cb3-17"><a href="ridge-regression.html#cb3-17" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">length</span>(beta1), <span class="fu">length</span>(beta2))</span>
<span id="cb3-18"><a href="ridge-regression.html#cb3-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-19"><a href="ridge-regression.html#cb3-19" aria-hidden="true" tabindex="-1"></a>    bh <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> <span class="dv">10</span><span class="sc">*</span><span class="fu">diag</span>(<span class="dv">2</span>)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb3-20"><a href="ridge-regression.html#cb3-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-21"><a href="ridge-regression.html#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the ridge solution</span></span>
<span id="cb3-22"><a href="ridge-regression.html#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb3-23"><a href="ridge-regression.html#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb3-24"><a href="ridge-regression.html#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(bh[<span class="dv">1</span>], bh[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb3-25"><a href="ridge-regression.html#cb3-25" aria-hidden="true" tabindex="-1"></a>    <span class="fu">box</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-6-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="bias-and-variance-of-ridge-regression" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Bias and Variance of Ridge Regression</h2>
<p>We can set a relationship between Ridge and OLS, assuming that the OLS estimator exist.</p>
<p><span class="math display">\[\begin{align}
\widehat{\boldsymbol \beta}^\text{ridge} =&amp; (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y}\\
=&amp; (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} (\mathbf{X}^\text{T}\mathbf{X}) \color{OrangeRed}{(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}\mathbf{y}}\\
=&amp; (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} (\mathbf{X}^\text{T}\mathbf{X}) \color{OrangeRed}{\widehat{\boldsymbol \beta}^\text{ols}}
\end{align}\]</span></p>
<p>This leads to a biased estimator (since the OLS estimator is unbiased) if we use any nonzero <span class="math inline">\(\lambda\)</span>.</p>
<ul>
<li>As <span class="math inline">\(\lambda \rightarrow 0\)</span>, the ridge solution is eventually the same as OLS</li>
<li>As <span class="math inline">\(\lambda \rightarrow \infty\)</span>, <span class="math inline">\(\widehat{\boldsymbol \beta}^\text{ridge} \rightarrow 0\)</span></li>
</ul>
<p>It can be easier to analyze a case with <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}= n \mathbf{I}\)</span>, i.e, with standardized and orthogonal columns in <span class="math inline">\(\mathbf{X}\)</span>. Note that in this case, each <span class="math inline">\(\beta_j^{\text{ols}}\)</span> is just the projection of <span class="math inline">\(\mathbf{y}\)</span> onto <span class="math inline">\(\mathbf{x}_j\)</span>, the <span class="math inline">\(j\)</span>th column of the design matrix. We also have</p>
<p><span class="math display">\[\begin{align}
\widehat{\boldsymbol \beta}^\text{ridge} =&amp; (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} (\mathbf{X}^\text{T}\mathbf{X}) \widehat{\boldsymbol \beta}^\text{ols}\\
=&amp; (\mathbf{I}+ \lambda \mathbf{I})^{-1}\widehat{\boldsymbol \beta}^\text{ols}\\
=&amp; (1 + \lambda)^{-1} \widehat{\boldsymbol \beta}^\text{ols}\\

\Longrightarrow \beta_j^{\text{ridge}} =&amp; \frac{1}{1 + \lambda} \beta_j^\text{ols}
\end{align}\]</span></p>
<p>Then in this case, the bias and variance of the ridge estimator can be explicitly expressed:</p>
<ul>
<li><span class="math inline">\(\text{Bias}(\beta_j^{\text{ridge}}) = \frac{-\lambda}{1 + \lambda} \beta_j^\text{ols}\)</span> (not zero)</li>
<li><span class="math inline">\(\text{Var}(\beta_j^{\text{ridge}}) = \frac{1}{(1 + \lambda)^2} \text{Var}(\beta_j^\text{ols})\)</span> (reduced from OLS)</li>
</ul>
<p>Of course, we can ask the question: is it worth it? We could proceed with a simple analysis of the MSE of <span class="math inline">\(\beta\)</span> (dropping <span class="math inline">\(j\)</span>):</p>
<p><span class="math display">\[\begin{align}
\text{MSE}(\beta) &amp;= \text{E}(\widehat{\beta} - \beta)^2 \\
&amp;= \text{E}[\widehat{\beta} - \text{E}(\widehat{\beta})]^2 + \text{E}[\widehat{\beta} - \beta]^2 \\
&amp;= \text{E}[\widehat{\beta} - \text{E}(\widehat{\beta})]^2 + 0 + [\text{E}(\widehat{\beta}) - \beta]^2 \\
&amp;= \text{Var}(\widehat{\beta}) + \text{Bias}^2.
\end{align}\]</span></p>
<p>This bias-variance breakdown formula will appear multiple times. Now, plug-in the results developed earlier based on the orthogonal design matrix, and investigate the derivative of the MSE of the Ridge estimator, we have</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \text{MSE}(\widehat{\beta}^\text{ridge})}{ \partial \beta} =&amp; \frac{\partial}{\partial \beta} \left[ \frac{1}{(1+\lambda)^2} \text{Var}(\widehat{\beta}^\text{ols}) + \frac{\lambda^2}{(1 + \lambda)^2} \beta^2 \right] \\
=&amp; \frac{2}{(1+\lambda)^3} \left[ \lambda \beta^2 - \text{Var}(\widehat{\beta}^\text{ols}) \right]
\end{align}\]</span></p>
<p>Note that when the derivative is negative, increasing <span class="math inline">\(\lambda\)</span> would decrease the MSE. This implies that we can reduce the MSE by choosing a small <span class="math inline">\(\lambda\)</span>. Of course the situation is much more involving when the columns in <span class="math inline">\(\mathbf{X}\)</span> are not orthogonal. However, the following analysis helps to understand a non-orthogonal case. It is essentially re-organizing the columns of <span class="math inline">\(\mathbf{X}\)</span> into its principle components so that they are still orthogonal.</p>
<p>Let’s first take a singular value decomposition (SVD) of <span class="math inline">\(\mathbf{X}\)</span>, with <span class="math inline">\(\mathbf{X}= \mathbf{U}\mathbf{D}\mathbf{V}^\text{T}\)</span>, then the columns in <span class="math inline">\(\mathbf{U}\)</span> form an orthonormal basis and columns in <span class="math inline">\(\mathbf{U}\mathbf{D}\)</span> are the <strong>principal components</strong> and <span class="math inline">\(\mathbf{V}\)</span> defines the principle directions. In addition, we have <span class="math inline">\(n \widehat{\boldsymbol \Sigma} = \mathbf{X}^\text{T}\mathbf{X}= \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}\)</span>. Assuming that <span class="math inline">\(p &lt; n\)</span>, and <span class="math inline">\(\mathbf{X}\)</span> has full column ranks, then the Ridge estimator fitted <span class="math inline">\(\mathbf{y}\)</span> value can be decomposed as</p>
<p><span class="math display">\[\begin{align}
\widehat{\mathbf{y}}^\text{ridge} =&amp; \mathbf{X}\widehat{\beta}^\text{ridge} \\ 
=&amp; \mathbf{X}(\mathbf{X}^\text{T}\mathbf{X}+ n \lambda)^{-1} \mathbf{X}^\text{T}\mathbf{y}\\
=&amp; \mathbf{U}\mathbf{D}\mathbf{V}^\text{T}( \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}+ n \lambda \mathbf{V}\mathbf{V}^\text{T})^{-1} \mathbf{V}\mathbf{D}\mathbf{U}^\text{T}\mathbf{y}\\
=&amp; \mathbf{U}\mathbf{D}^2 (n \lambda + \mathbf{D}^2)^{-1} \mathbf{U}^\text{T}\mathbf{y}\\
=&amp; \sum_{j = 1}^p \mathbf{u}_j \left( \frac{d_j^2}{n \lambda + d_j^2} \mathbf{u}_j^\text{T}\mathbf{y}\right),
\end{align}\]</span></p>
<p>where <span class="math inline">\(d_j\)</span> is the <span class="math inline">\(j\)</span>th eigenvalue of the PCA. Hence, the Ridge regression fitted value can be understood as</p>
<ul>
<li>Perform PCA of <span class="math inline">\(\mathbf{X}\)</span></li>
<li>Project <span class="math inline">\(\mathbf{y}\)</span> onto the PCs</li>
<li>Shrink the projection <span class="math inline">\(\mathbf{u}_j^\text{T}\mathbf{y}\)</span> by the factor <span class="math inline">\(d_j^2 / (n \lambda + d_j^2)\)</span></li>
<li>Reassemble the PCs using all the shrunken length</li>
</ul>
</div>
<div id="degrees-of-freedom" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Degrees of Freedom</h2>

<div id="refs" class="references csl-bib-body hanging-indent">
<div class="csl-entry">
Hoerl, Arthur E, and Robert W Kennard. 1970. <span>“Ridge Regression: Biased Estimation for Nonorthogonal Problems.”</span> <em>Technometrics</em> 12 (1): 55–67.
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-hoerl1970ridge" class="csl-entry">
Hoerl, Arthur E, and Robert W Kennard. 1970. <span>“Ridge Regression: Biased Estimation for Nonorthogonal Problems.”</span> <em>Technometrics</em> 12 (1): 55–67.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="optimization-basics.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "serif",
"size": 1
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
