<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 18 Boosting | Statistical Learning and Machine Learning with R</title>
  <meta name="description" content="A textbook for STAT 542 and 432 at UIUC" />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 18 Boosting | Statistical Learning and Machine Learning with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A textbook for STAT 542 and 432 at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 18 Boosting | Statistical Learning and Machine Learning with R" />
  
  <meta name="twitter:description" content="A textbook for STAT 542 and 432 at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2022-04-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="random-forests.html"/>
<link rel="next" href="k-means.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Learning and Machine Learning with R</a></li>

<li class="divider"></li>
<li><a href="index.html#preface">Preface<span></span></a>
<ul>
<li><a href="index.html#target-audience">Target Audience<span></span></a></li>
<li><a href="index.html#whats-covered">What’s Covered?<span></span></a></li>
<li><a href="index.html#acknowledgements">Acknowledgements<span></span></a></li>
<li><a href="index.html#license">License<span></span></a></li>
</ul></li>
<li class="part"><span><b>I Basics Knowledge<span></span></b></span></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> R and RStudio<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Installing R and RStudio<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-basic"><i class="fa fa-check"></i><b>1.2</b> Resources and Guides<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>1.3</b> Basic Mathematical Operations<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-objects"><i class="fa fa-check"></i><b>1.4</b> Data Objects<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#readin-and-save-data"><i class="fa fa-check"></i><b>1.5</b> Readin and save data<span></span></a></li>
<li class="chapter" data-level="1.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-and-defining-functions"><i class="fa fa-check"></i><b>1.6</b> Using and defining functions<span></span></a></li>
<li class="chapter" data-level="1.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#distribution-and-random-numbers"><i class="fa fa-check"></i><b>1.7</b> Distribution and random numbers<span></span></a></li>
<li class="chapter" data-level="1.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-packages-and-other-resources"><i class="fa fa-check"></i><b>1.8</b> Using packages and other resources<span></span></a></li>
<li class="chapter" data-level="1.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#practice-questions"><i class="fa fa-check"></i><b>1.9</b> Practice questions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>2</b> RMarkdown<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="rmarkdown.html"><a href="rmarkdown.html#basics-and-resources"><i class="fa fa-check"></i><b>2.1</b> Basics and Resources<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="rmarkdown.html"><a href="rmarkdown.html#formatting-text"><i class="fa fa-check"></i><b>2.2</b> Formatting Text<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-r-code"><i class="fa fa-check"></i><b>2.3</b> Adding <code>R</code> Code<span></span></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rmarkdown.html"><a href="rmarkdown.html#r-chunks"><i class="fa fa-check"></i><b>2.3.1</b> <code>R</code> Chunks<span></span></a></li>
<li class="chapter" data-level="2.3.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-r"><i class="fa fa-check"></i><b>2.3.2</b> Inline <code>R</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rmarkdown.html"><a href="rmarkdown.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data<span></span></a></li>
<li class="chapter" data-level="2.5" data-path="rmarkdown.html"><a href="rmarkdown.html#working-directory"><i class="fa fa-check"></i><b>2.5</b> Working Directory<span></span></a></li>
<li class="chapter" data-level="2.6" data-path="rmarkdown.html"><a href="rmarkdown.html#plotting"><i class="fa fa-check"></i><b>2.6</b> Plotting<span></span></a></li>
<li class="chapter" data-level="2.7" data-path="rmarkdown.html"><a href="rmarkdown.html#chunk-options"><i class="fa fa-check"></i><b>2.7</b> Chunk Options<span></span></a></li>
<li class="chapter" data-level="2.8" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-math-with-latex"><i class="fa fa-check"></i><b>2.8</b> Adding Math with LaTeX<span></span></a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="rmarkdown.html"><a href="rmarkdown.html#displaystyle-latex"><i class="fa fa-check"></i><b>2.8.1</b> Displaystyle LaTeX<span></span></a></li>
<li class="chapter" data-level="2.8.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-latex"><i class="fa fa-check"></i><b>2.8.2</b> Inline LaTex<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rmarkdown.html"><a href="rmarkdown.html#output-options"><i class="fa fa-check"></i><b>2.9</b> Output Options<span></span></a></li>
<li class="chapter" data-level="2.10" data-path="rmarkdown.html"><a href="rmarkdown.html#try-it"><i class="fa fa-check"></i><b>2.10</b> Try It!<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html"><i class="fa fa-check"></i><b>3</b> Linear Algebra Basics<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#definition"><i class="fa fa-check"></i><b>3.1</b> Definition<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="optimization-basics.html"><a href="optimization-basics.html"><i class="fa fa-check"></i><b>4</b> Optimization Basics<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="optimization-basics.html"><a href="optimization-basics.html#basic-concept"><i class="fa fa-check"></i><b>4.1</b> Basic Concept<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="optimization-basics.html"><a href="optimization-basics.html#global_local"><i class="fa fa-check"></i><b>4.2</b> Global vs. Local Optima<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="optimization-basics.html"><a href="optimization-basics.html#example-linear-regression-using-optim"><i class="fa fa-check"></i><b>4.3</b> Example: Linear Regression using <code>optim()</code><span></span></a></li>
<li class="chapter" data-level="4.4" data-path="optimization-basics.html"><a href="optimization-basics.html#first-and-second-order-properties"><i class="fa fa-check"></i><b>4.4</b> First and Second Order Properties<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="optimization-basics.html"><a href="optimization-basics.html#algorithm"><i class="fa fa-check"></i><b>4.5</b> Algorithm<span></span></a></li>
<li class="chapter" data-level="4.6" data-path="optimization-basics.html"><a href="optimization-basics.html#second-order-methods"><i class="fa fa-check"></i><b>4.6</b> Second-order Methods<span></span></a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="optimization-basics.html"><a href="optimization-basics.html#newtons-method"><i class="fa fa-check"></i><b>4.6.1</b> Newton’s Method<span></span></a></li>
<li class="chapter" data-level="4.6.2" data-path="optimization-basics.html"><a href="optimization-basics.html#quasi-newton-methods"><i class="fa fa-check"></i><b>4.6.2</b> Quasi-Newton Methods<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="optimization-basics.html"><a href="optimization-basics.html#first-order-methods"><i class="fa fa-check"></i><b>4.7</b> First-order Methods<span></span></a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent"><i class="fa fa-check"></i><b>4.7.1</b> Gradient Descent<span></span></a></li>
<li class="chapter" data-level="4.7.2" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent-example-linear-regression"><i class="fa fa-check"></i><b>4.7.2</b> Gradient Descent Example: Linear Regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate"><i class="fa fa-check"></i><b>4.8</b> Coordinate Descent<span></span></a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent-example-linear-regression"><i class="fa fa-check"></i><b>4.8.1</b> Coordinate Descent Example: Linear Regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="optimization-basics.html"><a href="optimization-basics.html#stocastic-gradient-descent"><i class="fa fa-check"></i><b>4.9</b> Stocastic Gradient Descent<span></span></a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="optimization-basics.html"><a href="optimization-basics.html#mini-batch-stocastic-gradient-descent"><i class="fa fa-check"></i><b>4.9.1</b> Mini-batch Stocastic Gradient Descent<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="optimization-basics.html"><a href="optimization-basics.html#lagrangian-multiplier-for-constrained-problems"><i class="fa fa-check"></i><b>4.10</b> Lagrangian Multiplier for Constrained Problems<span></span></a></li>
</ul></li>
<li class="part"><span><b>II Linear and Penalized Linear Models<span></span></b></span></li>
<li class="chapter" data-level="5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html"><i class="fa fa-check"></i><b>5</b> Linear Regression and Model Selection<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#example-real-estate-data"><i class="fa fa-check"></i><b>5.1</b> Example: real estate data<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#notation-and-basic-properties"><i class="fa fa-check"></i><b>5.2</b> Notation and Basic Properties<span></span></a></li>
<li class="chapter" data-level="5.3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-the-lm-function"><i class="fa fa-check"></i><b>5.3</b> Using the <code>lm()</code> Function<span></span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#adding-covariates"><i class="fa fa-check"></i><b>5.3.1</b> Adding Covariates<span></span></a></li>
<li class="chapter" data-level="5.3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#categorical-variables"><i class="fa fa-check"></i><b>5.3.2</b> Categorical Variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-criteria"><i class="fa fa-check"></i><b>5.4</b> Model Selection Criteria<span></span></a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-marrows-c_p"><i class="fa fa-check"></i><b>5.4.1</b> Using Marrows’ <span class="math inline">\(C_p\)</span><span></span></a></li>
<li class="chapter" data-level="5.4.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-aic-and-bic"><i class="fa fa-check"></i><b>5.4.2</b> Using AIC and BIC<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-algorithms"><i class="fa fa-check"></i><b>5.5</b> Model Selection Algorithms<span></span></a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps"><i class="fa fa-check"></i><b>5.5.1</b> Best Subset Selection with <code>leaps</code><span></span></a></li>
<li class="chapter" data-level="5.5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step"><i class="fa fa-check"></i><b>5.5.2</b> Step-wise regression using <code>step()</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#marrows-cp"><i class="fa fa-check"></i><b>5.6</b> Derivation of Marrows’ <span class="math inline">\(C_p\)</span><span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>6</b> Ridge Regression<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="ridge-regression.html"><a href="ridge-regression.html#motivation-correlated-variables-and-convexity"><i class="fa fa-check"></i><b>6.1</b> Motivation: Correlated Variables and Convexity<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="ridge-regression.html"><a href="ridge-regression.html#ridge-penalty-and-the-reduced-variation"><i class="fa fa-check"></i><b>6.2</b> Ridge Penalty and the Reduced Variation<span></span></a></li>
<li class="chapter" data-level="6.3" data-path="ridge-regression.html"><a href="ridge-regression.html#bias-and-variance-of-ridge-regression"><i class="fa fa-check"></i><b>6.3</b> Bias and Variance of Ridge Regression<span></span></a></li>
<li class="chapter" data-level="6.4" data-path="ridge-regression.html"><a href="ridge-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>6.4</b> Degrees of Freedom<span></span></a></li>
<li class="chapter" data-level="6.5" data-path="ridge-regression.html"><a href="ridge-regression.html#using-the-lm.ridge-function"><i class="fa fa-check"></i><b>6.5</b> Using the <code>lm.ridge()</code> function<span></span></a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue"><i class="fa fa-check"></i><b>6.5.1</b> Scaling Issue<span></span></a></li>
<li class="chapter" data-level="6.5.2" data-path="ridge-regression.html"><a href="ridge-regression.html#multiple-lambda-values"><i class="fa fa-check"></i><b>6.5.2</b> Multiple <span class="math inline">\(\lambda\)</span> values<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="ridge-regression.html"><a href="ridge-regression.html#cross-validation"><i class="fa fa-check"></i><b>6.6</b> Cross-validation<span></span></a></li>
<li class="chapter" data-level="6.7" data-path="ridge-regression.html"><a href="ridge-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>6.7</b> Leave-one-out cross-validation<span></span></a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#generalized-cross-validation"><i class="fa fa-check"></i><b>6.7.1</b> Generalized cross-validation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="ridge-regression.html"><a href="ridge-regression.html#the-glmnet-package"><i class="fa fa-check"></i><b>6.8</b> The <code>glmnet</code> package<span></span></a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue-1"><i class="fa fa-check"></i><b>6.8.1</b> Scaling Issue<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>7</b> Lasso<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="lasso.html"><a href="lasso.html#one-variable-lasso-and-shrinkage"><i class="fa fa-check"></i><b>7.1</b> One-Variable Lasso and Shrinkage<span></span></a></li>
<li class="chapter" data-level="7.2" data-path="lasso.html"><a href="lasso.html#constrained-optimization-view"><i class="fa fa-check"></i><b>7.2</b> Constrained Optimization View<span></span></a></li>
<li class="chapter" data-level="7.3" data-path="lasso.html"><a href="lasso.html#the-solution-path"><i class="fa fa-check"></i><b>7.3</b> The Solution Path<span></span></a></li>
<li class="chapter" data-level="7.4" data-path="lasso.html"><a href="lasso.html#path-wise-coordinate-descent"><i class="fa fa-check"></i><b>7.4</b> Path-wise Coordinate Descent<span></span></a></li>
<li class="chapter" data-level="7.5" data-path="lasso.html"><a href="lasso.html#using-the-glmnet-package"><i class="fa fa-check"></i><b>7.5</b> Using the <code>glmnet</code> package<span></span></a></li>
<li class="chapter" data-level="7.6" data-path="lasso.html"><a href="lasso.html#elastic-net"><i class="fa fa-check"></i><b>7.6</b> Elastic-Net<span></span></a></li>
</ul></li>
<li class="part"><span><b>III Nonparametric Models<span></span></b></span></li>
<li class="chapter" data-level="8" data-path="spline.html"><a href="spline.html"><i class="fa fa-check"></i><b>8</b> Spline<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="spline.html"><a href="spline.html#from-linear-to-nonlinear"><i class="fa fa-check"></i><b>8.1</b> From Linear to Nonlinear<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="spline.html"><a href="spline.html#a-motivating-example-and-polynomials"><i class="fa fa-check"></i><b>8.2</b> A Motivating Example and Polynomials<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="spline.html"><a href="spline.html#piecewise-polynomials"><i class="fa fa-check"></i><b>8.3</b> Piecewise Polynomials<span></span></a></li>
<li class="chapter" data-level="8.4" data-path="spline.html"><a href="spline.html#splines"><i class="fa fa-check"></i><b>8.4</b> Splines<span></span></a></li>
<li class="chapter" data-level="8.5" data-path="spline.html"><a href="spline.html#spline-basis"><i class="fa fa-check"></i><b>8.5</b> Spline Basis<span></span></a></li>
<li class="chapter" data-level="8.6" data-path="spline.html"><a href="spline.html#natural-cubic-spline"><i class="fa fa-check"></i><b>8.6</b> Natural Cubic Spline<span></span></a></li>
<li class="chapter" data-level="8.7" data-path="spline.html"><a href="spline.html#smoothing-spline"><i class="fa fa-check"></i><b>8.7</b> Smoothing Spline<span></span></a></li>
<li class="chapter" data-level="8.8" data-path="spline.html"><a href="spline.html#fitting-smoothing-splines"><i class="fa fa-check"></i><b>8.8</b> Fitting Smoothing Splines<span></span></a></li>
<li class="chapter" data-level="8.9" data-path="spline.html"><a href="spline.html#extending-splines-to-multiple-varibles"><i class="fa fa-check"></i><b>8.9</b> Extending Splines to Multiple Varibles<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html"><i class="fa fa-check"></i><b>9</b> K-Neariest Neighber<span></span></a>
<ul>
<li class="chapter" data-level="9.1" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#definition-1"><i class="fa fa-check"></i><b>9.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="9.2" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-k"><i class="fa fa-check"></i><b>9.2</b> Tuning <span class="math inline">\(k\)</span><span></span></a></li>
<li class="chapter" data-level="9.3" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>9.3</b> The Bias-variance Trade-off<span></span></a></li>
<li class="chapter" data-level="9.4" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#knn-for-classification"><i class="fa fa-check"></i><b>9.4</b> KNN for Classification<span></span></a></li>
<li class="chapter" data-level="9.5" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-1-an-artificial-data"><i class="fa fa-check"></i><b>9.5</b> Example 1: An artificial data<span></span></a></li>
<li class="chapter" data-level="9.6" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-with-the-caret-package"><i class="fa fa-check"></i><b>9.6</b> Tuning with the <code>caret</code> Package<span></span></a></li>
<li class="chapter" data-level="9.7" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#distance-measures"><i class="fa fa-check"></i><b>9.7</b> Distance Measures<span></span></a></li>
<li class="chapter" data-level="9.8" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#nn-error-bound"><i class="fa fa-check"></i><b>9.8</b> 1NN Error Bound<span></span></a></li>
<li class="chapter" data-level="9.9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-2-handwritten-digit-data"><i class="fa fa-check"></i><b>9.9</b> Example 2: Handwritten Digit Data<span></span></a></li>
<li class="chapter" data-level="9.10" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>9.10</b> Curse of Dimensionality<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html"><i class="fa fa-check"></i><b>10</b> Kernel Smoothing<span></span></a>
<ul>
<li class="chapter" data-level="10.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#knn-vs.-kernel"><i class="fa fa-check"></i><b>10.1</b> KNN vs. Kernel<span></span></a></li>
<li class="chapter" data-level="10.2" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#kernel-density-estimations"><i class="fa fa-check"></i><b>10.2</b> Kernel Density Estimations<span></span></a></li>
<li class="chapter" data-level="10.3" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>10.3</b> Bias-variance trade-off<span></span></a></li>
<li class="chapter" data-level="10.4" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#gaussian-kernel-regression"><i class="fa fa-check"></i><b>10.4</b> Gaussian Kernel Regression<span></span></a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off-1"><i class="fa fa-check"></i><b>10.4.1</b> Bias-variance Trade-off<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#choice-of-kernel-functions"><i class="fa fa-check"></i><b>10.5</b> Choice of Kernel Functions<span></span></a></li>
<li class="chapter" data-level="10.6" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-linear-regression"><i class="fa fa-check"></i><b>10.6</b> Local Linear Regression<span></span></a></li>
<li class="chapter" data-level="10.7" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-polynomial-regression"><i class="fa fa-check"></i><b>10.7</b> Local Polynomial Regression<span></span></a></li>
<li class="chapter" data-level="10.8" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#r-implementations"><i class="fa fa-check"></i><b>10.8</b> R Implementations<span></span></a></li>
</ul></li>
<li class="part"><span><b>IV Classification Models<span></span></b></span></li>
<li class="chapter" data-level="11" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Logistic Regression<span></span></a>
<ul>
<li class="chapter" data-level="11.1" data-path="logistic-regression.html"><a href="logistic-regression.html#modeling-binary-outcomes"><i class="fa fa-check"></i><b>11.1</b> Modeling Binary Outcomes<span></span></a></li>
<li class="chapter" data-level="11.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-cleveland-clinic-heart-disease-data"><i class="fa fa-check"></i><b>11.2</b> Example: Cleveland Clinic Heart Disease Data<span></span></a></li>
<li class="chapter" data-level="11.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-the-parameters"><i class="fa fa-check"></i><b>11.3</b> Interpretation of the Parameters<span></span></a></li>
<li class="chapter" data-level="11.4" data-path="logistic-regression.html"><a href="logistic-regression.html#solving-a-logistic-regression"><i class="fa fa-check"></i><b>11.4</b> Solving a Logistic Regression<span></span></a></li>
<li class="chapter" data-level="11.5" data-path="logistic-regression.html"><a href="logistic-regression.html#example-south-africa-heart-data"><i class="fa fa-check"></i><b>11.5</b> Example: South Africa Heart Data<span></span></a></li>
<li class="chapter" data-level="11.6" data-path="logistic-regression.html"><a href="logistic-regression.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>11.6</b> Penalized Logistic Regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>12</b> Discriminant Analysis<span></span></a>
<ul>
<li class="chapter" data-level="12.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#bayes-rule"><i class="fa fa-check"></i><b>12.1</b> Bayes Rule<span></span></a></li>
<li class="chapter" data-level="12.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>12.2</b> Example: Linear Discriminant Analysis (LDA)<span></span></a></li>
<li class="chapter" data-level="12.3" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>12.3</b> Linear Discriminant Analysis<span></span></a></li>
<li class="chapter" data-level="12.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>12.4</b> Example: Quadratic Discriminant Analysis (QDA)<span></span></a></li>
<li class="chapter" data-level="12.5" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>12.5</b> Quadratic Discriminant Analysis<span></span></a></li>
<li class="chapter" data-level="12.6" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-the-hand-written-digit-data"><i class="fa fa-check"></i><b>12.6</b> Example: the Hand Written Digit Data<span></span></a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning Algorithms<span></span></b></span></li>
<li class="chapter" data-level="13" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>13</b> Support Vector Machines<span></span></a>
<ul>
<li class="chapter" data-level="13.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>13.1</b> Maximum-margin Classifier<span></span></a></li>
<li class="chapter" data-level="13.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-separable-svm"><i class="fa fa-check"></i><b>13.2</b> Linearly Separable SVM<span></span></a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#from-primal-to-dual"><i class="fa fa-check"></i><b>13.2.1</b> From Primal to Dual<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-non-separable-svm-with-slack-variables"><i class="fa fa-check"></i><b>13.3</b> Linearly Non-separable SVM with Slack Variables<span></span></a></li>
<li class="chapter" data-level="13.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-saheart-data"><i class="fa fa-check"></i><b>13.4</b> Example: <code>SAheart</code> Data<span></span></a></li>
<li class="chapter" data-level="13.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-svm-via-kernel-trick"><i class="fa fa-check"></i><b>13.5</b> Nonlinear SVM via Kernel Trick<span></span></a></li>
<li class="chapter" data-level="13.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-mixture.example-data"><i class="fa fa-check"></i><b>13.6</b> Example: <code>mixture.example</code> Data<span></span></a></li>
<li class="chapter" data-level="13.7" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-as-a-penalized-model"><i class="fa fa-check"></i><b>13.7</b> SVM as a Penalized Model<span></span></a></li>
<li class="chapter" data-level="13.8" data-path="support-vector-machines.html"><a href="support-vector-machines.html#kernel-and-feature-maps-another-example"><i class="fa fa-check"></i><b>13.8</b> Kernel and Feature Maps: Another Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html"><i class="fa fa-check"></i><b>14</b> Reproducing Kernel Hilbert Space<span></span></a>
<ul>
<li class="chapter" data-level="14.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#constructing-the-rkhs"><i class="fa fa-check"></i><b>14.1</b> Constructing the RKHS<span></span></a></li>
<li class="chapter" data-level="14.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#properties-of-rkhs"><i class="fa fa-check"></i><b>14.2</b> Properties of RKHS<span></span></a></li>
<li class="chapter" data-level="14.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-representer-theorem"><i class="fa fa-check"></i><b>14.3</b> The Representer Theorem<span></span></a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html"><i class="fa fa-check"></i><b>15</b> Kernel Ridge Regression<span></span></a>
<ul>
<li class="chapter" data-level="15.1" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#example-linear-kernel-and-ridge-regression"><i class="fa fa-check"></i><b>15.1</b> Example: Linear Kernel and Ridge Regression<span></span></a></li>
<li class="chapter" data-level="15.2" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#example-alternative-view"><i class="fa fa-check"></i><b>15.2</b> Example: Alternative View<span></span></a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html"><i class="fa fa-check"></i><b>16</b> Classification and Regression Trees<span></span></a>
<ul>
<li class="chapter" data-level="16.1" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#example-classification-tree"><i class="fa fa-check"></i><b>16.1</b> Example: Classification Tree<span></span></a></li>
<li class="chapter" data-level="16.2" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#splitting-a-node"><i class="fa fa-check"></i><b>16.2</b> Splitting a Node<span></span></a></li>
<li class="chapter" data-level="16.3" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#regression-trees"><i class="fa fa-check"></i><b>16.3</b> Regression Trees<span></span></a></li>
<li class="chapter" data-level="16.4" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#predicting-a-target-point"><i class="fa fa-check"></i><b>16.4</b> Predicting a Target Point<span></span></a></li>
<li class="chapter" data-level="16.5" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#tuning-a-tree-model"><i class="fa fa-check"></i><b>16.5</b> Tuning a Tree Model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>17</b> Random Forests<span></span></a>
<ul>
<li class="chapter" data-level="17.1" data-path="random-forests.html"><a href="random-forests.html#bagging-predictors"><i class="fa fa-check"></i><b>17.1</b> Bagging Predictors<span></span></a></li>
<li class="chapter" data-level="17.2" data-path="random-forests.html"><a href="random-forests.html#random-forests-1"><i class="fa fa-check"></i><b>17.2</b> Random Forests<span></span></a></li>
<li class="chapter" data-level="17.3" data-path="random-forests.html"><a href="random-forests.html#effect-of-mtry"><i class="fa fa-check"></i><b>17.3</b> Effect of <code>mtry</code><span></span></a></li>
<li class="chapter" data-level="17.4" data-path="random-forests.html"><a href="random-forests.html#effect-of-nodesize"><i class="fa fa-check"></i><b>17.4</b> Effect of <code>nodesize</code><span></span></a></li>
<li class="chapter" data-level="17.5" data-path="random-forests.html"><a href="random-forests.html#variable-importance"><i class="fa fa-check"></i><b>17.5</b> Variable Importance<span></span></a></li>
<li class="chapter" data-level="17.6" data-path="random-forests.html"><a href="random-forests.html#kernel-view-of-random-forets"><i class="fa fa-check"></i><b>17.6</b> Kernel view of Random Forets<span></span></a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>18</b> Boosting<span></span></a>
<ul>
<li class="chapter" data-level="18.1" data-path="boosting.html"><a href="boosting.html#adaboost"><i class="fa fa-check"></i><b>18.1</b> AdaBoost<span></span></a></li>
<li class="chapter" data-level="18.2" data-path="boosting.html"><a href="boosting.html#training-error-of-adaboost"><i class="fa fa-check"></i><b>18.2</b> Training Error of AdaBoost<span></span></a></li>
<li class="chapter" data-level="18.3" data-path="boosting.html"><a href="boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>18.3</b> Gradient Boosting<span></span></a></li>
</ul></li>
<li class="part"><span><b>VI Unsupervised Learning<span></span></b></span></li>
<li class="chapter" data-level="19" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>19</b> K-Means<span></span></a>
<ul>
<li class="chapter" data-level="19.1" data-path="k-means.html"><a href="k-means.html#basic-concepts"><i class="fa fa-check"></i><b>19.1</b> Basic Concepts<span></span></a></li>
<li class="chapter" data-level="19.2" data-path="k-means.html"><a href="k-means.html#example-1-iris-data"><i class="fa fa-check"></i><b>19.2</b> Example 1: <code>iris</code> data<span></span></a></li>
<li class="chapter" data-level="19.3" data-path="k-means.html"><a href="k-means.html#example-2-clustering-of-image-pixels"><i class="fa fa-check"></i><b>19.3</b> Example 2: clustering of image pixels<span></span></a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>20</b> Hierarchical Clustering<span></span></a>
<ul>
<li class="chapter" data-level="20.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#basic-concepts-1"><i class="fa fa-check"></i><b>20.1</b> Basic Concepts<span></span></a></li>
<li class="chapter" data-level="20.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-1-iris-data-1"><i class="fa fa-check"></i><b>20.2</b> Example 1: <code>iris</code> data<span></span></a></li>
<li class="chapter" data-level="20.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-2-rna-expression-data"><i class="fa fa-check"></i><b>20.3</b> Example 2: RNA Expression Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>21</b> Principle Component Analysis<span></span></a>
<ul>
<li class="chapter" data-level="21.1" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html#basic-concepts-2"><i class="fa fa-check"></i><b>21.1</b> Basic Concepts<span></span></a>
<ul>
<li class="chapter" data-level="21.1.1" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html#note-scaling"><i class="fa fa-check"></i><b>21.1.1</b> Note: Scaling<span></span></a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html#example-1-iris-data-2"><i class="fa fa-check"></i><b>21.2</b> Example 1: <code>iris</code> Data<span></span></a></li>
<li class="chapter" data-level="21.3" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html#example-2-handwritten-digits"><i class="fa fa-check"></i><b>21.3</b> Example 2: Handwritten Digits<span></span></a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="self-organizing-map.html"><a href="self-organizing-map.html"><i class="fa fa-check"></i><b>22</b> Self-Organizing Map<span></span></a>
<ul>
<li class="chapter" data-level="22.1" data-path="self-organizing-map.html"><a href="self-organizing-map.html#basic-concepts-3"><i class="fa fa-check"></i><b>22.1</b> Basic Concepts<span></span></a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="spectral-clustering.html"><a href="spectral-clustering.html"><i class="fa fa-check"></i><b>23</b> Spectral Clustering<span></span></a>
<ul>
<li class="chapter" data-level="23.1" data-path="spectral-clustering.html"><a href="spectral-clustering.html#basic-concepts-4"><i class="fa fa-check"></i><b>23.1</b> Basic Concepts<span></span></a></li>
</ul></li>
<li class="part"><span><b>VII Reference<span></span></b></span></li>
<li class="chapter" data-level="24" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>24</b> Reference<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/teazrq/SMLR" target="blank">&copy; 2022 Ruoqing Zhu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning and Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="boosting" class="section level1 hasAnchor" number="18">
<h1><span class="header-section-number">Chapter 18</span> Boosting<a href="boosting.html#boosting" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Boosting is another ensemble model, created in the form of</p>
<p><span class="math display">\[F_T(x) = \sum_{t = 1}^T \alpha_t f_t(x)\]</span></p>
<p>However, it is different from random forest, in which each <span class="math inline">\(f_t(x)\)</span> is learned parallelly. These <span class="math inline">\(f_t(x)\)</span>’s are called weak learners and are constructed <strong>sequentially</strong>, with coefficients <span class="math inline">\(\alpha_t\)</span>’s to represent their weights. The most classical model, AdaBoost was proposed by <span class="citation"><a href="#ref-freund1997decision" role="doc-biblioref">Freund and Schapire</a> (<a href="#ref-freund1997decision" role="doc-biblioref">1997</a>)</span> for classification problems, and a more statically view of this model called gradient boosting machines <span class="citation">(<a href="#ref-friedman2001greedy" role="doc-biblioref">Friedman 2001</a>)</span> can handle any loss function we commonly use. We will first introduce AdaBoost and then discuss gradient boosting.</p>
<div id="adaboost" class="section level2 hasAnchor" number="18.1">
<h2><span class="header-section-number">18.1</span> AdaBoost<a href="boosting.html#adaboost" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Following our common notation, we observe a set of data <span class="math inline">\(\{\mathbf{x}_i, y_i\}_{i=1}^n\)</span>. Similar to SVM, we code <span class="math inline">\(y_i\)</span>s as <span class="math inline">\(-1\)</span> or <span class="math inline">\(1\)</span>. The AdaBoost works by creating <span class="math inline">\(F_T(x)\)</span> sequentially and use <span class="math inline">\(\text{sign}(F_T(x))\)</span> as the classification rule. The algorithm is given in the following:</p>
<ul>
<li>Initiate weights <span class="math inline">\(w_i^{(1)} = 1/n\)</span>, for <span class="math inline">\(i = 1, \ldots, n\)</span></li>
<li>For <span class="math inline">\(t = 1, \ldots, T\)</span>, do
<ul>
<li>Fit a classifier <span class="math inline">\(f_t(x)\)</span> to the training data with subject weights <span class="math inline">\(w_i^{(t)}\)</span>’s.</li>
<li>Compute the weighed error rate
<span class="math display">\[\epsilon_t = \sum_{i=1}^n w_i^{(t)} \mathbf{1}\{y_i \neq f_t(x_i) \}\]</span></li>
<li>Compute
<span class="math display">\[\alpha_t = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t}\]</span></li>
<li>Update subject weights
<span class="math display">\[w_i^{(t + 1)} = \frac{1}{Z_t} w_i^{(t)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\}\]</span>
where <span class="math inline">\(Z_t\)</span> is a normalizing constant make <span class="math inline">\(w_i^{(t + 1)}\)</span>’s sum up to 1:
<span class="math display">\[Z_t = \sum_{i=1}^n w_i^{(t)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\}\]</span></li>
</ul></li>
<li>Output the final model
<span class="math display">\[F_T(x) = \sum_{t = 1}^T \alpha_t f_t(x)\]</span>
and the decision rule is <span class="math inline">\(\text{sign}(F_T(x))\)</span>.</li>
</ul>
<p>An important mechanism in AdaBoost is the weight update step. We can notice that the weight is increased if <span class="math inline">\(\exp\big\{ - \alpha_t y_i f_t(x_i) \big\}\)</span> is larger than 1. This is simply when <span class="math inline">\(y_i f_t(x_i)\)</span> is negative, i.e., subject <span class="math inline">\(i\)</span> got mis-classified by <span class="math inline">\(f_t\)</span> at this iteration. Hence, during the next iteration <span class="math inline">\(t+1\)</span>, the model <span class="math inline">\(f_{(t+1)}\)</span> will more likely to address this subject. Here, <span class="math inline">\(f_t\)</span> can be any classification model, for example, we could use a tree model. The following figures demonstrate this idea of updating weights and aggregate the learners.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="boosting.html#cb1-1" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="fl">0.1</span>)</span>
<span id="cb1-2"><a href="boosting.html#cb1-2" aria-hidden="true" tabindex="-1"></a>  x2 <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>,</span>
<span id="cb1-3"><a href="boosting.html#cb1-3" aria-hidden="true" tabindex="-1"></a>         <span class="fl">0.8</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.2</span>)</span>
<span id="cb1-4"><a href="boosting.html#cb1-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-5"><a href="boosting.html#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the data</span></span>
<span id="cb1-6"><a href="boosting.html#cb1-6" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, </span>
<span id="cb1-7"><a href="boosting.html#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb1-8"><a href="boosting.html#cb1-8" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">cbind</span>(<span class="st">&quot;x1&quot;</span> <span class="ot">=</span> x1, <span class="st">&quot;x2&quot;</span> <span class="ot">=</span> x2)</span>
<span id="cb1-9"><a href="boosting.html#cb1-9" aria-hidden="true" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="st">&quot;x1&quot;</span> <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">1.1</span>, <span class="fl">0.01</span>), <span class="st">&quot;x2&quot;</span> <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">0.9</span>, <span class="fl">0.01</span>))</span>
<span id="cb1-10"><a href="boosting.html#cb1-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-11"><a href="boosting.html#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot data</span></span>
<span id="cb1-12"><a href="boosting.html#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb1-13"><a href="boosting.html#cb1-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb1-14"><a href="boosting.html#cb1-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb1-15"><a href="boosting.html#cb1-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-16"><a href="boosting.html#cb1-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit gbm with 3 trees</span></span>
<span id="cb1-17"><a href="boosting.html#cb1-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(gbm)</span>
<span id="cb1-18"><a href="boosting.html#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: package &#39;gbm&#39; was built under R version 4.1.3</span></span>
<span id="cb1-19"><a href="boosting.html#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Loaded gbm 2.1.8</span></span>
<span id="cb1-20"><a href="boosting.html#cb1-20" aria-hidden="true" tabindex="-1"></a>  gbm.fit <span class="ot">=</span> <span class="fu">gbm</span>(y <span class="sc">~</span>., <span class="fu">data.frame</span>(x1, x2, <span class="at">y=</span> <span class="fu">as.numeric</span>(y <span class="sc">==</span> <span class="dv">1</span>)), </span>
<span id="cb1-21"><a href="boosting.html#cb1-21" aria-hidden="true" tabindex="-1"></a>                <span class="at">distribution=</span><span class="st">&quot;adaboost&quot;</span>, <span class="at">interaction.depth =</span> <span class="dv">1</span>, </span>
<span id="cb1-22"><a href="boosting.html#cb1-22" aria-hidden="true" tabindex="-1"></a>                <span class="at">n.minobsinnode =</span> <span class="dv">1</span>, <span class="at">n.trees =</span> <span class="dv">3</span>, </span>
<span id="cb1-23"><a href="boosting.html#cb1-23" aria-hidden="true" tabindex="-1"></a>                <span class="at">shrinkage =</span> <span class="dv">1</span>, <span class="at">bag.fraction =</span> <span class="dv">1</span>)</span>
<span id="cb1-24"><a href="boosting.html#cb1-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-25"><a href="boosting.html#cb1-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># you may peek into each tree</span></span>
<span id="cb1-26"><a href="boosting.html#cb1-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pretty.gbm.tree</span>(gbm.fit, <span class="at">i.tree =</span> <span class="dv">1</span>)</span>
<span id="cb1-27"><a href="boosting.html#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="do">##   SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight Prediction</span></span>
<span id="cb1-28"><a href="boosting.html#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="do">## 0        0          0.25        1         2           3            2.5     10       0.00</span></span>
<span id="cb1-29"><a href="boosting.html#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="do">## 1       -1          1.00       -1        -1          -1            0.0      2       1.00</span></span>
<span id="cb1-30"><a href="boosting.html#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="do">## 2       -1         -0.25       -1        -1          -1            0.0      8      -0.25</span></span>
<span id="cb1-31"><a href="boosting.html#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="do">## 3       -1          0.00       -1        -1          -1            0.0     10       0.00</span></span>
<span id="cb1-32"><a href="boosting.html#cb1-32" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-33"><a href="boosting.html#cb1-33" aria-hidden="true" tabindex="-1"></a>  <span class="co"># we can view the predicted decision rule</span></span>
<span id="cb1-34"><a href="boosting.html#cb1-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb1-35"><a href="boosting.html#cb1-35" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb1-36"><a href="boosting.html#cb1-36" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb1-37"><a href="boosting.html#cb1-37" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">predict</span>(gbm.fit, xgrid)</span>
<span id="cb1-38"><a href="boosting.html#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="do">## Using 3 trees...</span></span>
<span id="cb1-39"><a href="boosting.html#cb1-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb1-40"><a href="boosting.html#cb1-40" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-4-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Here is a rundown of the algorithm. Let’s initialize all weights as <span class="math inline">\(1/n\)</span>. We only used trees with a single split as weak learners. The first tree is splitting at <span class="math inline">\(X_1 = 0.25\)</span>. After the first split, we need to adjust the weights.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="boosting.html#cb2-1" aria-hidden="true" tabindex="-1"></a>  w1 <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb2-2"><a href="boosting.html#cb2-2" aria-hidden="true" tabindex="-1"></a>  f1 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x[, <span class="dv">1</span>] <span class="sc">&lt;</span> <span class="fl">0.25</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb2-3"><a href="boosting.html#cb2-3" aria-hidden="true" tabindex="-1"></a>  e1 <span class="ot">=</span> <span class="fu">sum</span>(w1<span class="sc">*</span>(<span class="fu">f1</span>(X) <span class="sc">!=</span> y))</span>
<span id="cb2-4"><a href="boosting.html#cb2-4" aria-hidden="true" tabindex="-1"></a>  a1 <span class="ot">=</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>e1)<span class="sc">/</span>e1)</span>
<span id="cb2-5"><a href="boosting.html#cb2-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-6"><a href="boosting.html#cb2-6" aria-hidden="true" tabindex="-1"></a>  w2 <span class="ot">=</span> w1<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span> a1<span class="sc">*</span>y<span class="sc">*</span><span class="fu">f1</span>(X))</span>
<span id="cb2-7"><a href="boosting.html#cb2-7" aria-hidden="true" tabindex="-1"></a>  w2 <span class="ot">=</span> w2<span class="sc">/</span><span class="fu">sum</span>(w2)</span>
<span id="cb2-8"><a href="boosting.html#cb2-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-9"><a href="boosting.html#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the first tree</span></span>
<span id="cb2-10"><a href="boosting.html#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb2-11"><a href="boosting.html#cb2-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb2-12"><a href="boosting.html#cb2-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb2-13"><a href="boosting.html#cb2-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-14"><a href="boosting.html#cb2-14" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">f1</span>(xgrid)</span>
<span id="cb2-15"><a href="boosting.html#cb2-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb2-16"><a href="boosting.html#cb2-16" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb2-17"><a href="boosting.html#cb2-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-18"><a href="boosting.html#cb2-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># weights after the first tree</span></span>
<span id="cb2-19"><a href="boosting.html#cb2-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb2-20"><a href="boosting.html#cb2-20" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb2-21"><a href="boosting.html#cb2-21" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">30</span><span class="sc">*</span>w2)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-6-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>We can notice that the observations got correctly classified will decrease their weights while those mis-classified will increase the weights.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="boosting.html#cb3-1" aria-hidden="true" tabindex="-1"></a>  f2 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x[, <span class="dv">2</span>] <span class="sc">&gt;</span> <span class="fl">0.65</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb3-2"><a href="boosting.html#cb3-2" aria-hidden="true" tabindex="-1"></a>  e2 <span class="ot">=</span> <span class="fu">sum</span>(w2<span class="sc">*</span>(<span class="fu">f2</span>(X) <span class="sc">!=</span> y))</span>
<span id="cb3-3"><a href="boosting.html#cb3-3" aria-hidden="true" tabindex="-1"></a>  a2 <span class="ot">=</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>e2)<span class="sc">/</span>e2)</span>
<span id="cb3-4"><a href="boosting.html#cb3-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-5"><a href="boosting.html#cb3-5" aria-hidden="true" tabindex="-1"></a>  w3 <span class="ot">=</span> w2<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span> a2<span class="sc">*</span>y<span class="sc">*</span><span class="fu">f2</span>(X))</span>
<span id="cb3-6"><a href="boosting.html#cb3-6" aria-hidden="true" tabindex="-1"></a>  w3 <span class="ot">=</span> w3<span class="sc">/</span><span class="fu">sum</span>(w3)</span>
<span id="cb3-7"><a href="boosting.html#cb3-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-8"><a href="boosting.html#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the second tree</span></span>
<span id="cb3-9"><a href="boosting.html#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb3-10"><a href="boosting.html#cb3-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb3-11"><a href="boosting.html#cb3-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">30</span><span class="sc">*</span>w2)</span>
<span id="cb3-12"><a href="boosting.html#cb3-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-13"><a href="boosting.html#cb3-13" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">f2</span>(xgrid)</span>
<span id="cb3-14"><a href="boosting.html#cb3-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb3-15"><a href="boosting.html#cb3-15" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb3-16"><a href="boosting.html#cb3-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-17"><a href="boosting.html#cb3-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># weights after the second tree</span></span>
<span id="cb3-18"><a href="boosting.html#cb3-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb3-19"><a href="boosting.html#cb3-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb3-20"><a href="boosting.html#cb3-20" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">30</span><span class="sc">*</span>w3)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-7-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>And then we have the third step. Combining all three steps and their decision function, we have the final classifier</p>
<p><span class="math display">\[\begin{align}
F_3(x) =&amp; \sum_{t=1}^3 \alpha_t f_t(x) \nonumber \\
=&amp; 0.4236 \cdot f_1(x) + 0.6496 \cdot f_2(x) + 0.9229 \cdot f_3(x)
\end{align}\]</span></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="boosting.html#cb4-1" aria-hidden="true" tabindex="-1"></a>  f3 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x[, <span class="dv">1</span>] <span class="sc">&lt;</span> <span class="fl">0.85</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb4-2"><a href="boosting.html#cb4-2" aria-hidden="true" tabindex="-1"></a>  e3 <span class="ot">=</span> <span class="fu">sum</span>(w3<span class="sc">*</span>(<span class="fu">f3</span>(X) <span class="sc">!=</span> y))</span>
<span id="cb4-3"><a href="boosting.html#cb4-3" aria-hidden="true" tabindex="-1"></a>  a3 <span class="ot">=</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>e3)<span class="sc">/</span>e3)</span>
<span id="cb4-4"><a href="boosting.html#cb4-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-5"><a href="boosting.html#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the third tree</span></span>
<span id="cb4-6"><a href="boosting.html#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb4-7"><a href="boosting.html#cb4-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb4-8"><a href="boosting.html#cb4-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">30</span><span class="sc">*</span>w3)</span>
<span id="cb4-9"><a href="boosting.html#cb4-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-10"><a href="boosting.html#cb4-10" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">f3</span>(xgrid)</span>
<span id="cb4-11"><a href="boosting.html#cb4-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb4-12"><a href="boosting.html#cb4-12" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb4-13"><a href="boosting.html#cb4-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-14"><a href="boosting.html#cb4-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the final decision rule </span></span>
<span id="cb4-15"><a href="boosting.html#cb4-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb4-16"><a href="boosting.html#cb4-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb4-17"><a href="boosting.html#cb4-17" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb4-18"><a href="boosting.html#cb4-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-19"><a href="boosting.html#cb4-19" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> a1<span class="sc">*</span><span class="fu">f1</span>(xgrid) <span class="sc">+</span> a2<span class="sc">*</span><span class="fu">f2</span>(xgrid) <span class="sc">+</span> a3<span class="sc">*</span><span class="fu">f3</span>(xgrid)</span>
<span id="cb4-20"><a href="boosting.html#cb4-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb4-21"><a href="boosting.html#cb4-21" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb4-22"><a href="boosting.html#cb4-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.25</span>) <span class="co"># f1</span></span>
<span id="cb4-23"><a href="boosting.html#cb4-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">h =</span> <span class="fl">0.65</span>) <span class="co"># f2</span></span>
<span id="cb4-24"><a href="boosting.html#cb4-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.85</span>) <span class="co"># f3</span></span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-8-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="training-error-of-adaboost" class="section level2 hasAnchor" number="18.2">
<h2><span class="header-section-number">18.2</span> Training Error of AdaBoost<a href="boosting.html#training-error-of-adaboost" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There is an interesting property about the boosting algorithm that if we can always find a classifier that performs better than random guessing at each iteration <span class="math inline">\(t\)</span>, then the training error will eventually converge to zero. This works by analyzing the weight after the last iteration <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[\begin{align}
w_i^{(T+1)} =&amp; \frac{1}{Z_T} w_i^{(T)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\} \nonumber \\
=&amp; \frac{1}{Z_1\cdots Z_T} w_i^{(1)} \prod_{t = 1}^T \exp\big\{ - \alpha_t y_i f_t(x_i) \big\} \nonumber \\
=&amp; \frac{1}{Z_1\cdots Z_T} \frac{1}{n} \exp\Big\{ - y_i \sum_{t = 1}^T \alpha_t f_t(x_i) \Big\}
\end{align}\]</span></p>
<p>Since <span class="math inline">\(\sum_{t = 1}^T \alpha_t f_t(x_i)\)</span> is just the model at the <span class="math inline">\(T\)</span>-th iteration, we can write it as <span class="math inline">\(F_T(x_i)\)</span>. Noticing that they sum up to 1, we have</p>
<p><span class="math display">\[1 = \sum_{i = 1}^n w_i^{(T+1)} = \frac{1}{Z_1\cdots Z_T} \frac{1}{n} \sum_{i = 1}^n \exp\big\{ - y_i F_T(x_i) \big\}\]</span>
and
<span class="math display">\[Z_1\cdots Z_T = \frac{1}{n} \sum_{i = 1}^n \exp\big\{ - y_i F_T(x_i) \big\}\]</span>
On the right-hand-side, this is the exponential loss after we fit the model. In fact, this quantity would bound above the 0/1 loss, since the exponential loss is <span class="math inline">\(\exp[ - y f(x) ]\)</span>,</p>
<ul>
<li>For correctly classified subjects, <span class="math inline">\(y f(x) &gt; 0\)</span>, and <span class="math inline">\(\exp[ - y f(x) ] &gt; 0\)</span></li>
<li>For incorrectly classified subjects, <span class="math inline">\(y f(x) &lt; 0\)</span> the exponential loss is larger than 1</li>
</ul>
<p>This means that</p>
<p><span class="math display">\[Z_1\cdots Z_T &gt; \frac{1}{n} \sum_{i = 1}^n \mathbf{1} \big\{ y_i \neq \text{sign}(F_T(x_i)) \big\}\]</span>
Hence, if we want the final model to have low training error, we should bound above the <span class="math inline">\(Z_t\)</span>’s. Recall that <span class="math inline">\(Z_t\)</span> is used to normalize the weights, we have</p>
<p><span class="math display">\[Z_t = \sum_i^{n} w_i^{(t)} \exp[ - \alpha_t y_i f_t(x_i) ].\]</span>
We have two cases at this iteration, <span class="math inline">\(y_i f(x_i) = 1\)</span> for correct subjects, and <span class="math inline">\(y_i f(x_i) = -1\)</span> for the incorrect ones, hence,
By our definition, <span class="math inline">\(\epsilon_t = \sum_i w_i^{(t)} \mathbf{1} \big\{ y_i \neq f_t(x_i) \big\}\)</span> is the proportion of weights for mis-classified samples.
<span class="math display">\[\begin{align}
Z_t =&amp; \,\,\sum_{i=1}^n w_i^{(t)} \exp[ - \alpha_t y_i f_t(x_i)] \nonumber\\
=&amp;\,\,\sum_{y_i = f_t(x_i)} w_i^{(t)} \exp[ - \alpha_t ] +  \sum_{y_i \neq f_t(x_i)} w_i^{(t)} \exp[ \alpha_t ] \nonumber\\
=&amp; \,\, \exp[ - \alpha_t ] \sum_{y_i = f_t(x_i)} w_i^{(t)} + \exp[ \alpha_t ] \sum_{y_i \neq f_t(x_i)} w_i^{(t)}
\end{align}\]</span></p>
<p>So we have</p>
<p><span class="math display">\[ Z_t = (1 - \epsilon_t) \exp[ - \alpha_t ] + \epsilon_t \exp[ \alpha_t ].\]</span></p>
<p>If we want to minimize the product of all <span class="math inline">\(Z_t\)</span>’s, we can consider minimizing each of them. Let’s consider this as a function of <span class="math inline">\(\alpha_t\)</span>, then by taking a derivative with respect to <span class="math inline">\(\alpha_t\)</span>, we have</p>
<p><span class="math display">\[ - (1 - \epsilon_t) \exp[ - \alpha_t ] + \epsilon_t \exp[ \alpha_t ] = 0\]</span>
and</p>
<p><span class="math display">\[\alpha_t = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t}.\]</span>
Plugging this back into <span class="math inline">\(Z_t\)</span>, we have</p>
<p><span class="math display">\[Z_t = 2 \sqrt{\epsilon_t(1-\epsilon_t)}\]</span>
Since <span class="math inline">\(\epsilon_t(1-\epsilon_t)\)</span> can only attain maximum of <span class="math inline">\(1/4\)</span>, <span class="math inline">\(Z_t\)</span> must be smaller than 1. This makes the product <span class="math inline">\(Z_1 \cdots Z_T\)</span> converging to 0. If we look at this more closely, by defining <span class="math inline">\(\gamma_t = \frac{1}{2} - \epsilon_t\)</span> as the improvement from a random model (with error <span class="math inline">\(1/2\)</span>), then</p>
<p><span class="math display">\[\begin{align}
Z_t =&amp; 2 \sqrt{\epsilon_t(1-\epsilon_t)} \nonumber \\
=&amp; \sqrt{1 - 4 \gamma_t^2} \nonumber \\
\leq&amp; \exp\big[ - 2 \gamma_t^2 \big]
\end{align}\]</span></p>
<p>The last equation is because by Taylor expansion, <span class="math inline">\(\exp\big[ - 4 \gamma_t^2 \big] \geq 1 - 4 \gamma_t^2\)</span>. Then, we can finally put all <span class="math inline">\(Z_t\)</span>’s together:</p>
<p><span class="math display">\[\begin{align}
\text{Training Error} =&amp; \sum_{i = 1}^n \mathbf{1} \big\{ y_i \neq \text{sign}(F_T(x_i)) \big\} \nonumber \\
=&amp; \sum_{i = 1}^n \exp \big[ - y_i \neq F_T(x_i) \big] \nonumber \\
=&amp; Z_1 \cdots Z_T \nonumber \\
\leq&amp; \exp \big[ - 2 \sum_{t=1}^T \gamma_t^2 \big],
\end{align}\]</span></p>
<p>which converges to 0 as long as <span class="math inline">\(\sum_{t=1}^T \gamma_t^2\)</span> accumulates up to infinite. But of course, in practice, it would increasing difficult find <span class="math inline">\(f_t(x)\)</span> that reduces the training error greatly.</p>
</div>
<div id="gradient-boosting" class="section level2 hasAnchor" number="18.3">
<h2><span class="header-section-number">18.3</span> Gradient Boosting<a href="boosting.html#gradient-boosting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s take an alternative view of this problem, we use an additive structure to fit models</p>
<p><span class="math display">\[F_T(x) = \sum_{t = 1}^T \alpha_t f(x; \boldsymbol \theta_t)\]</span></p>
<p>by minimizing a loss function</p>
<p><span class="math display">\[\underset{\{\alpha_t, \boldsymbol \theta_t\}_{t=1}^T}{\min} \sum_{i=1}^n L\big(y_i, F_T(x_i)\big)\]</span>
In this framework, we may choose a loss function <span class="math inline">\(L\)</span> that is suitable for the problem, and also choose the base learner <span class="math inline">\(f(x; \boldsymbol \theta)\)</span> with parameter <span class="math inline">\(\boldsymbol \theta\)</span>. Examples of this include linear function, spline, tree, etc.. While it maybe difficult to minimize over all parameters <span class="math inline">\(\{\alpha_t, \boldsymbol \theta_t\}_{t=1}^T\)</span>, we may consider doing this in a stage-wise fashion. The algorithm could work in the following way:</p>
<ul>
<li>Set <span class="math inline">\(F_0(x) = 0\)</span></li>
<li>For <span class="math inline">\(t = 1, \ldots, T\)</span>
<ul>
<li>Choose <span class="math inline">\((\alpha_t, \boldsymbol \theta_t)\)</span> to minimize the loss
<span class="math display">\[\underset{\alpha, \boldsymbol \theta}{\min} \,\, \sum_{i=1}^n L\big(y_i, F_{t-1}(x_i) + \alpha f(x_i; \boldsymbol \theta)\big)\]</span></li>
<li>Update <span class="math inline">\(F_t(x) = F_{t-1}(x) + \alpha_t f(x; \boldsymbol \theta_t)\)</span></li>
</ul></li>
<li>Output <span class="math inline">\(F_T(x)\)</span> as the final model</li>
</ul>
<p>The previous AdaBoost example is using exponential loss function. Also, it doesn’t pick an optimal <span class="math inline">\(f(x; \boldsymbol \theta)\)</span> at each step. We just need a model that is better than random. The step size <span class="math inline">\(\alpha_t\)</span> is optimized at each <span class="math inline">\(t\)</span> given the fitted <span class="math inline">\(f(x; \boldsymbol \theta_t)\)</span>.</p>
<p>Another example is the forward stage-wise linear regression. In this case, we fit a single variable linear model at each step <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[f(x, j) = \text{sign}\big(\text{Cor}(X_j, \mathbf{r})\big) X_j\]</span>
* <span class="math inline">\(\mathbf{r}\)</span> is the residual, as <span class="math inline">\(r_i = y_i - F_{t-1}(x_i)\)</span>
* <span class="math inline">\(j\)</span> is the index that has the largest absolute correlation with <span class="math inline">\(\mathbf{r}\)</span></p>
<p>Then we give a very small step size <span class="math inline">\(\alpha_t\)</span>, say, <span class="math inline">\(\alpha_t = 10^{-5}\)</span>, and with sign equal to the correlation between <span class="math inline">\(X_j\)</span>. In this case, <span class="math inline">\(F_t(x)\)</span> is almost equivalent to the Lasso solution path, as <span class="math inline">\(t\)</span> increases.</p>
<p>We may notice that <span class="math inline">\(r_i\)</span> is in fact the negative gradient of the squared-error loss, as a function of the fitted function:</p>
<p><span class="math display">\[r_{it} = - \left[ \frac{\partial \, \big(y_i - F(x_i)\big)^2 }{\partial \, F(x_i)} \right]_{F(x_i) = F_{t-1}(x_i)}\]</span>
and we are essentially fitting a weak leaner <span class="math inline">\(f_t(x)\)</span> to the residuals and update the fitted model <span class="math inline">\(F_t(x)\)</span>. The following example shows the result of using a tree leaner as <span class="math inline">\(f_t(x)\)</span>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="boosting.html#cb5-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(gbm)</span>
<span id="cb5-2"><a href="boosting.html#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="boosting.html#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># a simple regression problem</span></span>
<span id="cb5-4"><a href="boosting.html#cb5-4" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb5-5"><a href="boosting.html#cb5-5" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.001</span>)</span>
<span id="cb5-6"><a href="boosting.html#cb5-6" aria-hidden="true" tabindex="-1"></a>  fx <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(<span class="dv">3</span><span class="sc">*</span>pi<span class="sc">*</span>x)</span>
<span id="cb5-7"><a href="boosting.html#cb5-7" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">fx</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x))</span>
<span id="cb5-8"><a href="boosting.html#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="boosting.html#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">ylab =</span> <span class="st">&quot;y&quot;</span>, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb5-10"><a href="boosting.html#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the true regression line</span></span>
<span id="cb5-11"><a href="boosting.html#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(x, <span class="fu">fx</span>(x), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-10-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We can see that the fitted model progressively approaximates the true function.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="boosting.html#cb6-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit regression boosting</span></span>
<span id="cb6-2"><a href="boosting.html#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># I use a very large shrinkage value for demonstrating the functions</span></span>
<span id="cb6-3"><a href="boosting.html#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># in practice you should use 0.1 or even smaller values for stability</span></span>
<span id="cb6-4"><a href="boosting.html#cb6-4" aria-hidden="true" tabindex="-1"></a>  gbm.fit <span class="ot">=</span> <span class="fu">gbm</span>(y<span class="sc">~</span>x, <span class="at">data =</span> <span class="fu">data.frame</span>(x, y), <span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span>,</span>
<span id="cb6-5"><a href="boosting.html#cb6-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">n.trees=</span><span class="dv">300</span>, <span class="at">shrinkage=</span><span class="fl">0.5</span>, <span class="at">bag.fraction=</span><span class="fl">0.8</span>)</span>
<span id="cb6-6"><a href="boosting.html#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="boosting.html#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># somehow, cross-validation for 1 dimensional problem creates error</span></span>
<span id="cb6-8"><a href="boosting.html#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gbm(y ~ ., data = data.frame(x, y), cv.folds = 3) # this produces an error  </span></span>
<span id="cb6-9"><a href="boosting.html#cb6-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-10"><a href="boosting.html#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the fitted regression function at several iterations</span></span>
<span id="cb6-11"><a href="boosting.html#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb6-12"><a href="boosting.html#cb6-12" aria-hidden="true" tabindex="-1"></a>  size<span class="ot">=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">50</span>,<span class="dv">100</span>,<span class="dv">300</span>)</span>
<span id="cb6-13"><a href="boosting.html#cb6-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-14"><a href="boosting.html#cb6-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)</span>
<span id="cb6-15"><a href="boosting.html#cb6-15" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb6-16"><a href="boosting.html#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb6-17"><a href="boosting.html#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">ylab =</span> <span class="st">&quot;y&quot;</span>, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb6-18"><a href="boosting.html#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(x, <span class="fu">fx</span>(x), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>)</span>
<span id="cb6-19"><a href="boosting.html#cb6-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-20"><a href="boosting.html#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this returns the fitted function, but not class</span></span>
<span id="cb6-21"><a href="boosting.html#cb6-21" aria-hidden="true" tabindex="-1"></a>    Fx <span class="ot">=</span> <span class="fu">predict</span>(gbm.fit, <span class="at">n.trees=</span>size[i])</span>
<span id="cb6-22"><a href="boosting.html#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(x, Fx, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb6-23"><a href="boosting.html#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="fu">paste</span>(<span class="st">&quot;# of Iterations = &quot;</span>, size[i]))</span>
<span id="cb6-24"><a href="boosting.html#cb6-24" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-11-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>This idea can be generalized to any loss function <span class="math inline">\(L\)</span>. This is the <strong>gradient boosting</strong> model:</p>
<ul>
<li>At each iteration <span class="math inline">\(t\)</span>, calculate ``pseudo-residuals’’, i.e., the negative gradient for each observation
<span class="math display">\[g_{it} = - \left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F(x_i) = F_{t-1}(x_i)}\]</span></li>
<li>Fit <span class="math inline">\(f_t(x, \boldsymbol \theta_t)\)</span> to pseudo-residual <span class="math inline">\(g_{it}\)</span>’s</li>
<li>Search for the best 
<span class="math display">\[\alpha_t = \underset{\alpha}{\arg\min} \sum_{i=1}^n L\big(y_i, F_{t-1}(x_i) + \alpha f(x_i; \boldsymbol \theta_t)\big)\]</span></li>
<li>Update <span class="math inline">\(F_t(x) = F_{t-1}(x) + \alpha_t f(x; \boldsymbol \theta_t)\)</span></li>
</ul>
<p>Hence, the only change when modeling different outcomes is to choose the loss function <span class="math inline">\(L\)</span>, and derive the pseudo-residuals</p>
<ul>
<li>For regression, the loss is <span class="math inline">\(\frac{1}{2} (y - f(x))^2\)</span>, and the pseudo-residual is <span class="math inline">\(y_i - f(x_i)\)</span></li>
<li>For quantile regression to model median, the loss is <span class="math inline">\(|y - f(x)|\)</span>, and the pseudo-residual is sign<span class="math inline">\((y_i - f(x_i))\)</span> \</li>
<li>For classification, we can use the deviance <span class="math inline">\(y\log(p) + (1-y)\log(1-p)\)</span>, and express <span class="math inline">\(p\)</span> as the log-odds of a scale predictor, i.e., <span class="math inline">\(f = \log(p/(1-p))\)</span>. Then the pseudo-residual is <span class="math inline">\(y_i - p(x_i)\)</span></li>
</ul>

</div>
</div>



<h3> Reference<a href="reference.html#reference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-freund1997decision" class="csl-entry">
Freund, Yoav, and Robert E Schapire. 1997. <span>“A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.”</span> <em>Journal of Computer and System Sciences</em> 55 (1): 119–39.
</div>
<div id="ref-friedman2001greedy" class="csl-entry">
Friedman, Jerome H. 2001. <span>“Greedy Function Approximation: A Gradient Boosting Machine.”</span> <em>Annals of Statistics</em>, 1189–1232.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="random-forests.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="k-means.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "serif",
"size": 1
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
