[
<<<<<<< HEAD
["optimization.html", "Chapter 5 Optimization", " Chapter 5 Optimization TODO: "]
=======
["index.html", "Statistical Learning with R Preface Target Audience What’s Covered? Acknowledgements License", " Statistical Learning with R Ruoqing Zhu 2018-09-07 Preface Welcome to Statistical Learning with R! I started this project during the summer of 2018 when I was preparing for the Stat 432 course. This course was taught by our former faculty member Dr. David Dalpiaz, who is currently at The Ohio State University. David introduced to me this awesome way of publishing website on GitHub, which is a very efficient approach for developing courses. Since I was also teaching Stat 542 (Statistical Learning) during the previous two years, I figured it could be beneficial to integrate what I have to this existing book by David and use it as the R material for both courses. As you can tell, I am not being very creative on the name, so `SLWR’ it is. You can find the source file of this book on my GitHub. Target Audience This book is targeted at advanced undergraduate to MS students in Statistics who have some or no prior statistical learning experience. Previous experience with both basic mathematics (mainly linear algebra), statistical modeling (such as linear regressions) and R are assumed. What’s Covered? I currently plan to include the following topics: Basics Knowledge Linear and Penalized Linear Regressions Unsupervised Learning Classification Non-parametric Statistical Models Machine Learning Models Appendix The goal of this book is to introduce not only how to run some of the popular statistical learning models in R, but also touches some basic algorithms and programming techniques for solving some of these models. For each section, the difficulty may gradually increase from an undergraduate level to a graduate level. It will be served as a supplement to An Introduction to Statistical Learning (James et al. 2013) for STAT 432 - Basics of Statistical Learning and The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Hastie, Tibshirani, and Friedman 2001) for STAT 542 - Statistical Learning at the University of Illinois at Urbana-Champaign. This book is under active development as I am teaching STAT 432 during Fall 2018. Hence, you may encounter errors ranging from typos to broken code, to poorly explained topics. If you do, please let me know! Simply send an email and I will make the changes as soon as possible (rqzhu AT illinois DOT edu). Or, if you know R Markdown and are familiar with GitHub, make a pull request and fix an issue yourself!. These contributions will be acknowledged. Acknowledgements The initial contents are derived from Dr. David Dalpiaz’s book. My STAT 542 course materials are also inspired by Dr. Feng Liang and Dr. John Marden who developed earlier versions of this course. And I also incorporated many online resources, such as bookdown: Authoring Books and Technical Documents with R Markdown R Programming for Data Science and others through Google search. License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. References "],
["r-rstudio-and-r-markdown.html", "Chapter 1 R, RStudio, and R Markdown 1.1 Resources and Guides 1.2 Basic Mathematical Operations 1.3 Read-in Data from Other Sources 1.4 Using Packages 1.5 Using R Markdown 1.6 Explore Yourself", " Chapter 1 R, RStudio, and R Markdown R is a free-to-use software that is very popular in statistical computing. You can download R from its official website. Another software that makes using R easier is RStudio, which is available at here. You can find many online guides that help you to set-up these two software, for example, this YouTube video. R Markdown is a built-in feature of RStudio. It works like an integration of LaTex and programming playground that compiles source code into nice-looking PDF, HTML, or MS Word files. This book is created using an extension of R Markdown, developed by Yihui Xie. 1.1 Resources and Guides There are many online resources for how to use R, RStudio, and R Markdown. For example, David Dalpiaz’s other online book Applied Statistics with R contains an introduction to using them. There are also other online documentation such as Install R and RStudio R tutorial Data in R Play-list (video) R and RStudio Play-list (video) R Markdown Cheat Sheet R Markdown Play-list (video) It is worth to mention that once you become a developer of R packages using C/C++ (add-on of R for performing specific tasks), and you also happen to use Windows like I do, you have to install this Rtools that contains compilers. This is also needed if you want to manually install any R package using a “source” (.tar.gz files) instead of using the so-called “binaries” (.zip files). 1.2 Basic Mathematical Operations We will briefly cover some basic R calculations and operations. If you want to see more information about a particular function or operator in R, the easiest way is to get the reference document. Put a question mark in front of a function name: # In a default R console window, this will open up a web browser. # In RStudio, this will be displayed at the ‘Help’ window at the bottom-right penal. ?log2 ?matrix Try type-in the following commands into your R console and start to explore yourself. Most of them are self-explanatory. Lines with a # in the front are comments, which will not be executed. Lines with ## in the front are outputs. # Basic mathematical operations 1 + 3 ## [1] 4 3*5 ## [1] 15 3^5 ## [1] 243 exp(2) ## [1] 7.389056 log(3) ## [1] 1.098612 log2(3) ## [1] 1.584963 factorial(5) ## [1] 120 1.2.1 Data Objects Data objects can be a complicated topic for people who never used R before. Most common data objects are vector, matrix, list, and data.frame. Operations on vectors are matrices are fairly intuitive. # creating a vector c(1,2,3,4) ## [1] 1 2 3 4 c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; # creating matrix from a vector matrix(c(1,2,3,4), 2, 2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 x = c(1,1,1,0,0,0); y = c(1,0,1,0,1,0) cbind(x,y) ## x y ## [1,] 1 1 ## [2,] 1 0 ## [3,] 1 1 ## [4,] 0 0 ## [5,] 0 1 ## [6,] 0 0 # matrix multiplication using &#39;%*%&#39; matrix(c(1,2,3,4), 2, 2) %*% t(cbind(x,y)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 4 1 4 0 3 0 ## [2,] 6 2 6 0 4 0 Simple mathematical operations on vectors and matrices are usually element-wise. You can easily extract certain elements of them by using the [] operator, like a C programming reference style. # some simple operations x[3] ## [1] 1 x[2:5] ## [1] 1 1 0 0 cbind(x,y)[1:2, ] ## x y ## [1,] 1 1 ## [2,] 1 0 (x + y)^2 ## [1] 4 1 4 0 1 0 length(x) ## [1] 6 dim(cbind(x,y)) ## [1] 6 2 # A warning will be issued when R detects something wrong. Results may still be produced. x + c(1,2,3,4) ## Warning in x + c(1, 2, 3, 4): longer object length is not a multiple of shorter object length ## [1] 2 3 4 4 1 2 list() simply creates a list of objects (of any type). However, some operators cannot be directly applied to a list in a similar way as to vectors or matrices. Model fitting results in R are usually stored as a list (for example, the lm() function used in Section ??. # creating a list x = list(c(1,2), &quot;hello&quot;, matrix(c(1,2,3,4), 2, 2)) x[[1]] ## [1] 1 2 data.frame() creates a list of vectors of equal length, and display them as a matrix-like object where each vector is a column of the matrix. It is mainly used for storing data. For example the famous iris data. The first four columns are numerical variables, while the last column is a categorical variable with three levels: setosa, versicolor, and virginica. # the head function peeks the first several rows of the dataset head(iris, n = 3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa # data frame can be called by each individual column iris$Species[c(1, 51, 101)] ## [1] setosa versicolor virginica ## Levels: setosa versicolor virginica # the summary function can be used to view all variables summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 1.3 Read-in Data from Other Sources Data can be imported from a variety of sources. More commonly, a dataset can be stored in .txt, .csv or other file formats. # read-in data birthrate = read.csv(&quot;data/birthrate.csv&quot;) head(birthrate) ## Year Birthrate ## 1 1917 183.1 ## 2 1918 183.9 ## 3 1919 163.1 ## 4 1920 179.5 ## 5 1921 181.4 ## 6 1922 173.4 # to see how many observations (rows) and variables (columns) in a dataset dim(birthrate) ## [1] 87 2 R data can also be saved into other formats. The more efficient way, assuming that you are going to load these file back to R in the future, is to save them as .RData file. Usually, for a large dataset, this reduces the time spend on reading in the data. # saving a object to .RData file save(birthrate, file = &quot;mydata.RData&quot;) # you can specify multiple objects too save(birthrate, iris, file = &quot;mydata.RData&quot;) # load the data again load(&quot;mydata.RData&quot;) # save data to .csv file write.csv(birthrate, file = &quot;mydata.csv&quot;) 1.4 Using Packages Packages are written and contributed to R by individuals. They provide additional features (functions or data) that serve particular needs. The package ElemStatLearn is created for the textbook (Hastie, Tibshirani, and Friedman 2001) and contains may popular datasets. A package needs to be installed and loaded to your local computer. # install packages install.packages(&quot;ElemStatLearn&quot;) # load package library(ElemStatLearn) # load a dataset from the package data(SAheart) head(SAheart) ## sbp tobacco ldl adiposity famhist typea obesity alcohol age chd ## 1 160 12.00 5.73 23.11 Present 49 25.30 97.20 52 1 ## 2 144 0.01 4.41 28.61 Absent 55 28.87 2.06 63 1 ## 3 118 0.08 3.48 32.28 Present 52 29.14 3.81 46 0 ## 4 170 7.50 6.41 38.03 Present 51 31.99 24.26 58 1 ## 5 134 13.60 3.50 27.78 Present 60 25.99 57.34 49 1 ## 6 132 6.20 6.47 36.21 Present 62 30.77 14.14 45 0 1.5 Using R Markdown R Markdown is a simple way of creating documents that contain both text (including mathematical equations) and R code chunks. Authors create a .Rmd file, which will be fed to knitr to generate the document. Fortunately, RStudio already integrated everything together and all you need is to open up a .Rmd file with RStudio and click a button . A sample file is prepared here. 1.6 Explore Yourself There is no guide that can exhaust all information. I found the best learning approach is to perform a specific task and google your way to the success. Oftentimes, Stack Overflow is my best friend, especially when I am developing new packages. Also, read the reference manual carefully if you use a particular package or function. A reference manual (for packages on CRAN) can always be found at https://cran.r-project.org/web/packages/package_name. References "],
["basics-of-probability-and-statistics.html", "Chapter 2 Basics of Probability and Statistics 2.1 Random Number Generation 2.2 Summary Statistics and Data Visualization", " Chapter 2 Basics of Probability and Statistics 2.1 Random Number Generation Random number generation is important for statistical simulation. R provides random number generators for many commonly used distributions, such as binomial (rbinom), normal (rnorm), t (rt) etc. The syntax is usually a letter “r” followed by the name of the distribution. # generate 10 independent Bernoulli random variables as a vector rbinom(n=10, size = 1, prob = 0.5) ## [1] 1 0 1 0 1 1 1 0 0 0 # 4 independent random standard normal variables rnorm(n=4) ## [1] -0.3894331 0.5683866 1.3121476 2.6499437 Setting the seed before generating random numbers will allow us to replicate the results when necessary. # after setting the seed, the two runs will generate exactly the same &quot;random&quot; numbers set.seed(1) rnorm(n=4, mean = 1, sd = 2) ## [1] -0.2529076 1.3672866 -0.6712572 4.1905616 set.seed(1) rnorm(n=4, mean = 1, sd = 2) ## [1] -0.2529076 1.3672866 -0.6712572 4.1905616 Some more complicated distributions require additional packages. For example, the MASS package can be used to generate the multivariate normal distribution. One needs to specify a vector of means and an invertable covariance matrix. library(MASS) P = 4 V &lt;- 0.5^abs(outer(1:P, 1:P, &quot;-&quot;)) mvrnorm(3, mu=1:P, Sigma=V) ## [,1] [,2] [,3] [,4] ## [1,] 2.3075926 0.6273378 2.875879 3.515928 ## [2,] 1.8215099 3.1535552 2.918031 4.528882 ## [3,] 0.2549359 1.6922485 2.920198 3.623822 2.2 Summary Statistics and Data Visualization Statistical functions that provide a summary of the data. x = rnorm(n=100, mean = 1, sd = 2) y = rnorm(n=100, mean = 2, sd = 1) sum(x) ## [1] 123.9597 mean(x) ## [1] 1.239597 var(x) ## [1] 3.12596 median(x) ## [1] 1.116457 quantile(x, c(0.25, 0.5, 0.75)) ## 25% 50% 75% ## 0.0115149 1.1164572 2.3955519 cor(x, y) ## [1] 0.05988467 For discrete data, we can use the table function. library(ElemStatLearn) data(SAheart) table(SAheart$famhist) ## ## Absent Present ## 270 192 table(SAheart[, c(&quot;famhist&quot;,&quot;chd&quot;)]) ## chd ## famhist 0 1 ## Absent 206 64 ## Present 96 96 Fisher’s exact test and the Chi-square test are tests of independence between two nominal variables. # We can test the association between family history (famhist) and # the indicator of coronary heart disease (chd) # using Fisher&#39;s Exact fisher.test(table(SAheart[, c(&quot;famhist&quot;,&quot;chd&quot;)])) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(SAheart[, c(&quot;famhist&quot;, &quot;chd&quot;)]) ## p-value = 6.658e-09 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.119573 4.891341 ## sample estimates: ## odds ratio ## 3.209996 # or the Chi-square test chisq.test(table(SAheart[, c(&quot;famhist&quot;,&quot;chd&quot;)])) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: table(SAheart[, c(&quot;famhist&quot;, &quot;chd&quot;)]) ## X-squared = 33.123, df = 1, p-value = 8.653e-09 For continuous variables, data visualization can be very helpful. There are many different ways to customize a plot, such as changing the color, shape, label, etc. For more advanced features, the R package ggplot2 is a very popular choice. # We use the birthrate data introduced earlier for this example birthrate = read.csv(&quot;data/birthrate.csv&quot;) plot(birthrate, pch = 19, col = &quot;darkorange&quot;, ylab = &quot;Birth Rate&quot;, main = &quot;U.S. birth rate (1917 - 2003, per 10000)&quot;) Correlations and correlation plots can be used to summarize more variables. However, be careful that factors may not be supported by this feature and could cause errors. # load the package with loading message suppressed suppressMessages(library(PerformanceAnalytics)) chart.Correlation(SAheart[, c(1:3)], histogram=TRUE, pch=&quot;+&quot;) 3-dimensional plot is also an alternative to visualize data. We demonstrate an example using the plot3D package and the scatter3D function. The observations are colored by the outcome class (chd). The package rgl can allow for an interactive plot with rotating and zooming. library(plot3D) scatter3D(SAheart$ldl, SAheart$age, log(1+SAheart$tobacco), xlab = &quot;LDL&quot;, ylab = &quot;Age&quot;, zlab = &quot;Tobacco&quot;, pch = 18, bty = &quot;u&quot;, col.var = SAheart$chd, col = ifelse(SAheart$chd == 1, &quot;darkorange&quot;, &quot;deepskyblue&quot;), colkey = FALSE) "],
["modeling-basics-in-r.html", "Chapter 3 Modeling Basics in R 3.1 Fitting Linear Regression 3.2 Model Diagnostics 3.3 Variable Transformations and Interactions 3.4 Model Selection 3.5 Prediction", " Chapter 3 Modeling Basics in R 3.1 Fitting Linear Regression A simple linear regression assumes the underlying model \\[Y = \\beta_0 + {\\boldsymbol\\beta}^\\text{T} X + \\epsilon,\\] where \\(\\beta_0\\) is an intercept and \\(\\boldsymbol\\beta\\) is a vector of coefficients corresponds to each covariate. With observed data, we can estimate the regression coefficients. Let’s use a classical dataset, the Boston Housing data (Harrison Jr and Rubinfeld 1978) from the MASS package. The goal of this dataset is to model the median house value (medv) using other predictors. library(MASS) data(Boston) # Fit a linear regression using all variables fit = lm(medv ~ ., data = Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ ., data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.595 -2.730 -0.518 1.777 26.199 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.646e+01 5.103e+00 7.144 3.28e-12 *** ## crim -1.080e-01 3.286e-02 -3.287 0.001087 ** ## zn 4.642e-02 1.373e-02 3.382 0.000778 *** ## indus 2.056e-02 6.150e-02 0.334 0.738288 ## chas 2.687e+00 8.616e-01 3.118 0.001925 ** ## nox -1.777e+01 3.820e+00 -4.651 4.25e-06 *** ## rm 3.810e+00 4.179e-01 9.116 &lt; 2e-16 *** ## age 6.922e-04 1.321e-02 0.052 0.958229 ## dis -1.476e+00 1.995e-01 -7.398 6.01e-13 *** ## rad 3.060e-01 6.635e-02 4.613 5.07e-06 *** ## tax -1.233e-02 3.760e-03 -3.280 0.001112 ** ## ptratio -9.527e-01 1.308e-01 -7.283 1.31e-12 *** ## black 9.312e-03 2.686e-03 3.467 0.000573 *** ## lstat -5.248e-01 5.072e-02 -10.347 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.745 on 492 degrees of freedom ## Multiple R-squared: 0.7406, Adjusted R-squared: 0.7338 ## F-statistic: 108.1 on 13 and 492 DF, p-value: &lt; 2.2e-16 The output can be overwhelming for beginners. Here, by specifying the model with medv ~ ., we are using all variables in this data as predictors, except medv itself. And by default, an intercept term is also included. However, we could also specify particular variables as predictors. For example, if per capita crime rate by town (crim), the average number of rooms (rm) are used to predict the price, and the weighted mean of distances to five Boston employment centres (dis), along with an intercept term, we specify the following fit = lm(medv ~ crim + rm + dis, data = Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ crim + rm + dis, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21.247 -2.930 -0.572 2.390 39.072 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -29.45838 2.60010 -11.330 &lt; 2e-16 *** ## crim -0.25405 0.03532 -7.193 2.32e-12 *** ## rm 8.34257 0.40870 20.413 &lt; 2e-16 *** ## dis 0.12627 0.14382 0.878 0.38 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.238 on 502 degrees of freedom ## Multiple R-squared: 0.5427, Adjusted R-squared: 0.5399 ## F-statistic: 198.6 on 3 and 502 DF, p-value: &lt; 2.2e-16 To read the output from a linear model, we usually pay attention to several key information, such as the coefficient and the p-value for each variable, and the overall model fitting F statistic and its p-value, which is almost 0 in this case. 3.2 Model Diagnostics To further evaluate this model fitting, we may plot the residuals (for assessing the normality) and the Cook’s distance for identifying potential influence observations. # setup the parameters for plotting 4 figures together, in a 2 by 2 structure par(mfrow = c(2, 2)) plot(fit) R also provides several functions for obtaining metrics related to unusual observations that may help this process. resid() provides the residual for each observation hatvalues() gives the leverage of each observation rstudent() give the studentized residual for each observation cooks.distance() calculates the influence of each observation head(resid(fit), n = 10) ## 1 2 3 4 5 6 7 ## -1.908839 -3.129506 3.596769 3.719837 5.286113 3.757775 1.523165 ## 8 9 10 ## 4.353400 -1.732947 -2.519590 head(hatvalues(fit), n = 10) ## 1 2 3 4 5 6 ## 0.002547887 0.002692042 0.005408817 0.005604599 0.006415502 0.004272416 ## 7 8 9 10 ## 0.004088718 0.004346169 0.007117245 0.006403949 head(rstudent(fit), n = 10) ## 1 2 3 4 5 6 ## -0.3061026 -0.5019650 0.5777474 0.5975886 0.8498652 0.6032833 ## 7 8 9 10 ## 0.2444363 0.6990195 -0.2785307 -0.4048546 head(cooks.distance(fit), n = 10) ## 1 2 3 4 5 ## 5.994417e-05 1.702892e-04 4.544127e-04 5.038330e-04 1.166558e-03 ## 6 7 8 9 10 ## 3.909005e-04 6.144012e-05 5.337764e-04 1.392832e-04 2.645454e-04 3.3 Variable Transformations and Interactions It appears that the residuals are not normally distributed because the QQ plot deviates from the diagonal line quite a lot. Sometimes variable transformations can be used to deal with this issue, but that may not fix it completely. Plotting can be useful for detecting ill-distributed variables and suggest potential transformations. For example, we may use the correlation plot to visualize them library(PerformanceAnalytics) chart.Correlation(Boston[, c(&quot;medv&quot;, &quot;crim&quot;, &quot;rm&quot;, &quot;dis&quot;)], histogram=TRUE, pch=&quot;+&quot;) It looks like both crim and dis have heavy tail on the right hand side and could benefit from a log or a power transformation. variable transformations can be easily specified within the lm() function. fit = lm(medv ~ log(crim) + rm + I(dis^0.5), data = Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ log(crim) + rm + I(dis^0.5), data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.767 -3.506 -0.589 2.501 40.035 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -25.1474 2.8674 -8.770 &lt; 2e-16 *** ## log(crim) -1.5033 0.1864 -8.067 5.36e-15 *** ## rm 8.0520 0.4096 19.656 &lt; 2e-16 *** ## I(dis^0.5) -2.1805 0.7644 -2.853 0.00451 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.155 on 502 degrees of freedom ## Multiple R-squared: 0.5548, Adjusted R-squared: 0.5521 ## F-statistic: 208.5 on 3 and 502 DF, p-value: &lt; 2.2e-16 Another approach is to consider polynomial transformations of the outcome variable, known as the Box-Cox transformation. # explore the Box-Cox transformation trans = boxcox(medv ~ log(crim) + rm + I(dis^0.5), data = Boston) # obtain the best power for performing the polynomial trans$x[which.max(trans$y)] ## [1] 0.2626263 # refit the model fit = lm(I(medv^0.2626263) ~ log(crim) + rm + I(dis^0.5), data = Boston) One can again reevaluate the model fitting results and repeat the process if necessary. However, keep in mind that this is could be a tedious process that may not end with a satisfactory solution. To further improve the model fitting we may also consider iterations and higher order terms such as fit = lm(medv ~ log(crim) + rm + rm*log(crim) + I(rm^2) + I(dis^0.5) + as.factor(chas)*rm, data = Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ log(crim) + rm + rm * log(crim) + I(rm^2) + ## I(dis^0.5) + as.factor(chas) * rm, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.476 -2.917 -0.486 2.414 35.630 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 72.0986 12.4270 5.802 1.17e-08 *** ## log(crim) 3.2224 1.1677 2.760 0.0060 ** ## rm -23.0419 3.9298 -5.863 8.27e-09 *** ## I(rm^2) 2.3857 0.3072 7.766 4.66e-14 *** ## I(dis^0.5) -1.0768 0.6729 -1.600 0.1102 ## as.factor(chas)1 14.6814 7.4142 1.980 0.0482 * ## log(crim):rm -0.7620 0.1845 -4.129 4.27e-05 *** ## rm:as.factor(chas)1 -1.6178 1.1357 -1.424 0.1549 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.321 on 498 degrees of freedom ## Multiple R-squared: 0.6699, Adjusted R-squared: 0.6653 ## F-statistic: 144.4 on 7 and 498 DF, p-value: &lt; 2.2e-16 3.4 Model Selection Suppose we have two candidate nested models, and we want to test if adding a set of new variables is significant in terms of predicting the outcome, this is essentially an F test. We can utilize the anova() function: fit = lm(medv ~ crim + rm + dis, data = Boston) fit2 = lm(medv ~ crim + rm + dis + chas + nox, data = Boston) anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: medv ~ crim + rm + dis ## Model 2: medv ~ crim + rm + dis + chas + nox ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 502 19536 ## 2 500 17457 2 2079.2 29.776 6.057e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It appears that adding the two additional variables chas and nox is significant. Selecting variables/models is a central topic in statistics. We could consider some classical tools such as the Akaike information criterion (Akaike 1998) or the Bayesian information criterion (Schwarz and others 1978). Incorporating the stepwise selection algorithm, we may find the best AIC model: # fit a full model that contains all variables full.model = lm(medv ~ ., data = Boston) # select the best AIC model by stepwise regression stepAIC = step(full.model, trace=0, direction=&quot;both&quot;) # the best set of variables being selected attr(stepAIC$terms, &quot;term.labels&quot;) ## [1] &quot;crim&quot; &quot;zn&quot; &quot;chas&quot; &quot;nox&quot; &quot;rm&quot; &quot;dis&quot; &quot;rad&quot; ## [8] &quot;tax&quot; &quot;ptratio&quot; &quot;black&quot; &quot;lstat&quot; 3.5 Prediction The predict() function is an extremely versatile function, for, prediction. When used on the result of a model fit using lm() it will, by default, return predictions for each of the data points used to fit the model. # the fitted value from a model fitting yhat1 = fit$fitted.values # predict on a set of testing data yhat2 = predict(fit) # they are the same all(yhat1 == yhat2) ## [1] FALSE We could also specify new data, which should be a data frame or tibble with the same column names as the predictors. new_obs = data.frame(crim = 0.3, rm = 6, dis = 5) predict(fit, newdata = new_obs) ## 1 ## 21.15216 We can also obtain the confidence interval for the mean response value of this new observation predict(fit, newdata = new_obs, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 21.15216 20.44473 21.8596 Lastly, we can alter the level using the level argument. Here we report a prediction interval instead of a confidence interval. predict(fit, newdata = new_obs, interval = &quot;prediction&quot;, level = 0.99) ## fit lwr upr ## 1 21.15216 4.995294 37.30903 References "],
["optimizations-in-r.html", "Chapter 4 Optimizations in `R’ 4.1 A simple optimization problem", " Chapter 4 Optimizations in `R’ TODO: Instead of specifically considering regression, change the focus of this chapter to modeling, with regression as an example. This chapter will recap the basics of performing regression analyses in R. For more detailed coverage, see Applied Statistics with R. We will use the Advertising data associated with Introduction to Statistical Learning. library(readr) Advertising = read_csv(&quot;data/Advertising.csv&quot;) After loading data into R, our first step should always be to inspect the data. We will start by simply printing some observations in order to understand the basic structure of the data. Advertising ## # A tibble: 200 x 4 ## TV Radio Newspaper Sales ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 230. 37.8 69.2 22.1 ## 2 44.5 39.3 45.1 10.4 ## 3 17.2 45.9 69.3 9.3 ## 4 152. 41.3 58.5 18.5 ## 5 181. 10.8 58.4 12.9 ## 6 8.7 48.9 75 7.2 ## 7 57.5 32.8 23.5 11.8 ## 8 120. 19.6 11.6 13.2 ## 9 8.6 2.1 1 4.8 ## 10 200. 2.6 21.2 10.6 ## # ... with 190 more rows Because the data was read using read_csv(), Advertising is a tibble. We see that there are a total of 200 observations and 4 variables, each of which is numeric. (Specifically double-precision vectors, but more importantly they are numbers.) For the purpose of this analysis, Sales will be the response variable. That is, we seek to understand the relationship between Sales, and the predictor variables: TV, Radio, and Newspaper. 4.1 A simple optimization problem After investigating the structure of the data, the next step should be to visualize the data. Since we have only numeric variables, we should consider scatter plots. "],
["regression-overview.html", "Chapter 5 Overview", " Chapter 5 Overview Chapter Status: This chapter is currently undergoing massive rewrites. Some code is mostly for the use of current students. Plotting code is often not best practice for plotting, but is instead useful for understanding. Supervised Learning Regression (Numeric Response) What do we want? To make predictions on unseen data. (Predicting on data we already have is easy…) In other words, we want a model that generalizes well. That is, generalizes to unseen data. How we will do this? By controlling the complexity of the model to guard against overfitting and underfitting. Model Parameters Tuning Parameters Why does manipulating the model complexity accomplish this? Because there is a bias-variance tradeoff. How do we know if our model generalizes? By evaluating metrics on test data. We will only ever fit (train) models on training data. All analyses will begin with a test-train split. For regression tasks, our metric will be RMSE. Classification (Categorical Response) The next section. Regression is a form of supervised learning. Supervised learning deals with problems where there are both an input and an output. Regression problems are the subset of supervised learning problems with a numeric output. Often one of the biggest differences between statistical learning, machine learning, artificial intelligence are the names used to describe variables and methods. The input can be called: input vector, feature vector, or predictors. The elements of these would be an input, feature, or predictor. The individual features can be either numeric or categorical. The output may be called: output, response, outcome, or target. The response must be numeric. As an aside, some textbooks and statisticians use the terms independent and dependent variables to describe the response and the predictors. However, this practice can be confusing as those terms have specific meanings in probability theory. Our goal is to find a rule, algorithm, or function which takes as input a feature vector, and outputs a response which is as close to the true value as possible. We often write the true, unknown relationship between the input and output \\(f(\\bf{x})\\). The relationship (model) we learn (fit, train), based on data, is written \\(\\hat{f}(\\bf{x})\\). From a statistical learning point-of-view, we write, \\[ Y = f(\\bf{x}) + \\epsilon \\] to indicate that the true response is a function of both the unknown relationship, as well as some unlearnable noise. \\[ \\text{RMSE}(\\hat{f}, \\text{Data}) = \\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i = 1}^{n}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\] \\[ \\text{RMSE}_{\\text{Train}} = \\text{RMSE}(\\hat{f}, \\text{Train Data}) = \\sqrt{\\frac{1}{n_{\\text{Tr}}}\\displaystyle\\sum_{i \\in \\text{Train}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\] \\[ \\text{RMSE}_{\\text{Test}} = \\text{RMSE}(\\hat{f}, \\text{Test Data}) = \\sqrt{\\frac{1}{n_{\\text{Te}}}\\displaystyle\\sum_{i \\in \\text{Test}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\] - TODO: RSS vs \\(R^2\\) vs RMSE Code for Plotting from Class ## load packages library(rpart) library(FNN) # simulate data ## signal f = function(x) { x ^ 3 } ## define data generating processs get_sim_data = function(f, sample_size = 50) { x = runif(n = sample_size, min = -1, max = 1) y = rnorm(n = sample_size, mean = f(x), sd = 0.15) data.frame(x, y) } ## simualte training data set.seed(42) sim_trn_data = get_sim_data(f = f) ## simulate testing data set.seed(3) sim_tst_data = get_sim_data(f = f) ## create grid for plotting x_grid = data.frame(x = seq(-1.5, 1.5, 0.001)) # fit models ## tree models tree_fit_l = rpart(y ~ x, data = sim_trn_data, control = rpart.control(cp = 0.500, minsplit = 2)) tree_fit_m = rpart(y ~ x, data = sim_trn_data, control = rpart.control(cp = 0.015, minsplit = 2)) tree_fit_h = rpart(y ~ x, data = sim_trn_data, control = rpart.control(cp = 0.000, minsplit = 2)) ## knn models knn_fit_l = knn.reg(train = sim_trn_data[&quot;x&quot;], y = sim_trn_data$y, test = x_grid, k = 40) knn_fit_m = knn.reg(train = sim_trn_data[&quot;x&quot;], y = sim_trn_data$y, test = x_grid, k = 5) knn_fit_h = knn.reg(train = sim_trn_data[&quot;x&quot;], y = sim_trn_data$y, test = x_grid, k = 1) ## polynomial models poly_fit_l = lm(y ~ poly(x, 1), data = sim_trn_data) poly_fit_m = lm(y ~ poly(x, 3), data = sim_trn_data) poly_fit_h = lm(y ~ poly(x, 22), data = sim_trn_data) # get predictions ## tree models tree_fit_l_pred = predict(tree_fit_l, newdata = x_grid) tree_fit_m_pred = predict(tree_fit_m, newdata = x_grid) tree_fit_h_pred = predict(tree_fit_h, newdata = x_grid) ## knn models knn_fit_l_pred = knn_fit_l$pred knn_fit_m_pred = knn_fit_m$pred knn_fit_h_pred = knn_fit_h$pred ## polynomial models poly_fit_l_pred = predict(poly_fit_l, newdata = x_grid) poly_fit_m_pred = predict(poly_fit_m, newdata = x_grid) poly_fit_h_pred = predict(poly_fit_h, newdata = x_grid) # plot fitted trees par(mfrow = c(1, 3)) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Tree Model, cp = 0.5&quot;, cex = 1.5) grid() lines(x_grid$x, tree_fit_l_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Tree Model, cp = 0.015&quot;, cex = 1.5) grid() lines(x_grid$x, tree_fit_m_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Tree Model, cp = 0.0&quot;, cex = 1.5) grid() lines(x_grid$x, tree_fit_h_pred, col = &quot;darkgrey&quot;, lwd = 2) # plot fitted KNN par(mfrow = c(1, 3)) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;KNN, k = 40&quot;, cex = 1.5) grid() lines(x_grid$x, knn_fit_l_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;KNN, k = 5&quot;, cex = 1.5) grid() lines(x_grid$x, knn_fit_m_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;KNN, k = 1&quot;, cex = 1.5) grid() lines(x_grid$x, knn_fit_h_pred, col = &quot;darkgrey&quot;, lwd = 2) # plot fitted polynomials par(mfrow = c(1, 3)) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Polynomial Model, degree = 1&quot;, cex = 1.5) grid() lines(x_grid$x, poly_fit_l_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Polynomial Model, degree = 3&quot;, cex = 1.5) grid() lines(x_grid$x, poly_fit_m_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Polynomial Model, degree = 22&quot;, cex = 1.5) grid() lines(x_grid$x, poly_fit_h_pred, col = &quot;darkgrey&quot;, lwd = 2) "]
>>>>>>> e725a8cb5308375bfaad1e4ca91108141f6e937d
]
