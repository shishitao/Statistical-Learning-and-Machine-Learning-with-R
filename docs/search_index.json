[
["index.html", "Statistical Learning with R Preface Target Audience What’s Covered? Acknowledgements License", " Statistical Learning with R Ruoqing Zhu 2018-05-14 Preface Welcome to Statistical Learning with R! I started this project during the summer of 2018 when I was preparing for the Stat 432 course. This course was taught by our former faculty member Dr. David Dalpiaz, who is currently at The Ohio State University. David introduced to me this awesome way of publishing website on GitHub, which is very efficient approach for developing courses. Since I was also teaching Stat 542 (Statistical Learning) during the previous two years, I figured it could be beneficial to integrate what I have to this existing book by David, and use it as the R material for both courses. As you can tell, I am not being very creative on the name, so `SLWR’ it is. You can find the source file of this book on my GitHub. Target Audience This book is targeted at advanced undergraduate to MS students in Statistics who have some or no prior statistical learning experience. Previous experience with both basic mathematics (mainly linear algebra), statistical modeling (such as linear regressions) and R are assumed. What’s Covered? I currently plan to include the following topics: Basics Knowledge Statistics and Probability Using R, RStudio and R Markdown Optimization Prelimiaries Linear and Penalized Linear Regressions Unsupervised Learning Classification Non-parametric Statistical Models Machine Learning Models Appendix The goal of this book is to introduce not only how to run some of the popular statistical learning models in R, but also touches some basic algorithms and programming techniques for solving some of these models. For each section, the difficulty may gradually increase from an undergraduate level to a graduate level. It will be served as a supplement to An Introduction to Statistical Learning for STAT 432 - Basics of Statistical Learning and The Elements of Statistical Learning: Data Mining, Inference, and Prediction for STAT 542 - Statistical Learning at the University of Illinois at Urbana-Champaign. This book is under active development as I am teaching Stat 432 during Fall 2018. Hence, you may encounter errors ranging from typos, to broken code, to poorly explained topics. If you do, please let me know! Simply send an email and I will make the changes as soon as possible (rqzhu AT illinois DOT edu). Or, if you know R Markdown and are familiar with GitHub, make a pull request and fix an issue yourself!. These contributions will be acknowledged. Acknowledgements The initial contents are derived from Dr. David Dalpiaz’s book. My Stat 542 course materials are also inspired by Dr. Feng Liang and Dr. John Marden who developed earlier versions of this course. And I also incorporated many online resources, such as bookdown: Authoring Books and Technical Documents with R Markdown R Programming for Data Science and others through Google search. License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["introducing-r-rstudio-and-r-markdown.html", "Chapter 1 Introducing R, RStudio and R Markdown 1.1 Resources and Guides 1.2 Demonstrating Examples", " Chapter 1 Introducing R, RStudio and R Markdown R is a free-to-use software that is very popular in statistical computing. You can download R from its official website. Another software that makes using R easier is RStudio, which is available at here. You can find many on-line guides that help you to set-up these two software, for example, this YouTube video. R Markdown is a build-in feature of RStudio. It works like an integration of LaTex and programming playground that complies source code into nice-looking PDF, html, or MS Word files. This book is created using an extension of R Markdown, developed by Yihui Xie. 1.1 Resources and Guides There are many online resources for how to use R, RStudio, and R Markdown. For example, David Dalpiaz’s other online book Applied Statistics with R contains an introduction to using them. There are also other online documentations such as Install R and RStudio R tutorial Data in R Playlist (video) R and RStudio Playlist (video) R Markdown Cheat Sheet R Markdown Playlist (video) It is worth to mention that once you become a developer of R packages using C/C++ (add-on of R for performing specific tasks), and you also happen to use Windows like I do, you have to install this Rtools that contains compilers. This is also needed if you want to mannually install any R package using a “source” (.tar.gz files) instead of using the so-called “binaries” (.zip files). 1.2 Demonstrating Examples We will briefly cover some basic R calculations and operations. 1.2.1 Basic Mathematical Operations Try type-in the following commands into your R console and start to explore yourself. Most of them are self-explanatory. Lines with a # in the front are comments, which will not be executed. Lines with ## in the front are outputs. # Basic mathematical operations 1 + 3 ## [1] 4 3*5 ## [1] 15 3^5 ## [1] 243 exp(2) ## [1] 7.389056 log(3) ## [1] 1.098612 log2(3) ## [1] 1.584963 factorial(5) ## [1] 120 1.2.2 Data Objects Data objects can be a complicated topic for people who never used R before. Most common data objects are vector, matrix, list, and `data.frame’. Operations on vectors are matrices are faily intuitive. # creating a vector c(1,2,3,4) ## [1] 1 2 3 4 # creating matrix from a vector matrix(c(1,2,3,4), 2, 2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 x = c(1,1,1,0,0,0); y = c(1,0,1,0,1,0) cbind(x,y) ## x y ## [1,] 1 1 ## [2,] 1 0 ## [3,] 1 1 ## [4,] 0 0 ## [5,] 0 1 ## [6,] 0 0 # matrix multiplication using &#39;%*%&#39; matrix(c(1,2,3,4), 2, 2) %*% t(cbind(x,y)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 4 1 4 0 3 0 ## [2,] 6 2 6 0 4 0 Simple mathematical operations on vectors and matrices are usually element-wise. You can easliy extract certain elements of them by using the [] operator, like a C programming reference style. # some simple operations x[3] ## [1] 1 x[2:5] ## [1] 1 1 0 0 cbind(x,y)[1:2, ] ## x y ## [1,] 1 1 ## [2,] 1 0 (x + y)^2 ## [1] 4 1 4 0 1 0 length(x) ## [1] 6 dim(cbind(x,y)) ## [1] 6 2 # A warning will be issued when R detects something wrong. Results may still be produced. x + c(1,2,3,4) ## Warning in x + c(1, 2, 3, 4): longer object length is not a multiple of ## shorter object length ## [1] 2 3 4 4 1 2 list() simply creates a list of objects (of any type). Random number generation is important for statistical simulation. # generate 10 Bernoulli random variables as a vector rbinom(n=10, size = 1, prob = 0.5) ## [1] 1 0 1 0 1 0 1 0 1 1 # 2 random normally distributed variables rnorm(n=4, mean = 1, sd = 2) ## [1] 5.7685581 2.9418756 0.2432349 1.1514992 If we need to replicate the results, we can set random seed # after setting the seed, the two runs will generate exactly the same &quot;random&quot; numbers set.seed(1) rnorm(n=4, mean = 1, sd = 2) ## [1] -0.2529076 1.3672866 -0.6712572 4.1905616 set.seed(1) rnorm(n=4, mean = 1, sd = 2) ## [1] -0.2529076 1.3672866 -0.6712572 4.1905616 Statistical functions that provides a summary of the data x = rnorm(n=100, mean = 1, sd = 2) y = rnorm(n=100, mean = 2, sd = 1) sum(x) ## [1] 118.4815 mean(x) ## [1] 1.184815 var(x) ## [1] 3.142351 median(x) ## [1] 1.148906 quantile(x, c(0.25, 0.5, 0.75)) ## 25% 50% 75% ## 0.0115149 1.1489063 2.2746083 cor(x, y) ## [1] -0.04261199 For discrete data, we usually use the table function set.seed(1); n = 1000 x = rbinom(n, size = 1, prob = 0.75) y = rbinom(n, size = 3, prob = c(0.4, 0.3, 0.2, 0.1)) table(x) ## x ## 0 1 ## 248 752 table(x, y) ## y ## x 0 1 2 3 ## 0 128 79 34 7 ## 1 342 267 125 18 For a mixture of discrete and continuous data (multiple variables), we often use a data.frame # data.frame is a special data structure that can store both factor and numeric variables z = runif(n, min = 18, max = 65) data = data.frame(&quot;Gender&quot; = as.factor(x), &quot;Group&quot; = as.factor(y), &quot;Age&quot; = z) levels(data$Gender) = c(&quot;male&quot;, &quot;female&quot;) levels(data$Group) = c(&quot;patient&quot;, &quot;physician&quot;, &quot;engineer&quot;, &quot;statistician&quot;) # a peek at the top 3 entries of the data head(data, 3) ## Gender Group Age ## 1 female physician 58.97484 ## 2 female physician 63.45826 ## 3 female patient 58.74506 # a brief summary summary(data) ## Gender Group Age ## male :248 patient :470 Min. :18.03 ## female:752 physician :346 1st Qu.:29.07 ## engineer :159 Median :40.51 ## statistician: 25 Mean :41.02 ## 3rd Qu.:53.43 ## Max. :64.99 # generate a 2 by 2 table that summarizes Gender and Group table(data[, 1:2]) ## Group ## Gender patient physician engineer statistician ## male 128 79 34 7 ## female 342 267 125 18 Fisher Exact Test and Chi-square test are tests of independence between two nominal variables. # the test p-value is not significant # recall that we generated the two variables independently. fisher.test(table(data[, 1:2])) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(data[, 1:2]) ## p-value = 0.3361 ## alternative hypothesis: two.sided # chi-square test chisq.test(table(data[, 1:2])) ## ## Pearson&#39;s Chi-squared test ## ## data: table(data[, 1:2]) ## X-squared = 3.3437, df = 3, p-value = 0.3416 For continuous variables, we can calculate and test Pearson’s correlation, Spearman’s rho or Kendall’ tau set.seed(1); n = 30 x = rnorm(n) y = x + rnorm(n, sd = 2) z = x + rnorm(n, sd = 2) # one can specify method = &quot;kendall&quot; or &quot;spearman&quot; to perform other correlations or tests cor(y, z) ## [1] 0.5810874 cor.test(y, z) ## ## Pearson&#39;s product-moment correlation ## ## data: y and z ## t = 3.7782, df = 28, p-value = 0.0007592 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.2792861 0.7784002 ## sample estimates: ## cor ## 0.5810874 A simple linear regression assumes the underlying model \\(Y = \\beta X + \\epsilon\\). With observed data, we can estimate the regression coefficients: # the lm() function is the most commonly used fit = lm(y~x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0404 -1.0099 -0.4594 1.1506 3.7069 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.2586 0.2964 0.873 0.39032 ## x 1.0838 0.3249 3.336 0.00241 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.617 on 28 degrees of freedom ## Multiple R-squared: 0.2844, Adjusted R-squared: 0.2588 ## F-statistic: 11.13 on 1 and 28 DF, p-value: 0.00241 A graphical view usually helps understand the data better. There are a variety of ways to customize your plot, such as color and shape. plot(x, y, xlab = &quot;x&quot;, ylab = &quot;y&quot;, main = &quot;A plot&quot;, col = &quot;darkorange&quot;, pch = 19) abline(a = fit$coefficients[1], b = fit$coefficients[2], col = &quot;deepskyblue&quot;, lty = 2, lwd = 2) legend(&quot;topleft&quot;, c(&quot;observed points&quot;, &quot;fitted line&quot;), lty = c(NA, 2), pch = c(19, NA), col = c(&quot;darkorange&quot;, &quot;deepskyblue&quot;)) R can read-in data from many different sources such as , , etc. For example, can be used to import files. The first argument should be specified as the path to the data file, or just the name of the file if the current working directory is the same as the data file. R objects, especially matrices, can be saved into these standard files. Use functions such as and to perform this. We can also save any object into file, which can be loaded later on. To do this try functions and . One of the most important features of R is its massive collection of packages. A package is like an add-on that can be downloaded and installed and perform additional function and analysis. # The MASS package can be used to generate multivariate normal distribution library(MASS) P = 4; N = 200 V &lt;- 0.5^abs(outer(1:P, 1:P, &quot;-&quot;)) X = as.matrix(mvrnorm(N, mu=rep(0,P), Sigma=V)) head(X, 3) ## [,1] [,2] [,3] [,4] ## [1,] -0.5324135 1.022266 0.54818061 0.32146101 ## [2,] 0.4287820 -1.612594 -1.79219165 -0.07651307 ## [3,] -0.5563969 -1.335632 0.03372782 -1.62713529 To get reference about a particular function, one can put a question mark in front of a function name to see details: # This will open up the web browser for the on-line document ?mvrnorm ?save.image "],
["modeling-basics-in-r.html", "Chapter 2 Modeling Basics in R 2.1 Visualization for Regression 2.2 The lm() Function 2.3 Hypothesis Testing 2.4 Prediction 2.5 Unusual Observations 2.6 Adding Complexity 2.7 rmarkdown", " Chapter 2 Modeling Basics in R TODO: Instead of specifically considering regression, change the focus of this chapter to modeling, with regression as an example. This chapter will recap the basics of performing regression analyses in R. For more detailed coverage, see Applied Statistics with R. We will use the Advertising data associated with Introduction to Statistical Learning. library(readr) Advertising = read_csv(&quot;data/Advertising.csv&quot;) After loading data into R, our first step should always be to inspect the data. We will start by simply printing some observations in order to understand the basic structure of the data. Advertising ## # A tibble: 200 x 4 ## TV Radio Newspaper Sales ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 230. 37.8 69.2 22.1 ## 2 44.5 39.3 45.1 10.4 ## 3 17.2 45.9 69.3 9.30 ## 4 152. 41.3 58.5 18.5 ## 5 181. 10.8 58.4 12.9 ## 6 8.70 48.9 75.0 7.20 ## 7 57.5 32.8 23.5 11.8 ## 8 120. 19.6 11.6 13.2 ## 9 8.60 2.10 1.00 4.80 ## 10 200. 2.60 21.2 10.6 ## # ... with 190 more rows Because the data was read using read_csv(), Advertising is a tibble. We see that there are a total of 200 observations and 4 variables, each of which is numeric. (Specifically double-precision vectors, but more importantly they are numbers.) For the purpose of this analysis, Sales will be the response variable. That is, we seek to understand the relationship between Sales, and the predictor variables: TV, Radio, and Newspaper. 2.1 Visualization for Regression After investigating the structure of the data, the next step should be to visualize the data. Since we have only numeric variables, we should consider scatter plots. We could do so for any individual predictor. plot(Sales ~ TV, data = Advertising, col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5, main = &quot;Sales vs Television Advertising&quot;) The pairs() function is a useful way to quickly visualize a number of scatter plots. pairs(Advertising) Often, we will be most interested in only the relationship between each predictor and the response. For this, we can use the featurePlot() function from the caret package. (We will use the caret package more and more frequently as we introduce new topics.) library(caret) featurePlot(x = Advertising[ , c(&quot;TV&quot;, &quot;Radio&quot;, &quot;Newspaper&quot;)], y = Advertising$Sales) We see that there is a clear increase in Sales as Radio or TV are increased. The relationship between Sales and Newspaper is less clear. How all of the predictors work together is also unclear, as there is some obvious correlation between Radio and TV. To investigate further, we will need to model the data. 2.2 The lm() Function The following code fits an additive linear model with Sales as the response and each remaining variable as a predictor. Note, by not using attach() and instead specifying the data = argument, we are able to specify this model without using each of the variable names directly. mod_1 = lm(Sales ~ ., data = Advertising) # mod_1 = lm(Sales ~ TV + Radio + Newspaper, data = Advertising) Note that the commented line is equivalent to the line that is run, but we will often use the response ~ . syntax when possible. 2.3 Hypothesis Testing The summary() function will return a large amount of useful information about a model fit using lm(). Much of it will be helpful for hypothesis testing including individual tests about each predictor, as well as the significance of the regression test. summary(mod_1) ## ## Call: ## lm(formula = Sales ~ ., data = Advertising) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.8277 -0.8908 0.2418 1.1893 2.8292 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.938889 0.311908 9.422 &lt;2e-16 *** ## TV 0.045765 0.001395 32.809 &lt;2e-16 *** ## Radio 0.188530 0.008611 21.893 &lt;2e-16 *** ## Newspaper -0.001037 0.005871 -0.177 0.86 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.686 on 196 degrees of freedom ## Multiple R-squared: 0.8972, Adjusted R-squared: 0.8956 ## F-statistic: 570.3 on 3 and 196 DF, p-value: &lt; 2.2e-16 mod_0 = lm(Sales ~ TV + Radio, data = Advertising) The anova() function is useful for comparing two models. Here we compare the full additive model, mod_1, to a reduced model mod_0. Essentially we are testing for the significance of the Newspaper variable in the additive model. anova(mod_0, mod_1) ## Analysis of Variance Table ## ## Model 1: Sales ~ TV + Radio ## Model 2: Sales ~ TV + Radio + Newspaper ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 197 556.91 ## 2 196 556.83 1 0.088717 0.0312 0.8599 Note that hypothesis testing is not our focus, so we omit many details. 2.4 Prediction The predict() function is an extremely versatile function, for, prediction. When used on the result of a model fit using lm() it will, by default, return predictions for each of the data points used to fit the model. (Here, we limit the printed result to the first 10.) head(predict(mod_1), n = 10) ## 1 2 3 4 5 6 7 ## 20.523974 12.337855 12.307671 17.597830 13.188672 12.478348 11.729760 ## 8 9 10 ## 12.122953 3.727341 12.550849 Note that the effect of the predict() function is dependent on the input to the function. Here, we are supplying as the first argument a model object of class lm. Because of this, predict() then runs the predict.lm() function. Thus, we should use ?predict.lm() for details. We could also specify new data, which should be a data frame or tibble with the same column names as the predictors. new_obs = data.frame(TV = 150, Radio = 40, Newspaper = 1) We can then use the predict() function for point estimates, confidence intervals, and prediction intervals. Using only the first two arguments, R will simply return a point estimate, that is, the “predicted value,” \\(\\hat{y}\\). predict(mod_1, newdata = new_obs) ## 1 ## 17.34375 If we specify an additional argument interval with a value of &quot;confidence&quot;, R will return a 95% confidence interval for the mean response at the specified point. Note that here R also gives the point estimate as fit. predict(mod_1, newdata = new_obs, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 17.34375 16.77654 17.91096 Lastly, we can alter the level using the level argument. Here we report a prediction interval instead of a confidence interval. predict(mod_1, newdata = new_obs, interval = &quot;prediction&quot;, level = 0.99) ## fit lwr upr ## 1 17.34375 12.89612 21.79138 2.5 Unusual Observations R provides several functions for obtaining metrics related to unusual observations. resid() provides the residual for each observation hatvalues() gives the leverage of each observation rstudent() give the studentized residual for each observation cooks.distance() calculates the influence of each observation head(resid(mod_1), n = 10) ## 1 2 3 4 5 6 ## 1.57602559 -1.93785482 -3.00767078 0.90217049 -0.28867186 -5.27834763 ## 7 8 9 10 ## 0.07024005 1.07704683 1.07265914 -1.95084872 head(hatvalues(mod_1), n = 10) ## 1 2 3 4 5 6 ## 0.025202848 0.019418228 0.039226158 0.016609666 0.023508833 0.047481074 ## 7 8 9 10 ## 0.014435091 0.009184456 0.030714427 0.017147645 head(rstudent(mod_1), n = 10) ## 1 2 3 4 5 6 ## 0.94680369 -1.16207937 -1.83138947 0.53877383 -0.17288663 -3.28803309 ## 7 8 9 10 ## 0.04186991 0.64099269 0.64544184 -1.16856434 head(cooks.distance(mod_1), n = 10) ## 1 2 3 4 5 ## 5.797287e-03 6.673622e-03 3.382760e-02 1.230165e-03 1.807925e-04 ## 6 7 8 9 10 ## 1.283058e-01 6.452021e-06 9.550237e-04 3.310088e-03 5.945006e-03 2.6 Adding Complexity We have a number of ways to add complexity to a linear model, even allowing a linear model to be used to model non-linear relationships. 2.6.1 Interactions Interactions can be introduced to the lm() procedure in a number of ways. We can use the : operator to introduce a single interaction of interest. mod_2 = lm(Sales ~ . + TV:Newspaper, data = Advertising) coef(mod_2) ## (Intercept) TV Radio Newspaper TV:Newspaper ## 3.8730824491 0.0392939602 0.1901312252 -0.0320449675 0.0002016962 The response ~ . ^ k syntax can be used to model all k-way interactions. (As well as the appropriate lower order terms.) Here we fit a model with all two-way interactions, and the lower order main effects. mod_3 = lm(Sales ~ . ^ 2, data = Advertising) coef(mod_3) ## (Intercept) TV Radio Newspaper ## 6.460158e+00 2.032710e-02 2.292919e-02 1.703394e-02 ## TV:Radio TV:Newspaper Radio:Newspaper ## 1.139280e-03 -7.971435e-05 -1.095976e-04 The * operator can be used to specify all interactions of a certain order, as well as all lower order terms according to the usual hierarchy. Here we see a three-way interaction and all lower order terms. mod_4 = lm(Sales ~ TV * Radio * Newspaper, data = Advertising) coef(mod_4) ## (Intercept) TV Radio ## 6.555887e+00 1.971030e-02 1.962160e-02 ## Newspaper TV:Radio TV:Newspaper ## 1.310565e-02 1.161523e-03 -5.545501e-05 ## Radio:Newspaper TV:Radio:Newspaper ## 9.062944e-06 -7.609955e-07 Note that, we have only been dealing with numeric predictors. Categorical predictors are often recorded as factor variables in R. library(tibble) cat_pred = tibble( x1 = factor(c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10), rep(&quot;C&quot;, 10))), x2 = runif(n = 30), y = rnorm(n = 30) ) cat_pred ## # A tibble: 30 x 3 ## x1 x2 y ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 0.558 -1.19 ## 2 A 0.870 0.595 ## 3 A 0.686 -0.558 ## 4 A 0.788 1.18 ## 5 A 0.979 -1.23 ## 6 A 0.661 -0.755 ## 7 A 0.0743 -0.992 ## 8 A 0.769 -0.476 ## 9 A 0.655 -0.866 ## 10 A 0.102 -1.49 ## # ... with 20 more rows Notice that in this simple simulated tibble, we have coerced x1 to be a factor variable, although this is not strictly necessary since the variable took values A, B, and C. When using lm(), even if not a factor, R would have treated x1 as such. Coercion to factor is more important if a categorical variable is coded for example as 1, 2 and 3. Otherwise it is treated as numeric, which creates a difference in the regression model. The following two models illustrate the effect of factor variables on linear models. cat_pred_mod_add = lm(y ~ x1 + x2, data = cat_pred) coef(cat_pred_mod_add) ## (Intercept) x1B x1C x2 ## -0.6421212 0.5973976 0.7949951 0.1047108 cat_pred_mod_int = lm(y ~ x1 * x2, data = cat_pred) coef(cat_pred_mod_int) ## (Intercept) x1B x1C x2 x1B:x2 x1C:x2 ## -1.3935618 2.0004628 1.2765280 1.3279982 -2.8159012 -0.7879916 2.6.2 Polynomials Polynomial terms can be specified using the inhibit function I() or through the poly() function. Note that these two methods produce different coefficients, but the same residuals! This is due to the poly() function using orthogonal polynomials by default. mod_5 = lm(Sales ~ TV + I(TV ^ 2), data = Advertising) coef(mod_5) ## (Intercept) TV I(TV^2) ## 6.114120e+00 6.726593e-02 -6.846934e-05 mod_6 = lm(Sales ~ poly(TV, degree = 2), data = Advertising) coef(mod_6) ## (Intercept) poly(TV, degree = 2)1 poly(TV, degree = 2)2 ## 14.022500 57.572721 -6.228802 all.equal(resid(mod_5), resid(mod_6)) ## [1] TRUE Polynomials and interactions can be mixed to create even more complex models. mod_7 = lm(Sales ~ . ^ 2 + poly(TV, degree = 3), data = Advertising) # mod_7 = lm(Sales ~ . ^ 2 + I(TV ^ 2) + I(TV ^ 3), data = Advertising) coef(mod_7) ## (Intercept) TV Radio ## 6.206394e+00 2.092726e-02 3.766579e-02 ## Newspaper poly(TV, degree = 3)1 poly(TV, degree = 3)2 ## 1.405289e-02 NA -9.925605e+00 ## poly(TV, degree = 3)3 TV:Radio TV:Newspaper ## 5.309590e+00 1.082074e-03 -5.690107e-05 ## Radio:Newspaper ## -9.924992e-05 Notice here that R ignores the first order term from poly(TV, degree = 3) as it is already in the model. We could consider using the commented line instead. 2.6.3 Transformations Note that we could also create more complex models, which allow for non-linearity, using transformations. Be aware, when doing so to the response variable, that this will affect the units of said variable. You may need to un-transform to compare to non-transformed models. mod_8 = lm(log(Sales) ~ ., data = Advertising) sqrt(mean(resid(mod_8) ^ 2)) # incorrect RMSE for Model 8 ## [1] 0.1849483 sqrt(mean(resid(mod_7) ^ 2)) # RMSE for Model 7 ## [1] 0.4813215 sqrt(mean(exp(resid(mod_8)) ^ 2)) # correct RMSE for Model 8 ## [1] 1.023205 2.7 rmarkdown The rmarkdown file for this chapter can be found here. The file was created using R version 3.4.4. The following packages (and their dependencies) were loaded in this file: ## [1] &quot;tibble&quot; &quot;caret&quot; &quot;ggplot2&quot; &quot;lattice&quot; &quot;readr&quot; "],
["optimizations-in-r.html", "Chapter 3 Optimizations in `R’ 3.1 A simple optimization problem", " Chapter 3 Optimizations in `R’ TODO: Instead of specifically considering regression, change the focus of this chapter to modeling, with regression as an example. This chapter will recap the basics of performing regression analyses in R. For more detailed coverage, see Applied Statistics with R. We will use the Advertising data associated with Introduction to Statistical Learning. library(readr) Advertising = read_csv(&quot;data/Advertising.csv&quot;) After loading data into R, our first step should always be to inspect the data. We will start by simply printing some observations in order to understand the basic structure of the data. Advertising ## # A tibble: 200 x 4 ## TV Radio Newspaper Sales ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 230. 37.8 69.2 22.1 ## 2 44.5 39.3 45.1 10.4 ## 3 17.2 45.9 69.3 9.30 ## 4 152. 41.3 58.5 18.5 ## 5 181. 10.8 58.4 12.9 ## 6 8.70 48.9 75.0 7.20 ## 7 57.5 32.8 23.5 11.8 ## 8 120. 19.6 11.6 13.2 ## 9 8.60 2.10 1.00 4.80 ## 10 200. 2.60 21.2 10.6 ## # ... with 190 more rows Because the data was read using read_csv(), Advertising is a tibble. We see that there are a total of 200 observations and 4 variables, each of which is numeric. (Specifically double-precision vectors, but more importantly they are numbers.) For the purpose of this analysis, Sales will be the response variable. That is, we seek to understand the relationship between Sales, and the predictor variables: TV, Radio, and Newspaper. 3.1 A simple optimization problem After investigating the structure of the data, the next step should be to visualize the data. Since we have only numeric variables, we should consider scatter plots. "],
["regression-overview.html", "Chapter 4 Overview", " Chapter 4 Overview Chapter Status: This chapter is currently undergoing massive rewrites. Some code is mostly for the use of current students. Plotting code is often not best practice for plotting, but is instead useful for understanding. Supervised Learning Regression (Numeric Response) What do we want? To make predictions on unseen data. (Predicting on data we already have is easy…) In other words, we want a model that generalizes well. That is, generalizes to unseen data. How we will do this? By controlling the complexity of the model to guard against overfitting and underfitting. Model Parameters Tuning Parameters Why does manipulating the model complexity accomplish this? Because there is a bias-variance tradeoff. How do we know if our model generalizes? By evaluating metrics on test data. We will only ever fit (train) models on training data. All analyses will begin with a test-train split. For regression tasks, our metric will be RMSE. Classification (Categorical Response) The next section. Regression is a form of supervised learning. Supervised learning deals with problems where there are both an input and an output. Regression problems are the subset of supervised learning problems with a numeric output. Often one of the biggest differences between statistical learning, machine learning, artificial intelligence are the names used to describe variables and methods. The input can be called: input vector, feature vector, or predictors. The elements of these would be an input, feature, or predictor. The individual features can be either numeric or categorical. The output may be called: output, response, outcome, or target. The response must be numeric. As an aside, some textbooks and statisticians use the terms independent and dependent variables to describe the response and the predictors. However, this practice can be confusing as those terms have specific meanings in probability theory. Our goal is to find a rule, algorithm, or function which takes as input a feature vector, and outputs a response which is as close to the true value as possible. We often write the true, unknown relationship between the input and output \\(f(\\bf{x})\\). The relationship (model) we learn (fit, train), based on data, is written \\(\\hat{f}(\\bf{x})\\). From a statistical learning point-of-view, we write, \\[ Y = f(\\bf{x}) + \\epsilon \\] to indicate that the true response is a function of both the unknown relationship, as well as some unlearnable noise. \\[ \\text{RMSE}(\\hat{f}, \\text{Data}) = \\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i = 1}^{n}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\] \\[ \\text{RMSE}_{\\text{Train}} = \\text{RMSE}(\\hat{f}, \\text{Train Data}) = \\sqrt{\\frac{1}{n_{\\text{Tr}}}\\displaystyle\\sum_{i \\in \\text{Train}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\] \\[ \\text{RMSE}_{\\text{Test}} = \\text{RMSE}(\\hat{f}, \\text{Test Data}) = \\sqrt{\\frac{1}{n_{\\text{Te}}}\\displaystyle\\sum_{i \\in \\text{Test}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\] - TODO: RSS vs \\(R^2\\) vs RMSE Code for Plotting from Class ## load packages library(rpart) library(FNN) # simulate data ## signal f = function(x) { x ^ 3 } ## define data generating processs get_sim_data = function(f, sample_size = 50) { x = runif(n = sample_size, min = -1, max = 1) y = rnorm(n = sample_size, mean = f(x), sd = 0.15) data.frame(x, y) } ## simualte training data set.seed(42) sim_trn_data = get_sim_data(f = f) ## simulate testing data set.seed(3) sim_tst_data = get_sim_data(f = f) ## create grid for plotting x_grid = data.frame(x = seq(-1.5, 1.5, 0.001)) # fit models ## tree models tree_fit_l = rpart(y ~ x, data = sim_trn_data, control = rpart.control(cp = 0.500, minsplit = 2)) tree_fit_m = rpart(y ~ x, data = sim_trn_data, control = rpart.control(cp = 0.015, minsplit = 2)) tree_fit_h = rpart(y ~ x, data = sim_trn_data, control = rpart.control(cp = 0.000, minsplit = 2)) ## knn models knn_fit_l = knn.reg(train = sim_trn_data[&quot;x&quot;], y = sim_trn_data$y, test = x_grid, k = 40) knn_fit_m = knn.reg(train = sim_trn_data[&quot;x&quot;], y = sim_trn_data$y, test = x_grid, k = 5) knn_fit_h = knn.reg(train = sim_trn_data[&quot;x&quot;], y = sim_trn_data$y, test = x_grid, k = 1) ## polynomial models poly_fit_l = lm(y ~ poly(x, 1), data = sim_trn_data) poly_fit_m = lm(y ~ poly(x, 3), data = sim_trn_data) poly_fit_h = lm(y ~ poly(x, 22), data = sim_trn_data) # get predictions ## tree models tree_fit_l_pred = predict(tree_fit_l, newdata = x_grid) tree_fit_m_pred = predict(tree_fit_m, newdata = x_grid) tree_fit_h_pred = predict(tree_fit_h, newdata = x_grid) ## knn models knn_fit_l_pred = knn_fit_l$pred knn_fit_m_pred = knn_fit_m$pred knn_fit_h_pred = knn_fit_h$pred ## polynomial models poly_fit_l_pred = predict(poly_fit_l, newdata = x_grid) poly_fit_m_pred = predict(poly_fit_m, newdata = x_grid) poly_fit_h_pred = predict(poly_fit_h, newdata = x_grid) # plot fitted trees par(mfrow = c(1, 3)) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Tree Model, cp = 0.5&quot;, cex = 1.5) grid() lines(x_grid$x, tree_fit_l_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Tree Model, cp = 0.015&quot;, cex = 1.5) grid() lines(x_grid$x, tree_fit_m_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Tree Model, cp = 0.0&quot;, cex = 1.5) grid() lines(x_grid$x, tree_fit_h_pred, col = &quot;darkgrey&quot;, lwd = 2) # plot fitted KNN par(mfrow = c(1, 3)) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;KNN, k = 40&quot;, cex = 1.5) grid() lines(x_grid$x, knn_fit_l_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;KNN, k = 5&quot;, cex = 1.5) grid() lines(x_grid$x, knn_fit_m_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;KNN, k = 1&quot;, cex = 1.5) grid() lines(x_grid$x, knn_fit_h_pred, col = &quot;darkgrey&quot;, lwd = 2) # plot fitted polynomials par(mfrow = c(1, 3)) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Polynomial Model, degree = 1&quot;, cex = 1.5) grid() lines(x_grid$x, poly_fit_l_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Polynomial Model, degree = 3&quot;, cex = 1.5) grid() lines(x_grid$x, poly_fit_m_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Polynomial Model, degree = 22&quot;, cex = 1.5) grid() lines(x_grid$x, poly_fit_h_pred, col = &quot;darkgrey&quot;, lwd = 2) "]
]
