[["3-linear-regression.html", "Chapter 3 Linear Regression 3.1 Example: real estate data 3.2 Notation and Basic Properties 3.3 Using the lm() Function 3.4 Model Selection Criteria and Algorithm", " Chapter 3 Linear Regression This chapter severs several purposes. First, we will review some basic knowledge of linear regression. This includes the concept of vector space, projection, which leads to estimating parameters of a linear regression. Most of these knowledge are covered in the prerequisite so you shouldnt find these concepts too difficult to understand. Secondly, we will mainly use the lm() function as an example to demonstrate some features of R. This includes extracting results, visualizations, handling categorical variables, prediction and model selection. These concepts will be useful for other models. 3.1 Example: real estate data This Real Estate data is provided on the UCI machine learning repository. The goal of this dataset is to predict the unit house price based on six different covariates: date: The transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.) age: The house age (unit: year) distance: The distance to the nearest MRT station (unit: meter) stores: The number of convenience stores in the living circle on foot (integer) latitude: Latitude (unit: degree) longitude: Longitude (unit: degree) price: House price of unit area realestate = read.csv(&quot;data/realestate.csv&quot;, row.names = 1) library(DT) datatable(realestate, filter = &quot;top&quot;, rownames = FALSE, options = list(pageLength = 8)) dim(realestate) ## [1] 414 7 3.2 Notation and Basic Properties We usually denote the observed covariates data as the design matrix \\(\\mathbf{X}\\), with dimension \\(n \\times p\\). Hence in this case, the dimension of \\(\\mathbf{X}\\) is \\(414 \\times 7\\). The \\(j\\)th variable is simply the \\(j\\)th column of this matrix, which is denoted as \\(\\mathbf{x}_j\\). The outcome \\(\\mathbf{y}\\) (price) is a vector of length \\(414\\). Please note that we usually use a bold symbol to represent a vector, while for a single element (scalar), such as the \\(j\\)th variable of subject \\(i\\), we use \\(x_{ij}\\). \\(\\bX\\) A linear regression concerns modeling the relationship (in matrix form) \\[\\by_{n \\times 1} = \\bX_{n \\times p} \\bbeta_{p \\times 1} + \\bepsilon_{n \\times 1}\\] And we know that the solution is obtained by minimizing the residual sum of squares (RSS): \\[ \\begin{align} \\widehat{\\bbeta} &amp;= \\underset{\\bbeta}{\\argmin} \\sum_{i=1}^n \\left(y_i - x_i^\\T \\bbeta \\right)^2 \\\\ &amp;= \\underset{\\bbeta}{\\argmin} \\big( \\mathbf y - \\mathbf{X} \\boldsymbol \\beta \\big)^\\T \\big( \\mathbf y - \\mathbf{X} \\boldsymbol \\beta \\big) \\end{align} \\] Classic solution can be obtained by taking the derivative of RSS w.r.t \\(\\bbeta\\) and set it to zero. This leads to the well known normal equation: \\[ \\begin{align} \\frac{\\partial \\text{RSS}}{\\partial \\bbeta} &amp;= -2 \\bX^\\T (\\by - \\bX \\bbeta) \\doteq 0 \\\\ \\Longrightarrow \\quad \\bX^\\T \\by &amp;= \\bX^\\T \\bX \\bbeta \\end{align} \\] Assuming that \\(\\bX\\) is full rank, then \\(\\bX^\\T \\bX\\) is invertible. Then, we have \\[ \\widehat{\\bbeta} = (\\bX^\\T \\bX)^{-1}\\bX^\\T \\by \\] Some additional concepts are frequently used. The fitted values \\(\\widehat \\by\\) are essentially the prediction of the original \\(n\\) training data points: \\[ \\begin{align} \\widehat{\\by} =&amp; \\bX \\bbeta\\\\ =&amp; \\underbrace{\\bX (\\bX^\\T \\bX)^{-1}\\bX^\\T}_{\\bH} \\by \\\\ \\doteq&amp; \\bH_{n \\times n} \\by \\end{align} \\] where \\(\\bH\\) is called the hat matrix. It is a projection matrix that projects any vector (\\(\\by\\) in our case) onto the column space of \\(\\bX\\). A project matrix enjoys two properties Symmetric: \\(\\bH^\\T = \\bH\\) Idempotent \\(\\bH\\bH = \\bH\\) The residuals \\(\\br\\) can also be obtained using the hat matrix: \\[ \\br = \\by - \\widehat \\by = (\\bI - \\bH) \\by\\] From the properties of a projection matrix, we also know that \\(\\br\\) should be orthogonal to any vector from the column space of \\(\\bX\\). Hence, \\[\\bX^\\T \\br = \\mathbf{0}_{p \\times 1}\\] The residuals is also used to estimate the error variance: \\[\\widehat\\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^n r_i^2 = \\frac{\\text{RSS}}{n-p}\\] When the data are indeed generated from a linear model, and with suitable conditions on the design matrix and random errors \\(\\bepsilon\\), we can conclude that \\(\\widehat \\bbeta\\) is an unbiased estimator of \\(\\bbeta\\). Its variance-covariance matrix satisfies \\[ \\begin{align} \\Var(\\widehat \\bbeta) &amp;= \\Var\\big( (\\bX^\\T \\bX)^{-1}\\bX^\\T \\by \\big) \\nonumber \\\\ &amp;= \\Var\\big( (\\bX^\\T \\bX)^{-1}\\bX^\\T (\\bX \\bbeta + \\bepsilon) \\big) \\nonumber \\\\ &amp;= \\Var\\big( (\\bX^\\T \\bX)^{-1}\\bX^\\T \\bepsilon) \\big) \\nonumber \\\\ &amp;= (\\bX^\\T \\bX)^{-1}\\bX^\\T \\bX (\\bX^\\T \\bX)^{-1} \\bI \\sigma^2 \\nonumber \\\\ &amp;= (\\bX^\\T \\bX)^{-1}\\sigma^2 \\end{align} \\] All of the above mentioned results are already implemented in R through the lm() function to fit a linear regression. 3.3 Using the lm() Function Lets consider a simple regression model that uses age and distance to explain price. We will save the fitted object as realestat.lmfit realestat.lmfit = lm(price ~ age + distance, data = realestate) This example contains three major components: data = specifies the dataset The outcome variable should be on the left hand side of ~ The covariates should be on the right hand side of ~ To look at the detailed model fitting results, use the summary() function summary(realestat.lmfit) ## ## Call: ## lm(formula = price ~ age + distance, data = realestate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.032 -4.742 -1.037 4.533 71.930 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.8855858 0.9677644 51.547 &lt; 2e-16 *** ## age -0.2310266 0.0420383 -5.496 6.84e-08 *** ## distance -0.0072086 0.0003795 -18.997 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.73 on 411 degrees of freedom ## Multiple R-squared: 0.4911, Adjusted R-squared: 0.4887 ## F-statistic: 198.3 on 2 and 411 DF, p-value: &lt; 2.2e-16 This can be viewed as either a convex optimization problem or projections on the \\(n\\) dimensional vector space. 3.3.1 Linear regression as an optimization # generate data for a simple linear regression set.seed(20) n = 100 x &lt;- cbind(1, rnorm(n)) y &lt;- x %*% c(1, 0.5) + rnorm(n) # calculate the residual sum of squares for a grid of beta values rss &lt;- function(b, x, y) sum((y - x %*% b)^2) b1 &lt;- b2 &lt;- seq(0, 2, length= 20) z = matrix(apply(expand.grid(b1, b2), 1, rss, x, y), 20, 20) # 3d plot for RSS par(mar = c(1,1,3,1)) persp(b1, b2, z, xlab = &quot;beta 1&quot;, ylab = &quot;beta 2&quot;, zlab = &quot;RSS&quot;, main=&quot;Residual Sum of Squares&quot;, col = &quot;springgreen&quot;, shade = 0.6, theta = 30, phi = 5) # The solution can be solved by any optimization algorithm optim(c(0, 0), rss, x = x, y = y)$par ## [1] 1.088813 0.679870 3.3.2 Linear regression as projections Another view is through projections in vector space. Consider each column of \\(\\mathbf{X}\\) as a vector, and project \\(\\mathbf{y}\\) onto the column space of \\(\\mathbf{X}\\). The project is \\[ \\widehat{\\mathbf{y}} = \\mathbf{X} (\\mathbf{X}^\\text{T} \\mathbf{X})^{-1}\\mathbf{X}^\\text{T} \\mathbf{y} \\doteq {\\mathbf{H}} \\mathbf{y}, \\] where \\(\\mathbf{H}\\) is a projection matrix. And the residuals are simply \\[ \\widehat{\\mathbf{e}} = \\mathbf{y} - \\widehat{\\mathbf{y}} = (\\mathbf{I} - \\mathbf{H}) \\mathbf{y} \\] When the number of variables is large, inverting \\(\\mathbf{X}^\\text{T} \\mathbf{X}\\) is expansive. The R function lm() does not calculate the inverse directly. Instead, QR decomposition can be used. You can try a larger \\(n\\) and \\(p\\) to see a significant difference. This is only for demonstration. They are not required for our course. # generate 100 observations with 3 variables set.seed(1) n = 1000 p = 500 x = matrix(rnorm(n*p), n, p) X = cbind(1, x) # the design matrix, including 1 as the first column # define the true beta, the first entry is the intercept b = as.matrix(c(1, 1, 0.5, rep(0, p-2))) # generate training y with Gaussian errors y = X %*% b + rnorm(n) # fit a linear regression model lm.fit = lm(y ~ x) # look at the coefficients beta hat head(lm.fit$coef) ## (Intercept) x1 x2 x3 x4 x5 ## 1.016479750 1.026143517 0.496792668 -0.017272409 0.005193304 0.034639107 # using normal equations by inverting the X&#39;X matrix: b = (X&#39;X)^-1 X&#39;y # however, this is very slow # check ?solve system.time({beta_hat = solve(t(X) %*% X) %*% t(X) %*% y}) ## user system elapsed ## 0.31 0.00 0.31 head(beta_hat) ## [,1] ## [1,] 1.016479750 ## [2,] 1.026143517 ## [3,] 0.496792668 ## [4,] -0.017272409 ## [5,] 0.005193304 ## [6,] 0.034639107 # you can avoid the inversion by specifying the linear equation system X&#39;X b = X&#39;y system.time({beta_hat = solve(t(X) %*% X, t(X) %*% y)}) ## user system elapsed ## 0.14 0.00 0.15 # A better approach is to use QR decomposition or the Cholesky decomposition # The following codes are not necessarily efficient, they are only for demonstration purpose # QR decomposition # direct calling the qr.coef function system.time({beta_hat = qr.coef(qr(X), y)}) ## user system elapsed ## 0.13 0.00 0.12 # or system.time({beta_hat = qr.solve(t(X) %*% X, t(X) %*% y)}) ## user system elapsed ## 0.17 0.00 0.17 # if you want to see what Q and R are QR = qr(X) Q = qr.Q(QR) R = qr.R(QR) # get inverse of R, you can check R %*% R_inv yourself # the backsolve/forwardsolve functions can be used to solve AX = b for upper/lower triangular matrix A # ?backsolve R_inv = backsolve(R, diag(p+1), upper.tri = TRUE, transpose = FALSE) beta_hat = R_inv %*% t(Q) %*% y # Cholesky Decomposition # the chol function gives upper triangular matrix # crossprod(X) = X&#39;X system.time({ R = chol(crossprod(X)) w = backsolve(R, t(X) %*% y, upper.tri = TRUE, transpose = TRUE) beta_hat = backsolve(R, w, upper.tri = TRUE, transpose = FALSE) }) ## user system elapsed ## 0.13 0.00 0.12 # or equivalently R = t(chol(crossprod(X))) w = forwardsolve(R, t(X) %*% y, upper.tri = FALSE, transpose = FALSE) beta_hat = forwardsolve(R, w, upper.tri = FALSE, transpose = TRUE) # the transpose = TRUE means that we are solving for R&#39;b = w instead of Rb = w 3.4 Model Selection Criteria and Algorithm 3.4.1 Example: diabetes dataset We use the diabetes dataset from the lars package as a demonstration of model selection. library(lars) ## Loaded lars 1.2 data(diabetes) diab = data.frame(cbind(diabetes$x, &quot;Y&quot; = diabetes$y)) # A Brief Description of the Diabetes Data (Efron et al, 2004): # Ten baseline variables: age, sex, body mass index, average blood pressure, and six blood serum # measurements were obtained for each of n = 442 diabetes patients, as well as # the response of interest, a quantitative measure of disease progression one year after baseline lmfit=lm(Y~., data=diab) # When we use normal distribution likelihood for the errors, there are 12 parameters # The function AIC() directly calculates the AIC score from a lm() fitted model n = nrow(diab) p = 11 # ?AIC AIC(lmfit) # a build-in function for calculating AIC using -2log likelihood ## [1] 4795.985 n*log(sum(residuals(lmfit)^2/n)) + n + n*log(2*pi) + 2 + 2*p ## [1] 4795.985 # In many standard R packages, the AIC is calculated by removing some constants from the likelihood # We will use this value as the default # ?extractAIC extractAIC(lmfit) # AIC for the full model ## [1] 11.000 3539.643 RSS = sum(residuals(lmfit)^2) n*log(RSS/n) + 2*p ## [1] 3539.643 # so the BIC for the full model is extractAIC(lmfit, k = log(n)) ## [1] 11.000 3584.648 n*log(RSS/n) + log(n)*p ## [1] 3584.648 # if we want to calculate Cp, use the formula RSS + 2*p*summary(lmfit)$sigma^2 ## [1] 1328502 # however, the scale of this is usually very large, we may consider the following version RSS/summary(lmfit)$sigma^2 + 2*p - n ## [1] 11 The step() function can be used to select the best model based on specified model selection criteria. # Model selection: stepwise algorithm # ?step # this function shows every step during the model selection step(lmfit, direction=&quot;both&quot;, k = 2) # k = 2 (AIC) is default; ## Start: AIC=3539.64 ## Y ~ age + sex + bmi + map + tc + ldl + hdl + tch + ltg + glu ## ## Df Sum of Sq RSS AIC ## - age 1 82 1264066 3537.7 ## - hdl 1 663 1264646 3537.9 ## - glu 1 3080 1267064 3538.7 ## - tch 1 3526 1267509 3538.9 ## &lt;none&gt; 1263983 3539.6 ## - ldl 1 5799 1269782 3539.7 ## - tc 1 10600 1274583 3541.3 ## - sex 1 45000 1308983 3553.1 ## - ltg 1 56015 1319998 3556.8 ## - map 1 72103 1336086 3562.2 ## - bmi 1 179028 1443011 3596.2 ## ## Step: AIC=3537.67 ## Y ~ sex + bmi + map + tc + ldl + hdl + tch + ltg + glu ## ## Df Sum of Sq RSS AIC ## - hdl 1 646 1264712 3535.9 ## - glu 1 3001 1267067 3536.7 ## - tch 1 3543 1267608 3536.9 ## &lt;none&gt; 1264066 3537.7 ## - ldl 1 5751 1269817 3537.7 ## - tc 1 10569 1274635 3539.4 ## + age 1 82 1263983 3539.6 ## - sex 1 45831 1309896 3551.4 ## - ltg 1 55963 1320029 3554.8 ## - map 1 73850 1337915 3560.8 ## - bmi 1 179079 1443144 3594.2 ## ## Step: AIC=3535.9 ## Y ~ sex + bmi + map + tc + ldl + tch + ltg + glu ## ## Df Sum of Sq RSS AIC ## - glu 1 3093 1267805 3535.0 ## - tch 1 3247 1267959 3535.0 ## &lt;none&gt; 1264712 3535.9 ## - ldl 1 7505 1272217 3536.5 ## + hdl 1 646 1264066 3537.7 ## + age 1 66 1264646 3537.9 ## - tc 1 26840 1291552 3543.2 ## - sex 1 46382 1311094 3549.8 ## - map 1 73536 1338248 3558.9 ## - ltg 1 97509 1362221 3566.7 ## - bmi 1 178537 1443249 3592.3 ## ## Step: AIC=3534.98 ## Y ~ sex + bmi + map + tc + ldl + tch + ltg ## ## Df Sum of Sq RSS AIC ## - tch 1 3686 1271491 3534.3 ## &lt;none&gt; 1267805 3535.0 ## - ldl 1 7472 1275277 3535.6 ## + glu 1 3093 1264712 3535.9 ## + hdl 1 738 1267067 3536.7 ## + age 1 0 1267805 3537.0 ## - tc 1 26378 1294183 3542.1 ## - sex 1 44686 1312491 3548.3 ## - map 1 82154 1349959 3560.7 ## - ltg 1 102520 1370325 3567.3 ## - bmi 1 189970 1457775 3594.7 ## ## Step: AIC=3534.26 ## Y ~ sex + bmi + map + tc + ldl + ltg ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1271491 3534.3 ## + tch 1 3686 1267805 3535.0 ## + glu 1 3532 1267959 3535.0 ## + hdl 1 395 1271097 3536.1 ## + age 1 11 1271480 3536.3 ## - ldl 1 39378 1310869 3545.7 ## - sex 1 41858 1313349 3546.6 ## - tc 1 65237 1336728 3554.4 ## - map 1 79627 1351119 3559.1 ## - bmi 1 190586 1462077 3594.0 ## - ltg 1 294094 1565585 3624.2 ## ## Call: ## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab) ## ## Coefficients: ## (Intercept) sex bmi map tc ldl ## 152.1 -226.5 529.9 327.2 -757.9 538.6 ## ltg ## 804.2 step(lmfit, direction=&quot;backward&quot;, trace=0) # trace=0 will not print intermediate results ## ## Call: ## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab) ## ## Coefficients: ## (Intercept) sex bmi map tc ldl ## 152.1 -226.5 529.9 327.2 -757.9 538.6 ## ltg ## 804.2 step(lm(Y~1, data=diab), scope=list(upper=lmfit, lower=~1), direction=&quot;forward&quot;, trace=0) ## ## Call: ## lm(formula = Y ~ bmi + ltg + map + tc + sex + ldl, data = diab) ## ## Coefficients: ## (Intercept) bmi ltg map tc sex ## 152.1 529.9 804.2 327.2 -757.9 -226.5 ## ldl ## 538.6 step(lmfit, direction=&quot;both&quot;, k=log(n), trace=0) # BIC (the default value for k=2, which corresponds to AIC) ## ## Call: ## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab) ## ## Coefficients: ## (Intercept) sex bmi map tc ldl ## 152.1 -226.5 529.9 327.2 -757.9 538.6 ## ltg ## 804.2 The leaps package will calculate the best model of each model size. Then we can add the penalties to the model fitting result and conclude the best model. ########################################################################## # Best subset model selection (Cp, AIC, and BIC): leaps ########################################################################## library(leaps) # performs an exhaustive search over models, and gives back the best model # (with low RSS) of each size. # the default maximum model size is nvmax=8 RSSleaps=regsubsets(as.matrix(diab[,-11]),diab[,11]) summary(RSSleaps, matrix=T) ## Subset selection object ## 10 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## sex FALSE FALSE ## bmi FALSE FALSE ## map FALSE FALSE ## tc FALSE FALSE ## ldl FALSE FALSE ## hdl FALSE FALSE ## tch FALSE FALSE ## ltg FALSE FALSE ## glu FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: exhaustive ## age sex bmi map tc ldl hdl tch ltg glu ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; RSSleaps=regsubsets(as.matrix(diab[,-11]),diab[,11], nvmax=10) summary(RSSleaps,matrix=T) ## Subset selection object ## 10 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## sex FALSE FALSE ## bmi FALSE FALSE ## map FALSE FALSE ## tc FALSE FALSE ## ldl FALSE FALSE ## hdl FALSE FALSE ## tch FALSE FALSE ## ltg FALSE FALSE ## glu FALSE FALSE ## 1 subsets of each size up to 10 ## Selection Algorithm: exhaustive ## age sex bmi map tc ldl hdl tch ltg glu ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 9 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; sumleaps=summary(RSSleaps,matrix=T) names(sumleaps) # components returned by summary(RSSleaps) ## [1] &quot;which&quot; &quot;rsq&quot; &quot;rss&quot; &quot;adjr2&quot; &quot;cp&quot; &quot;bic&quot; &quot;outmat&quot; &quot;obj&quot; sumleaps$which ## (Intercept) age sex bmi map tc ldl hdl tch ltg glu ## 1 TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## 2 TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## 3 TRUE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE ## 4 TRUE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE TRUE FALSE ## 5 TRUE FALSE TRUE TRUE TRUE FALSE FALSE TRUE FALSE TRUE FALSE ## 6 TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE TRUE FALSE ## 7 TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE FALSE ## 8 TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE ## 9 TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## 10 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE msize=apply(sumleaps$which,1,sum) n=dim(diab)[1] p=dim(diab)[2] Cp = sumleaps$rss/(summary(lmfit)$sigma^2) + 2*msize - n; AIC = n*log(sumleaps$rss/n) + 2*msize; BIC = n*log(sumleaps$rss/n) + msize*log(n); cbind(Cp, sumleaps$cp) ## Cp ## 1 148.352561 148.352561 ## 2 47.072229 47.072229 ## 3 30.663634 30.663634 ## 4 21.998461 21.998461 ## 5 9.148045 9.148045 ## 6 5.560162 5.560162 ## 7 6.303221 6.303221 ## 8 7.248522 7.248522 ## 9 9.028080 9.028080 ## 10 11.000000 11.000000 cbind(BIC, sumleaps$bic) # It seems regsubsets uses a formula for BIC different from the one we used. ## BIC ## 1 3665.879 -174.1108 ## 2 3586.331 -253.6592 ## 3 3575.249 -264.7407 ## 4 3571.077 -268.9126 ## 5 3562.469 -277.5210 ## 6 3562.900 -277.0899 ## 7 3567.708 -272.2819 ## 8 3572.720 -267.2702 ## 9 3578.585 -261.4049 ## 10 3584.648 -255.3424 BIC-sumleaps$bic # But the two just differ by a constant, so won&#39;t affect the model selection result. ## 1 2 3 4 5 6 7 8 9 10 ## 3839.99 3839.99 3839.99 3839.99 3839.99 3839.99 3839.99 3839.99 3839.99 3839.99 n*log(sum((diab[,11] - mean(diab[,11]))^2/n)) # the difference is the score of an intercept model ## [1] 3839.99 # Rescale Cp, AIC, BIC to (0,1). inrange &lt;- function(x) { (x - min(x)) / (max(x) - min(x)) } Cp = sumleaps$cp; Cp = inrange(Cp); BIC = sumleaps$bic; BIC = inrange(BIC); AIC = n*log(sumleaps$rss/n) + 2*msize; AIC = inrange(AIC); plot(range(msize), c(0, 1.1), type=&quot;n&quot;, xlab=&quot;Model Size (with Intercept)&quot;, ylab=&quot;Model Selection Criteria&quot;) points(msize, Cp, col=&quot;red&quot;, type=&quot;b&quot;) points(msize, AIC, col=&quot;blue&quot;, type=&quot;b&quot;) points(msize, BIC, col=&quot;black&quot;, type=&quot;b&quot;) legend(&quot;topright&quot;, lty=rep(1,3), col=c(&quot;red&quot;, &quot;blue&quot;, &quot;black&quot;), legend=c(&quot;Cp&quot;, &quot;AIC&quot;, &quot;BIC&quot;)) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
