[
["index.html", "Statistical Learning with R Preface Target Audience What’s Covered? Acknowledgements License", " Statistical Learning with R Ruoqing Zhu 2019-09-09 Preface Welcome to Statistical Learning with R! I started this project during the summer of 2018 when I was preparing for the Stat 432 course. This course was taught by our former faculty member Dr. David Dalpiaz, who is currently at The Ohio State University. David introduced to me this awesome way of publishing website on GitHub, which is a very efficient approach for developing courses. Since I was also teaching Stat 542 (Statistical Learning) during the previous two years, I figured it could be beneficial to integrate what I have to this existing book by David and use it as the R material for both courses. As you can tell, I am not being very creative on the name, so `SLWR’ it is. You can find the source file of this book on my GitHub. Target Audience This book is targeted at advanced undergraduate to MS students in Statistics who have some or no prior statistical learning experience. Previous experience with both basic mathematics (mainly linear algebra), statistical modeling (such as linear regressions) and R are assumed. What’s Covered? I currently plan to include the following topics: Basics Knowledge Linear and Penalized Linear Regressions Unsupervised Learning Classification Non-parametric Statistical Models Machine Learning Models Appendix The goal of this book is to introduce not only how to run some of the popular statistical learning models in R, but also touches some basic algorithms and programming techniques for solving some of these models. For each section, the difficulty may gradually increase from an undergraduate level to a graduate level. It will be served as a supplement to An Introduction to Statistical Learning (James et al. 2013) for STAT 432 - Basics of Statistical Learning and The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Hastie, Tibshirani, and Friedman 2001) for STAT 542 - Statistical Learning at the University of Illinois at Urbana-Champaign. This book is under active development as I am teaching STAT 432 during Fall 2018. Hence, you may encounter errors ranging from typos to broken code, to poorly explained topics. If you do, please let me know! Simply send an email and I will make the changes as soon as possible (rqzhu AT illinois DOT edu). Or, if you know R Markdown and are familiar with GitHub, make a pull request and fix an issue yourself!. These contributions will be acknowledged. Acknowledgements The initial contents are derived from Dr. David Dalpiaz’s book. My STAT 542 course materials are also inspired by Dr. Feng Liang and Dr. John Marden who developed earlier versions of this course. And I also incorporated many online resources, such as bookdown: Authoring Books and Technical Documents with R Markdown R Programming for Data Science and others through Google search. License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. References "],
["r-and-rstudio.html", "Chapter 1 R and RStudio 1.1 Resources and Guides 1.2 Basic Mathematical Operations 1.3 Read-in Data from Other Sources 1.4 Using Packages 1.5 Explore Yourself", " Chapter 1 R and RStudio R is a free-to-use software that is very popular in statistical computing. You can download R from its official website. Another software that makes using R easier is RStudio, which is available at here. You can find many online guides that help you to set-up these two software, for example, this YouTube video. R Markdown is a built-in feature of RStudio. It works like an integration of LaTex and programming playground that compiles source code into nice-looking PDF, HTML, or MS Word files. This book is created using an extension of R Markdown, developed by Yihui Xie. 1.1 Resources and Guides There are many online resources for how to use R, RStudio, and R Markdown. For example, David Dalpiaz’s other online book Applied Statistics with R contains an introduction to using them. There are also other online documentation such as Install R and RStudio R tutorial Data in R Play-list (video) R and RStudio Play-list (video) R Markdown Cheat Sheet R Markdown Play-list (video) It is worth to mention that once you become a developer of R packages using C/C++ (add-on of R for performing specific tasks), and you also happen to use Windows like I do, you have to install this Rtools that contains compilers. This is also needed if you want to manually install any R package using a “source” (.tar.gz files) instead of using the so-called “binaries” (.zip files). 1.2 Basic Mathematical Operations We will briefly cover some basic R calculations and operations. If you want to see more information about a particular function or operator in R, the easiest way is to get the reference document. Put a question mark in front of a function name: # In a default R console window, this will open up a web browser. # In RStudio, this will be displayed at the ‘Help’ window at the bottom-right penal. ?log2 ?matrix Try type-in the following commands into your R console and start to explore yourself. Most of them are self-explanatory. Lines with a # in the front are comments, which will not be executed. Lines with ## in the front are outputs. # Basic mathematical operations 1 + 3 ## [1] 4 3*5 ## [1] 15 3^5 ## [1] 243 exp(2) ## [1] 7.389056 log(3) ## [1] 1.098612 log2(3) ## [1] 1.584963 factorial(5) ## [1] 120 1.2.1 Data Objects Data objects can be a complicated topic for people who never used R before. Most common data objects are vector, matrix, list, and data.frame. Operations on vectors are matrices are fairly intuitive. # creating a vector c(1,2,3,4) ## [1] 1 2 3 4 c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; # creating matrix from a vector matrix(c(1,2,3,4), 2, 2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 x = c(1,1,1,0,0,0); y = c(1,0,1,0,1,0) cbind(x,y) ## x y ## [1,] 1 1 ## [2,] 1 0 ## [3,] 1 1 ## [4,] 0 0 ## [5,] 0 1 ## [6,] 0 0 # matrix multiplication using &#39;%*%&#39; matrix(c(1,2,3,4), 2, 2) %*% t(cbind(x,y)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 4 1 4 0 3 0 ## [2,] 6 2 6 0 4 0 Simple mathematical operations on vectors and matrices are usually element-wise. You can easily extract certain elements of them by using the [] operator, like a C programming reference style. # some simple operations x[3] ## [1] 1 x[2:5] ## [1] 1 1 0 0 cbind(x,y)[1:2, ] ## x y ## [1,] 1 1 ## [2,] 1 0 (x + y)^2 ## [1] 4 1 4 0 1 0 length(x) ## [1] 6 dim(cbind(x,y)) ## [1] 6 2 # A warning will be issued when R detects something wrong. Results may still be produced. x + c(1,2,3,4) ## Warning in x + c(1, 2, 3, 4): longer object length is not a multiple of shorter object length ## [1] 2 3 4 4 1 2 list() simply creates a list of objects (of any type). However, some operators cannot be directly applied to a list in a similar way as to vectors or matrices. Model fitting results in R are usually stored as a list (for example, the lm() function used in Section ??. # creating a list x = list(c(1,2), &quot;hello&quot;, matrix(c(1,2,3,4), 2, 2)) x[[1]] ## [1] 1 2 data.frame() creates a list of vectors of equal length, and display them as a matrix-like object where each vector is a column of the matrix. It is mainly used for storing data. For example the famous iris data. The first four columns are numerical variables, while the last column is a categorical variable with three levels: setosa, versicolor, and virginica. # the head function peeks the first several rows of the dataset head(iris, n = 3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa # data frame can be called by each individual column iris$Species[c(1, 51, 101)] ## [1] setosa versicolor virginica ## Levels: setosa versicolor virginica # the summary function can be used to view all variables summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 1.3 Read-in Data from Other Sources Data can be imported from a variety of sources. More commonly, a dataset can be stored in .txt, .csv or other file formats. # read-in data birthrate = read.csv(&quot;data/birthrate.csv&quot;) head(birthrate) ## Year Birthrate ## 1 1917 183.1 ## 2 1918 183.9 ## 3 1919 163.1 ## 4 1920 179.5 ## 5 1921 181.4 ## 6 1922 173.4 # to see how many observations (rows) and variables (columns) in a dataset dim(birthrate) ## [1] 87 2 R data can also be saved into other formats. The more efficient way, assuming that you are going to load these file back to R in the future, is to save them as .RData file. Usually, for a large dataset, this reduces the time spend on reading in the data. # saving a object to .RData file save(birthrate, file = &quot;mydata.RData&quot;) # you can specify multiple objects too save(birthrate, iris, file = &quot;mydata.RData&quot;) # load the data again load(&quot;mydata.RData&quot;) # save data to .csv file write.csv(birthrate, file = &quot;mydata.csv&quot;) 1.4 Using Packages Packages are written and contributed to R by individuals. They provide additional features (functions or data) that serve particular needs. The package ElemStatLearn is created for the textbook (Hastie, Tibshirani, and Friedman 2001) and contains may popular datasets. A package needs to be installed and loaded to your local computer. # install packages install.packages(&quot;ElemStatLearn&quot;) # load package library(ElemStatLearn) # load a dataset from the package data(SAheart) head(SAheart) ## sbp tobacco ldl adiposity famhist typea obesity alcohol age chd ## 1 160 12.00 5.73 23.11 Present 49 25.30 97.20 52 1 ## 2 144 0.01 4.41 28.61 Absent 55 28.87 2.06 63 1 ## 3 118 0.08 3.48 32.28 Present 52 29.14 3.81 46 0 ## 4 170 7.50 6.41 38.03 Present 51 31.99 24.26 58 1 ## 5 134 13.60 3.50 27.78 Present 60 25.99 57.34 49 1 ## 6 132 6.20 6.47 36.21 Present 62 30.77 14.14 45 0 1.5 Explore Yourself There is no guide that can exhaust all information. I found the best learning approach is to perform a specific task and google your way to the success. Oftentimes, Stack Overflow is my best friend, especially when I am developing new packages. Also, read the reference manual carefully if you use a particular package or function. A reference manual (for packages on CRAN) can always be found at https://cran.r-project.org/web/packages/package_name. References "],
["rmarkdown-basics.html", "Chapter 2 RMarkdown Basics 2.1 Acknowledgement 2.2 Getting Started 2.3 Adding R 2.4 Importing Data 2.5 Working Directory 2.6 Packages 2.7 Plotting 2.8 Chunk Options 2.9 Adding Math with LaTeX 2.10 Output Options 2.11 Try It!", " Chapter 2 RMarkdown Basics 2.1 Acknowledgement This section was originally created by David Dalpiaz. The original file can be downloaded from his R4SL online text book. I made some slight modifications. This is by no means a comprehensive guide, and there are many other resources available online. 2.2 Getting Started RMarkdown at its core is a combination of R and Markdown used to generate reproducible reports for data analyses. Markdown and R are mixed together in a .Rmd file, which can then be rendered into a number of formats including .html, .pdf, and .docx. There will be a strong preference for .html in this course. Have a look at (this source file) to see how this document was generated! It should be read alongside the rendered .html to best understand how everything works. Alternatively, you could render the .Rmd inside RStudio, and you’ll automatically have both side-by-side. You can also modify the .Rmd along the way, and see what effects your modifications have. Formatting text is easy. Bold can be done using ** or __ before and after the text. Italics can be done using * or _ before and after the text. For example, This is bold. This is italics. and this is bold italics. This text appears as monospaced. Unordered list element 1. Unordered list element 2. Unordered list element 3. Ordered list element 1. Ordered list element 2. Ordered list element 3. We could mix lists and links. Note that a link can be constructed in the format [display text](http link). If colors are desired, we can customize it using, for example, [\\textcolor{blue}{display text}](http link). A default link: RMarkdown Documentation colored link 1: colored link 2: Tables are sometimes tricky using Markdown. See the above link for a helpful Markdown table generator. A B C 1 2 3 Do Re Mi 2.3 Adding R So far we have only used Markdown to create html. This is useful by itself, but the real power of RMarkdown comes when we add R. There are two ways we can do this. We can use R code chunks, or run R inline. 2.3.1 R Chunks The following is an example of an R code chunk # define function get_sd = function(x, biased = FALSE) { n = length(x) - 1 * !biased sqrt((1 / n) * sum((x - mean(x)) ^ 2)) } # generate random sample data set.seed(42) (test_sample = rnorm(n = 10, mean = 2, sd = 5)) ## [1] 8.8547922 -0.8234909 3.8156421 5.1643130 4.0213416 1.4693774 ## [7] 9.5576100 1.5267048 12.0921186 1.6864295 # run function on generated data get_sd(test_sample) ## [1] 4.177244 There is a lot going on here. In the .Rmd file, notice the syntax that creates and ends the chunk. Also note that example_chunk is the chunk name. Everything between the start and end syntax must be valid R code. Chunk names are not necessary, but can become useful as your documents grow in size. In this example, we define a function, generate some random data in a reproducible manner, displayed the data, then ran our function. 2.3.2 Inline R R can also be run in the middle of the exposition. For example, the mean of the data we generated is 4.7364838. 2.4 Importing Data When using RMarkdown, any time you knit your document to its final form, say .html, a number of programs run in the background. Your current R environment seen in RStudio will be reset. Any objects you created while working interactively inside RStudio will be ignored. Essentially a new R session will be spawned in the background and the code in your document is run there from start to finish. For this reason, things such as importing data must be explicitly coded into your document. library(readr) example_data = read_table(&quot;data/skincancer.txt&quot;) The above loads the online file. In many cases, you will load a file that is locally stored in your own computer. In that case, you can either specify the full file path, or simply use, for example read_csv(&quot;filename.csv&quot;) if that file is stored at your working directory. The working directory will usually be the directory that contains your .Rmd file. You are recommended to reference data in this manner. Note that we use the newer read_csv() from the readr package instead of the default read.csv(). 2.5 Working Directory Whenever R code is run, there is always a current working directory. This allows for relative references to external files, in addition to absolute references. Since the working directory when knitting a file is always the directory that contains the .Rmd file, it can be helpful to set the working directory inside RStudio to match while working interactively. To do so, select Session &gt; Set Working Directory &gt; To Source File Location while editing a .Rmd file. This will set the working directory to the path that contains the .Rmd. You can also use getwd() and setwd() to manipulate your working directory programmatically. These should only be used interactively. Using them inside an RMarkdown document would likely result in lessened reproducibility. As of recent RStudio updates, this practice is not always necessary when working interactively. If lines of code are being “Output Inline,” then the working directory is automatically the directory which contains the .Rmd file. 2.6 Packages Packages are key to using R. The community generated packages are a large part of R’s success, and it is extremely rare to perform an analysis without using at least some packages. Once installed, packages must be loaded before they are used, so again, since your environment is initialized with nothing during knitting, these must be included in your RMarkdown file. #install.packages(&quot;ggplot2&quot;) library(ggplot2) Here we load the ggplot2 package, which should be installed interactively before knitting the file. The install command is included for reference, but commented out. It could be left uncommented, but then the package would re-install every time you knit your document. #install.packages(&quot;rmarkdown&quot;) Note that rmarkdown is actually a package in R! If R never prompts you to install rmarkdown and its associated packages when first creating an RMarkdown document, use the above command to install them manually. 2.7 Plotting The following generates a boring plot, which displays the skin cancer mortality plot(Mort ~ Lat, data = example_data) This next plot, uses data from the package ggplot2 to create a more interesting plot. Notice it is huge in the resulting document, since we have modified some chunk options in the RMarkdown file to manipulate its size. plot(Mort ~ Lat, data = example_data, xlab = &quot;Latitude&quot;, ylab = &quot;Skin Cancer Mortality Rate&quot;, main = &quot;Skin Cancer Mortality vs. State Latitude&quot;, pch = 19, cex = 1.5, col = &quot;deepskyblue&quot;) But you can also notice that the labels and the plots becomes disporportional when the figure size is set too small. This can be resolved using a scalling option such as out.width = '40%, but enlarge the original figure size: 2.8 Chunk Options We have already seen chunk options fig.height, fig.width, and out.width which modified the size of plots from a particular chunk. There are many chunk options, but we will discuss some others which are frequently used including; eval, echo, message, and warning. If you noticed, the plot above was displayed without showing the code. install.packages(&quot;rmarkdown&quot;) ?log View(mpg) Using eval = FALSE the above chunk displays the code, but it is not run. We’ve already discussed not wanting install code to run. The ? code pulls up documentation of a function. This will spawn a browser window when knitting, or potentially crash during knitting. Similarly, using View() is an issue with RMarkdown. Inside RStudio, this would pull up a window which displays the data. However, when knitting, R runs in the background and RStudio is not modifying the View() function. This, on OSX especially, usually causes knitting to fail. ## [1] &quot;Hello World!&quot; Above, we see output, but no code! This is done using echo = FALSE, which is often useful. x = 1:10 y = 1:10 summary(lm(y ~ x)) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.661e-16 -1.157e-16 4.273e-17 2.153e-16 4.167e-16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.123e-15 2.458e-16 4.571e+00 0.00182 ** ## x 1.000e+00 3.961e-17 2.525e+16 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.598e-16 on 8 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 6.374e+32 on 1 and 8 DF, p-value: &lt; 2.2e-16 The above code produces a warning, for reasons we will discuss later. Sometimes, in final reports, it is nice to hide these, which we have done here. message = FALSE and warning = FALSE can be used to do so. Messages are often created when loading packages to give the user information about the effects of loading the package. These should be suppressed in final reports. Be careful about suppressing these messages and warnings too early in an analysis as you could potentially miss important information! 2.9 Adding Math with LaTeX Another benefit of RMarkdown is the ability to add Latex for mathematics typesetting. Like R code, there are two ways we can include Latex; displaystyle and inline. Note that use of LaTeX is somewhat dependent on the resulting file format. For example, it cannot be used at all with .docx. To use it with .pdf you must have LaTeX installed on your machine. With .html the LaTeX is not actually rendered during knitting, but actually rendered in your browser using MathJax. 2.9.1 Displaystyle LaTeX Displaystyle is used for larger equations which appear centered on their own line. This is done by putting $$ before and after the mathematical equation. \\[ \\widehat \\sigma = \\sqrt{\\frac{1}{n - 1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\] 2.9.2 Inline LaTex We could mix LaTeX commands in the middle of exposition, for example: \\(t = 2\\). We could actually mix R with Latex as well! For example: \\(\\bar{x} = 4.7364838\\). 2.10 Output Options At the beginning of the document, there is a code which describes some metadata and settings of the document. For this file, that code is: title: &quot;RMarkdown Template&quot; author: &quot;Your Name&quot; date: &quot;Aug 26, 2018&quot; output: html_document: toc: yes This describes the output format as html, defines the theme, and toc tells R to automatically create a Table of Contents based on the headers and sub-headers you have defined using #. You can remove this line if that’s not what you needed. You can edit this yourself, or click the settings button at the top of the document and select Output Options.... Here you can explore other themes and syntax highlighting options, as well as many additional options. Using this method will automatically modify this information in the document. 2.11 Try It! Be sure to play with this document! Change it. Break it. Fix it. The best way to learn RMarkdown (or really almost anything) is to try, fail, then find out what you did wrong. RStudio has provided a number of beginner tutorials which have been greatly improved recently and detail many of the specifics potentially not covered in this document. RMarkdown is continually improving, and this document covers only the very basics. "],
["basics-of-probability-and-statistics.html", "Chapter 3 Basics of Probability and Statistics 3.1 Random Number Generation 3.2 Summary Statistics and Data Visualization", " Chapter 3 Basics of Probability and Statistics 3.1 Random Number Generation Random number generation is important for statistical simulation. R provides random number generators for many commonly used distributions, such as binomial (rbinom), normal (rnorm), t (rt) etc. The syntax is usually a letter “r” followed by the name of the distribution. # generate 10 independent Bernoulli random variables as a vector rbinom(n=10, size = 1, prob = 0.5) ## [1] 0 0 1 1 0 1 0 1 0 1 # 4 independent random standard normal variables rnorm(n=4) ## [1] -1.84093366 0.84755588 0.09527354 0.92080407 Setting the seed before generating random numbers will allow us to replicate the results when necessary. # after setting the seed, the two runs will generate exactly the same &quot;random&quot; numbers set.seed(1) rnorm(n=4, mean = 1, sd = 2) ## [1] -0.2529076 1.3672866 -0.6712572 4.1905616 set.seed(1) rnorm(n=4, mean = 1, sd = 2) ## [1] -0.2529076 1.3672866 -0.6712572 4.1905616 Some more complicated distributions require additional packages. For example, the MASS package can be used to generate the multivariate normal distribution. One needs to specify a vector of means and an invertable covariance matrix. library(MASS) P = 4 V &lt;- 0.5^abs(outer(1:P, 1:P, &quot;-&quot;)) mvrnorm(3, mu=1:P, Sigma=V) ## [,1] [,2] [,3] [,4] ## [1,] 2.3075926 0.6273378 2.875879 3.515928 ## [2,] 1.8215099 3.1535552 2.918031 4.528882 ## [3,] 0.2549359 1.6922485 2.920198 3.623822 3.2 Summary Statistics and Data Visualization Statistical functions that provide a summary of the data. x = rnorm(n=100, mean = 1, sd = 2) y = rnorm(n=100, mean = 2, sd = 1) sum(x) ## [1] 123.9597 mean(x) ## [1] 1.239597 var(x) ## [1] 3.12596 median(x) ## [1] 1.116457 quantile(x, c(0.25, 0.5, 0.75)) ## 25% 50% 75% ## 0.0115149 1.1164572 2.3955519 cor(x, y) ## [1] 0.05988467 For discrete data, we can use the table function. library(ElemStatLearn) data(SAheart) table(SAheart$famhist) ## ## Absent Present ## 270 192 table(SAheart[, c(&quot;famhist&quot;,&quot;chd&quot;)]) ## chd ## famhist 0 1 ## Absent 206 64 ## Present 96 96 Fisher’s exact test and the Chi-square test are tests of independence between two nominal variables. # We can test the association between family history (famhist) and # the indicator of coronary heart disease (chd) # using Fisher&#39;s Exact fisher.test(table(SAheart[, c(&quot;famhist&quot;,&quot;chd&quot;)])) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(SAheart[, c(&quot;famhist&quot;, &quot;chd&quot;)]) ## p-value = 6.658e-09 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.119573 4.891341 ## sample estimates: ## odds ratio ## 3.209996 # or the Chi-square test chisq.test(table(SAheart[, c(&quot;famhist&quot;,&quot;chd&quot;)])) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: table(SAheart[, c(&quot;famhist&quot;, &quot;chd&quot;)]) ## X-squared = 33.123, df = 1, p-value = 8.653e-09 For continuous variables, data visualization can be very helpful. There are many different ways to customize a plot, such as changing the color, shape, label, etc. For more advanced features, the R package ggplot2 is a very popular choice. # We use the birthrate data introduced earlier for this example birthrate = read.csv(&quot;data/birthrate.csv&quot;) plot(birthrate, pch = 19, col = &quot;darkorange&quot;, ylab = &quot;Birth Rate&quot;, main = &quot;U.S. birth rate (1917 - 2003, per 10000)&quot;) Correlations and correlation plots can be used to summarize more variables. However, be careful that factors may not be supported by this feature and could cause errors. # load the package with loading message suppressed suppressMessages(library(PerformanceAnalytics)) chart.Correlation(SAheart[, c(1:3)], histogram=TRUE, pch=&quot;+&quot;) 3-dimensional plot is also an alternative to visualize data. We demonstrate an example using the plot3D package and the scatter3D function. The observations are colored by the outcome class (chd). The package rgl can allow for an interactive plot with rotating and zooming. library(plot3D) scatter3D(SAheart$ldl, SAheart$age, log(1+SAheart$tobacco), xlab = &quot;LDL&quot;, ylab = &quot;Age&quot;, zlab = &quot;Tobacco&quot;, pch = 18, bty = &quot;u&quot;, col.var = SAheart$chd, col = ifelse(SAheart$chd == 1, &quot;darkorange&quot;, &quot;deepskyblue&quot;), colkey = FALSE) "],
["modeling-basics.html", "Chapter 4 Modeling Basics 4.1 Fitting Linear Regression 4.2 Model Diagnostics 4.3 Variable Transformations and Interactions 4.4 Model Selection 4.5 Prediction", " Chapter 4 Modeling Basics 4.1 Fitting Linear Regression A simple linear regression assumes the underlying model \\[Y = \\beta_0 + {\\boldsymbol\\beta}^\\text{T} X + \\epsilon,\\] where \\(\\beta_0\\) is an intercept and \\(\\boldsymbol\\beta\\) is a vector of coefficients corresponds to each covariate. With observed data, we can estimate the regression coefficients. Let’s use a classical dataset, the Boston Housing data (Harrison Jr and Rubinfeld 1978) from the MASS package. The goal of this dataset is to model the median house value (medv) using other predictors. library(MASS) data(Boston) # Fit a linear regression using all variables fit = lm(medv ~ ., data = Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ ., data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.595 -2.730 -0.518 1.777 26.199 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.646e+01 5.103e+00 7.144 3.28e-12 *** ## crim -1.080e-01 3.286e-02 -3.287 0.001087 ** ## zn 4.642e-02 1.373e-02 3.382 0.000778 *** ## indus 2.056e-02 6.150e-02 0.334 0.738288 ## chas 2.687e+00 8.616e-01 3.118 0.001925 ** ## nox -1.777e+01 3.820e+00 -4.651 4.25e-06 *** ## rm 3.810e+00 4.179e-01 9.116 &lt; 2e-16 *** ## age 6.922e-04 1.321e-02 0.052 0.958229 ## dis -1.476e+00 1.995e-01 -7.398 6.01e-13 *** ## rad 3.060e-01 6.635e-02 4.613 5.07e-06 *** ## tax -1.233e-02 3.760e-03 -3.280 0.001112 ** ## ptratio -9.527e-01 1.308e-01 -7.283 1.31e-12 *** ## black 9.312e-03 2.686e-03 3.467 0.000573 *** ## lstat -5.248e-01 5.072e-02 -10.347 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.745 on 492 degrees of freedom ## Multiple R-squared: 0.7406, Adjusted R-squared: 0.7338 ## F-statistic: 108.1 on 13 and 492 DF, p-value: &lt; 2.2e-16 The output can be overwhelming for beginners. Here, by specifying the model with medv ~ ., we are using all variables in this data as predictors, except medv itself. And by default, an intercept term is also included. However, we could also specify particular variables as predictors. For example, if per capita crime rate by town (crim), the average number of rooms (rm) are used to predict the price, and the weighted mean of distances to five Boston employment centres (dis), along with an intercept term, we specify the following fit = lm(medv ~ crim + rm + dis, data = Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ crim + rm + dis, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21.247 -2.930 -0.572 2.390 39.072 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -29.45838 2.60010 -11.330 &lt; 2e-16 *** ## crim -0.25405 0.03532 -7.193 2.32e-12 *** ## rm 8.34257 0.40870 20.413 &lt; 2e-16 *** ## dis 0.12627 0.14382 0.878 0.38 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.238 on 502 degrees of freedom ## Multiple R-squared: 0.5427, Adjusted R-squared: 0.5399 ## F-statistic: 198.6 on 3 and 502 DF, p-value: &lt; 2.2e-16 To read the output from a linear model, we usually pay attention to several key information, such as the coefficient and the p-value for each variable, and the overall model fitting F statistic and its p-value, which is almost 0 in this case. 4.2 Model Diagnostics To further evaluate this model fitting, we may plot the residuals (for assessing the normality) and the Cook’s distance for identifying potential influence observations. # setup the parameters for plotting 4 figures together, in a 2 by 2 structure par(mfrow = c(2, 2)) plot(fit) R also provides several functions for obtaining metrics related to unusual observations that may help this process. resid() provides the residual for each observation hatvalues() gives the leverage of each observation rstudent() give the studentized residual for each observation cooks.distance() calculates the influence of each observation head(resid(fit), n = 10) ## 1 2 3 4 5 6 7 ## -1.908839 -3.129506 3.596769 3.719837 5.286113 3.757775 1.523165 ## 8 9 10 ## 4.353400 -1.732947 -2.519590 head(hatvalues(fit), n = 10) ## 1 2 3 4 5 6 ## 0.002547887 0.002692042 0.005408817 0.005604599 0.006415502 0.004272416 ## 7 8 9 10 ## 0.004088718 0.004346169 0.007117245 0.006403949 head(rstudent(fit), n = 10) ## 1 2 3 4 5 6 ## -0.3061026 -0.5019650 0.5777474 0.5975886 0.8498652 0.6032833 ## 7 8 9 10 ## 0.2444363 0.6990195 -0.2785307 -0.4048546 head(cooks.distance(fit), n = 10) ## 1 2 3 4 5 ## 5.994417e-05 1.702892e-04 4.544127e-04 5.038330e-04 1.166558e-03 ## 6 7 8 9 10 ## 3.909005e-04 6.144012e-05 5.337764e-04 1.392832e-04 2.645454e-04 4.3 Variable Transformations and Interactions It appears that the residuals are not normally distributed because the QQ plot deviates from the diagonal line quite a lot. Sometimes variable transformations can be used to deal with this issue, but that may not fix it completely. Plotting can be useful for detecting ill-distributed variables and suggest potential transformations. For example, we may use the correlation plot to visualize them library(PerformanceAnalytics) chart.Correlation(Boston[, c(&quot;medv&quot;, &quot;crim&quot;, &quot;rm&quot;, &quot;dis&quot;)], histogram=TRUE, pch=&quot;+&quot;) It looks like both crim and dis have heavy tail on the right hand side and could benefit from a log or a power transformation. variable transformations can be easily specified within the lm() function. fit = lm(medv ~ log(crim) + rm + I(dis^0.5), data = Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ log(crim) + rm + I(dis^0.5), data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.767 -3.506 -0.589 2.501 40.035 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -25.1474 2.8674 -8.770 &lt; 2e-16 *** ## log(crim) -1.5033 0.1864 -8.067 5.36e-15 *** ## rm 8.0520 0.4096 19.656 &lt; 2e-16 *** ## I(dis^0.5) -2.1805 0.7644 -2.853 0.00451 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.155 on 502 degrees of freedom ## Multiple R-squared: 0.5548, Adjusted R-squared: 0.5521 ## F-statistic: 208.5 on 3 and 502 DF, p-value: &lt; 2.2e-16 Another approach is to consider polynomial transformations of the outcome variable, known as the Box-Cox transformation. # explore the Box-Cox transformation trans = boxcox(medv ~ log(crim) + rm + I(dis^0.5), data = Boston) # obtain the best power for performing the polynomial trans$x[which.max(trans$y)] ## [1] 0.2626263 # refit the model fit = lm(I(medv^0.2626263) ~ log(crim) + rm + I(dis^0.5), data = Boston) One can again reevaluate the model fitting results and repeat the process if necessary. However, keep in mind that this is could be a tedious process that may not end with a satisfactory solution. To further improve the model fitting we may also consider iterations and higher order terms such as fit = lm(medv ~ log(crim) + rm + rm*log(crim) + I(rm^2) + I(dis^0.5) + as.factor(chas)*rm, data = Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ log(crim) + rm + rm * log(crim) + I(rm^2) + ## I(dis^0.5) + as.factor(chas) * rm, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.476 -2.917 -0.486 2.414 35.630 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 72.0986 12.4270 5.802 1.17e-08 *** ## log(crim) 3.2224 1.1677 2.760 0.0060 ** ## rm -23.0419 3.9298 -5.863 8.27e-09 *** ## I(rm^2) 2.3857 0.3072 7.766 4.66e-14 *** ## I(dis^0.5) -1.0768 0.6729 -1.600 0.1102 ## as.factor(chas)1 14.6814 7.4142 1.980 0.0482 * ## log(crim):rm -0.7620 0.1845 -4.129 4.27e-05 *** ## rm:as.factor(chas)1 -1.6178 1.1357 -1.424 0.1549 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.321 on 498 degrees of freedom ## Multiple R-squared: 0.6699, Adjusted R-squared: 0.6653 ## F-statistic: 144.4 on 7 and 498 DF, p-value: &lt; 2.2e-16 4.4 Model Selection Suppose we have two candidate nested models, and we want to test if adding a set of new variables is significant in terms of predicting the outcome, this is essentially an F test. We can utilize the anova() function: fit = lm(medv ~ crim + rm + dis, data = Boston) fit2 = lm(medv ~ crim + rm + dis + chas + nox, data = Boston) anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: medv ~ crim + rm + dis ## Model 2: medv ~ crim + rm + dis + chas + nox ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 502 19536 ## 2 500 17457 2 2079.2 29.776 6.057e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It appears that adding the two additional variables chas and nox is significant. Selecting variables/models is a central topic in statistics. We could consider some classical tools such as the Akaike information criterion (Akaike 1998) or the Bayesian information criterion (Schwarz and others 1978). Incorporating the stepwise selection algorithm, we may find the best AIC model: # fit a full model that contains all variables full.model = lm(medv ~ ., data = Boston) # select the best AIC model by stepwise regression stepAIC = step(full.model, trace=0, direction=&quot;both&quot;) # the best set of variables being selected attr(stepAIC$terms, &quot;term.labels&quot;) ## [1] &quot;crim&quot; &quot;zn&quot; &quot;chas&quot; &quot;nox&quot; &quot;rm&quot; &quot;dis&quot; &quot;rad&quot; ## [8] &quot;tax&quot; &quot;ptratio&quot; &quot;black&quot; &quot;lstat&quot; 4.5 Prediction The predict() function is an extremely versatile function, for, prediction. When used on the result of a model fit using lm() it will, by default, return predictions for each of the data points used to fit the model. # the fitted value from a model fitting yhat1 = fit$fitted.values # predict on a set of testing data yhat2 = predict(fit) # they are the same all(yhat1 == yhat2) ## [1] FALSE We could also specify new data, which should be a data frame or tibble with the same column names as the predictors. new_obs = data.frame(crim = 0.3, rm = 6, dis = 5) predict(fit, newdata = new_obs) ## 1 ## 21.15216 We can also obtain the confidence interval for the mean response value of this new observation predict(fit, newdata = new_obs, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 21.15216 20.44473 21.8596 Lastly, we can alter the level using the level argument. Here we report a prediction interval instead of a confidence interval. predict(fit, newdata = new_obs, interval = &quot;prediction&quot;, level = 0.99) ## fit lwr upr ## 1 21.15216 4.995294 37.30903 References "],
["optimization.html", "Chapter 5 Optimization", " Chapter 5 Optimization TODO: "],
["k-means-clustering.html", "Chapter 6 K-means Clustering 6.1 Basic Concepts 6.2 Example 1: iris data 6.3 Example 2: clustering of image pixels", " Chapter 6 K-means Clustering 6.1 Basic Concepts The \\(k\\)-means clustering algorithm attemps to solve the following optimization problem: \\[ \\underset{C, \\, \\{m_k\\}_{k=1}^K}\\min \\sum_{k=1}^K \\sum_{C(i) = k} \\lVert x_i - m_k \\rVert^2, \\] where \\(C(\\cdot): \\{1, \\ldots, n\\} \\rightarrow \\{1, \\ldots, K\\}\\) is a cluster assignment function, and \\(m_k\\)’s are the cluster means. To solve this problem, \\(k\\)-means uses an iterative approach that updates \\(C(\\cdot)\\) and \\(m_k\\)’s alternatively. Suppose we have a set of six observations. We first randomly assign them into two clusters (initiate a random \\(C\\) function). Based on this cluster assignment, we can calculate the corresponding cluster mean \\(m_k\\)’s. Then we will assign each observation to the closest cluster mean. In this example, only the blue point on the top will be moved to a new cluster. Then the cluster means can then be recalculated. When there is nothing to move anymore, the algorithm stops. Keep in mind that we started with a random cluster assignment, and this objective function is not convex. Hence we may obtain different results if started with different values. The solution is to try different starting points and use the best final results. This can be tuned using the nstart parameter in the kmeans() function. # some random data set.seed(1) mat = matrix(rnorm(1000), 50, 20) # if we use only one starting point kmeans(mat, centers = 3, nstart = 1)$tot.withinss ## [1] 885.8913 # if we use multiple starting point and pick the best one kmeans(mat, centers = 3, nstart = 100)$tot.withinss ## [1] 883.8241 6.2 Example 1: iris data We use the classical iris data as an example. This dataset contains three different classes, but the goal here is to learn the clusters without knowing the class labels. # plot the original data using two varaibles head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa library(ggplot2) ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() The last two variables in the iris data carry more information on separating the three classes. Hence we will only use the Petal.Length and Petal.Width. library(colorspace) par(mar = c(3, 2, 4, 2), xpd = TRUE) MASS::parcoord(iris[, -5], col = rainbow_hcl(3)[iris$Species], var.label = TRUE, lwd = 2) legend(x = 1.2, y = 1.3, cex = 1, legend = as.character(levels(iris$Species)), fill = rainbow_hcl(3), horiz = TRUE) Let’s perfrom the \\(k\\)-means clustering set.seed(1) # k mean clustering iris.kmean &lt;- kmeans(iris[, 3:4], centers = 3, nstart = 20) # the center of each class iris.kmean$centers ## Petal.Length Petal.Width ## 1 1.462000 0.246000 ## 2 5.595833 2.037500 ## 3 4.269231 1.342308 # the within cluster variation iris.kmean$withinss ## [1] 2.02200 16.29167 13.05769 # the between cluster variation iris.kmean$betweenss ## [1] 519.524 # plot the fitted clusters vs. the truth iris.kmean$cluster &lt;- as.factor(iris.kmean$cluster) ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + # true cluster geom_point(alpha = 0.3, size = 3.5) + scale_color_manual(values = c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;)) + geom_point(col = c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;)[iris.kmean$cluster]) # fitted cluster 6.3 Example 2: clustering of image pixels Let’s first load and plot an image of Leo. library(jpeg) img&lt;-readJPEG(&quot;images/leo.jpg&quot;) # generate a blank image par(mar=rep(0.2, 4)) plot(c(0, 400), c(0, 500), xaxt = &#39;n&#39;, yaxt = &#39;n&#39;, bty = &#39;n&#39;, pch = &#39;&#39;, ylab = &#39;&#39;, xlab = &#39;&#39;) rasterImage(img, 0, 0, 400, 500) For a jpg file, each pixel is stored as a vector with 3 elements — representing red, green and blue intensities. However, by the way, that this objective img being constructed, it is stored as a 3d array. The first two dimensions are the height and width of the figure. We need to vectorize them and treat each pixel as an observation. dim(img) ## [1] 500 400 3 # this apply function applies vecterization to each layer (r/g/b) of the image. img_expand = apply(img, 3, c) # and now we have the desired data matrix dim(img_expand) ## [1] 200000 3 Before performing the \\(k\\)-mean clustering, let’s have a quick peek at the data in a 3d view. Since there are too many observations, we randomly sample a few. library(scatterplot3d) set.seed(1) sub_pixels = sample(1:nrow(img_expand), 1000) sub_img_expand = img_expand[sub_pixels, ] scatterplot3d(sub_img_expand, pch = 19, xlab = &quot;red&quot;, ylab = &quot;green&quot;, zlab = &quot;blue&quot;, color = rgb(sub_img_expand[,1], sub_img_expand[,2], sub_img_expand[,3])) The next step is to perform the \\(k\\)-mean and obtain the cluster label. For example, let’s try 5 clusters. kmeanfit &lt;- kmeans(img_expand, 5) # to produce the new graph, we simply replicate the cluster mean for all observations in the same cluster new_img_expand = kmeanfit$centers[kmeanfit$cluster, ] # now we need to convert this back to the array that can be plotted as an image. # this is a lazy way to do it, but get the job done new_img = img new_img[, , 1] = matrix(new_img_expand[,1], 500, 400) new_img[, , 2] = matrix(new_img_expand[,2], 500, 400) new_img[, , 3] = matrix(new_img_expand[,3], 500, 400) # plot the new image par(mar=rep(0.2, 4)) plot(c(0, 400), c(0, 500), xaxt = &#39;n&#39;, yaxt = &#39;n&#39;, bty = &#39;n&#39;, pch = &#39;&#39;, ylab = &#39;&#39;, xlab = &#39;&#39;) rasterImage(new_img, 0, 0, 400, 500) With this technique, we can easily reproduce results with different \\(k\\) values. Apparently, as \\(k\\) increases, we get better resolution. \\(k = 30\\) seems to recover the original image fairly well. ## Warning: did not converge in 10 iterations "],
["hierarchical-clustering.html", "Chapter 7 Hierarchical Clustering 7.1 Basic Concepts 7.2 Example 1: iris data 7.3 Example 2: RNA Expression Data", " Chapter 7 Hierarchical Clustering 7.1 Basic Concepts Suppose we have a set of six observations: The goal is to progressively group them together until there is only one group. During this entire process, only the pair-wise distances are used to determine which observations are grouped. Hence, the R function hclust() will take a distance matrix and perform the clustering. A distance matrix is an \\(n \\times n\\) matrix. As a default, we can use the Euclidean distance. # the Euclidean distance can be computed using dist() as.matrix(dist(x)) ## 1 2 3 4 5 6 ## 1 0.000000 1.2294164 1.7864196 1.1971565 1.4246185 1.5698349 ## 2 1.229416 0.0000000 2.3996575 0.8727261 1.9243764 2.2708670 ## 3 1.786420 2.3996575 0.0000000 2.8586738 0.4782442 0.2448835 ## 4 1.197156 0.8727261 2.8586738 0.0000000 2.4219048 2.6741260 ## 5 1.424618 1.9243764 0.4782442 2.4219048 0.0000000 0.4204479 ## 6 1.569835 2.2708670 0.2448835 2.6741260 0.4204479 0.0000000 We then use this distance matrix in the hierarchical clustering algorithm. # the Euclidean distance can be computed using dist() as.matrix(dist(x)) ## 1 2 3 4 5 6 ## 1 0.000000 1.2294164 1.7864196 1.1971565 1.4246185 1.5698349 ## 2 1.229416 0.0000000 2.3996575 0.8727261 1.9243764 2.2708670 ## 3 1.786420 2.3996575 0.0000000 2.8586738 0.4782442 0.2448835 ## 4 1.197156 0.8727261 2.8586738 0.0000000 2.4219048 2.6741260 ## 5 1.424618 1.9243764 0.4782442 2.4219048 0.0000000 0.4204479 ## 6 1.569835 2.2708670 0.2448835 2.6741260 0.4204479 0.0000000 # pass the distance matrix to hclust() # we use a complete link function hcfit &lt;- hclust(dist(x), method = &quot;complete&quot;) par(mar=c(2, 2, 2, 1)) plot(hcfit) Hence, the merging process is exactly the same as we demonstrated previous. The height of each split represents how separated the two subsets are. Selecting the number of clusters is still a tricky problem. Usually, we pick a cutoff where the height of the next split is short. Hence, the above example fits well with two clusters. 7.2 Example 1: iris data For this example, we use all variables in the distance calculation and still use the default complete linkage. iris_hc &lt;- hclust(dist(iris[, 3:4])) plot(iris_hc) This does not seem to perform very well, considering that we know the true number of classes is three. Hence, let’s try some other linkage functions. iris_hc &lt;- hclust(dist(iris[, 3:4]), method = &quot;average&quot;) plot(iris_hc, hang = -1) This looks better. Now we can also consider using other approaches to plot this result. For example, the ape package provides some interesting choices. library(ape) plot(as.phylo(iris_hc), type = &quot;unrooted&quot;, cex = 0.6, no.margin = TRUE) We can also add the true class colors to the plot. This plot is motivated by the dendextend package vignettes. Of course in a realistic situation, we wouldn’t know what the true class is. 7.3 Example 2: RNA Expression Data We use a tissue gene expression dataset from the tissuesGeneExpression library, available from bioconductor. I prepared the data to include only 100 genes. You can download the data from the course website. If we simply plot the data using a heatmap. By default, a heatmap uses red to denote higher values, and yellow for lower values. Note that we first plot the data without organizing the columns or rows. The data is also standardized based on columns (genes). load(&quot;data/tissue.Rda&quot;) dim(expression) ## [1] 189 100 table(tissue) ## tissue ## cerebellum colon endometrium hippocampus kidney liver placenta ## 38 34 15 31 39 26 6 head(expression[, 1:3]) ## 211298_s_at 203540_at 211357_s_at ## GSM11805.CEL.gz 7.710426 5.856596 12.618471 ## GSM11814.CEL.gz 4.741010 5.813841 5.116707 ## GSM11823.CEL.gz 11.730652 5.986338 13.206078 ## GSM11830.CEL.gz 5.061337 6.316815 9.780614 ## GSM12067.CEL.gz 4.955245 6.561705 8.589003 ## GSM12075.CEL.gz 10.469501 5.880740 13.050554 heatmap(scale(expression), Rowv = NA, Colv = NA) hierarchical clustering may help us discover interesting patterns. If we reorganize the columns and rows based on the clusters, then it may reveal underlying subclass of issues, or subgroup of genes. heatmap(scale(expression)) Note that there are many other R packages that produce more interesting plots. For example, you can try the heatmaply package. "],
["principle-component-analysis.html", "Chapter 8 Principle Component Analysis 8.1 Basic Concepts 8.2 Example 1: iris Data 8.3 Example 2: Handwritten Digits", " Chapter 8 Principle Component Analysis 8.1 Basic Concepts The goal of PCA is to find a direction of data that displays the largest variance. A nice demonstration of this search of direction is provided at this r-bloggers: Suppose we have a data matrix with \\(n\\) observations and \\(p\\) variables. Principle Component Analysis (PCA) is always done by centering the variables, i.e., subtract column means from each column of the \\(n \\times p\\) data matrix. par(mfrow=c(1,2)) # generate some random data from a 2-dimensional normal distribution. library(MASS) n = 1000 Sigma = matrix(c(0.5, -0.65, -0.65, 1), 2, 2) x = mvrnorm(n, c(1, 2), Sigma) par(mar=c(2, 2, 2, 0.3)) plot(x, main = &quot;Before Centering&quot;, xlim = c(-5, 5), ylim= c(-5, 5), pch = 19, cex = 0.5) abline(h = 0, col = &quot;red&quot;) abline(v = 0, col = &quot;red&quot;) par(mar=c(2, 2, 2, 0.3)) x = scale(x, scale = FALSE) plot(x, main = &quot;After Centering&quot;, xlim = c(-5, 5), ylim= c(-5, 5), pch = 19, cex = 0.5) abline(h = 0, col = &quot;red&quot;) abline(v = 0, col = &quot;red&quot;) For our two-dimensional case, we are trying to find a line (direction) on this plain, such that if all points are projected onto this line, their coordinates have the largest variance, compared with any other line. par(mar=c(2, 2, 0.3, 0.3)) plot(x, xlim = c(-3.5, 3.5), ylim= c(-3.5, 3.5), pch = 19, cex = 0.5) abline(h = 0, col = &quot;red&quot;) abline(v = 0, col = &quot;red&quot;) # using pca pc1 = princomp(x)$loadings[,1] abline(a = 0, b = pc1[2]/pc1[1], col = &quot;deepskyblue&quot;, lwd = 4) We can then take the residuals (after projecting onto this line), and find the largest variance direction of the residuals ## Comp.1 Comp.2 ## 1.1648967 0.2274221 We usually visualize the data in these two directions instead of the original covariates. Note that the coordinates on the PC’s can be obtained using either the scores in the fitted object of princomp, or simply multiply the original data matrix by the loadings. pcafit &lt;- princomp(x) # the new coordinates on PC&#39;s head(pcafit$scores) ## Comp.1 Comp.2 ## [1,] -0.3277569 0.02028922 ## [2,] -2.6787397 -0.17797288 ## [3,] 0.3638739 0.02107160 ## [4,] -0.3741278 -0.15935442 ## [5,] -0.4738269 -0.06945660 ## [6,] 1.0116895 -0.00997124 # direct calculation based on projection head(x %*% pcafit$loadings) ## Comp.1 Comp.2 ## [1,] -0.3277569 0.02028922 ## [2,] -2.6787397 -0.17797288 ## [3,] 0.3638739 0.02107160 ## [4,] -0.3741278 -0.15935442 ## [5,] -0.4738269 -0.06945660 ## [6,] 1.0116895 -0.00997124 # visualize the data on the PCs # Note that the both axies are scaled par(mar=c(4, 4.2, 0.3, 0.3)) plot(pcafit$scores[,1], pcafit$scores[,2], xlab = &quot;First PC&quot;, ylab = &quot;Second PC&quot;, pch = 19, cex.lab = 1.5) abline(h = 0, col = &quot;deepskyblue&quot;, lwd = 4) abline(v = 0, col = &quot;darkorange&quot;, lwd = 4) Note that there are many different functions in R that performs PCA. princomp and prcomp are the most popular ones. 8.1.1 Note: Scaling You should always center the variables when performing PCA, however, whether to use scaling (force each variable to have a standard deviation of 1) depends on the particular application. When you have variables that are extremely disproportionate, e.g., age vs. RNA expression, scaling should be used. This is to prevent some variables from dominating the PC loadings due to their large scales. When all the variables are of the similar type, e.g., color intensities of pixels in a figure, it is better to use the original scale. This is because the variables with larger variations may carry more signal. Scaling may lose that information. 8.2 Example 1: iris Data We use the iris data again. All four variables are considered in this analysis. We plot the first and second PC directions. iris_pc &lt;- prcomp(iris[, 1:4]) library(ggplot2) ggplot(data = data.frame(iris_pc$x), aes(x=PC1, y=PC2)) + geom_point(color=c(&quot;chartreuse4&quot;, &quot;darkorange&quot;, &quot;deepskyblue&quot;)[iris$Species], size = 3) One may be interested in plotting all pair-wise direction to see if lower PC’s provide useful information. pairs(iris_pc$x, col=c(&quot;chartreuse4&quot;, &quot;darkorange&quot;, &quot;deepskyblue&quot;)[iris$Species], pch = 19) However, usually, the lower PC’s are less informative. This can also be speculated from the eigenvalue plot, which shows how influential each PC is. plot(iris_pc, type = &quot;l&quot;, pch = 19, main = &quot;Iris PCA eigen-values&quot;) Feature contributions to the PC can be accessed through the magnitude of the loadings. This table shows that Petal.Length is the most influential variable on the first PC, with loading \\(\\approx 0.8567\\). iris_pc$rotation ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.36138659 -0.65658877 0.58202985 0.3154872 ## Sepal.Width -0.08452251 -0.73016143 -0.59791083 -0.3197231 ## Petal.Length 0.85667061 0.17337266 -0.07623608 -0.4798390 ## Petal.Width 0.35828920 0.07548102 -0.54583143 0.7536574 We can further visualize this on a plot. This can be helpful when the number of variables is large. features = row.names(iris_pc$rotation) ggplot(data = data.frame(iris_pc$rotation), aes(x=PC1, y=PC2, label=features,color=features)) + geom_point(size = 3) + geom_text(size=3) 8.3 Example 2: Handwritten Digits The handwritten zip code digits data contains 7291 training data and 2007 testing data. Each image is a \\(16 \\times 16\\)-pixel gray-scale image. Hence they are converted to a vector of 256 variables. library(ElemStatLearn) # Handwritten Digit Recognition Data # the first column is the true digit dim(zip.train) ## [1] 7291 257 Here is a sample of some images: Let’s do a simpler task, using just three letters: 1, 4 and 8. zip.sub = zip.train[zip.train[,1] %in% c(1,4,8), -1] zip.sub.truth = as.factor(zip.train[zip.train[,1] %in% c(1,4,8), 1]) dim(zip.sub) ## [1] 2199 256 zip_pc = prcomp(zip.sub) plot(zip_pc, type = &quot;l&quot;, pch = 19, main = &quot;Digits 1, 4, and 8 PCA eigen-values&quot;) The eigenvalue results suggest that the first two principal components are much more influential than the rest. A pair-wise PC plot of the first four PC’s may further confirm that speculation. pairs(zip_pc$x[, 1:4], col=c(&quot;chartreuse4&quot;, &quot;darkorange&quot;, &quot;deepskyblue&quot;)[zip.sub.truth], pch = 19) Let’s look at the first two PCs more closely. Even without knowing the true class (no colors) we can still vaguely see 3 clusters. library(ggplot2) ggplot(data = data.frame(zip_pc$x), aes(x=PC1, y=PC2)) + geom_point(size = 2) Finally, let’s briefly look at the results of PCA for all 10 different digits. Of course, more PC’s are needed for this task. You can also plot other PC’s to get more information. library(colorspace) zip_pc &lt;- prcomp(zip.train) plot(zip_pc, type = &quot;l&quot;, pch = 19, main = &quot;All Digits PCA eigen-values&quot;) ggplot(data = data.frame(prcomp(zip.train)$x), aes(x=PC1, y=PC2)) + geom_point(color = rainbow_hcl(10)[zip.train[,1]+1], size = 1) "],
["regression-overview.html", "Chapter 9 Overview", " Chapter 9 Overview Chapter Status: This chapter is currently undergoing massive rewrites. Some code is mostly for the use of current students. Plotting code is often not best practice for plotting, but is instead useful for understanding. Supervised Learning Regression (Numeric Response) What do we want? To make predictions on unseen data. (Predicting on data we already have is easy…) In other words, we want a model that generalizes well. That is, generalizes to unseen data. How we will do this? By controlling the complexity of the model to guard against overfitting and underfitting. Model Parameters Tuning Parameters Why does manipulating the model complexity accomplish this? Because there is a bias-variance tradeoff. How do we know if our model generalizes? By evaluating metrics on test data. We will only ever fit (train) models on training data. All analyses will begin with a test-train split. For regression tasks, our metric will be RMSE. Classification (Categorical Response) The next section. Regression is a form of supervised learning. Supervised learning deals with problems where there are both an input and an output. Regression problems are the subset of supervised learning problems with a numeric output. Often one of the biggest differences between statistical learning, machine learning, artificial intelligence are the names used to describe variables and methods. The input can be called: input vector, feature vector, or predictors. The elements of these would be an input, feature, or predictor. The individual features can be either numeric or categorical. The output may be called: output, response, outcome, or target. The response must be numeric. As an aside, some textbooks and statisticians use the terms independent and dependent variables to describe the response and the predictors. However, this practice can be confusing as those terms have specific meanings in probability theory. Our goal is to find a rule, algorithm, or function which takes as input a feature vector, and outputs a response which is as close to the true value as possible. We often write the true, unknown relationship between the input and output \\(f(\\bf{x})\\). The relationship (model) we learn (fit, train), based on data, is written \\(\\hat{f}(\\bf{x})\\). From a statistical learning point-of-view, we write, \\[ Y = f(\\bf{x}) + \\epsilon \\] to indicate that the true response is a function of both the unknown relationship, as well as some unlearnable noise. \\[ \\text{RMSE}(\\hat{f}, \\text{Data}) = \\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i = 1}^{n}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\] \\[ \\text{RMSE}_{\\text{Train}} = \\text{RMSE}(\\hat{f}, \\text{Train Data}) = \\sqrt{\\frac{1}{n_{\\text{Tr}}}\\displaystyle\\sum_{i \\in \\text{Train}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\] \\[ \\text{RMSE}_{\\text{Test}} = \\text{RMSE}(\\hat{f}, \\text{Test Data}) = \\sqrt{\\frac{1}{n_{\\text{Te}}}\\displaystyle\\sum_{i \\in \\text{Test}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\] - TODO: RSS vs \\(R^2\\) vs RMSE Code for Plotting from Class ## load packages library(rpart) library(FNN) # simulate data ## signal f = function(x) { x ^ 3 } ## define data generating processs get_sim_data = function(f, sample_size = 50) { x = runif(n = sample_size, min = -1, max = 1) y = rnorm(n = sample_size, mean = f(x), sd = 0.15) data.frame(x, y) } ## simualte training data set.seed(42) sim_trn_data = get_sim_data(f = f) ## simulate testing data set.seed(3) sim_tst_data = get_sim_data(f = f) ## create grid for plotting x_grid = data.frame(x = seq(-1.5, 1.5, 0.001)) # fit models ## tree models tree_fit_l = rpart(y ~ x, data = sim_trn_data, control = rpart.control(cp = 0.500, minsplit = 2)) tree_fit_m = rpart(y ~ x, data = sim_trn_data, control = rpart.control(cp = 0.015, minsplit = 2)) tree_fit_h = rpart(y ~ x, data = sim_trn_data, control = rpart.control(cp = 0.000, minsplit = 2)) ## knn models knn_fit_l = knn.reg(train = sim_trn_data[&quot;x&quot;], y = sim_trn_data$y, test = x_grid, k = 40) knn_fit_m = knn.reg(train = sim_trn_data[&quot;x&quot;], y = sim_trn_data$y, test = x_grid, k = 5) knn_fit_h = knn.reg(train = sim_trn_data[&quot;x&quot;], y = sim_trn_data$y, test = x_grid, k = 1) ## polynomial models poly_fit_l = lm(y ~ poly(x, 1), data = sim_trn_data) poly_fit_m = lm(y ~ poly(x, 3), data = sim_trn_data) poly_fit_h = lm(y ~ poly(x, 22), data = sim_trn_data) # get predictions ## tree models tree_fit_l_pred = predict(tree_fit_l, newdata = x_grid) tree_fit_m_pred = predict(tree_fit_m, newdata = x_grid) tree_fit_h_pred = predict(tree_fit_h, newdata = x_grid) ## knn models knn_fit_l_pred = knn_fit_l$pred knn_fit_m_pred = knn_fit_m$pred knn_fit_h_pred = knn_fit_h$pred ## polynomial models poly_fit_l_pred = predict(poly_fit_l, newdata = x_grid) poly_fit_m_pred = predict(poly_fit_m, newdata = x_grid) poly_fit_h_pred = predict(poly_fit_h, newdata = x_grid) # plot fitted trees par(mfrow = c(1, 3)) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Tree Model, cp = 0.5&quot;, cex = 1.5) grid() lines(x_grid$x, tree_fit_l_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Tree Model, cp = 0.015&quot;, cex = 1.5) grid() lines(x_grid$x, tree_fit_m_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Tree Model, cp = 0.0&quot;, cex = 1.5) grid() lines(x_grid$x, tree_fit_h_pred, col = &quot;darkgrey&quot;, lwd = 2) # plot fitted KNN par(mfrow = c(1, 3)) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;KNN, k = 40&quot;, cex = 1.5) grid() lines(x_grid$x, knn_fit_l_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;KNN, k = 5&quot;, cex = 1.5) grid() lines(x_grid$x, knn_fit_m_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;KNN, k = 1&quot;, cex = 1.5) grid() lines(x_grid$x, knn_fit_h_pred, col = &quot;darkgrey&quot;, lwd = 2) # plot fitted polynomials par(mfrow = c(1, 3)) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Polynomial Model, degree = 1&quot;, cex = 1.5) grid() lines(x_grid$x, poly_fit_l_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Polynomial Model, degree = 3&quot;, cex = 1.5) grid() lines(x_grid$x, poly_fit_m_pred, col = &quot;darkgrey&quot;, lwd = 2) plot(y ~ x, data = sim_trn_data, col = &quot;dodgerblue&quot;, pch = 20, main = &quot;Polynomial Model, degree = 22&quot;, cex = 1.5) grid() lines(x_grid$x, poly_fit_h_pred, col = &quot;darkgrey&quot;, lwd = 2) "]
]
