[["optimization-basics.html", "Chapter 4 Optimization Basics 4.1 Basic Concept 4.2 Global vs. Local Optima 4.3 Example: Linear Regression using optim() 4.4 First and Second Order Propoerties", " Chapter 4 Optimization Basics Optimization is heavily involved in statistics and machine learning. Almost all methods introduced in this book can be viewed as some form of optimization. It would be good to have some prior knowledge of it so that later chapters can use these concepts without difficulties. Especially, one should be familiar with concepts such as constrains, gradient methods, and be able to implement them using existing R functions. Since optimization is such a broad topic, we refer readers to Boyd and Vandenberghe (2004) and Nocedal and Wright (2006) for more further reading. We will use a slightly different set of notations in this Chapter so that we are consistent with the literature. This means that for the most part, we will use \\(x\\) as our parameter of interest and optimize a function \\(f(x)\\). This is in contrast to optimizing \\(\\theta\\) in a statistical model \\(f_\\theta(x)\\) where \\(x\\) is the observed data. However, in the example of linear regression, we may again switch back to the regular notation of \\(x^\\text{T} \\boldsymbol \\beta\\). These transitions will only happen under clear context and should not create ambiguity. 4.1 Basic Concept We usually consider a convex optimization problem (non-convex problems are a bit too involving although we will also see some examples of that), meaning that we optimize (minimize) a convex function in a convex domain. A convex function \\(f(\\mathbf{x})\\) maps some subset \\(C \\in \\mathbb{R}^p\\) into \\(\\mathbb{R}^p\\), but enjoys the property that \\[ f(t \\mathbf{x}_1 + (1 - t) \\mathbf{x}_2) \\leq t f(\\mathbf{x}_1) + ( 1- t) f(\\mathbf{x}_2), \\] for all \\(t \\in [0, 1]\\) and any two points \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\) in the domain of \\(f\\). Note that if you have a concave function (the bowl faces downwards) then \\(-f(\\mathbf{x})\\) would be convex. Examples of convex functions: Univariate functions: \\(x^2\\), \\(\\exp(x)\\), \\(-log(x)\\) Affine map: \\(a^\\text{T}\\mathbf{x}+ b\\) is both convex and concave A quadratic function \\(\\frac{1}{2}\\mathbf{x}^\\text{T}\\mathbf{A}\\mathbf{x}+ b^\\text{T}\\mathbf{x}+ c\\), if \\(\\mathbf{A}\\) is positive semidefinite All \\(p\\) norms are convex, following the Triangle inequality and properties of a norm. A sin function is neither convex or convave On the other hand, a convex set \\(C\\) means that if we have two points \\(x_1\\) and \\(x_2\\) in \\(C\\), the line segment joining these two points has to lie within \\(C\\), i.e., \\[\\mathbf{x}_1, \\mathbf{x}_2 \\in C \\quad \\Longrightarrow \\quad t \\mathbf{x}_1 + (1 - t) \\mathbf{x}_2 \\in C,\\] for all \\(t \\in [0, 1]\\). Examples of convex set include Real line: \\(\\mathbb{R}\\) Norm ball: \\(\\{ \\mathbf{x}: \\lVert \\mathbf{x}\\rVert \\leq r \\}\\) Hyperplane: \\(\\{ \\mathbf{x}: a^\\text{T}\\mathbf{x}= b \\}\\) Consider a simple optimization problem: \\[ \\text{minimize} \\quad f(x_1, x_2) = x_1^2 + x_2^2\\] Clearly, \\(f(x_1, x_2)\\) is a convex function, and we know that the solution of this problem is \\(x_1 = x_2 = 0\\). However, the problem might be a bit more complicated if we restrict that in a certain (convex) region, for example, \\[\\begin{align} &amp;\\underset{x_1, x_2}{\\text{minimize}} &amp; \\quad f(x_1, x_2) &amp;= x_1^2 + x_2^2 \\\\ &amp;\\text{subject to} &amp; x_1 + x_2 &amp;\\leq -1 \\\\ &amp; &amp; x_1 + x_2 &amp;&gt; -2 \\end{align}\\] Here the convex set \\(C = \\{x_1, x_2 \\in \\mathbb{R}: x_1 + x_2 \\leq -1 \\,\\, \\text{and} \\,\\, x_1 + x_2 &gt; -2\\}\\). And our problem looks like the following, which attains it minimum at \\((-0.5, -0.5)\\). In general, we will be dealing with a problem in the form of \\[\\begin{align} &amp;\\underset{\\mathbf{x}}{\\text{minimize}} &amp; \\quad f(\\mathbf{x}) \\\\ &amp;\\text{subject to} &amp; g_i(\\mathbf{x}) &amp; \\leq 0, \\, i = 1,\\ldots, m \\\\ &amp; &amp; h_j(\\mathbf{x}) &amp;= 0, \\, j = 1,\\ldots, k \\end{align}\\] where \\(g_i(\\mathbf{x})\\)s are a set of inequality constrains, and \\(h_j(\\mathbf{x})\\)s are equality constrains. There are established result showing what type of constrains would lead to a convex set, but lets assuming for now that we will be dealing a well behaved problem. We shall see in later chapters that many models such as, Lasso, Ridge and support vector machines can all be formulated into this form. 4.2 Global vs. Local Optima Although we would like to deal with convex optimization problems, non-convex problems appears more and more frequently. For example, deep learning models are almost always non-convex except overly simplified ones. However, for convex optimization problems, a local minimum is also a global minimum, i.e., a \\(x^\\ast\\) such that for any \\(x\\) in the feasible set, \\(f(x^\\ast) \\leq f(x)\\). This can be achieved by a variety of descent algorithms, to be introduced. However, for non-convex problems, we may still be interested in a local minimum, which satisfies that for any \\(x\\) in a neighboring set of \\(x^\\ast\\), \\(f(x^\\ast) \\leq f(x)\\). The comparison of these two cases can be demonstrated in the following plots. Again, a descent algorithm can help us find a local minimum, except for some very special cases, such as a saddle point. However, we will not discuss these issues in this book. 4.3 Example: Linear Regression using optim() Although completely not necessary, we may also view linear regression as an optimization problem. This is of course an unconstrained problem, meaning that \\(C \\in \\mathbb{R}^p\\). Such problems can be solved using the optim() function. Also, lets temporarily switch back to the \\(\\boldsymbol \\beta\\) notation of parameters. Hence, if we observe a set of observations \\(\\{\\mathbf{x}_i, y_i\\}_{i = 1}^n\\), our optimization problem is to minimize the objection function, i.e., residual sum of squares (RSS): \\[\\begin{align} \\underset{\\boldsymbol \\beta}{\\text{minimize}} \\quad f(\\boldsymbol \\beta) = \\frac{1}{n} \\sum_i (y_i - \\mathbf{x}_i^\\text{T}\\boldsymbol \\beta)^2 \\\\ \\end{align}\\] We generate 200 random observations, and also write a function to calculate the RSS for any given \\(\\boldsymbol \\beta\\) values. The objective function looks like the following: # generate data from a simple linear model set.seed(20) n = 200 x &lt;- cbind(1, rnorm(n)) y &lt;- x %*% c(0.5, 1) + rnorm(n) # calculate the residual sum of squares for a grid of beta values rss &lt;- function(b, trainx, trainy) sum((trainy - trainx %*% b)^2) b0 &lt;- b1 &lt;- seq(0, 2, length = 20) z = matrix(apply(expand.grid(b0, b1), 1, rss, x, y), 20, 20) bestpoint = data.frame(&quot;x&quot; = 0.5, &quot;y&quot; = 1, &quot;z&quot; = rss(c(0.5, 1), x, y)) # 3d plot of RSS using `plotly` library(plotly) plot_ly(x = b0, y = b1) %&gt;% layout(plot_bgcolor=&#39;rgb(254, 247, 234)&#39;) %&gt;% layout(paper_bgcolor=&#39;transparent&#39;) %&gt;% add_surface(z = t(z), colorscale = &#39;Viridis&#39;) %&gt;% layout(scene = list(xaxis = list(title = &quot;beta0&quot;), yaxis = list(title = &quot;beta1&quot;), zaxis = list(title = &quot;RSS&quot;))) %&gt;% add_markers(data = bestpoint, x = ~x, y = ~y, z = ~z, marker = list(size = 6, color = &quot;red&quot;, symbol = 104)) Now the question is how to solve this problem. The optim() function uses the following syntax: # The solution can be solved by any optimization algorithm lm.optim &lt;- optim(par = c(2, 2), fn = rss, trainx = x, trainy = y) The par argument specifies an initial value, in this case, \\(\\beta_0 = \\beta_1 = 2\\) The fn argument specifies the name of an R function that can calculate the objective function. Please note that the first argument in this function has be the parameter being optimized, i.e, \\(\\boldsymbol \\beta\\). Also, it must be a vector, not a matrix or other types. The arguments trainx = x, trainy = y specifies any additional arguments that the objective function fn, i.e., rss needs. It behaves the same as if you are supplying this to rss. lm.optim ## $par ## [1] 0.4532072 0.9236502 ## ## $value ## [1] 203.5623 ## ## $counts ## function gradient ## 63 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL The result shows that the estimated parameters ($par) are 0.453 and 0.924, with a functional value 203.562. The convergence code is 0, meaning that the algorithm converged. The parameter estimates are almost the same as lm(), with small numerical errors. # The solution form lm() summary(lm(y ~ x - 1))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## x1 0.4530498 0.07177854 6.311772 1.773276e-09 ## x2 0.9236934 0.07226742 12.781602 1.136397e-27 What we will be introducing in the following are some basic approaches to solve such a numerical problem. 4.4 First and Second Order Propoerties "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
