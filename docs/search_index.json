[["index.html", "Statistical Learning and Machine Learning with R Preface Target Audience Whats Covered? Acknowledgements License", " Statistical Learning and Machine Learning with R Ruoqing Zhu, PhD 2021-09-19 Preface Welcome to Statistical Learning and Machine Learning with R! I started this project during the summer of 2018 when I was preparing for the Stat 432 course. At that time, our faculty member Dr. David Dalpiaz, had decided to move to The Ohio State University (although he moved back to UIUC later on). David introduced to me this awesome way of publishing website on GitHub, which is a very efficient approach for developing courses. Since I have also taught Stat 542 (Statistical Learning) for several years, I figured it could be beneficial to integrate what I have to this existing book by David and use it as the R material for both courses. For Stat 542, the main focus is to learn the numerical optimization behind these learning algorithms, and also be familiar with the theoretical background. As you can tell, I am not being very creative on the name, so SMLR it is. You can find the source file of this book on my GitHub. Target Audience This book can be suitable for students ranging from advanced undergraduate to first/second year Ph.D students who have prior knowledge in statistics. Although a student at the masters level will likely benefit most from the material. Previous experience with both basic mathematics (mainly linear algebra), statistical modeling (such as linear regressions) and R are assumed. Whats Covered? I currently plan to include the following topics: Basic Knowledge R, R Studio and R Markdown Linear regression and linear algebra Numerical optimization Penalized linear models and the bias-variance trade-off Classification Non-parametric Statistical Models Machine Learning Models Unsupervised Learning Appendix The goal of this book is to introduce not only how to run some of the popular statistical learning models in R, but also to know the algorithms and programming techniques for solving these models. For each section, the difficulty will gradually increase from an undergraduate level to a graduate level. It will be served as a supplement to An Introduction to Statistical Learning (James et al. 2013) for STAT 432 - Basics of Statistical Learning and to The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Hastie, Tibshirani, and Friedman 2001) for STAT 542 - Statistical Learning at the University of Illinois at Urbana-Champaign. This book is under active development. Hence, you may encounter errors ranging from typos to broken code, to poorly explained topics. If you do, please let me know! Simply send an email and I will make the changes as soon as possible (rqzhu AT illinois DOT edu). Or, if you know R Markdown and are familiar with GitHub, make a pull request and fix an issue yourself! These contributions will be acknowledged. Acknowledgements The initial contents are derived from Dr. David Dalpiazs book. My STAT 542 course materials are also inspired by Dr. Feng Liang and Dr. John Marden who developed earlier versions of this course. And I also incorporated many online resources, which I cannot put into a comprehensive list. License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Reference "],["r-and-rstudio.html", "Chapter 1 R and RStudio 1.1 Installing R and RStudio 1.2 Resources and Guides 1.3 Basic Mathematical Operations 1.4 Data Objects 1.5 Readin and save data 1.6 Using and defining functions 1.7 Distribution and random numbers 1.8 Using packages and other resources 1.9 Practice questions", " Chapter 1 R and RStudio 1.1 Installing R and RStudio The first step is to download and install R and RStudio. Most steps should be self-explanatory. You can also find many online guides for step-by-step instruction, such as this YouTube video. However, be aware that some details may have been changed over the years. After installing both, open your RStudio, you should see four panes, which can be seen below: Source pane on top-left where you write code in to files Console on bottom-left where the code is inputted into R Environment (and other tabs) on top-right where you can see current variables and objects you defined File (and other tabs) on bottom-right which is essentially a file borrower We will mainly use the left two panes. You can either directly input code into the console to run for results, or edit your code in a file and run them in chunks or as a whole. 1.2 Resources and Guides There are many online resources for how to use R, RStudio. For example, David Dalpiazs other online book Applied Statistics with R contains an introduction to using them. There are also other online documentation such as Install R and RStudio R tutorial Data in R Play-list (video) R and RStudio Play-list (video) It is worth to mention that once you become an advanced user, and possibly a developer of R packages using C/C++ (add-on of R for performing specific tasks), and you also happen to use Windows like I do, you will have to install Rtools that contains the gcc compilers. This is also needed if you want to install any R package from a source (.tar.gz) file instead of using the so-called binaries (.zip files). 1.3 Basic Mathematical Operations Basic R calculations and operations should be self-explanatory. Try to type-in the following commands into your R console and start to explore yourself. Lines with a # in the front are comments, which will not be executed. Lines with ## in the front are outputs you should expect. # Basic mathematical operations 1 + 3 ## [1] 4 1 - 3 ## [1] -2 1 * 3 ## [1] 3 1 / 3 ## [1] 0.3333333 3^5 ## [1] 243 4^(-1/2) ## [1] 0.5 pi ## [1] 3.141593 # some math functions sqrt(4) ## [1] 2 exp(1) ## [1] 2.718282 log(3) ## [1] 1.098612 log2(16) ## [1] 4 log(15, base = 3) ## [1] 2.464974 factorial(5) ## [1] 120 sin(pi) ## [1] 1.224606e-16 If you want to see more information about a particular function or operator in R, the easiest way is to get the reference document. Put a question mark in front of a function name: # In a default R console window, this will open up a web browser. # In RStudio, this will be displayed at the Help window at the bottom-right penal (Help tab). ?log10 ?cos 1.4 Data Objects Data objects can be a complicated topic for people who never used R before. The most common data objects are vector, matrix, list, and data.frame. They are defined using a specific syntax. To define a vector, we use c followed by (), where the elements within the parenthesis are separated using comma. You can save the vector and name as something else. For example # creating a vector c(1,2,3,4) ## [1] 1 2 3 4 c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; # define a new vector object, called `x` x = c(1,1,1,0,0,0) After defining this object x, it should also appear on your top-right environment pane. To access elements in an object, we use the [] operator, like a C programming reference style. # getting the second element in x x[2] ## [1] 1 # getting the second to the fourth element in x x[2:4] ## [1] 1 1 0 Similarly, we can create and access elements in a matrix: # create a matrix by providing all of its elements # the elements are filled to the matrix by column matrix(c(1,2,3,4), 2, 2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 # create a matrix by column-bind vectors y = c(1,0,1,0,1,0) cbind(x, y) ## x y ## [1,] 1 1 ## [2,] 1 0 ## [3,] 1 1 ## [4,] 0 0 ## [5,] 0 1 ## [6,] 0 0 # access elements in a matrix # Note that in R, upper and lower cases are treated as two different objects X = matrix(c(1:16), 4, 4) X ## [,1] [,2] [,3] [,4] ## [1,] 1 5 9 13 ## [2,] 2 6 10 14 ## [3,] 3 7 11 15 ## [4,] 4 8 12 16 X[2, 3] ## [1] 10 X[1, ] ## [1] 1 5 9 13 # getting a sub-matrix of X X[1:2, 3:4] ## [,1] [,2] ## [1,] 9 13 ## [2,] 10 14 Mathematical operations on vectors and matrices are, by default, element-wise. For matrix multiplications, you should use %*%. # adding two vectors (x + y)^2 ## [1] 4 1 4 0 1 0 # getting the length of a vector length(x) ## [1] 6 # matrix multiplication X %*% X ## [,1] [,2] [,3] [,4] ## [1,] 90 202 314 426 ## [2,] 100 228 356 484 ## [3,] 110 254 398 542 ## [4,] 120 280 440 600 # getting the dimension of a matrix dim(X) ## [1] 4 4 # A warning will be issued when R detects something wrong # Results may still be produced however y + c(1,2,3,4) ## Warning in y + c(1, 2, 3, 4): longer object length is not a multiple of shorter object length ## [1] 2 2 4 4 2 2 list() creates a list of objects (of any type). However, some operators cannot be directly applied to a list in a similar way as to vectors or matrices. Model fitting results in R are usually stored as a list. For example, the lm() function, which will be introduced later. # creating a list x = list(c(1,2), &quot;hello&quot;, matrix(c(1,2,3,4), 2, 2)) # accessing its elements using double brackets `[[]]` x[[1]] ## [1] 1 2 data.frame() creates a list of vectors of equal length, and display them as a matrix-like object, in which each vector is a column of the matrix. It is mainly used for storing data. This will be our most frequently used data object for analysis. For example, in the famous iris data, the first four columns are numerical variables, while the last column is a categorical variable with three levels: setosa, versicolor, and virginica: # The iris data is included with base R, so we can use them directly # This will create a copy of the data into your environment data(iris) # the head function peeks the first several rows of the dataset head(iris, n = 3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa # each column usually contains a column (variable) name colnames(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; # data frame can be called by each individual column, which will be a vector # iris$Species iris$Species[2:4] ## [1] setosa setosa setosa ## Levels: setosa versicolor virginica # the summary function can be used to view summary statistics of all variables summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 factor is a special type of vector. It is frequently used to store a categorical variable with more than two categories. The last column of the iris data is a factor. You need to be a little bit careful when dealing with factor variables when during modeling since some functions do not take care of them automatically or they do it in a different way than you thought. For example, changing a factor variable into numerical ones will ignore any potential relationship among different categories. levels(iris$Species) ## [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; as.numeric(iris$Species) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [48] 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [95] 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [142] 3 3 3 3 3 3 3 3 3 1.5 Readin and save data Data can be imported from a variety of sources. More commonly, a dataset can be stored in .txt and .csv files. Such data reading methods require specific structures in the source file: the first row should contain column names, and there should be equal number of elements in each row. Hence you should always check your file before reading them in. # read-in data birthrate = read.csv(&quot;data/birthrate.csv&quot;) head(birthrate) ## Year Birthrate ## 1 1917 183.1 ## 2 1918 183.9 ## 3 1919 163.1 ## 4 1920 179.5 ## 5 1921 181.4 ## 6 1922 173.4 # to see how many observations (rows) and variables (columns) in a dataset dim(birthrate) ## [1] 87 2 R data can also be saved into other formats. The more efficient way, assuming that you are going to load these file back to R in the future, is to save them as .RData file. Usually, for a larger dataset, this reduces the time spend on reading the data. # saving a object to .RData file save(birthrate, file = &quot;mydata.RData&quot;) # you can specify multiple objects to be saved into the same file save(birthrate, iris, file = &quot;mydata.RData&quot;) # load the data again back to your environment load(&quot;mydata.RData&quot;) # alternatively, you can also save data to a .csv file write.csv(birthrate, file = &quot;mydata.csv&quot;) # you can notice that this .csv file contains an extra column of &quot;ID number&quot;, without a column name # Hence, when you read this file back into R, you should specify `row.names = 1` to indicate that. # Otherwise this will produce an error read.csv(file = &quot;mydata.csv&quot;, row.names = 1) 1.6 Using and defining functions We have already used many functions. You can also define your own functions, and even build them into packages (more on this later) for other people to use. This is the main advantage of R. For example, lets consider writing a function that returns the minimum and maximum of a vector. Suppose we already know the min() and max() functions. myrange &lt;- function(x) # x is the argument that your function takes in { return(c(min(x), max(x))) # return a vector that contains two elements } x = 1:10 myrange(x) ## [1] 1 10 # R already has this function range(x) ## [1] 1 10 1.7 Distribution and random numbers Three distributions that are most frequently used in this course are Bernoulli, Gaussian (normal), and \\(t\\) distributions. Bernoulli distributions can be used to describe binary variables, while Gaussian distribution is often used to describe continuous ones. The following code generates some random variables # read the documentation of rbinom() using ?rbinom x = rbinom(100, 1, 0.4) table(x) ## x ## 0 1 ## 62 38 However, this result cannot be replicated by others, since the next time we run this code, the random numbers will be different. Hence it is important to set and keep the random seed when a random algorithm is involved. The following code will always generate the same result set.seed(1) x = rbinom(100, 1, 0.4) y = rnorm(100) # by default, this is mean 0 and variance 1 table(x) ## x ## 0 1 ## 57 43 hist(y) boxplot(y ~ x) 1.8 Using packages and other resources Packages are written and contributed to R by individuals. They provide additional features (functions or data) that serve particular needs. For example, the ggplot2 package is developed by the RStudio team that provides nice features to plot data. We will have more examples of this later on, but first, lets install and load the package so that we can use these features. More details will be provided in the data visualization section. # to install a package install.packages(&quot;ggplot2&quot;) # to load the package library(ggplot2) # use the ggplot() function to produce a plot # Sepal.Length is the horizontal axis # Sepal.Width is the vertical axis # Species labels are used as color ggplot(iris, aes(Sepal.Length, Sepal.Width, colour = Species)) + geom_point() You may also noticed that in our previous examples, all tables only displayed the first several rows. One may be interested in looking at the entire dataset, however, it would take too much space to display the whole table. Here is a package that would allow you to display it in a compact window. It also provides searching and sorting tools. You can integrate this into your R Markdown reports. library(DT) datatable(iris, filter = &quot;top&quot;, rownames = FALSE, options = list(pageLength = 5)) Often times, you may want to perform a new task and you dont know what function can be used to achieve that. Google Search or Stack Overflow are probably your best friends. I used to encounter this problem: I have a list of objects, and each of them is a vector. I then need to extract the first element of all these vectors. However, doing this using a for-loop can be slow, and I am also interested in a cleaner code. So I found this post, which provided a simple answer: # create the list a = list(c(1,1,1), c(2,2,2), c(3,3,3)) # extract the first element in each vector of the list sapply(a, &quot;[[&quot;, 1) ## [1] 1 2 3 1.9 Practice questions Attach a new numerical column to the iris data, as the product of Petal.Length and Petal.Width and name the column as Petal.Prod. iris = cbind(iris, &quot;Petal.Prod&quot; = iris$Petal.Length*iris$Petal.Width) head(iris) Attach a new numerical column to the iris data, with value 1 if the observation is setosa, 2 for versicolor and 3 for virginica, and name the column as Species.Num. iris = cbind(iris, &quot;Species.Num&quot; = as.numeric(iris$Species)) head(iris) Change Species.Num to a factor variable such that it takes value Type1 if the observation is setosa and NA otherwise. iris$Species.Num = as.factor(ifelse(iris$Species == &quot;setosa&quot;, &quot;Type1&quot;, &quot;NA&quot;)) head(iris) Define a function that takes in a numerical vector, and output the mean of that vector. Do this without using the mean() and sum() function. mymean &lt;- function(x) { sum = 0 for (i in 1:length(x)) sum = sum + x[i] return(sum = sum / length(x)) } x = 1:10 mymean(x) mean(x) "],["rmarkdown.html", "Chapter 2 RMarkdown 2.1 Basics and Resources 2.2 Formatting Text 2.3 Adding R Code 2.4 Importing Data 2.5 Working Directory 2.6 Plotting 2.7 Chunk Options 2.8 Adding Math with LaTeX 2.9 Output Options 2.10 Try It!", " Chapter 2 RMarkdown 2.1 Basics and Resources R Markdown is a built-in feature of RStudio. It integrates plain text with chunks of R code in to a single file, which is extremely useful when constructing class notes or building a website. A .rmd file can be compiled into nice-looking .html, .pdf, and .docx file. For example, this entire guide is created using R Markdown. With RStudio, you can install R Markdown from R console using the following code. Note that this should be automatically done the first time you create and compile a .rmd file in RStudio. # Install R Markdown from CRAN install.packages(&quot;rmarkdown&quot;) Again there are many online guides for R Markdown, and these may not be the best ones. R Markdown: The Definitive Guide R Markdown Cheat Sheet R Markdown Play-list (video) To get started, create an R Markdown template file by clicking File -&gt; New File -&gt; R Markdown... You can then Knit the template file and start to explore its features. Please note that this guide is provided in the .html format. However, your homework report should be in .pdf format. This can be done by selecting the Knit to PDF option from the Knit button. Again there are many online guides, and these may not be the best ones. R Markdown Cheat Sheet R Markdown Play-list (video) 2.2 Formatting Text Formatting text is easy. Bold can be done using ** or __ before and after the text. Italics can be done using * or _ before and after the text. For example, This is bold. This is italics. and this is bold italics. This text appears as monospaced. Unordered list element 1. Unordered list element 2. Unordered list element 3. Ordered list element 1. Ordered list element 2. Ordered list element 3. We could mix lists and links. Note that a link can be constructed in the format [display text](http link). If colors are desired, we can customize it using, for example, [\\textcolor{blue}{display text}](http link). But this only works in .pdf format. For .html, use &lt;span style=\"color: red;\"&gt;text&lt;/span&gt;. A default link: RMarkdown Documentation colored link 1: (Not shown because it only works in PDF) colored link 2: Table Generator (only works in HTML) Tables are sometimes tricky using Markdown. See the above link for a helpful Markdown table generator. Note that you can also adjust the alignment by using a : sign. A B C 1 2 3 Middle Left Right 2.3 Adding R Code So far we have only used Markdown to create the text part. This is useful by itself, but the real power of RMarkdown comes when we add R. There are two ways we can do this. We can use R code chunks, or run R inline. 2.3.1 R Chunks The following is an example of an R code chunk. Start the chunk with ```{r} and end with ```: ```{r} \\(\\quad\\) set.seed(123) \\(\\quad\\) rnorm(5) ``` This generates five random observations from the standard normal distribution. We also set the seed so that the results can be later on replicated. The result looks like the following set.seed(123) rnorm(5) ## [1] -0.56047565 -0.23017749 1.55870831 0.07050839 0.12928774 # define function get_sd = function(x, biased = FALSE) { n = length(x) - 1 * !biased sqrt((1 / n) * sum((x - mean(x)) ^ 2)) } # generate random sample data set.seed(42) (test_sample = rnorm(n = 10, mean = 2, sd = 5)) ## [1] 8.8547922 -0.8234909 3.8156421 5.1643130 4.0213416 1.4693774 9.5576100 1.5267048 ## [9] 12.0921186 1.6864295 # run function on generated data get_sd(test_sample) ## [1] 4.177244 There is a lot going on here. In the .Rmd file, notice the syntax that creates and ends the chunk. Also note that example_chunk is the chunk name. Everything between the start and end syntax must be valid R code. Chunk names are not necessary, but can become useful as your documents grow in size. In this example, we define a function, generate some random data in a reproducible manner, displayed the data, then ran our function. 2.3.2 Inline R R can also be run in the middle of the exposition. For example, the mean of the data we generated is 4.7364838. 2.4 Importing Data When using RMarkdown, any time you knit your document to its final form, say .html, a number of programs run in the background. Your current R environment seen in RStudio will be reset. Any objects you created while working interactively inside RStudio will be ignored. Essentially a new R session will be spawned in the background and the code in your document is run there from start to finish. For this reason, things such as importing data must be explicitly coded into your document. library(readr) example_data = read_table(&quot;data/skincancer.txt&quot;) The above loads the online file. In many cases, you will load a file that is locally stored in your own computer. In that case, you can either specify the full file path, or simply use, for example read_csv(\"filename.csv\") if that file is stored at your working directory. The working directory will usually be the directory that contains your .Rmd file. You are recommended to reference data in this manner. Note that we use the newer read_csv() from the readr package instead of the default read.csv(). 2.5 Working Directory Whenever R code is run, there is always a current working directory. This allows for relative references to external files, in addition to absolute references. Since the working directory when knitting a file is always the directory that contains the .Rmd file, it can be helpful to set the working directory inside RStudio to match while working interactively. To do so, select Session &gt; Set Working Directory &gt; To Source File Location while editing a .Rmd file. This will set the working directory to the path that contains the .Rmd. You can also use getwd() and setwd() to manipulate your working directory programmatically. These should only be used interactively. Using them inside an RMarkdown document would likely result in lessened reproducibility. As of recent RStudio updates, this practice is not always necessary when working interactively. If lines of code are being Output Inline, then the working directory is automatically the directory which contains the .Rmd file. 2.6 Plotting The following generates a simple plot, which displays the skin cancer mortality. By default, the figure is aligned on the left, with size 3 by 5 inches. plot(Mort ~ Lat, data = example_data) In our R introduction, we used ggplot2 to create a more interesting plot. You may also polish a plot with basic functions. Notice it is huge in the resulting document, since we have modified some chunk options (fig.height = 6, fig.width = 8) in the RMarkdown file to manipulate its size. plot(Mort ~ Lat, data = example_data, xlab = &quot;Latitude&quot;, ylab = &quot;Skin Cancer Mortality Rate&quot;, main = &quot;Skin Cancer Mortality vs. State Latitude&quot;, pch = 19, cex = 1.5, col = &quot;deepskyblue&quot;) But you can also notice that the labels and the plots becomes disproportional when the figure size is set too small. This can be resolved using a scaling option such as out.width = '60%, but enlarge the original figure size. We also align the figure at the center using fig.align = 'center' 2.7 Chunk Options We have already seen chunk options fig.height, fig.width, and out.width which modified the size of plots from a particular chunk. There are many chunk options, but we will discuss some others which are frequently used including; eval, echo, message, and warning. If you noticed, the plot above was displayed without showing the code. install.packages(&quot;rmarkdown&quot;) ?log View(mpg) Using eval = FALSE the above chunk displays the code, but it is not run. Weve already discussed not wanting install code to run. The ? code pulls up documentation of a function. This will spawn a browser window when knitting, or potentially crash during knitting. Similarly, using View() is an issue with RMarkdown. Inside RStudio, this would pull up a window which displays the data. However, when knitting, R runs in the background and RStudio is not modifying the View() function. This, on OSX especially, usually causes knitting to fail. ## [1] &quot;Hello World!&quot; Above, we see output, but no code! This is done using echo = FALSE, which is often useful. x = 1:10 y = 1:10 summary(lm(y ~ x)) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.661e-16 -1.157e-16 4.273e-17 2.153e-16 4.167e-16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.123e-15 2.458e-16 4.571e+00 0.00182 ** ## x 1.000e+00 3.961e-17 2.525e+16 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.598e-16 on 8 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 6.374e+32 on 1 and 8 DF, p-value: &lt; 2.2e-16 The above code produces a warning, for reasons we will discuss later. Sometimes, in final reports, it is nice to hide these, which we have done here. message = FALSE and warning = FALSE can be used to do so. Messages are often created when loading packages to give the user information about the effects of loading the package. These should be suppressed in final reports. Be careful about suppressing these messages and warnings too early in an analysis as you could potentially miss important information! 2.8 Adding Math with LaTeX Another benefit of RMarkdown is the ability to add Latex for mathematics typesetting. Like R code, there are two ways we can include Latex; displaystyle and inline. Note that use of LaTeX is somewhat dependent on the resulting file format. For example, it cannot be used at all with .docx. To use it with .pdf you must have LaTeX installed on your machine. With .html the LaTeX is not actually rendered during knitting, but actually rendered in your browser using MathJax. 2.8.1 Displaystyle LaTeX Displaystyle is used for larger equations which appear centered on their own line. This is done by putting $$ before and after the mathematical equation. \\[ \\widehat \\sigma = \\sqrt{\\frac{1}{n - 1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\] 2.8.2 Inline LaTex We could mix LaTeX commands in the middle of exposition, for example: \\(t = 2\\). We could actually mix R with Latex as well! For example: \\(\\bar{x} = 4.7364838\\). 2.9 Output Options At the beginning of the document, there is a code which describes some metadata and settings of the document. The default code looks like title: &quot;R Notebook&quot; output: html_notebook You can easily add your name and date to it, and add a Table of Contents, using toc: yes. Note that the following code would specify the theme of an html file. title: &quot;My RMarkdown Template&quot; author: &quot;Your Name&quot; date: &quot;Aug 26, 2021&quot; output: html_document: toc: yes You can edit this yourself, or click the settings button at the top of the document and select Output Options.... Here you can explore other themes and syntax highlighting options, as well as many additional options. Using this method will automatically modify this information in the document. 2.10 Try It! Be sure to play with this document! Change it. Break it. Fix it. The best way to learn RMarkdown (or really almost anything) is to try, fail, then find out what you did wrong. RStudio has provided a number of beginner tutorials which have been greatly improved recently and detail many of the specifics potentially not covered in this document. RMarkdown is continually improving, and this document covers only the very basics. "],["linear-regression-and-model-selection.html", "Chapter 3 Linear Regression and Model Selection 3.1 Example: real estate data 3.2 Notation and Basic Properties 3.3 Using the lm() Function 3.4 Model Selection Criteria 3.5 Model Selection Algorithms 3.6 Derivation of Marrows \\(C_p\\)", " Chapter 3 Linear Regression and Model Selection This chapter severs several purposes. First, we will review some basic knowledge of linear regression. This includes the concept of vector space, projection, which leads to estimating parameters of a linear regression. Most of these knowledge are covered in the prerequisite so you shouldnt find these concepts too difficult to understand. Secondly, we will mainly use the lm() function as an example to demonstrate some features of R. This includes extracting results, visualizations, handling categorical variables, prediction and model selection. These concepts will be useful for other models. Finally, we will introduce several model selection criteria and algorithms to perform model selection. 3.1 Example: real estate data This Real Estate data (Yeh and Hsu 2018) is provided on the UCI machine learning repository. The goal of this dataset is to predict the unit house price based on six different covariates: date: The transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.) age: The house age (unit: year) distance: The distance to the nearest MRT station (unit: meter) stores: The number of convenience stores in the living circle on foot (integer) latitude: Latitude (unit: degree) longitude: Longitude (unit: degree) price: House price of unit area realestate = read.csv(&quot;data/realestate.csv&quot;, row.names = 1) library(DT) datatable(realestate, filter = &quot;top&quot;, rownames = FALSE, options = list(pageLength = 8)) dim(realestate) ## [1] 414 7 3.2 Notation and Basic Properties We usually denote the observed covariates data as the design matrix \\(\\mathbf{X}\\), with dimension \\(n \\times p\\). Hence in this case, the dimension of \\(\\mathbf{X}\\) is \\(414 \\times 7\\). The \\(j\\)th variable is simply the \\(j\\)th column of this matrix, which is denoted as \\(\\mathbf{x}_j\\). The outcome \\(\\mathbf{y}\\) (price) is a vector of length \\(414\\). Please note that we usually use a bold symbol to represent a vector, while for a single element (scalar), such as the \\(j\\)th variable of subject \\(i\\), we use \\(x_{ij}\\). A linear regression concerns modeling the relationship (in matrix form) \\[\\mathbf{y}_{n \\times 1} = \\mathbf{X}_{n \\times p} \\boldsymbol \\beta_{p \\times 1} + \\boldsymbol \\epsilon_{n \\times 1}\\] And we know that the solution is obtained by minimizing the residual sum of squares (RSS): \\[ \\begin{align} \\widehat{\\boldsymbol \\beta} &amp;= \\underset{\\boldsymbol \\beta}{\\mathop{\\mathrm{arg\\,min}}} \\sum_{i=1}^n \\left(y_i - x_i^\\text{T}\\boldsymbol \\beta\\right)^2 \\\\ &amp;= \\underset{\\boldsymbol \\beta}{\\mathop{\\mathrm{arg\\,min}}} \\big( \\mathbf y - \\mathbf{X} \\boldsymbol \\beta \\big)^\\text{T}\\big( \\mathbf y - \\mathbf{X} \\boldsymbol \\beta \\big) \\end{align} \\] Classic solution can be obtained by taking the derivative of RSS w.r.t \\(\\boldsymbol \\beta\\) and set it to zero. This leads to the well known normal equation: \\[ \\begin{align} \\frac{\\partial \\text{RSS}}{\\partial \\boldsymbol \\beta} &amp;= -2 \\mathbf{X}^\\text{T}(\\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta) \\doteq 0 \\\\ \\Longrightarrow \\quad \\mathbf{X}^\\text{T}\\mathbf{y}&amp;= \\mathbf{X}^\\text{T}\\mathbf{X}\\boldsymbol \\beta \\end{align} \\] Assuming that \\(\\mathbf{X}\\) is full rank, then \\(\\mathbf{X}^\\text{T}\\mathbf{X}\\) is invertible. Then, we have \\[ \\widehat{\\boldsymbol \\beta} = (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}\\mathbf{y} \\] Some additional concepts are frequently used. The fitted values \\(\\widehat{\\mathbf{y}}\\) are essentially the prediction of the original \\(n\\) training data points: \\[ \\begin{align} \\widehat{\\mathbf{y}} =&amp; \\mathbf{X}\\boldsymbol \\beta\\\\ =&amp; \\underbrace{\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}}_{\\mathbf{H}} \\mathbf{y}\\\\ \\doteq&amp; \\mathbf{H}_{n \\times n} \\mathbf{y} \\end{align} \\] where \\(\\mathbf{H}\\) is called the hat matrix. It is a projection matrix that projects any vector (\\(\\mathbf{y}\\) in our case) onto the column space of \\(\\mathbf{X}\\). A project matrix enjoys two properties Symmetric: \\(\\mathbf{H}^\\text{T}= \\mathbf{H}\\) Idempotent \\(\\mathbf{H}\\mathbf{H}= \\mathbf{H}\\) The residuals \\(\\mathbf{r}\\) can also be obtained using the hat matrix: \\[ \\mathbf{r}= \\mathbf{y}- \\widehat{\\mathbf{y}} = (\\mathbf{I}- \\mathbf{H}) \\mathbf{y}\\] From the properties of a projection matrix, we also know that \\(\\mathbf{r}\\) should be orthogonal to any vector from the column space of \\(\\mathbf{X}\\). Hence, \\[\\mathbf{X}^\\text{T}\\mathbf{r}= \\mathbf{0}_{p \\times 1}\\] The residuals is also used to estimate the error variance: \\[\\widehat\\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^n r_i^2 = \\frac{\\text{RSS}}{n-p}\\] When the data are indeed generated from a linear model, and with suitable conditions on the design matrix and random errors \\(\\boldsymbol \\epsilon\\), we can conclude that \\(\\widehat{\\boldsymbol \\beta}\\) is an unbiased estimator of \\(\\boldsymbol \\beta\\). Its variance-covariance matrix satisfies \\[ \\begin{align} \\text{Var}(\\widehat{\\boldsymbol \\beta}) &amp;= \\text{Var}\\big( (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}\\mathbf{y}\\big) \\nonumber \\\\ &amp;= \\text{Var}\\big( (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}(\\mathbf{X}\\boldsymbol \\beta+ \\boldsymbol \\epsilon) \\big) \\nonumber \\\\ &amp;= \\text{Var}\\big( (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}\\boldsymbol \\epsilon) \\big) \\nonumber \\\\ &amp;= (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{I}\\sigma^2 \\nonumber \\\\ &amp;= (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\sigma^2 \\end{align} \\] All of the above mentioned results are already implemented in R through the lm() function to fit a linear regression. 3.3 Using the lm() Function Lets consider a simple regression that uses age and distance to model price. We will save the fitted object as lm.fit lm.fit = lm(price ~ age + distance, data = realestate) This syntax contains three components: data = specifies the dataset The outcome variable should be on the left hand side of ~ The covariates should be on the right hand side of ~ To look at the detailed model fitting results, use the summary() function lm.summary = summary(lm.fit) lm.summary ## ## Call: ## lm(formula = price ~ age + distance, data = realestate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.032 -4.742 -1.037 4.533 71.930 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.8855858 0.9677644 51.547 &lt; 2e-16 *** ## age -0.2310266 0.0420383 -5.496 6.84e-08 *** ## distance -0.0072086 0.0003795 -18.997 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.73 on 411 degrees of freedom ## Multiple R-squared: 0.4911, Adjusted R-squared: 0.4887 ## F-statistic: 198.3 on 2 and 411 DF, p-value: &lt; 2.2e-16 This shows that both age and distance are highly significant for predicting the price. In fact, this fitted object (lm.fit) and the summary object (lm.summary) are both saved as a list. This is pretty common to handle an output object with many things involved. We may peek into this object to see what are provided using a $ after the object. The str() function can also display all the items in a list. str(lm.summary) Usually, printing out the summary is sufficient. However, further details can be useful for other purposes. For example, if we interested in the residual vs. fits plot, we may use plot(lm.fit$fitted.values, lm.fit$residuals, xlab = &quot;Fitted Values&quot;, ylab = &quot;Residuals&quot;, col = &quot;darkorange&quot;, pch = 19, cex = 0.5) It seems that the error variance is not constant (as a function of the fitted values), hence additional techniques may be required to handle this issue. However, that is beyond the scope of this book. 3.3.1 Adding Covariates It is pretty simple if we want to include additional variables. This is usually done by connecting them with the + sign on the right hand side of ~. R also provide convenient ways to include interactions and higher order terms. The following code with the interaction term between age and distance, and a squared term of distance should be self-explanatory. lm.fit2 = lm(price ~ age + distance + age*distance + I(distance^2), data = realestate) summary(lm.fit2) ## ## Call: ## lm(formula = price ~ age + distance + age * distance + I(distance^2), ## data = realestate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.117 -4.160 -0.594 3.548 69.683 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.454e+01 1.099e+00 49.612 &lt; 2e-16 *** ## age -2.615e-01 4.931e-02 -5.302 1.87e-07 *** ## distance -1.603e-02 1.133e-03 -14.152 &lt; 2e-16 *** ## I(distance^2) 1.907e-06 2.416e-07 7.892 2.75e-14 *** ## age:distance 8.727e-06 4.615e-05 0.189 0.85 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.939 on 409 degrees of freedom ## Multiple R-squared: 0.5726, Adjusted R-squared: 0.5684 ## F-statistic: 137 on 4 and 409 DF, p-value: &lt; 2.2e-16 If you choose to include all covariates presented in the data, then simply use . on the right hand side of ~. However, you should always be careful when doing this because some dataset would contain meaningless variables such as subject ID. lm.fit3 = lm(price ~ ., data = realestate) 3.3.2 Categorical Variables The store variable has several different values. We can see that it has 11 different values. One strategy is to model this as a continuous variable. However, we may also consider to discretize it. For example, we may create a new variable, say store.cat, defined as follows table(realestate$stores) ## ## 0 1 2 3 4 5 6 7 8 9 10 ## 67 46 24 46 31 67 37 31 30 25 10 # define a new factor variable realestate$store.cat = as.factor((realestate$stores &gt; 0) + (realestate$stores &gt; 4)) table(realestate$store.cat) ## ## 0 1 2 ## 67 147 200 levels(realestate$store.cat) = c(&quot;None&quot;, &quot;Several&quot;, &quot;Many&quot;) head(realestate$store.cat) ## [1] Many Many Many Many Many Several ## Levels: None Several Many This variable is defined as a factor, which is often used for categorical variables. Since this variable has three different categories, if we include it in the linear regression, it will introduce two additional variables (using the third as the reference): lm.fit3 = lm(price ~ age + distance + store.cat, data = realestate) summary(lm.fit3) ## ## Call: ## lm(formula = price ~ age + distance + store.cat, data = realestate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.656 -5.360 -0.868 3.913 76.797 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 43.712887 1.751608 24.956 &lt; 2e-16 *** ## age -0.229882 0.040095 -5.733 1.91e-08 *** ## distance -0.005404 0.000470 -11.497 &lt; 2e-16 *** ## store.catSeveral 0.838228 1.435773 0.584 0.56 ## store.catMany 8.070551 1.646731 4.901 1.38e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.279 on 409 degrees of freedom ## Multiple R-squared: 0.5395, Adjusted R-squared: 0.535 ## F-statistic: 119.8 on 4 and 409 DF, p-value: &lt; 2.2e-16 There are usually two types of categorical variables: Ordinal: the numbers representing each category is ordered, e.g., how many stores in the neighborhood. Oftentimes nominal data can be treated as a continuous variable. Nominal: they are not ordered and can be represented using either numbers or letters, e.g., ethnic group. The above example is treating store.cat as a nominal variable, and the lm() function is using dummy variables for each category. This should be our default approach to handle nominal variables. 3.4 Model Selection Criteria We will use the diabetes dataset from the lars package as a demonstration of model selection. Ten baseline variables include age, sex, body mass index, average blood pressure, and six blood serum measurements. These measurements were obtained for each of n = 442 diabetes patients, as well as the outcome of interest, a quantitative measure of disease progression one year after baseline. More details are available in Efron et al. (2004). Our goal is to select a linear model, preferably with a small number of variables, that can predict the outcome. To select the best model, commonly used strategies include Marrows \\(C_p\\), AIC (Akaike information criterion) and BIC (Bayesian information criterion). Further derivations will be provide at a later section. # load the diabetes data library(lars) ## Loaded lars 1.2 data(diabetes) diab = data.frame(cbind(diabetes$x, &quot;Y&quot; = diabetes$y)) # fit linear regression with all covariates lm.fit = lm(Y~., data=diab) The idea of model selection is to apply some penalty on the number of parameters used in the model. In general, they are usually in the form of \\[\\text{Goodness-of-Fit} + \\text{Complexity Penality}\\] 3.4.1 Using Marrows \\(C_p\\) For example, the Marrows \\(C_p\\) criterion minimize the following quantity (a derivation is provided at Section 3.6): \\[\\text{RSS} + 2 p \\widehat\\sigma_{\\text{full}}^2\\] Note that the \\(\\sigma_{\\text{full}}^2\\) refers to the residual variance estimation based on the full model, i.e., will all variables. Hence, this formula cannot be used when \\(p &gt; n\\) because you would not be able to obtain a valid estimation of \\(\\sigma_{\\text{full}}^2\\). Nonetheless, we can calculate this quantity with the diabetes dataset # number of variables (including intercept) p = 11 n = nrow(diab) # obtain residual sum of squares RSS = sum(residuals(lm.fit)^2) # use the formula directly to calculate the Cp criterion Cp = RSS + 2*p*summary(lm.fit)$sigma^2 Cp ## [1] 1328502 We can compare this with another sub-model, say, with just age and glu: lm.fit_sub = lm(Y~ age + glu, data=diab) # obtain residual sum of squares RSS_sub = sum(residuals(lm.fit_sub)^2) # use the formula directly to calculate the Cp criterion Cp_sub = RSS_sub + 2*3*summary(lm.fit)$sigma^2 Cp_sub ## [1] 2240019 Comparing this with the previous one, the full model is better. 3.4.2 Using AIC and BIC Calculating the AIC and BIC criteria in R is a lot simpler, with the existing functions. The AIC score is given by \\[-2 \\text{Log-likelihood} + 2 p,\\] while the BIC score is given by \\[-2 \\text{Log-likelihood} + \\log(n) p,\\] Interestingly, when assuming that the error distribution is Gaussian, the log-likelihood part is just a function of the RSS. In general, AIC performs similarly to \\(C_p\\), while BIC tend to select a much smaller set due to the larger penalty. Theoretically, both AIC and \\(C_p\\) are interested in the prediction error, regardless of whether the model is specified correctly, while BIC is interested in selecting the true set of variables, while assuming that the true model is being considered. The AIC score can be done using the AIC() function. We can match this result by writing out the normal density function and plug in the estimated parameters. Note that this requires one additional parameter, which is the variance. Hence the total number of parameters is 12. We can calculate this with our own code: # ?AIC # a build-in function for calculating AIC using -2log likelihood AIC(lm.fit) ## [1] 4795.985 # Match the result n*log(RSS/n) + n + n*log(2*pi) + 2 + 2*p ## [1] 4795.985 Alternatively, the extractAIC() function can calculate both AIC and BIC. However, note that the n + n*log(2*pi) + 2 part in the above code does not change regardless of how many parameters we use. Hence, this quantify does not affect the comparison between different models. Then we can safely remove this part and only focus on the essential ones. # ?extractAIC # AIC for the full model extractAIC(lm.fit) ## [1] 11.000 3539.643 n*log(RSS/n) + 2*p ## [1] 3539.643 # BIC for the full model extractAIC(lm.fit, k = log(n)) ## [1] 11.000 3584.648 n*log(RSS/n) + log(n)*p ## [1] 3584.648 Now, we can compare AIC or BIC using of two different models and select whichever one that gives a smaller value. For example the AIC of the previous sub-model is # AIC for the sub-model extractAIC(lm.fit_sub) ## [1] 3.000 3773.077 3.5 Model Selection Algorithms In previous examples, we have to manually fit two models and calculate their respective selection criteria and compare them. This is a rather tedious process if we have many variables and a huge number of combinations to consider. To automatically compare different models and select the best one, there are two common computational approaches: best subset regression and step-wise regression. As their name suggest, the best subset selection will exhaust all possible combination of variables, while the step-wise regression would adjust the model by adding or subtracting one variable at a time to reach the best model. 3.5.1 Best Subset Selection with leaps Since the penalty is only affected by the number of variables, we may first choose the best model with the smallest RSS for each model size, and then compare across these models by attaching the penalty terms of their corresponding sizes. The leaps package can be used to calculate the best model of each model size. It essentially performs an exhaustive search, however, still utilizing some tricks to skip some really bad models. Note that the leaps package uses the data matrix directly, instead of specifying a formula. library(leaps) # The package specifies the X matrix and outcome y vector RSSleaps = regsubsets(x = as.matrix(diab[, -11]), y = diab[, 11]) summary(RSSleaps, matrix=T) ## Subset selection object ## 10 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## sex FALSE FALSE ## bmi FALSE FALSE ## map FALSE FALSE ## tc FALSE FALSE ## ldl FALSE FALSE ## hdl FALSE FALSE ## tch FALSE FALSE ## ltg FALSE FALSE ## glu FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: exhaustive ## age sex bmi map tc ldl hdl tch ltg glu ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; The results is summarized in a matrix, with each row representing a model size. The \"*\" sign indicates that the variable is include in the model for the corresponding size. Hence, there should be only one of such in the first row, two in the second row, etc. By default, the algorithm would only consider models up to size 8. This is controlled by the argument nvmax. If we want to consider larger model sizes, then set this to a larger number. However, be careful that this many drastically increase the computational cost. # Consider maximum of 10 variables RSSleaps = regsubsets(x = as.matrix(diab[, -11]), y = diab[, 11], nvmax = 10) summary(RSSleaps,matrix=T) ## Subset selection object ## 10 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## sex FALSE FALSE ## bmi FALSE FALSE ## map FALSE FALSE ## tc FALSE FALSE ## ldl FALSE FALSE ## hdl FALSE FALSE ## tch FALSE FALSE ## ltg FALSE FALSE ## glu FALSE FALSE ## 1 subsets of each size up to 10 ## Selection Algorithm: exhaustive ## age sex bmi map tc ldl hdl tch ltg glu ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 9 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; # Obtain the matrix that indicates the variables sumleaps = summary(RSSleaps, matrix = T) # This object includes the RSS results, which is needed to calculate the scores sumleaps$rss ## [1] 1719582 1416694 1362708 1331430 1287879 1271491 1267805 1264712 1264066 1263983 # This matrix indicates whether a variable is in the best model(s) sumleaps$which ## (Intercept) age sex bmi map tc ldl hdl tch ltg glu ## 1 TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## 2 TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## 3 TRUE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE ## 4 TRUE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE TRUE FALSE ## 5 TRUE FALSE TRUE TRUE TRUE FALSE FALSE TRUE FALSE TRUE FALSE ## 6 TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE TRUE FALSE ## 7 TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE FALSE ## 8 TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE ## 9 TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## 10 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE # The package automatically produces the Cp statistic sumleaps$cp ## [1] 148.352561 47.072229 30.663634 21.998461 9.148045 5.560162 6.303221 7.248522 ## [9] 9.028080 11.000000 We can calculate different model selection criteria with the best models of each size. The model fitting result already produces the \\(C_p\\) and BIC results. However, please note that both quantities are modified slightly. For the \\(C_p\\) statistics, the quantity is divided by the estimated error variance, and also adjust for the sample size. For the BIC, the difference is a constant regardless of the model size. Hence these difference do will not affect the model selection result because the modification is the same regardless of the number of variables. modelsize=apply(sumleaps$which,1,sum) Cp = sumleaps$rss/(summary(lm.fit)$sigma^2) + 2*modelsize - n; AIC = n*log(sumleaps$rss/n) + 2*modelsize; BIC = n*log(sumleaps$rss/n) + modelsize*log(n); # Comparing the Cp scores cbind(&quot;Our Cp&quot; = Cp, &quot;leaps Cp&quot; = sumleaps$cp) ## Our Cp leaps Cp ## 1 148.352561 148.352561 ## 2 47.072229 47.072229 ## 3 30.663634 30.663634 ## 4 21.998461 21.998461 ## 5 9.148045 9.148045 ## 6 5.560162 5.560162 ## 7 6.303221 6.303221 ## 8 7.248522 7.248522 ## 9 9.028080 9.028080 ## 10 11.000000 11.000000 # Comparing the BIC results. The difference is a constant, # which is the score of an intercept model cbind(&quot;Our BIC&quot; = BIC, &quot;leaps BIC&quot; = sumleaps$bic, &quot;Difference&quot; = BIC-sumleaps$bic, &quot;Intercept Score&quot; = n*log(sum((diab[,11] - mean(diab[,11]))^2/n))) ## Our BIC leaps BIC Difference Intercept Score ## 1 3665.879 -174.1108 3839.99 3839.99 ## 2 3586.331 -253.6592 3839.99 3839.99 ## 3 3575.249 -264.7407 3839.99 3839.99 ## 4 3571.077 -268.9126 3839.99 3839.99 ## 5 3562.469 -277.5210 3839.99 3839.99 ## 6 3562.900 -277.0899 3839.99 3839.99 ## 7 3567.708 -272.2819 3839.99 3839.99 ## 8 3572.720 -267.2702 3839.99 3839.99 ## 9 3578.585 -261.4049 3839.99 3839.99 ## 10 3584.648 -255.3424 3839.99 3839.99 Finally, we may select the best model, using any of the criteria. The following code would produced a plot to visualize it. We can see that BIC selects 6 variables, while both AIC and \\(C_p\\) selects 7. # Rescale Cp, AIC and BIC to (0,1). inrange &lt;- function(x) { (x - min(x)) / (max(x) - min(x)) } Cp = inrange(Cp) BIC = inrange(BIC) AIC = inrange(AIC) plot(range(modelsize), c(0, 0.4), type=&quot;n&quot;, xlab=&quot;Model Size (with Intercept)&quot;, ylab=&quot;Model Selection Criteria&quot;, cex.lab = 1.5) points(modelsize, Cp, col = &quot;green4&quot;, type = &quot;b&quot;, pch = 19) points(modelsize, AIC, col = &quot;orange&quot;, type = &quot;b&quot;, pch = 19) points(modelsize, BIC, col = &quot;purple&quot;, type = &quot;b&quot;, pch = 19) legend(&quot;topright&quot;, legend=c(&quot;Cp&quot;, &quot;AIC&quot;, &quot;BIC&quot;), col=c(&quot;green4&quot;, &quot;orange&quot;, &quot;purple&quot;), lty = rep(1, 3), pch = 19, cex = 1.7) 3.5.2 Step-wise regression using step() The idea of step-wise regression is very simple: we start with a certain model (e.g. the intercept or the full mode), and add or subtract one variable at a time by making the best decision to improve the model selection score. The step() function implements this procedure. The following example starts with the full model and uses AIC as the selection criteria (default of the function). After removing several variables, the model ends up with six predictors. # k = 2 (AIC) is default; step(lm.fit, direction=&quot;both&quot;, k = 2) ## Start: AIC=3539.64 ## Y ~ age + sex + bmi + map + tc + ldl + hdl + tch + ltg + glu ## ## Df Sum of Sq RSS AIC ## - age 1 82 1264066 3537.7 ## - hdl 1 663 1264646 3537.9 ## - glu 1 3080 1267064 3538.7 ## - tch 1 3526 1267509 3538.9 ## &lt;none&gt; 1263983 3539.6 ## - ldl 1 5799 1269782 3539.7 ## - tc 1 10600 1274583 3541.3 ## - sex 1 45000 1308983 3553.1 ## - ltg 1 56015 1319998 3556.8 ## - map 1 72103 1336086 3562.2 ## - bmi 1 179028 1443011 3596.2 ## ## Step: AIC=3537.67 ## Y ~ sex + bmi + map + tc + ldl + hdl + tch + ltg + glu ## ## Df Sum of Sq RSS AIC ## - hdl 1 646 1264712 3535.9 ## - glu 1 3001 1267067 3536.7 ## - tch 1 3543 1267608 3536.9 ## &lt;none&gt; 1264066 3537.7 ## - ldl 1 5751 1269817 3537.7 ## - tc 1 10569 1274635 3539.4 ## + age 1 82 1263983 3539.6 ## - sex 1 45831 1309896 3551.4 ## - ltg 1 55963 1320029 3554.8 ## - map 1 73850 1337915 3560.8 ## - bmi 1 179079 1443144 3594.2 ## ## Step: AIC=3535.9 ## Y ~ sex + bmi + map + tc + ldl + tch + ltg + glu ## ## Df Sum of Sq RSS AIC ## - glu 1 3093 1267805 3535.0 ## - tch 1 3247 1267959 3535.0 ## &lt;none&gt; 1264712 3535.9 ## - ldl 1 7505 1272217 3536.5 ## + hdl 1 646 1264066 3537.7 ## + age 1 66 1264646 3537.9 ## - tc 1 26840 1291552 3543.2 ## - sex 1 46382 1311094 3549.8 ## - map 1 73536 1338248 3558.9 ## - ltg 1 97509 1362221 3566.7 ## - bmi 1 178537 1443249 3592.3 ## ## Step: AIC=3534.98 ## Y ~ sex + bmi + map + tc + ldl + tch + ltg ## ## Df Sum of Sq RSS AIC ## - tch 1 3686 1271491 3534.3 ## &lt;none&gt; 1267805 3535.0 ## - ldl 1 7472 1275277 3535.6 ## + glu 1 3093 1264712 3535.9 ## + hdl 1 738 1267067 3536.7 ## + age 1 0 1267805 3537.0 ## - tc 1 26378 1294183 3542.1 ## - sex 1 44686 1312491 3548.3 ## - map 1 82154 1349959 3560.7 ## - ltg 1 102520 1370325 3567.3 ## - bmi 1 189970 1457775 3594.7 ## ## Step: AIC=3534.26 ## Y ~ sex + bmi + map + tc + ldl + ltg ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1271491 3534.3 ## + tch 1 3686 1267805 3535.0 ## + glu 1 3532 1267959 3535.0 ## + hdl 1 395 1271097 3536.1 ## + age 1 11 1271480 3536.3 ## - ldl 1 39378 1310869 3545.7 ## - sex 1 41858 1313349 3546.6 ## - tc 1 65237 1336728 3554.4 ## - map 1 79627 1351119 3559.1 ## - bmi 1 190586 1462077 3594.0 ## - ltg 1 294094 1565585 3624.2 ## ## Call: ## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab) ## ## Coefficients: ## (Intercept) sex bmi map tc ldl ltg ## 152.1 -226.5 529.9 327.2 -757.9 538.6 804.2 We can also use different settings, such as which model to start with, which is the minimum/maximum model, and do we allow to adding/subtracting. # use BIC (k = log(n))instead of AIC # trace = 0 will suppress the output of intermediate steps step(lm.fit, direction=&quot;both&quot;, k = log(n), trace=0) ## ## Call: ## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab) ## ## Coefficients: ## (Intercept) sex bmi map tc ldl ltg ## 152.1 -226.5 529.9 327.2 -757.9 538.6 804.2 # Start with an intercept model, and use forward selection (adding only) step(lm(Y~1, data=diab), scope=list(upper=lm.fit, lower=~1), direction=&quot;forward&quot;, trace=0) ## ## Call: ## lm(formula = Y ~ bmi + ltg + map + tc + sex + ldl, data = diab) ## ## Coefficients: ## (Intercept) bmi ltg map tc sex ldl ## 152.1 529.9 804.2 327.2 -757.9 -226.5 538.6 We can see that these results are slightly different from the best subset selection. So which is better? Of course the best subset selection is better because it considers all possible candidates, which step-wise regression may stuck at a sub-optimal model, while adding and subtracting any variable do not benefit further. Hence, the results of step-wise regression may be unstable. On the other hand, best subset selection not really feasible for high-dimensional problems because of the computational cost. 3.6 Derivation of Marrows \\(C_p\\) Suppose we have a set of training data \\(\\cal{D}_n = \\{x_i, \\color{DodgerBlue}{y_i}\\}_{i=1}^n\\) and a set of testing data, with the same covariates \\(\\cal{D}_n^\\ast = \\{x_i, \\color{OrangeRed}{y_i^\\ast}\\}_{i=1}^n\\). Hence, this is an in-sample prediction problem. However, the \\(\\color{OrangeRed}{y_i^\\ast}\\)s are newly observed. Assuming that the data are generated from a linear model, i.e., in vector form, \\[\\color{DodgerBlue}{\\mathbf{y}}= \\boldsymbol \\mu+ \\color{DodgerBlue}{\\mathbf{e}}= \\mathbf{X}\\boldsymbol \\beta+ \\color{DodgerBlue}{\\mathbf{e}},\\] and \\[\\color{OrangeRed}{\\mathbf{y}^\\ast}= \\boldsymbol \\mu+ \\color{OrangeRed}{\\mathbf{e}^\\ast}= \\mathbf{X}\\boldsymbol \\beta+ \\color{OrangeRed}{\\mathbf{e}^\\ast},\\] where the error terms are i.i.d with mean 0 and variance \\(\\sigma^2\\). We want to know what is the best model that predicts \\(\\color{OrangeRed}{\\mathbf{y}^\\ast}\\). Lets look at the testing error first: \\[\\begin{align} \\text{E}[\\color{OrangeRed}{\\text{Testing Error}}] =&amp; ~\\text{E}\\lVert \\color{OrangeRed}{\\mathbf{y}^\\ast}- \\mathbf{X}\\color{DodgerBlue}{\\widehat{\\boldsymbol \\beta}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\color{OrangeRed}{\\mathbf{y}^\\ast}- \\mathbf{X}\\boldsymbol \\beta) + (\\mathbf{X}\\boldsymbol \\beta- \\mathbf{X}\\color{DodgerBlue}{\\widehat{\\boldsymbol \\beta}}) \\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert \\color{OrangeRed}{\\mathbf{e}^\\ast}\\rVert^2 + \\text{E}\\lVert \\mathbf{X}(\\color{DodgerBlue}{\\widehat{\\boldsymbol \\beta}}- \\boldsymbol \\beta) \\rVert^2 \\\\ =&amp; ~\\color{OrangeRed}{n \\sigma^2} + \\text{E}\\big[ \\text{Trace}\\big( (\\color{DodgerBlue}{\\widehat{\\boldsymbol \\beta}}- \\boldsymbol \\beta)^\\text{T}\\mathbf{X}^\\text{T}\\mathbf{X}(\\color{DodgerBlue}{\\widehat{\\boldsymbol \\beta}}- \\boldsymbol \\beta) \\big) \\big] \\\\ =&amp; ~\\color{OrangeRed}{n \\sigma^2} + \\text{Trace}\\big(\\mathbf{X}^\\text{T}\\mathbf{X}\\text{Cov}(\\color{DodgerBlue}{\\widehat{\\boldsymbol \\beta}})\\big) \\\\ =&amp; ~\\color{OrangeRed}{n \\sigma^2} + \\color{DodgerBlue}{p \\sigma^2}. \\end{align}\\] In the above, we used properties \\(\\text{Trace}(ABC) = \\text{Trace}(CBA)\\) \\(\\text{E}[\\text{Trace}(A)] = \\text{Trace}(\\text{E}[A])\\) On the other hand, the training error is \\[\\begin{align} \\text{E}[\\color{DodgerBlue}{\\text{Training Error}}] =&amp; ~\\text{E}\\lVert \\mathbf{y}- \\color{DodgerBlue}{\\mathbf{y}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\mathbf{I}- \\mathbf{H})(\\mathbf{X}\\boldsymbol \\beta+ \\color{DodgerBlue}{\\mathbf{e}}) \\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\mathbf{I}- \\mathbf{H})\\color{DodgerBlue}{\\mathbf{e}}\\rVert^2 \\\\ =&amp; ~\\text{E}[\\text{Trace}(\\color{DodgerBlue}{\\mathbf{e}}^\\text{T}(\\mathbf{I}- \\mathbf{H})^\\text{T}(\\mathbf{I}- \\mathbf{H}) \\color{DodgerBlue}{\\mathbf{e}})]\\\\ =&amp; ~\\text{Trace}((\\mathbf{I}- \\mathbf{H})^\\text{T}(\\mathbf{I}- \\mathbf{H}) \\text{Cov}(\\color{DodgerBlue}{\\mathbf{e}})]\\\\ =&amp; ~\\color{DodgerBlue}{(n - p) \\sigma^2}. \\end{align}\\] In the above, we further used properties \\(\\mathbf{H}\\) and \\(\\mathbf{I}- \\mathbf{H}\\) are projection matrices \\(\\mathbf{H}\\mathbf{X}= \\mathbf{X}\\) If we contrast the two results above, the difference between the training and testing errors is \\(2 p \\sigma^2\\). Hence, if we can obtain a valid estimation of \\(\\sigma^2\\), then the training error plus \\(2 p \\widehat{\\sigma}^2\\) is a good approximation of the testing error, which we want to minimize. And that is exactly what Marrows \\(C_p\\) does. We can also generalize this result to the case when the underlying model is not a linear model. Assume that \\[\\color{DodgerBlue}{\\mathbf{y}}= f(\\mathbf{X}) + \\color{DodgerBlue}{\\mathbf{e}}= \\boldsymbol \\mu+ \\color{DodgerBlue}{\\mathbf{e}},\\] and \\[\\color{OrangeRed}{\\mathbf{y}^\\ast}= f(\\mathbf{X}) + \\color{OrangeRed}{\\mathbf{e}^\\ast}= \\boldsymbol \\mu+ \\color{OrangeRed}{\\mathbf{e}^\\ast}.\\] In this case, a linear model would not estimate \\(\\boldsymbol \\mu\\). Instead, it is only capable to produce the best linear approximation of \\(\\boldsymbol \\mu\\) using the columns in \\(\\mathbf{X}\\), which is \\(\\mathbf{H}\\boldsymbol \\mu\\), the projection of \\(\\boldsymbol \\mu\\) on the column space of \\(\\mathbf{X}\\). In general, \\(\\mathbf{H}\\boldsymbol \\mu\\neq \\boldsymbol \\mu\\), and the remaining part \\(\\boldsymbol \\mu- \\mathbf{H}\\boldsymbol \\mu\\) is called bias. This is a new concept that will appear frequently in this book. Selection variables will essentially trade between bias and variance of a model. The following derivation shows this phenomenon: \\[\\begin{align} \\text{E}[\\color{OrangeRed}{\\text{Testing Error}}] =&amp; ~\\text{E}\\lVert \\color{OrangeRed}{\\mathbf{y}^\\ast}- \\mathbf{X}\\color{DodgerBlue}{\\widehat{\\boldsymbol \\beta}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert \\color{OrangeRed}{\\mathbf{y}^\\ast}- \\mathbf{H}\\color{DodgerBlue}{\\mathbf{y}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\color{OrangeRed}{\\mathbf{y}^\\ast}- \\boldsymbol \\mu) + (\\boldsymbol \\mu- \\mathbf{H}\\boldsymbol \\mu) + (\\mathbf{H}\\boldsymbol \\mu- \\mathbf{H}\\color{DodgerBlue}{\\mathbf{y}}) \\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert \\color{OrangeRed}{\\mathbf{y}^\\ast}- \\boldsymbol \\mu\\rVert^2 + \\text{E}\\lVert \\boldsymbol \\mu- \\mathbf{H}\\boldsymbol \\mu\\rVert^2 + \\text{E}\\lVert \\mathbf{H}\\boldsymbol \\mu- \\mathbf{H}\\color{DodgerBlue}{\\mathbf{y}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert \\color{OrangeRed}{\\mathbf{e}^\\ast}\\rVert^2 + \\text{E}\\lVert \\boldsymbol \\mu- \\mathbf{H}\\boldsymbol \\mu\\rVert^2 + \\text{E}\\lVert \\mathbf{H}\\color{DodgerBlue}{\\mathbf{e}}\\rVert^2 \\\\ =&amp; ~\\color{OrangeRed}{n \\sigma^2} + \\text{Bias}^2 + \\color{DodgerBlue}{p \\sigma^2}, \\end{align}\\] while the training error is \\[\\begin{align} \\text{E}[\\color{DodgerBlue}{\\text{Training Error}}] =&amp; ~\\text{E}\\lVert \\color{DodgerBlue}{\\mathbf{y}}- \\mathbf{X}\\color{DodgerBlue}{\\widehat{\\boldsymbol \\beta}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert \\color{DodgerBlue}{\\mathbf{y}}- \\mathbf{H}\\color{DodgerBlue}{\\mathbf{y}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\mathbf{I}- \\mathbf{H})(\\boldsymbol \\mu+ \\color{DodgerBlue}{\\mathbf{e}}) \\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\mathbf{I}- \\mathbf{H})\\boldsymbol \\mu\\rVert^2 + \\text{E}\\lVert (\\mathbf{I}- \\mathbf{H})\\color{DodgerBlue}{\\mathbf{e}}\\rVert^2\\\\ =&amp; ~\\text{Bias}^2 + \\color{DodgerBlue}{(n - p) \\sigma^2}. \\end{align}\\] We can notice again that the difference is \\(2p\\sigma^2\\). Note that this is regardless of whether the linear model is correct or not. Reference "],["optimization-basics.html", "Chapter 4 Optimization Basics 4.1 Basic Concept 4.2 Global vs. Local Optima 4.3 Example: Linear Regression using optim() 4.4 First and Second Order Properties 4.5 Algorithm 4.6 Second-order Methods 4.7 First-order Methods 4.8 Coordinate Descent 4.9 Stocastic Gradient Descent 4.10 Lagrangian Multiplier for Constrained Problems", " Chapter 4 Optimization Basics Optimization is heavily involved in statistics and machine learning. Almost all methods introduced in this book can be viewed as some form of optimization. It would be good to have some prior knowledge of it so that later chapters can use these concepts without difficulties. Especially, one should be familiar with concepts such as constrains, gradient methods, and be able to implement them using existing R functions. Since optimization is such a broad topic, we refer readers to Boyd and Vandenberghe (2004) and Nocedal and Wright (2006) for more further reading. We will use a slightly different set of notations in this Chapter so that we are consistent with the literature. This means that for the most part, we will use \\(x\\) as our parameter of interest and optimize a function \\(f(x)\\). This is in contrast to optimizing \\(\\theta\\) in a statistical model \\(f_\\theta(x)\\) where \\(x\\) is the observed data. However, in the example of linear regression, we may again switch back to the regular notation of \\(x^\\text{T} \\boldsymbol \\beta\\). These transitions will only happen under clear context and should not create ambiguity. 4.1 Basic Concept We usually consider a convex optimization problem (non-convex problems are a bit too involving although we will also see some examples of that), meaning that we optimize (minimize) a convex function in a convex domain. A convex function \\(f(\\mathbf{x})\\) maps some subset \\(C \\in \\mathbb{R}^p\\) into \\(\\mathbb{R}^p\\), but enjoys the property that \\[ f(t \\mathbf{x}_1 + (1 - t) \\mathbf{x}_2) \\leq t f(\\mathbf{x}_1) + ( 1- t) f(\\mathbf{x}_2), \\] for all \\(t \\in [0, 1]\\) and any two points \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\) in the domain of \\(f\\). Note that if you have a concave function (the bowl faces downwards) then \\(-f(\\mathbf{x})\\) would be convex. Examples of convex functions: Univariate functions: \\(x^2\\), \\(\\exp(x)\\), \\(-log(x)\\) Affine map: \\(a^\\text{T}\\mathbf{x}+ b\\) is both convex and concave A quadratic function \\(\\frac{1}{2}\\mathbf{x}^\\text{T}\\mathbf{A}\\mathbf{x}+ b^\\text{T}\\mathbf{x}+ c\\), if \\(\\mathbf{A}\\) is positive semidefinite All \\(p\\) norms are convex, following the Triangle inequality and properties of a norm. A sin function is neither convex or concave On the other hand, a convex set \\(C\\) means that if we have two points \\(x_1\\) and \\(x_2\\) in \\(C\\), the line segment joining these two points has to lie within \\(C\\), i.e., \\[\\mathbf{x}_1, \\mathbf{x}_2 \\in C \\quad \\Longrightarrow \\quad t \\mathbf{x}_1 + (1 - t) \\mathbf{x}_2 \\in C,\\] for all \\(t \\in [0, 1]\\). Examples of convex set include Real line: \\(\\mathbb{R}\\) Norm ball: \\(\\{ \\mathbf{x}: \\lVert \\mathbf{x}\\rVert \\leq r \\}\\) Hyperplane: \\(\\{ \\mathbf{x}: a^\\text{T}\\mathbf{x}= b \\}\\) Consider a simple optimization problem: \\[ \\text{minimize} \\quad f(x_1, x_2) = x_1^2 + x_2^2\\] Clearly, \\(f(x_1, x_2)\\) is a convex function, and we know that the solution of this problem is \\(x_1 = x_2 = 0\\). However, the problem might be a bit more complicated if we restrict that in a certain (convex) region, for example, \\[\\begin{align} &amp;\\underset{x_1, x_2}{\\text{minimize}} &amp; \\quad f(x_1, x_2) &amp;= x_1^2 + x_2^2 \\\\ &amp;\\text{subject to} &amp; x_1 + x_2 &amp;\\leq -1 \\\\ &amp; &amp; x_1 + x_2 &amp;&gt; -2 \\end{align}\\] Here the convex set \\(C = \\{x_1, x_2 \\in \\mathbb{R}: x_1 + x_2 \\leq -1 \\,\\, \\text{and} \\,\\, x_1 + x_2 &gt; -2\\}\\). And our problem looks like the following, which attains it minimum at \\((-0.5, -0.5)\\). In general, we will be dealing with a problem in the form of \\[\\begin{align} &amp;\\underset{\\mathbf{x}}{\\text{minimize}} &amp; \\quad f(\\mathbf{x}) \\\\ &amp;\\text{subject to} &amp; g_i(\\mathbf{x}) &amp; \\leq 0, \\, i = 1,\\ldots, m \\\\ &amp; &amp; h_j(\\mathbf{x}) &amp;= 0, \\, j = 1,\\ldots, k \\end{align}\\] where \\(g_i(\\mathbf{x})\\)s are a set of inequality constrains, and \\(h_j(\\mathbf{x})\\)s are equality constrains. There are established result showing what type of constrains would lead to a convex set, but lets assuming for now that we will be dealing a well behaved problem. We shall see in later chapters that many models such as, Lasso, Ridge and support vector machines can all be formulated into this form. 4.2 Global vs. Local Optima Although we would like to deal with convex optimization problems, non-convex problems appears more and more frequently. For example, deep learning models are almost always non-convex except overly simplified ones. However, for convex optimization problems, a local minimum is also a global minimum, i.e., a \\(x^\\ast\\) such that for any \\(x\\) in the feasible set, \\(f(x^\\ast) \\leq f(x)\\). This can be achieved by a variety of descent algorithms, to be introduced. However, for non-convex problems, we may still be interested in a local minimum, which satisfies that for any \\(x\\) in a neighboring set of \\(x^\\ast\\), \\(f(x^\\ast) \\leq f(x)\\). The comparison of these two cases can be demonstrated in the following plots. Again, a descent algorithm can help us find a local minimum, except for some very special cases, such as a saddle point. However, we will not discuss these issues in this book. 4.3 Example: Linear Regression using optim() Although completely not necessary, we may also view linear regression as an optimization problem. This is of course an unconstrained problem, meaning that \\(C \\in \\mathbb{R}^p\\). Such problems can be solved using the optim() function. Also, lets temporarily switch back to the \\(\\boldsymbol \\beta\\) notation of parameters. Hence, if we observe a set of observations \\(\\{\\mathbf{x}_i, y_i\\}_{i = 1}^n\\), our optimization problem is to minimize the objection function, i.e., residual sum of squares (RSS): \\[\\begin{align} \\underset{\\boldsymbol \\beta}{\\text{minimize}} \\quad f(\\boldsymbol \\beta) = \\frac{1}{n} \\sum_i (y_i - \\mathbf{x}_i^\\text{T}\\boldsymbol \\beta)^2 \\\\ \\end{align}\\] We generate 200 random observations, and also write a function to calculate the RSS for any given \\(\\boldsymbol \\beta\\) values. The objective function looks like the following: # generate data from a simple linear model set.seed(20) n = 200 x &lt;- cbind(1, rnorm(n)) y &lt;- x %*% c(0.5, 1) + rnorm(n) # calculate the residual sum of squares for a grid of beta values rss &lt;- function(b, trainx, trainy) sum((trainy - trainx %*% b)^2) Now the question is how to solve this problem. The optim() function uses the following syntax: # The solution can be solved by any optimization algorithm lm.optim &lt;- optim(par = c(2, 2), fn = rss, trainx = x, trainy = y) The par argument specifies an initial value, in this case, \\(\\beta_0 = \\beta_1 = 2\\) The fn argument specifies the name of an R function that can calculate the objective function. Please note that the first argument in this function has be the parameter being optimized, i.e, \\(\\boldsymbol \\beta\\). Also, it must be a vector, not a matrix or other types. The arguments trainx = x, trainy = y specifies any additional arguments that the objective function fn, i.e., rss needs. It behaves the same as if you are supplying this to rss. lm.optim ## $par ## [1] 0.4532072 0.9236502 ## ## $value ## [1] 203.5623 ## ## $counts ## function gradient ## 63 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL The result shows that the estimated parameters ($par) are 0.453 and 0.924, with a functional value 203.562. The convergence code is 0, meaning that the algorithm converged. The parameter estimates are almost the same as lm(), with small numerical errors. # The solution form lm() summary(lm(y ~ x - 1))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## x1 0.4530498 0.07177854 6.311772 1.773276e-09 ## x2 0.9236934 0.07226742 12.781602 1.136397e-27 What we will be introducing in the following are some basic approaches to solve such a numerical problem. We will start with unconstrained problems, then introduce constrained problems. 4.4 First and Second Order Properties These properties are usually applied to unconstrained optimization problems. They are essentially just describing the landscape around a point \\(\\mathbf{x}^\\ast\\) such that it becomes the local optimizer. Since we generally concerns a convex problem, a local solution is also the global solution. However, these properties are still generally applied when solving a non-convex problem. Note that these statements are multi-dimensional. First-Order Necessary Conditions: If \\(f\\) is continuously differentiable in an open neighborhood of local minimum \\(\\mathbf{x}^\\ast\\), then \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\). When we have a point \\(\\mathbf{x}^\\ast\\) with \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\), we call \\(\\mathbf{x}^\\ast\\) a stationary point. This is only a necessary condition, but not sufficient. Since example, \\(f(x) = x^3\\) has zero derivative at \\(x = 0\\), but this is not an optimizer. The figure in @ref(global_local) also contains such a point. TO further strengthen this, we have Second-order Necessary Conditions: If \\(f\\) is twice continuously differentiable in an open neighborhood of local minimum \\(\\mathbf{x}^\\ast\\), then \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\) and \\(\\nabla^2 f(\\mathbf{x}^\\ast)\\) is positive semi-definite. This does rule out some cases, with a higher cost (\\(f\\) needs to be twice continuously differentiable). But requiring positive semi-definite would not ensure everything. The same example \\(f(x) = x^3\\) still satisfies this, but its not a local minimum. A positive definite \\(\\nabla^2 f(\\mathbf{x}^\\ast)\\) would be sufficient: Second-order Sufficient Conditions: \\(f\\) is twice continuously differentiable in an open neighborhood of \\(\\mathbf{x}^\\ast\\). If \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\) and \\(\\nabla^2 f(\\mathbf{x}^\\ast)\\) is positive definite, i.e., \\[ \\nabla^2 f(\\mathbf{x}) = \\left(\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_i \\partial x_j}\\right) = \\mathbf{H}(\\mathbf{x}) \\succeq 0 \\] then \\(\\mathbf{x}^\\ast\\) is a strict local minimizer of \\(f\\). Here \\(\\mathbf{H}(\\mathbf{x})\\) is called the Hessian matrix, which will be frequently used in second-order methods. 4.5 Algorithm Most optimization algorithms follow the same idea: starting from a point \\(\\mathbf{x}^{(0)}\\) (which is usually specified by the user) and move to a new point \\(\\mathbf{x}^{(1)}\\) that improves the objective function value. Repeatedly performing this to get a sequence of points \\(\\mathbf{x}^{(0)}, \\mathbf{x}^{(1)}, \\ldots\\) until the certain stopping criterion is reached. A stopping criterion could be Using the gradient of the objective function: \\(\\lVert \\nabla f(\\mathbf{x}^{(k)}) \\rVert &lt; \\epsilon\\) Using the (relative) change of distance: \\(\\lVert \\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)} \\rVert / \\lVert \\mathbf{x}^{(k-1)}\\rVert&lt; \\epsilon\\) or \\(\\lVert \\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)} \\rVert &lt; \\epsilon\\) Using the (relative) change of functional value: \\(| f(\\mathbf{x}^{(k)}) - f(\\mathbf{x}^{(k-1)})| &lt; \\epsilon\\) or \\(| f(\\mathbf{x}^{(k)}) - f(\\mathbf{x}^{(k-1)})| / |f(\\mathbf{x}^{(k)})| &lt; \\epsilon\\) Stop at a pre-specified number of iterations. Most algorithms differ in terms of how to move from the current point \\(\\mathbf{x}^{(k)}\\) to the next, better target point \\(\\mathbf{x}^{(k+1)}\\). This may depend on the smoothness or structure of \\(f\\), constrains on the domain, computational complexity, memory limitation, and many others. 4.6 Second-order Methods 4.6.1 Newtons Method Now, lets discuss several specific methods. One of the oldest one is Newtons method. This is motivated form a quadratic approximation (essentially Taylor expansion) at a current point \\(\\mathbf{x}\\), \\[f(\\mathbf{x}^\\ast) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\text{T}(\\mathbf{x}^\\ast - \\mathbf{x}) + \\frac{1}{2} (\\mathbf{x}^\\ast - \\mathbf{x})^\\text{T}\\mathbf{H}(\\mathbf{x}) (\\mathbf{x}^\\ast - \\mathbf{x})\\] Our goal is to find a new stationary point \\(\\mathbf{x}^\\ast\\) such that \\(\\nabla f(\\mathbf{x}^\\ast) = 0\\). By taking derivative of the above equation on both sides, with respect to \\(\\mathbf{x}^\\ast\\), we need \\[0 = \\nabla f(\\mathbf{x}^\\ast) = 0 + \\nabla f(\\mathbf{x}) + (\\mathbf{x}^\\ast - \\mathbf{x})^\\text{T}\\mathbf{H}(\\mathbf{x})\\] which leads to \\[\\mathbf{x}^\\ast = \\mathbf{x}- \\mathbf{H}(\\mathbf{x})^{-1} \\nabla f(\\mathbf{x}).\\] Hence, if we are currently at a point \\(\\mathbf{x}^{(k)}\\), we need to calculate the gradient \\(\\nabla f(\\mathbf{x}^{(k)})\\) and Hessian \\(\\mathbf{H}(\\mathbf{x})\\) at this point, then move to the new point using \\(\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\mathbf{H}(\\mathbf{x}^{(k)})^{-1} \\nabla f(\\mathbf{x}^{(k)})\\). Some properties and things to concern regarding Newtons method: Newtons method is scale invariant, meaning that you do not need to worry about the step size. It is automatically taken care of by the Hessian matrix. However, in practice, the local approximation may not be accurate, which makes the new point \\(\\mathbf{x}^{(k+1)}\\) behaves differently than what we expect. Hence, it might still be safe to introduce a smaller step size \\(\\delta \\in (0, 1)\\) and move with \\[\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\delta \\, \\mathbf{H}(\\mathbf{x}^{(k)})^{-1} \\nabla f(\\mathbf{x}^{(k)})\\] There are also alternatives to take care of the step size. For example, line search is frequently used, which will try to find the optimal \\(\\delta\\) that minimizes the function \\[f(\\mathbf{x}^{(k)} + \\delta \\mathbf{v})\\] where the direction \\(\\mathbf{v}\\) in this case is \\(\\mathbf{v}= \\mathbf{H}(\\mathbf{x}^{(k)})^{-1} \\nabla f(\\mathbf{x}^{(k)})\\). It is also popular to use backtracking line search, which reduces the computational cost. The idea is to start with a large \\(\\delta\\) and gradually reduces it by a certain proportion if the new point doesnt significantly improves, i.e., \\[f(\\mathbf{x}^{(k)} + \\delta \\mathbf{v}) &gt; f(\\mathbf{x}^{(k)}) - \\frac{1}{2} \\delta \\nabla f(\\mathbf{x}^{(k)})^\\text{T}\\mathbf{v}\\] Note that when the direction \\(\\mathbf{v}\\) is \\(\\mathbf{H}(\\mathbf{x}^{(k)})^{-1} \\nabla f(\\mathbf{x}^{(k)})\\), \\(\\nabla f(\\mathbf{x}^{(k)})^\\text{T}\\mathbf{v}\\) is essentially the norm defined by the Hessian matrix. When you do not have the explicit formula of Hessian and even the gradient, you may numerically approximate the derivative using the definition. For example, we could use \\[ \\frac{f(\\mathbf{x}^{(k)} + \\epsilon) - f(\\mathbf{x}^{(k)})}{\\epsilon} \\] with \\(\\epsilon\\) small enough, e.g., \\(10^{-5}\\). However, this is very costly for the Hessian matrix if the number of variables is large. 4.6.2 Quasi-Newton Methods Since the idea of Newtons method is to solve a vector \\(\\mathbf{v}\\) such that \\[\\mathbf{H}(\\mathbf{x}^{(k)}) \\mathbf{v}= - \\nabla f(\\mathbf{x}^{(k)}), \\] If \\(\\mathbf{H}\\) is difficult to compute, we may use some matrix to substitute it. For example, if we simplify use the identity matrix \\(\\mathbf{I}\\), then this reduces to a first-order method to be introduced later. However, if we can get a good approximation, we can still solve this linear system and get to a better point. Then the question is how to obtain such matrix in a computationally inexpensive way. The BroydenFletcherGoldfarbShanno (BFSG) algorithm is such an approach by iteratively updating its (inverse) estimation. The algorithm proceed as follows: Start with \\(x^{(0)}\\) and a positive definite matrix, e.g., \\(\\mathbf{B}^{(0)} = \\mathbf{I}\\) For \\(k = 0, 1, 2, \\ldots\\), Search a updating direction by solving the linear system \\(\\mathbf{B}^{(k)} \\mathbf{p}_k = - \\nabla f(\\mathbf{x}^{(k)})\\) Perform line search in the direction of \\(\\mathbf{v}_k\\) and obtain the next point \\(\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\delta \\mathbf{p}_k\\) Update the approximation by \\[ \\mathbf{B}^{(k+1)} = \\mathbf{B}^{(k)} + \\frac{\\mathbf{y}_k^\\text{T}\\mathbf{y}_{k}}{ \\mathbf{y}_{k}^\\text{T}\\mathbf{s}_{k} } - \\frac{\\mathbf{B}^{(k)}\\mathbf{s}_{k}\\mathbf{s}_{k}^\\text{T}{\\mathbf{B}^{(k)}}^\\text{T}}{\\mathbf{s}_{k}^\\text{T}\\mathbf{B}^{(k)} \\mathbf{s}_{k} }, \\] where \\(\\mathbf{y}_k = \\nabla f(\\mathbf{x}^{(k+1)}) - \\nabla f(\\mathbf{x}^{(k)})\\) and \\(\\mathbf{s}_{k} = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{(k)}\\). The BFGS is performing a rank-two update by assuming that \\[ \\mathbf{B}^{(k+1)} = \\mathbf{B}^{(k)} + a \\mathbf{u}\\mathbf{u}^\\text{T}+ b \\mathbf{v}\\mathbf{v}^\\text{T},\\] Alternatives of such type of methods include the symmetric rank-one and Davidon-Fletcher-Powell (DFP) updates. 4.7 First-order Methods 4.7.1 Gradient Descent When simply using \\(\\mathbf{H}= \\mathbf{I}\\), we update \\[\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\delta \\nabla f(\\mathbf{x}^{(k)}).\\] However, it is then crucial to figure out the step size \\(\\delta\\). A step size too large may not even converge at all, however, a step size too small will take too many iterations to converge. Alternatively, line search could be used. 4.7.2 Gradient Descent Example: Linear Regression We use linear regression as an example. The objective function for linear regression is: \\[ \\ell(\\boldsymbol \\beta) = \\frac{1}{2n}||\\mathbf{y} - \\mathbf{X} \\boldsymbol \\beta ||^2 \\] with solution is \\[\\widehat{\\boldsymbol \\beta} = \\left(\\mathbf{X}^\\text{T}\\mathbf{X}\\right)^{-1} \\mathbf{X}^\\text{T} \\mathbf{y} \\] par(mfrow=c(1,1)) library(MASS) set.seed(3) n = 200 # create some data with linear model X = mvrnorm(n, c(0, 0), matrix(c(1,0.7, 0.7, 1), 2,2)) y = rnorm(n, mean = 2*X[,1] + X[,2]) beta1 &lt;- seq(-1, 4, 0.005) beta2 &lt;- seq(-1, 4, 0.005) allbeta &lt;- data.matrix(expand.grid(beta1, beta2)) rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y), length(beta1), length(beta2)) # quantile levels for drawing contour quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75) # plot the contour contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) box() # the truth b = solve(t(X) %*% X) %*% t(X) %*% y points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 2) We use an optimization approach to solve this problem. By taking the derivative with respect to \\(\\boldsymbol \\beta\\), we have the gradient \\[ \\begin{align} \\frac{\\partial \\ell(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta} = -\\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^\\text{T} \\boldsymbol \\beta) x_i. \\end{align} \\] To perform the optimization, we will first set an initial beta value, say \\(\\boldsymbol \\beta = \\mathbf{0}\\) for all entries, then proceed with the update \\[ \\boldsymbol \\beta^\\text{new} = \\boldsymbol \\beta^\\text{old} - \\frac{\\partial \\ell(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta} \\times \\delta.\\] Lets set \\(\\delta = 0.2\\) for now. The following function performs gradient descent. # gradient descent function, which also record the path mylm_g &lt;- function(x, y, b0 = rep(0, ncol(x)), # initial value delta = 0.2, # step size epsilon = 1e-6, #stopping rule maxitr = 5000) # maximum iterations { if (!is.matrix(x)) stop(&quot;x must be a matrix&quot;) if (!is.vector(y)) stop(&quot;y must be a vector&quot;) if (nrow(x) != length(y)) stop(&quot;number of observations different&quot;) # initialize beta values allb = matrix(b0, 1, length(b0)) # iterative update for (k in 1:maxitr) { # the new beta value b1 = b0 + t(x) %*% (y - x %*% b0) * delta / length(y) # record the new beta allb = rbind(allb, as.vector(b1)) # stopping rule if (max(abs(b0 - b1)) &lt; epsilon) break; # reset beta0 b0 = b1 } if (k == maxitr) cat(&quot;maximum iteration reached\\n&quot;) return(list(&quot;allb&quot; = allb, &quot;beta&quot; = b1)) } # fit the model mybeta = mylm_g(X, y, b0 = c(0, 1)) par(bg=&quot;transparent&quot;) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(mybeta$allb[,1], mybeta$allb[,2], type = &quot;b&quot;, col = &quot;red&quot;, pch = 19) points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 1.5) box() The descent path is very smooth because we choose a very small step size. However, if the step size is too large, we may observe unstable results or even unable to converge. For example, if We set \\(\\delta = 1\\) or \\(\\delta = 1.5\\). par(mfrow=c(1,2)) par(mar=c(2,2,2,2)) # fit the model with a larger step size mybeta = mylm_g(X, y, b0 = c(0, 1), delta = 1) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(mybeta$allb[,1], mybeta$allb[,2], type = &quot;b&quot;, col = &quot;red&quot;, pch = 19) points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 1.5) box() # and even larger mybeta = mylm_g(X, y, b0 = c(0, 1), delta = 1.5, maxitr = 6) ## maximum iteration reached contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(mybeta$allb[,1], mybeta$allb[,2], type = &quot;b&quot;, col = &quot;red&quot;, pch = 19) points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 1.5) box() 4.8 Coordinate Descent Instead of updating all parameters at a time, we can also update one parameter each time. The Gauss-Seidel style coordinate descent algorithm at the \\(k\\)th iteration will sequentially update all \\(p\\) parameters: \\[\\begin{align} x_1^{(k+1)} &amp;= \\underset{\\color{OrangeRed}{x_1}}{\\arg\\min} \\quad f(\\color{OrangeRed}{x_1}, x_2^{(k)}, \\ldots, x_p^{(k)}) \\nonumber \\\\ x_2^{(k+1)} &amp;= \\underset{\\color{OrangeRed}{x_2}}{\\arg\\min} \\quad f(x_1^{\\color{DodgerBlue}{(k+1)}}, \\color{OrangeRed}{\\mathbf{x}_2}, \\ldots, x_p^{(k)}) \\nonumber \\\\ \\cdots &amp;\\nonumber \\\\ x_p^{(k+1)} &amp;= \\underset{\\color{OrangeRed}{x_p}}{\\arg\\min} \\quad f(x_1^{\\color{DodgerBlue}{(k+1)}}, x_2^{\\color{DodgerBlue}{(k+1)}}, \\ldots, \\color{OrangeRed}{x_p}) \\nonumber \\\\ \\end{align}\\] Note that after updating one coordinate, the new parameter value is used for updating the next coordinate. After we complete this loop, all \\(j\\) are updated to their new values, and we proceed to the next step. Another type of update is the Jacobi style, which can be performed in parallel at the \\(k\\)th iteration: \\[\\begin{align} x_1^{(k+1)} &amp;= \\underset{\\color{OrangeRed}{x_1}}{\\arg\\min} \\quad f(\\color{OrangeRed}{x_1}, x_2^{(k)}, \\ldots, x_p^{(k)}) \\nonumber \\\\ x_2^{(k+1)} &amp;= \\underset{\\color{OrangeRed}{x_2}}{\\arg\\min} \\quad f(x_1^{(k+1)}, \\color{OrangeRed}{\\mathbf{x}_2}, \\ldots, x_p^{(k)}) \\nonumber \\\\ \\cdots &amp;\\nonumber \\\\ x_p^{(k+1)} &amp;= \\underset{\\color{OrangeRed}{x_p}}{\\arg\\min} \\quad f(x_1^{(k+1)}, x_2^{(k+1)}, \\ldots, \\color{OrangeRed}{x_p}) \\nonumber \\\\ \\end{align}\\] For differentiable convex functions \\(f\\), we can ensure that if all parameters are optimized then the entire problem is also optimized. If \\(f\\) is not differentiable, we may have trouble (see the example on wiki). However, there are also cases where coordinate descent would still guarantee a convergence, e.g., a sperable case: \\[f(\\mathbf{x}) = g(\\mathbf{x}) + \\sum_{j=1}^p h_j(x_j)\\] This is the Lasso formulation which will be discussed in later section. 4.8.1 Coordinate Descent Example: Linear Regression Coordinate descent for linear regression is not really necessary. However, we will still use this as an example. Note that the update for a single parameter is \\[ \\underset{\\boldsymbol \\beta_j}{\\text{argmin}} \\frac{1}{2n} ||\\mathbf{y}- X_j \\beta_j - \\mathbf{X}_{(-j)} \\boldsymbol \\beta_{(-j)} ||^2 \\] where \\(\\mathbf{X}_{(-j)}\\) is the data matrix without the \\(j\\)th column. Note that when updating \\(\\beta_j\\) coordinate-wise, we can first calculate the residual defined as \\(\\mathbf{r} = \\mathbf{y} - \\mathbf{X}_{(-j)} \\boldsymbol \\beta_{(-j)}\\) which does not depend on \\(\\beta_j\\), and optimize the rest of the formula for \\(\\beta_j\\). This is essentially the same as performing a one-dimensional regression by regressing \\(\\mathbf{r}\\) on \\(X_j\\) and obtain the update. \\[ \\beta_j = \\frac{X_j^T \\mathbf{r}}{X_j^T X_j} \\] The coordinate descent usually does not involve choosing a step size. Note that the following function is NOT efficient because there are a lot of wasted calculations. It is only for demonstration purpose. Here we use the Gauss-Seidel style update. # gradient descent function, which also record the path mylm_c &lt;- function(x, y, b0 = rep(0, ncol(x)), epsilon = 1e-6, maxitr = 5000) { if (!is.matrix(x)) stop(&quot;x must be a matrix&quot;) if (!is.vector(y)) stop(&quot;y must be a vector&quot;) if (nrow(x) != length(y)) stop(&quot;number of observations different&quot;) # initialize beta values allb = matrix(b0, 1, length(b0)) # iterative update for (k in 1:maxitr) { # initiate a vector for new beta b1 = b0 for (j in 1:ncol(x)) { # calculate the residual r = y - x[, -j, drop = FALSE] %*% b1[-j] # update jth coordinate b1[j] = t(r) %*% x[,j] / (t(x[,j, drop = FALSE]) %*% x[,j]) # record the update allb = rbind(allb, as.vector(b1)) } if (max(abs(b0 - b1)) &lt; epsilon) break; # reset beta0 b0 = b1 } if (k == maxitr) cat(&quot;maximum iteration reached\\n&quot;) return(list(&quot;allb&quot; = allb, &quot;beta&quot; = b1)) } # fit the model mybeta = mylm_c(X, y, b0 = c(0, 3)) par(mfrow=c(1,1)) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(mybeta$allb[,1], mybeta$allb[,2], type = &quot;b&quot;, col = &quot;red&quot;, pch = 19) points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 1.5) box() 4.9 Stocastic Gradient Descent The main advantage of using Stochastic Gradient Descent (SGD) is its computational speed. Calculating the gradient using all observations can be costly. Instead, we consider update the parameter based on a single observation. Hence, the gradient is defined as \\[ \\frac{\\partial \\ell_i(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta} = - (y_i - x_i^\\text{T} \\boldsymbol \\beta) x_i. \\] Compared with using all observations, this is \\(1/n\\) of the cost. However, because this is rater not accurate for each iteration, but can still converge in the long run. There is a decay rate involved in SGD step size. If the step size does not decreases to 0, the algorithm cannot converge. However, it also has to sum up to infinite to allow us to go as far as we can. For example, a choice could be \\(\\delta_k = 1/k\\), hence \\(\\sum \\delta_k = \\infty\\) and \\(\\sum \\delta_k^2 &lt; \\infty\\). # gradient descent function, which also record the path mylm_sgd &lt;- function(x, y, b0 = rep(0, ncol(x)), delta = 0.05, maxitr = 10) { if (!is.matrix(x)) stop(&quot;x must be a matrix&quot;) if (!is.vector(y)) stop(&quot;y must be a vector&quot;) if (nrow(x) != length(y)) stop(&quot;number of observations different&quot;) # initialize beta values allb = matrix(b0, 1, length(b0)) # iterative update for (k in 1:maxitr) { # going through all samples for (i in sample(1:nrow(x))) { # update based on the gradient of a single subject b0 = b0 + (y[i] - sum(x[i, ] * b0)) * x[i, ] * delta # record the update allb = rbind(allb, as.vector(b0)) # learning rate decay delta = delta * 1/k } } return(list(&quot;allb&quot; = allb, &quot;beta&quot; = b0)) } # fit the model mybeta = mylm_sgd(X, y, b0 = c(0, 1), maxitr = 3) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(mybeta$allb[,1], mybeta$allb[,2], type = &quot;b&quot;, col = &quot;red&quot;, pch = 19) points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 1.5) box() 4.9.1 Mini-batch Stocastic Gradient Descent Instead of using just one observation, we could also consider splitting the data into several small batches and use one batch of sample to calculate the gradient at each iteration. # gradient descent function, which also record the path mylm_sgd_mb &lt;- function(x, y, b0 = rep(0, ncol(x)), delta = 0.3, maxitr = 20) { if (!is.matrix(x)) stop(&quot;x must be a matrix&quot;) if (!is.vector(y)) stop(&quot;y must be a vector&quot;) if (nrow(x) != length(y)) stop(&quot;number of observations different&quot;) # initiate batches with 10 observations each batch = sample(rep(1:floor(nrow(x)/10), length.out = nrow(x))) # initialize beta values allb = matrix(b0, 1, length(b0)) # iterative update for (k in 1:maxitr) { for (i in 1:max(batch)) # loop through batches { # update based on the gradient of a single subject b0 = b0 + t(x[batch==i, ]) %*% (y[batch==i] - x[batch==i, ] %*% b0) * delta / sum(batch==i) # record the update allb = rbind(allb, as.vector(b0)) # learning rate decay delta = delta * 1/k } } return(list(&quot;allb&quot; = allb, &quot;beta&quot; = b0)) } # fit the model mybeta = mylm_sgd_mb(X, y, b0 = c(0, 1), maxitr = 3) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(mybeta$allb[,1], mybeta$allb[,2], type = &quot;b&quot;, col = &quot;red&quot;, pch = 19) points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 1.5) box() You may further play around with these tuning parameters to see how sensitive the optimization is to them. A stopping rule can be difficult to determine, hence in practice, early stop is also used. 4.10 Lagrangian Multiplier for Constrained Problems Constrained optimization problems appear very frequently. Both Lasso and Ridge regressions can be viewed as constrained problems, while support vector machines (SVM) is another example, which will be introduced later on. Lets investigate this using a toy example. Suppose we have an optimization problem \\[\\text{minimize} \\quad f(x, y) = x^2 + y^2\\] \\[\\text{subj. to} \\quad g(x, y) = xy - 4 = 0\\] x &lt;- seq(-5, 5, 0.05) y &lt;- seq(-5, 5, 0.05) mygrid &lt;- data.matrix(expand.grid(x, y)) f &lt;- matrix(mygrid[,1]^2 + mygrid[,2]^2, length(x), length(y)) f2 &lt;- matrix(mygrid[,1]*mygrid[,2], length(x), length(y)) # plot the contour par(mar=c(2,2,2,2)) contour(x, y, f, levels = c(0.2, 1, 2, 4, 8, 16)) contour(x, y, f2, levels = 4, add = TRUE, col = &quot;blue&quot;, lwd = 2) box() lines(seq(1, 3, 0.01), 4- seq(1, 3, 0.01), type = &quot;l&quot;, col = &quot;darkorange&quot;, lwd = 3) points(2, 2, col = &quot;red&quot;, pch = 19, cex = 2) points(-2, -2, col = &quot;red&quot;, pch = 19, cex = 2) The problem itself is very simple. We know that the optimizer is the red dot. But an interesting point of view is to look at the level curves of the objective function. As it is growing (expanding), there is one point (the red dot) at which level curve barely touches the constrain curve (blue line). This should be the optimizer. But this also implies that the tangent line (orange line) of this leveling curve must coincide with the tangent line of the constraint. Noticing that the tangent line can be obtained by taking the derivative of the function, this observation implies that gradients of the two functions (the objective function and the constraint function) must be a multiple of the other. Hence, \\[ \\begin{align} &amp; \\bigtriangledown f = \\lambda \\bigtriangledown g \\\\ \\\\ \\Longrightarrow \\qquad &amp; \\begin{cases} 2x = \\lambda y &amp; \\text{by taking derivative w.r.t.} \\,\\, x\\\\ 2y = \\lambda x &amp; \\text{by taking derivative w.r.t.} \\,\\, y\\\\ xy - 4 = 0 &amp; \\text{the constraint itself} \\end{cases} \\end{align} \\] The three equations put together is very easy to solve. We have \\(x = y = 0\\) or \\(\\lambda = \\pm 2\\) based on the first two equations. The first one is not feasible based on the constraint. The second solution leads to two feasible solutions: \\(x = y = 2\\) or \\(x = y = -2\\). Hence, we now know that there are two solutions. Now, looking back at the equation \\(\\bigtriangledown f = \\lambda \\bigtriangledown g\\), this is simply the derivative of the Lagrangian function defined as \\[{\\cal L}(x, y, \\lambda) = f(x, y) - \\lambda g(x, y),\\] while solving for the solution of the constrained problem becomes finding the stationary point of the Lagrangian. Be aware that in some cases, the solution you found can be maximizers instead of minimizers. Hence, its necessary to compare all of them and see which one is smaller. Reference "],["ridge-regression.html", "Chapter 5 Ridge Regression 5.1 Motivation: Correlated Variables and Convexity 5.2 Bias and Variance of Ridge Regression 5.3 Degrees of Freedom 5.4 Using the lm.ridge() function 5.5 Cross-validation 5.6 Leave-one-out cross-validation 5.7 The glmnet package", " Chapter 5 Ridge Regression Ridge regression was proposed by Hoerl and Kennard (1970), but is also a special case of Tikhonov regularization. The essential idea is very simple: Knowing that the ordinary least squares (OLS) solution is not unique in an ill-posed problem, i.e., \\(\\mathbf{X}^\\text{T}\\mathbf{X}\\) is not invertible, a ridge regression adds a ridge (diagonal matrix) on \\(\\mathbf{X}^\\text{T}\\mathbf{X}\\): \\[\\widehat{\\boldsymbol \\beta}^\\text{ridge} = (\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y},\\] It provides a solution of linear regression when multicollinearity happens, especially when the number of variables is larger than the sample size. Alternatively, this is also the solution of a regularized least square estimator. We add an \\(\\ell_2\\) penalty to the residual sum of squares, i.e., \\[ \\begin{align} \\widehat{\\boldsymbol \\beta}^\\text{ridge} =&amp; \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} (\\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta)^\\text{T}(\\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta) + n \\lambda \\lVert\\boldsymbol \\beta\\rVert^2\\\\ =&amp; \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^\\text{T}\\boldsymbol \\beta)^2 + \\lambda \\sum_{i=1}^n \\beta_j^2, \\end{align} \\] for some penalty \\(\\lambda &gt; 0\\). Another approach that leads to the ridge regression is a constraint on the \\(\\ell_2\\) norm of the parameters, which will be introduced in the next Chapter. Ridge regression is used extensively in genetic analyses to address small-\\(n\\)-large-\\(p\\) problems. We will start with a motivation example and then discuss the bias-variance trade-off issue. 5.1 Motivation: Correlated Variables and Convexity Ridge regression has many advantages. Most notably, it can address highly correlated variables. From an optimization point of view, having highly correlated variables means that the objective function (\\(\\ell_2\\) loss) becomes flat along certain directions in the parameter domain. This can be seen from the following example, where the true parameters are both \\(1\\) while the estimated parameters concludes almost all effects to the first variable. You can change different seed to observe the variability of these parameter estimates and notice that they are quite large. Instead, if we fit a ridge regression, the parameter estimates are relatively stable. library(MASS) set.seed(2) n = 30 # create highly correlated variables and a linear model X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2)) y = rnorm(n, mean = X[,1] + X[,2]) # compare parameter estimates summary(lm(y~X-1))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## X1 1.8461255 1.294541 1.42608527 0.1648987 ## X2 0.0990278 1.321283 0.07494822 0.9407888 # note that the true parameters are all 1&#39;s # Be careful that the `lambda` parameter in lm.ridge is our (n*lambda) lm.ridge(y~X-1, lambda=5) ## X1 X2 ## 0.9413221 0.8693253 The variance of both \\(\\beta_1\\) and \\(\\beta_2\\) are quite large. This is expected because we know from linear regression that the variance of \\(\\widehat{\\boldsymbol \\beta}\\) is \\(\\sigma^2 (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\). However, since the columns of \\(\\mathbf{X}\\) are highly correlated, the smallest eigenvalue of \\(\\mathbf{X}^\\text{T}\\mathbf{X}\\) is close to 0, making the largest eigenvalue of \\((\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\) very large. This can also be interpreted through an optimization point of view. The objective function for an OLS estimator is demonstrated in the following. beta1 &lt;- seq(0, 3, 0.005) beta2 &lt;- seq(-1, 2, 0.005) allbeta &lt;- data.matrix(expand.grid(beta1, beta2)) rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y), length(beta1), length(beta2)) # quantile levels for drawing contour quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75) # plot the contour contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) box() # the truth points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) # the data betahat &lt;- coef(lm(y~X-1)) points(betahat[1], betahat[2], pch = 19, col = &quot;blue&quot;, cex = 2) As an alternative, if we add a ridge regression penalty, the contour is forced to be more convex due to the added eigenvalues. Here is a plot of the Ridge \\(\\ell_2\\) penalty. Hence, by adding this to the OLS objective function, the solution is more stable. This may be interpreted in several different ways such as: 1) the objective function is more convex; 2) the variance of the estimator is smaller. However, this causes some bias too. Choosing the tuning parameter is a balance of the bias-variance trade-off, which will be discussed in the following. par(mfrow=c(1, 2)) # adding a L2 penalty to the objective function rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + b %*% b, X, y), length(beta1), length(beta2)) # the ridge solution bh = solve(t(X) %*% X + diag(2)) %*% t(X) %*% y contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) points(bh[1], bh[2], pch = 19, col = &quot;blue&quot;, cex = 2) box() # adding a larger penalty rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + 10*b %*% b, X, y), length(beta1), length(beta2)) bh = solve(t(X) %*% X + 10*diag(2)) %*% t(X) %*% y # the ridge solution contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) points(bh[1], bh[2], pch = 19, col = &quot;blue&quot;, cex = 2) box() 5.2 Bias and Variance of Ridge Regression We can set a relationship between Ridge and OLS, assuming that the OLS estimator exist. \\[\\begin{align} \\widehat{\\boldsymbol \\beta}^\\text{ridge} =&amp; (\\mathbf{X}^\\text{T}\\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}\\\\ =&amp; (\\mathbf{X}^\\text{T}\\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} (\\mathbf{X}^\\text{T}\\mathbf{X}) \\color{OrangeRed}{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}}\\\\ =&amp; (\\mathbf{X}^\\text{T}\\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} (\\mathbf{X}^\\text{T}\\mathbf{X}) \\color{OrangeRed}{\\widehat{\\boldsymbol \\beta}^\\text{ols}} \\end{align}\\] This leads to a biased estimator (since the OLS estimator is unbiased) if we use any nonzero \\(\\lambda\\). As \\(\\lambda \\rightarrow 0\\), the ridge solution is eventually the same as OLS As \\(\\lambda \\rightarrow \\infty\\), \\(\\widehat{\\boldsymbol \\beta}^\\text{ridge} \\rightarrow 0\\) It can be easier to analyze a case with \\(\\mathbf{X}^\\text{T}\\mathbf{X}= n \\mathbf{I}\\), i.e, with standardized and orthogonal columns in \\(\\mathbf{X}\\). Note that in this case, each \\(\\beta_j^{\\text{ols}}\\) is just the projection of \\(\\mathbf{y}\\) onto \\(\\mathbf{x}_j\\), the \\(j\\)th column of the design matrix. We also have \\[\\begin{align} \\widehat{\\boldsymbol \\beta}^\\text{ridge} =&amp; (\\mathbf{X}^\\text{T}\\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} (\\mathbf{X}^\\text{T}\\mathbf{X}) \\widehat{\\boldsymbol \\beta}^\\text{ols}\\\\ =&amp; (\\mathbf{I}+ \\lambda \\mathbf{I})^{-1}\\widehat{\\boldsymbol \\beta}^\\text{ols}\\\\ =&amp; (1 + \\lambda)^{-1} \\widehat{\\boldsymbol \\beta}^\\text{ols}\\\\ \\Longrightarrow \\beta_j^{\\text{ridge}} =&amp; \\frac{1}{1 + \\lambda} \\beta_j^\\text{ols} \\end{align}\\] Then in this case, the bias and variance of the ridge estimator can be explicitly expressed: \\(\\text{Bias}(\\beta_j^{\\text{ridge}}) = \\frac{-\\lambda}{1 + \\lambda} \\beta_j^\\text{ols}\\) (not zero) \\(\\text{Var}(\\beta_j^{\\text{ridge}}) = \\frac{1}{(1 + \\lambda)^2} \\text{Var}(\\beta_j^\\text{ols})\\) (reduced from OLS) Of course, we can ask the question: is it worth it? We could proceed with a simple analysis of the MSE of \\(\\beta\\) (dropping \\(j\\)): \\[\\begin{align} \\text{MSE}(\\beta) &amp;= \\text{E}(\\widehat{\\beta} - \\beta)^2 \\\\ &amp;= \\text{E}[\\widehat{\\beta} - \\text{E}(\\widehat{\\beta})]^2 + \\text{E}[\\widehat{\\beta} - \\beta]^2 \\\\ &amp;= \\text{E}[\\widehat{\\beta} - \\text{E}(\\widehat{\\beta})]^2 + 0 + [\\text{E}(\\widehat{\\beta}) - \\beta]^2 \\\\ &amp;= \\text{Var}(\\widehat{\\beta}) + \\text{Bias}^2. \\end{align}\\] This bias-variance breakdown formula will appear multiple times. Now, plug-in the results developed earlier based on the orthogonal design matrix, and investigate the derivative of the MSE of the Ridge estimator, we have \\[\\begin{align} \\frac{\\partial \\text{MSE}(\\widehat{\\beta}^\\text{ridge})}{ \\partial \\beta} =&amp; \\frac{\\partial}{\\partial \\beta} \\left[ \\frac{1}{(1+\\lambda)^2} \\text{Var}(\\widehat{\\beta}^\\text{ols}) + \\frac{\\lambda^2}{(1 + \\lambda)^2} \\beta^2 \\right] \\\\ =&amp; \\frac{2}{(1+\\lambda)^3} \\left[ \\lambda \\beta^2 - \\text{Var}(\\widehat{\\beta}^\\text{ols}) \\right] \\end{align}\\] Note that when the derivative is negative, increasing \\(\\lambda\\) would decrease the MSE. This implies that we can reduce the MSE by choosing a small \\(\\lambda\\). Of course the situation is much more involving when the columns in \\(\\mathbf{X}\\) are not orthogonal. However, the following analysis helps to understand a non-orthogonal case. It is essentially re-organizing the columns of \\(\\mathbf{X}\\) into its principle components so that they are still orthogonal. Lets first take a singular value decomposition (SVD) of \\(\\mathbf{X}\\), with \\(\\mathbf{X}= \\mathbf{U}\\mathbf{D}\\mathbf{V}^\\text{T}\\), then the columns in \\(\\mathbf{U}\\) form an orthonormal basis and columns in \\(\\mathbf{U}\\mathbf{D}\\) are the principal components and \\(\\mathbf{V}\\) defines the principle directions. In addition, we have \\(n \\widehat{\\boldsymbol \\Sigma} = \\mathbf{X}^\\text{T}\\mathbf{X}= \\mathbf{V}\\mathbf{D}^2 \\mathbf{V}^\\text{T}\\). Assuming that \\(p &lt; n\\), and \\(\\mathbf{X}\\) has full column ranks, then the Ridge estimator fitted \\(\\mathbf{y}\\) value can be decomposed as \\[\\begin{align} \\widehat{\\mathbf{y}}^\\text{ridge} =&amp; \\mathbf{X}\\widehat{\\beta}^\\text{ridge} \\\\ =&amp; \\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda)^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}\\\\ =&amp; \\mathbf{U}\\mathbf{D}\\mathbf{V}^\\text{T}( \\mathbf{V}\\mathbf{D}^2 \\mathbf{V}^\\text{T}+ n \\lambda \\mathbf{V}\\mathbf{V}^\\text{T})^{-1} \\mathbf{V}\\mathbf{D}\\mathbf{U}^\\text{T}\\mathbf{y}\\\\ =&amp; \\mathbf{U}\\mathbf{D}^2 (n \\lambda + \\mathbf{D}^2)^{-1} \\mathbf{U}^\\text{T}\\mathbf{y}\\\\ =&amp; \\sum_{j = 1}^p \\mathbf{u}_j \\left( \\frac{d_j^2}{n \\lambda + d_j^2} \\mathbf{u}_j^\\text{T}\\mathbf{y}\\right), \\end{align}\\] where \\(d_j\\) is the \\(j\\)th eigenvalue of the PCA. Hence, the Ridge regression fitted value can be understood as Perform PCA of \\(\\mathbf{X}\\) Project \\(\\mathbf{y}\\) onto the PCs Shrink the projection \\(\\mathbf{u}_j^\\text{T}\\mathbf{y}\\) by the factor \\(d_j^2 / (n \\lambda + d_j^2)\\) Reassemble the PCs using all the shrunken length Hence, the bias-variance notion can be understood as the trade-off on these derived directions \\(\\mathbf{u}_j\\) and their corresponding parameters \\(\\mathbf{u}_j^\\text{T}\\mathbf{y}\\). 5.3 Degrees of Freedom We know that for a linear model, the degrees of freedom (DF) is simply the number of parameters used. There is a formal definition, using \\[\\begin{align} \\text{DF}(\\widehat{f}) =&amp; \\frac{1}{\\sigma^2} \\text{Cov}(\\widehat{y}_i, y_i)\\\\ =&amp; \\frac{1}{\\sigma^2} \\text{Trace}[\\text{Cov}(\\widehat{\\mathbf{y}}, \\mathbf{y})] \\end{align}\\] We can check that for a linear regression (assuming the intercept is already included in \\(\\mathbf{X}\\)), the DF is \\[\\begin{align} \\frac{1}{\\sigma^2} \\text{Trace}[\\text{Cov}(\\widehat{\\mathbf{y}}^\\text{ols}, \\mathbf{y})] =&amp; \\frac{1}{\\sigma^2} \\text{Trace}[\\text{Cov}(\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}, \\mathbf{y})] \\\\ =&amp; \\text{Trace}(\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}) \\\\ =&amp; \\text{Trace}(\\mathbf{I}_{p\\times p})\\\\ =&amp; p \\end{align}\\] For the Ridge regression, we can perform the same analysis on ridge regression. \\[\\begin{align} \\frac{1}{\\sigma^2} \\text{Trace}[\\text{Cov}(\\widehat{\\mathbf{y}}^\\text{ridge}, \\mathbf{y})] =&amp; \\frac{1}{\\sigma^2} \\text{Trace}[\\text{Cov}(\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}, \\mathbf{y})] \\\\ =&amp; \\text{Trace}(\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}) \\\\ =&amp; \\text{Trace}(\\mathbf{U}\\mathbf{D}\\mathbf{V}^\\text{T}( \\mathbf{V}\\mathbf{D}^2 \\mathbf{V}^\\text{T}+ n \\lambda \\mathbf{V}\\mathbf{V}^\\text{T})^{-1} \\mathbf{V}\\mathbf{D}\\mathbf{U}^\\text{T})\\\\ =&amp; \\sum_{j = 1}^p \\frac{d_j^2}{d_j^2 + n\\lambda} \\end{align}\\] Note that this is smaller than \\(p\\) as long as \\(\\lambda \\neq 0\\). This implies that the Ridge regression does not use the full potential of all \\(p\\) variables, since there is a risk of over-fitting. 5.4 Using the lm.ridge() function We have seen how the lm.ridge() can be used to fit a Ridge regression. However, keep in mind that the lambda parameter used in the function actually specifies the \\(n \\lambda\\) entirely we used in our notation. However, regardless, our goal is mainly to tune this parameter to achieve a good balance of bias-variance trade off. However, the difficulty here is to evaluate the performance without knowing the truth. Lets first use a simulated example, in which we do know the truth and then introduce the cross-validation approach for real data where we do not know the truth. We use the prostate cancer data prostate from the ElemStatLearn package. The dataset contains 8 explanatory variables and one outcome lpsa, the log prostate-specific antigen value. # ElemStatLearn is currently archived, install a previous version # library(devtools) # install_version(&quot;ElemStatLearn&quot;, version = &quot;2015.6.26&quot;, repos = &quot;http://cran.r-project.org&quot;) library(ElemStatLearn) head(prostate) ## lcavol lweight age lbph svi lcp gleason pgg45 lpsa train ## 1 -0.5798185 2.769459 50 -1.386294 0 -1.386294 6 0 -0.4307829 TRUE ## 2 -0.9942523 3.319626 58 -1.386294 0 -1.386294 6 0 -0.1625189 TRUE ## 3 -0.5108256 2.691243 74 -1.386294 0 -1.386294 7 20 -0.1625189 TRUE ## 4 -1.2039728 3.282789 58 -1.386294 0 -1.386294 6 0 -0.1625189 TRUE ## 5 0.7514161 3.432373 62 -1.386294 0 -1.386294 6 0 0.3715636 TRUE ## 6 -1.0498221 3.228826 50 -1.386294 0 -1.386294 6 0 0.7654678 TRUE 5.4.1 Scaling Issue We can use lm.ridge() with a fixed \\(\\lambda\\) value, as we have shown in the previous example. Its syntax is again similar to the lm() function, with an additional argument lambda. We can also compare that with our own code. # lm.ridge function from the MASS package lm.ridge(lpsa ~., data = prostate[, 1:9], lambda = 1) ## lcavol lweight age lbph svi lcp gleason ## 0.14716982 0.55209405 0.61998311 -0.02049376 0.09488234 0.74846397 -0.09399009 0.05227074 ## pgg45 ## 0.00424397 # using our own code X = cbind(1, data.matrix(prostate[, 1:8])) y = prostate[, 9] solve(t(X) %*% X + diag(9)) %*% t(X) %*% y ## [,1] ## 0.07941225 ## lcavol 0.55985143 ## lweight 0.60398302 ## age -0.01957258 ## lbph 0.09395770 ## svi 0.68809341 ## lcp -0.08863685 ## gleason 0.06288206 ## pgg45 0.00416878 However, they look different. This is because ridge regression has a scaling issue: it would shrink parameters differently if the corresponding covariates have different scales. This can be seen from our previous development of the SVD analysis. Since the shrinkage is the same for all \\(d_j\\)s, it would apply a larger shrinkage for small \\(d_j\\). A commonly used approach to deal with the scaling issue is to standardize all covariates such that they are treated the same way. In addition, we will also center both \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) before performing the ridge regression. An interesting consequence of centering is that we do not need the intercept anymore, since \\(\\mathbf{X}\\boldsymbol \\beta= \\mathbf{0}\\) for all \\(\\boldsymbol \\beta\\). One last point is that when performing scaling, lm.ridge() use the \\(n\\) factor instead of \\(n-1\\) when calculating the standard deviation. Hence, incorporating all these, we have # perform centering and scaling X = scale(data.matrix(prostate[, 1:8]), center = TRUE, scale = TRUE) # use n instead of (n-1) for standardization n = nrow(X) X = X * sqrt(n / (n-1)) # center y but not scaling y = scale(prostate[, 9], center = TRUE, scale = FALSE) # getting the estimated parameter mybeta = solve(t(X) %*% X + diag(8)) %*% t(X) %*% y ridge.fit = lm.ridge(lpsa ~., data = prostate[, 1:9], lambda = 1) # note that $coef obtains the coefficients internally from lm.ridge # however coef() would transform these back to the original scale version cbind(mybeta, ridge.fit$coef) ## [,1] [,2] ## lcavol 0.64734891 0.64734891 ## lweight 0.26423507 0.26423507 ## age -0.15178989 -0.15178989 ## lbph 0.13694453 0.13694453 ## svi 0.30825889 0.30825889 ## lcp -0.13074243 -0.13074243 ## gleason 0.03755141 0.03755141 ## pgg45 0.11907848 0.11907848 5.4.2 Multiple \\(\\lambda\\) values Since we now face the problem of bias-variance trade-off, we can fit the model with multiple \\(\\lambda\\) values and select the best. This can be done using the following code. library(MASS) fit = lm.ridge(lpsa~., data = prostate[, -10], lambda = seq(0, 100, by=0.2)) For each \\(\\lambda\\), the coefficients of all variables are recorded. The plot shows how these coefficients change as a function of \\(\\lambda\\). We can easily see that as \\(\\lambda\\) becomes larger, the coefficients are shrunken towards 0. This is consistent with our understanding of the bias. On the very left hand size of the plot, the value of each parameter corresponds to the OLS result since no penalty is applied. Be careful that the coefficients of the fitted objects fit$coef are scaled by the standard deviation of the covariates. If you need the original scale, make sure to use coef(fit). matplot(coef(fit)[, -1], type = &quot;l&quot;, xlab = &quot;Lambda&quot;, ylab = &quot;Coefficients&quot;) text(rep(50, 8), coef(fit)[1,-1], colnames(prostate)[1:8]) title(&quot;Prostate Cancer Data: Ridge Coefficients&quot;) To select the best \\(\\lambda\\) value, there can be several different methods. We will discuss two approaches among them: \\(k\\)-fold cross-validation and generalized cross-validation (GCV). 5.5 Cross-validation Cross-validation (CV) is a technique to evaluate the performance of a model on an independent set of data. The essential idea is to separate out a subset of the data and do not use that part during the training, while using it for testing. We can then rotate to or sample a different subset as the testing data. Different cross-validation methods differs on the mechanisms of generating such testing data. \\(K\\)-fold cross-validation is probably the the most popular among them. The method works in the following steps: Randomly split the data into \\(K\\) equal portions For each \\(k\\) in \\(1, \\ldots, K\\): use the \\(k\\)th portion as the testing data and the rest as training data, obtain the testing error Average all \\(K\\) testing errors Here is a graphical demonstration of a \\(10\\)-fold CV: There are also many other cross-validation procedures, for example, the Monte Carlo cross-validation randomly splits the data into training and testing (instead of fix \\(K\\) portions) each time and repeat the process as many times as we like. The benefit of such procedure is that if this is repeated enough times, the estimated testing error becomes fairly stable, and not affected much by the random mechanism. On the other hand, we can also repeat the entire \\(K\\)-fold CV process many times, then average the errors. This is also trying to reduced the influence of randomness. 5.6 Leave-one-out cross-validation Regarding the randomness, the leave-one-out cross-validation is completely nonrandom. It is essentially the \\(k\\)-fold CV approach, but with \\(k\\) equal to \\(n\\), the sample size. A standard approach would require to re-fit the model \\(n\\) times, however, some linear algebra can show that there is an equivalent form using the Hat matrix when fitting a linear regression: \\[\\begin{align} \\text{CV}(n) =&amp; \\frac{1}{n}\\sum_{i=1}^n (y_i - \\widehat{y}_{[i]})^2\\\\ =&amp; \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\widehat{y}_i}{1 - \\mathbf{H}_{ii}} \\right)^2, \\end{align}\\] where \\(\\widehat{y}_{i}\\) is the fitted value using the whole dataset, but \\(\\widehat{y}_{[i]}\\) is the prediction of \\(i\\)th observation using the data without it when fitting the model. And \\(\\mathbf{H}_{ii}\\) is the \\(i\\)th diagonal element of the hat matrix \\(\\mathbf{H}= \\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}\\). The proof is essentially an application of the ShermanMorrisonWoodbury (SMW) formula, which is also used when deriving the rank-one update of a quasi-Newton optimization approach. Proof. Denote \\(\\mathbf{X}_{[i]}\\) and \\(\\mathbf{y}_{[i]}\\) the data derived from \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\), but with the \\(i\\) observation (\\(x_i\\), \\(y_i\\)) removed. We then have the properties that \\[\\mathbf{X}_{[i]}^\\text{T}\\mathbf{X}_{[i]} = \\mathbf{X}^\\text{T}\\mathbf{X}- x_i x_i^\\text{T}, \\] and \\[\\mathbf{H}_{ii} = x_i^\\text{T}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} x_i.\\] By the SMW formula, we have \\[(\\mathbf{X}_{[i]}^\\text{T}\\mathbf{X}_{[i]})^{-1} = (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} + \\frac{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}x_i x_i^\\text{T}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}}{ 1 - \\mathbf{H}_{ii}}, \\] Further notice that \\[\\mathbf{X}_{[i]}^\\text{T}\\mathbf{y}_{[i]} = \\mathbf{X}^\\text{T}\\mathbf{y}- x_i y_i, \\] we can then reconstruct the fitted parameter when observation \\(i\\) is removed: \\[\\begin{align} \\widehat{\\boldsymbol \\beta}_{[i]} =&amp; (\\mathbf{X}_{[i]}^\\text{T}\\mathbf{X}_{[i]})^{-1} \\mathbf{X}_{[i]}^\\text{T}\\mathbf{y}_{[i]} \\\\ =&amp; \\left[ (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} + \\frac{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}x_i x_i^\\text{T}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}}{ 1 - \\mathbf{H}_{ii}} \\right] (\\mathbf{X}^\\text{T}\\mathbf{y}- x_i y_i)\\\\ =&amp; (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}+ \\left[ - (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} x_i y_i + \\frac{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}x_i x_i^\\text{T}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}}{ 1 - \\mathbf{H}_{ii}} (\\mathbf{X}^\\text{T}\\mathbf{y}- x_i y_i) \\right] \\\\ =&amp; \\widehat{\\boldsymbol \\beta} - \\frac{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} x_i}{1 - \\mathbf{H}_{ii}} \\left[ y_i (1 - \\mathbf{H}_{ii}) - x_i^\\text{T}\\widehat{\\boldsymbol \\beta} + \\mathbf{H}_{ii} y_i \\right]\\\\ =&amp; \\widehat{\\boldsymbol \\beta} - \\frac{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} x_i}{1 - \\mathbf{H}_{ii}} \\left( y_i - x_i^\\text{T}\\widehat{\\boldsymbol \\beta} \\right) \\end{align}\\] Then the error of the \\(i\\)th obervation from the leave-one-out model is \\[\\begin{align} y _i - \\widehat{y}_{[i]} =&amp; y _i - x_i^\\text{T}\\widehat{\\boldsymbol \\beta}_{[i]} \\\\ =&amp; y _i - x_i^\\text{T}\\left[ \\widehat{\\boldsymbol \\beta} - \\frac{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} x_i}{1 - \\mathbf{H}_{ii}} \\left( y_i - x_i^\\text{T}\\widehat{\\boldsymbol \\beta} \\right) \\right]\\\\ =&amp; y _i - x_i^\\text{T}\\widehat{\\boldsymbol \\beta} + \\frac{x_i^\\text{T}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} x_i}{1 - \\mathbf{H}_{ii}} \\left( y_i - x_i^\\text{T}\\widehat{\\boldsymbol \\beta} \\right)\\\\ =&amp; y _i - x_i^\\text{T}\\widehat{\\boldsymbol \\beta} + \\frac{\\mathbf{H}_{ii}}{1 - \\mathbf{H}_{ii}} \\left( y_i - x_i^\\text{T}\\widehat{\\boldsymbol \\beta} \\right)\\\\ =&amp; \\frac{y_i - x_i^\\text{T}\\widehat{\\boldsymbol \\beta}}{1 - \\mathbf{H}_{ii}} \\end{align}\\] This completes the proof. 5.6.1 Generalized cross-validation The generalized cross-validation (GCV, Golub, Heath, and Wahba (1979)) is a modified version of the leave-one-out CV: \\[\\text{GCV}(\\lambda) = \\frac{\\sum_{i=1}^n (y_i - x_i^\\text{T}\\widehat{\\boldsymbol \\beta}^\\text{ridge}_\\lambda)}{(n - \\text{Trace}{\\mathbf{S}_\\lambda})}\\] where \\(\\mathbf{S}_\\lambda\\) is the hat matrix corresponding to the ridge regression: \\[\\mathbf{S}_\\lambda = \\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X}+ \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\text{T}\\] The following plot shows how GCV value changes as a function of \\(\\lambda\\). # use GCV to select the best lambda plot(fit$lambda[1:500], fit$GCV[1:500], type = &quot;l&quot;, col = &quot;darkorange&quot;, ylab = &quot;GCV&quot;, xlab = &quot;Lambda&quot;, lwd = 3) title(&quot;Prostate Cancer Data: GCV&quot;) We can select the best \\(\\lambda\\) that produces the smallest GCV. fit$lambda[which.min(fit$GCV)] ## [1] 6.8 round(coef(fit)[which.min(fit$GCV), ], 4) ## lcavol lweight age lbph svi lcp gleason pgg45 ## 0.0170 0.4949 0.6050 -0.0169 0.0863 0.6885 -0.0420 0.0634 0.0034 5.7 The glmnet package The glmnet package implements the \\(k\\)-fold cross-validation. To perform a ridge regression with cross-validation, we need to use the cv.glmnet() function with \\(alpha = 0\\). Here, the \\(\\alpha\\) is a parameter that controls the \\(\\ell_2\\) and \\(\\ell_1\\) (Lasso) penalties. In addition, the lambda values are also automatically selected, on the log-scale. library(glmnet) ## Loading required package: Matrix ## Loaded glmnet 4.1-2 set.seed(3) fit2 = cv.glmnet(data.matrix(prostate[, 1:8]), prostate$lpsa, nfolds = 10, alpha = 0) plot(fit2$glmnet.fit, &quot;lambda&quot;) It is useful to plot the cross-validation error against the \\(\\lambda\\) values , then select the corresponding \\(\\lambda\\) with the smallest error. The corresponding coefficient values can be obtained using the s = \"lambda.min\" option in the coef() function. However, this can still be subject to over-fitting, and sometimes practitioners use s = \"lambda.1se\" to select a slightly heavier penalized version based on the variations observed from different folds. plot(fit2) coef(fit2, s = &quot;lambda.min&quot;) ## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 0.011566730 ## lcavol 0.492211875 ## lweight 0.604155671 ## age -0.016727236 ## lbph 0.085820464 ## svi 0.685477645 ## lcp -0.039717080 ## gleason 0.063806235 ## pgg45 0.003411982 coef(fit2, s = &quot;lambda.1se&quot;) ## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 0.035381751 ## lcavol 0.264613824 ## lweight 0.421408729 ## age -0.002555681 ## lbph 0.049916919 ## svi 0.452500471 ## lcp 0.075346975 ## gleason 0.083894617 ## pgg45 0.002615235 Reference "],["lasso.html", "Chapter 6 Lasso 6.1 One-Variable Lasso and Shrinkage", " Chapter 6 Lasso Lasso (Tibshirani 1996) is among the most popular machine learning models. Different from the Ridge regression, its adds \\(\\ell_1\\) penalty on the fitted parameters: \\[ \\begin{align} \\widehat{\\boldsymbol \\beta}^\\text{ridge} =&amp; \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} (\\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta)^\\text{T}(\\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta) + n \\lambda \\lVert\\boldsymbol \\beta\\rVert_1\\\\ =&amp; \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^\\text{T}\\boldsymbol \\beta)^2 + \\lambda \\sum_{i=1}^n |\\beta_j|, \\end{align} \\] The main advantage of adding such a penalty is that small \\(\\widehat{\\beta}_j\\) values can be shrunk to zero. This may prevents over-fitting and also improve the interpretability especially when the number of variables is large. We will analyze the Lasso starting with a single variable case, and then discuss the application of coordinate descent algorithm to obtain the solution. 6.1 One-Variable Lasso and Shrinkage To illustrate how Lasso shrink a parameter estimate to zero, lets consider an orthogonal design matrix case, , i.e., \\(\\mathbf{X}^\\text{T}\\mathbf{X}= n \\mathbf{I}\\), which will eventually reduce to a one-variable problem. Note that the intercept term is not essential because we can always pre-center the observed data \\(x_i\\) and \\(y_i\\)s so that they can be recovered after this one variable problem. Our objective function is \\[\\frac{1}{n}\\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert^2 + \\lambda \\lVert\\boldsymbol \\beta\\rVert_1\\] We are going to relate the solution the OLS solution, which exists in this case because \\(\\mathbf{X}^\\text{T}\\mathbf{X}\\) is invertible. Hence, we have \\[\\begin{align} &amp;\\frac{1}{n}\\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert^2 + \\lambda \\lVert\\boldsymbol \\beta\\rVert_1\\\\ =&amp;\\frac{1}{n}\\lVert \\mathbf{y}- \\color{OrangeRed}{\\mathbf{X}\\widehat{\\boldsymbol \\beta}^\\text{ols} + \\mathbf{X}\\widehat{\\boldsymbol \\beta}^\\text{ols}} - \\mathbf{X}\\boldsymbol \\beta\\rVert^2 + \\lambda \\lVert\\boldsymbol \\beta\\rVert_1\\\\ =&amp;\\frac{1}{n}\\lVert \\mathbf{y}- \\mathbf{X}\\widehat{\\boldsymbol \\beta}^\\text{ols} \\rVert^2 + \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\boldsymbol \\beta}^\\text{ols} - \\mathbf{X}\\boldsymbol \\beta\\rVert^2 + \\lambda \\lVert\\boldsymbol \\beta\\rVert_1 \\end{align}\\] The cross-term is zero because the OLS residual term is orthogonal to the columns of \\(\\mathbf{X}\\): \\[\\begin{align} &amp;2(\\mathbf{y}- \\mathbf{X}\\widehat{\\boldsymbol \\beta}^\\text{ols})^\\text{T}(\\mathbf{X}\\widehat{\\boldsymbol \\beta}^\\text{ols} - \\mathbf{X}\\boldsymbol \\beta)\\\\ =&amp; 2\\mathbf{r}^\\text{T}\\mathbf{X}(\\widehat{\\boldsymbol \\beta}^\\text{ols} - \\boldsymbol \\beta)\\\\ =&amp; 0 \\end{align}\\] Then we just need to optimize the part that involves \\(\\boldsymbol \\beta\\): \\[\\begin{align} &amp;\\underset{\\boldsymbol \\beta}{\\mathop{\\mathrm{arg\\,min}}} \\frac{1}{n}\\lVert \\mathbf{y}- \\mathbf{X}\\widehat{\\boldsymbol \\beta}^\\text{ols} \\rVert^2 + \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\boldsymbol \\beta}^\\text{ols} - \\mathbf{X}\\boldsymbol \\beta\\rVert^2 + \\lambda \\lVert\\boldsymbol \\beta\\rVert_1\\\\ =&amp;\\underset{\\boldsymbol \\beta}{\\mathop{\\mathrm{arg\\,min}}} \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\boldsymbol \\beta}^\\text{ols} - \\mathbf{X}\\boldsymbol \\beta\\rVert^2 + \\lambda \\lVert\\boldsymbol \\beta\\rVert_1\\\\ =&amp;\\underset{\\boldsymbol \\beta}{\\mathop{\\mathrm{arg\\,min}}} \\frac{1}{n} (\\widehat{\\boldsymbol \\beta}^\\text{ols} - \\boldsymbol \\beta)^\\text{T}\\mathbf{X}^\\text{T}\\mathbf{X}(\\widehat{\\boldsymbol \\beta}^\\text{ols} - \\boldsymbol \\beta) + \\lambda \\lVert\\boldsymbol \\beta\\rVert_1\\\\ =&amp;\\underset{\\boldsymbol \\beta}{\\mathop{\\mathrm{arg\\,min}}} \\frac{1}{n} (\\widehat{\\boldsymbol \\beta}^\\text{ols} - \\boldsymbol \\beta)^\\text{T}n \\mathbf{I}(\\widehat{\\boldsymbol \\beta}^\\text{ols} - \\boldsymbol \\beta) + \\lambda \\lVert\\boldsymbol \\beta\\rVert_1\\\\ =&amp;\\underset{\\boldsymbol \\beta}{\\mathop{\\mathrm{arg\\,min}}} \\sum_{j = 1}^p (\\widehat{\\boldsymbol \\beta}^\\text{ols}_j - \\boldsymbol \\beta_j )^2 + \\lambda \\sum_j |\\boldsymbol \\beta_j|\\\\ \\end{align}\\] This is a separable problem meaning that we can solve each \\(\\beta_j\\) independently since they do not interfere each other. Then the univariate problem is \\[\\underset{\\beta}{\\mathop{\\mathrm{arg\\,min}}} \\,\\, (\\beta - a)^2 + \\lambda |\\beta|\\] We learned that to solve for an optimizer, we can set the gradient to be zero. However, the function is not everywhere differentiable. Still, we can separate this into two cases: \\(\\beta &gt; 0\\) and \\(\\beta &lt; 0\\). For the positive side, we have \\[\\begin{align} 0 =&amp; \\frac{\\partial}{\\partial \\beta} \\,\\, (\\beta - a)^2 + \\lambda |\\beta| = 2 (\\beta - a) + \\lambda \\\\ \\Longrightarrow \\quad \\beta =&amp;\\, a - \\lambda/2 \\end{align}\\] However, this will maintain positive only when \\(\\beta\\) is greater than \\(a - \\lambda/2\\). The negative size is similar. And whenever \\(\\beta\\) falls in between, it will be shrunk to zero. Overall, for our previous univariate optimization problem, the solution is \\[\\begin{align} \\hat\\beta_j^\\text{lasso} &amp;= \\begin{cases} \\hat\\beta_j^\\text{ols} - \\lambda/2 &amp; \\text{if} \\quad \\hat\\beta_j^\\text{ols} &gt; \\lambda/2 \\\\ 0 &amp; \\text{if} \\quad |\\hat\\beta_j^\\text{ols}| &lt; \\lambda/2 \\\\ \\hat\\beta_j^\\text{ols} + \\lambda/2 &amp; \\text{if} \\quad \\hat\\beta_j^\\text{ols} &lt; -\\lambda/2 \\\\ \\end{cases}\\\\ &amp;= \\text{sign}(\\hat\\beta_j^\\text{ols}) \\left(|\\hat\\beta_j^\\text{ols}| - \\lambda/2 \\right)_+ \\end{align}\\] This implies that when \\(\\lambda\\) is large enough, the estimated \\(\\beta\\) parameter of Lasso will be shrunk towards zero. The following animated figure demonstrates how adding an \\(\\ell_1\\) penalty can change the optimizer. Reference "],["reference.html", "Chapter 7 Reference", " Chapter 7 Reference "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
