[["index.html", "Statistical Learning with R Preface Target Audience Whats Covered? Acknowledgements License", " Statistical Learning with R Ruoqing Zhu 2021-06-23 Preface Welcome to Statistical Learning with R! I started this project during the summer of 2018 when I was preparing for the Stat 432 course. At that time, our faculty member Dr. David Dalpiaz, had decided to move to The Ohio State University (although he later on moved back to UIUC). David introduced to me this awesome way of publishing website on GitHub, which is a very efficient approach for developing courses. Since I was also teaching Stat 542 (Statistical Learning) for several years, I figured it could be beneficial to integrate what I have to this existing book by David and use it as the R material for both courses. As you can tell, I am not being very creative on the name, so `SLWR it is. You can find the source file of this book on my GitHub. Target Audience This book is targeted at advanced undergraduate to first/second year Ph.D students in Statistics who have prior knowledge in statistics. Previous experience with both basic mathematics (mainly linear algebra), statistical modeling (such as linear regressions) and R are assumed. Whats Covered? I currently plan to include the following topics: Basics Knowledge Linear and Penalized Linear Regressions Unsupervised Learning Classification Non-parametric Statistical Models Machine Learning Models Appendix The goal of this book is to introduce not only how to run some of the popular statistical learning models in R, but also touches some basic algorithms and programming techniques for solving some of these models. For each section, the difficulty may gradually increase from an undergraduate level to a graduate level. It will be served as a supplement to An Introduction to Statistical Learning (James et al. 2013) for STAT 432 - Basics of Statistical Learning and The Elements of Statistical Learning: Data Mining, Inference, and Prediction (Hastie, Tibshirani, and Friedman 2001) for STAT 542 - Statistical Learning at the University of Illinois at Urbana-Champaign. This book is under active development as I am teaching STAT 432 during Fall 2018. Hence, you may encounter errors ranging from typos to broken code, to poorly explained topics. If you do, please let me know! Simply send an email and I will make the changes as soon as possible (rqzhu AT illinois DOT edu). Or, if you know R Markdown and are familiar with GitHub, make a pull request and fix an issue yourself!. These contributions will be acknowledged. Acknowledgements The initial contents are derived from Dr. David Dalpiazs book. My STAT 542 course materials are also inspired by Dr. Feng Liang and Dr. John Marden who developed earlier versions of this course. And I also incorporated many online resources, such as bookdown: Authoring Books and Technical Documents with R Markdown R Programming for Data Science and others through Google search. License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. References "],["r-and-rstudio.html", "Chapter 1 R and RStudio 1.1 Resources and Guides 1.2 Basic Mathematical Operations 1.3 Read-in Data from Other Sources 1.4 Using Packages 1.5 Explore Yourself", " Chapter 1 R and RStudio R is a free-to-use software that is very popular in statistical computing. You can download R from its official website. Another software that makes using R easier is RStudio, which is available at here. You can find many online guides that help you to set-up these two software, for example, this YouTube video. R Markdown is a built-in feature of RStudio. It works like an integration of LaTex and programming playground that compiles source code into nice-looking PDF, HTML, or MS Word files. This book is created using an extension of R Markdown, developed by Yihui Xie. 1.1 Resources and Guides There are many online resources for how to use R, RStudio, and R Markdown. For example, David Dalpiazs other online book Applied Statistics with R contains an introduction to using them. There are also other online documentation such as Install R and RStudio R tutorial Data in R Play-list (video) R and RStudio Play-list (video) R Markdown Cheat Sheet R Markdown Play-list (video) It is worth to mention that once you become a developer of R packages using C/C++ (add-on of R for performing specific tasks), and you also happen to use Windows like I do, you have to install this Rtools that contains compilers. This is also needed if you want to manually install any R package using a source (.tar.gz files) instead of using the so-called binaries (.zip files). 1.2 Basic Mathematical Operations We will briefly cover some basic R calculations and operations. If you want to see more information about a particular function or operator in R, the easiest way is to get the reference document. Put a question mark in front of a function name: # In a default R console window, this will open up a web browser. # In RStudio, this will be displayed at the Help window at the bottom-right penal. ?log2 ?matrix Try type-in the following commands into your R console and start to explore yourself. Most of them are self-explanatory. Lines with a # in the front are comments, which will not be executed. Lines with ## in the front are outputs. # Basic mathematical operations 1 + 3 ## [1] 4 3*5 ## [1] 15 3^5 ## [1] 243 exp(2) ## [1] 7.389056 log(3) ## [1] 1.098612 log2(3) ## [1] 1.584963 factorial(5) ## [1] 120 1.2.1 Data Objects Data objects can be a complicated topic for people who never used R before. Most common data objects are vector, matrix, list, and data.frame. Operations on vectors are matrices are fairly intuitive. # creating a vector c(1,2,3,4) ## [1] 1 2 3 4 c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; # creating matrix from a vector matrix(c(1,2,3,4), 2, 2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 x = c(1,1,1,0,0,0); y = c(1,0,1,0,1,0) cbind(x,y) ## x y ## [1,] 1 1 ## [2,] 1 0 ## [3,] 1 1 ## [4,] 0 0 ## [5,] 0 1 ## [6,] 0 0 # matrix multiplication using &#39;%*%&#39; matrix(c(1,2,3,4), 2, 2) %*% t(cbind(x,y)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 4 1 4 0 3 0 ## [2,] 6 2 6 0 4 0 Simple mathematical operations on vectors and matrices are usually element-wise. You can easily extract certain elements of them by using the [] operator, like a C programming reference style. # some simple operations x[3] ## [1] 1 x[2:5] ## [1] 1 1 0 0 cbind(x,y)[1:2, ] ## x y ## [1,] 1 1 ## [2,] 1 0 (x + y)^2 ## [1] 4 1 4 0 1 0 length(x) ## [1] 6 dim(cbind(x,y)) ## [1] 6 2 # A warning will be issued when R detects something wrong. Results may still be produced. x + c(1,2,3,4) ## Warning in x + c(1, 2, 3, 4): longer object length is not a multiple of shorter object length ## [1] 2 3 4 4 1 2 list() simply creates a list of objects (of any type). However, some operators cannot be directly applied to a list in a similar way as to vectors or matrices. Model fitting results in R are usually stored as a list (for example, the lm() function used in Section ??. # creating a list x = list(c(1,2), &quot;hello&quot;, matrix(c(1,2,3,4), 2, 2)) x[[1]] ## [1] 1 2 data.frame() creates a list of vectors of equal length, and display them as a matrix-like object where each vector is a column of the matrix. It is mainly used for storing data. For example the famous iris data. The first four columns are numerical variables, while the last column is a categorical variable with three levels: setosa, versicolor, and virginica. # the head function peeks the first several rows of the dataset head(iris, n = 3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa # data frame can be called by each individual column iris$Species[c(1, 51, 101)] ## [1] setosa versicolor virginica ## Levels: setosa versicolor virginica # the summary function can be used to view all variables summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 1.3 Read-in Data from Other Sources Data can be imported from a variety of sources. More commonly, a dataset can be stored in .txt, .csv or other file formats. # read-in data birthrate = read.csv(&quot;data/birthrate.csv&quot;) head(birthrate) ## Year Birthrate ## 1 1917 183.1 ## 2 1918 183.9 ## 3 1919 163.1 ## 4 1920 179.5 ## 5 1921 181.4 ## 6 1922 173.4 # to see how many observations (rows) and variables (columns) in a dataset dim(birthrate) ## [1] 87 2 R data can also be saved into other formats. The more efficient way, assuming that you are going to load these file back to R in the future, is to save them as .RData file. Usually, for a large dataset, this reduces the time spend on reading in the data. # saving a object to .RData file save(birthrate, file = &quot;mydata.RData&quot;) # you can specify multiple objects too save(birthrate, iris, file = &quot;mydata.RData&quot;) # load the data again load(&quot;mydata.RData&quot;) # save data to .csv file write.csv(birthrate, file = &quot;mydata.csv&quot;) 1.4 Using Packages Packages are written and contributed to R by individuals. They provide additional features (functions or data) that serve particular needs. The package ElemStatLearn is created for the textbook (Hastie, Tibshirani, and Friedman 2001) and contains may popular datasets. A package needs to be installed and loaded to your local computer. # install packages install.packages(&quot;ElemStatLearn&quot;) # load package library(ElemStatLearn) # load a dataset from the package data(SAheart) head(SAheart) ## sbp tobacco ldl adiposity famhist typea obesity alcohol age chd ## 1 160 12.00 5.73 23.11 Present 49 25.30 97.20 52 1 ## 2 144 0.01 4.41 28.61 Absent 55 28.87 2.06 63 1 ## 3 118 0.08 3.48 32.28 Present 52 29.14 3.81 46 0 ## 4 170 7.50 6.41 38.03 Present 51 31.99 24.26 58 1 ## 5 134 13.60 3.50 27.78 Present 60 25.99 57.34 49 1 ## 6 132 6.20 6.47 36.21 Present 62 30.77 14.14 45 0 1.5 Explore Yourself There is no guide that can exhaust all information. I found the best learning approach is to perform a specific task and google your way to the success. Oftentimes, Stack Overflow is my best friend, especially when I am developing new packages. Also, read the reference manual carefully if you use a particular package or function. A reference manual (for packages on CRAN) can always be found at https://cran.r-project.org/web/packages/package_name. References "],["rmarkdown-basics.html", "Chapter 2 RMarkdown Basics 2.1 Acknowledgement 2.2 Getting Started 2.3 Adding R 2.4 Importing Data 2.5 Working Directory 2.6 Packages 2.7 Plotting 2.8 Chunk Options 2.9 Adding Math with LaTeX 2.10 Output Options 2.11 Try It!", " Chapter 2 RMarkdown Basics 2.1 Acknowledgement This section was originally created by David Dalpiaz. The original file can be downloaded from his R4SL online text book. I made some slight modifications. This is by no means a comprehensive guide, and there are many other resources available online. 2.2 Getting Started RMarkdown at its core is a combination of R and Markdown used to generate reproducible reports for data analyses. Markdown and R are mixed together in a .Rmd file, which can then be rendered into a number of formats including .html, .pdf, and .docx. There will be a strong preference for .html in this course. Have a look at (this source file) to see how this document was generated! It should be read alongside the rendered .html to best understand how everything works. Alternatively, you could render the .Rmd inside RStudio, and youll automatically have both side-by-side. You can also modify the .Rmd along the way, and see what effects your modifications have. Formatting text is easy. Bold can be done using ** or __ before and after the text. Italics can be done using * or _ before and after the text. For example, This is bold. This is italics. and this is bold italics. This text appears as monospaced. Unordered list element 1. Unordered list element 2. Unordered list element 3. Ordered list element 1. Ordered list element 2. Ordered list element 3. We could mix lists and links. Note that a link can be constructed in the format [display text](http link). If colors are desired, we can customize it using, for example, [\\textcolor{blue}{display text}](http link). A default link: RMarkdown Documentation colored link 1: colored link 2: Tables are sometimes tricky using Markdown. See the above link for a helpful Markdown table generator. A B C 1 2 3 Do Re Mi 2.3 Adding R So far we have only used Markdown to create html. This is useful by itself, but the real power of RMarkdown comes when we add R. There are two ways we can do this. We can use R code chunks, or run R inline. 2.3.1 R Chunks The following is an example of an R code chunk # define function get_sd = function(x, biased = FALSE) { n = length(x) - 1 * !biased sqrt((1 / n) * sum((x - mean(x)) ^ 2)) } # generate random sample data set.seed(42) (test_sample = rnorm(n = 10, mean = 2, sd = 5)) ## [1] 8.8547922 -0.8234909 3.8156421 5.1643130 4.0213416 1.4693774 ## [7] 9.5576100 1.5267048 12.0921186 1.6864295 # run function on generated data get_sd(test_sample) ## [1] 4.177244 There is a lot going on here. In the .Rmd file, notice the syntax that creates and ends the chunk. Also note that example_chunk is the chunk name. Everything between the start and end syntax must be valid R code. Chunk names are not necessary, but can become useful as your documents grow in size. In this example, we define a function, generate some random data in a reproducible manner, displayed the data, then ran our function. 2.3.2 Inline R R can also be run in the middle of the exposition. For example, the mean of the data we generated is 4.7364838. 2.4 Importing Data When using RMarkdown, any time you knit your document to its final form, say .html, a number of programs run in the background. Your current R environment seen in RStudio will be reset. Any objects you created while working interactively inside RStudio will be ignored. Essentially a new R session will be spawned in the background and the code in your document is run there from start to finish. For this reason, things such as importing data must be explicitly coded into your document. library(readr) example_data = read_table(&quot;data/skincancer.txt&quot;) The above loads the online file. In many cases, you will load a file that is locally stored in your own computer. In that case, you can either specify the full file path, or simply use, for example read_csv(\"filename.csv\") if that file is stored at your working directory. The working directory will usually be the directory that contains your .Rmd file. You are recommended to reference data in this manner. Note that we use the newer read_csv() from the readr package instead of the default read.csv(). 2.5 Working Directory Whenever R code is run, there is always a current working directory. This allows for relative references to external files, in addition to absolute references. Since the working directory when knitting a file is always the directory that contains the .Rmd file, it can be helpful to set the working directory inside RStudio to match while working interactively. To do so, select Session &gt; Set Working Directory &gt; To Source File Location while editing a .Rmd file. This will set the working directory to the path that contains the .Rmd. You can also use getwd() and setwd() to manipulate your working directory programmatically. These should only be used interactively. Using them inside an RMarkdown document would likely result in lessened reproducibility. As of recent RStudio updates, this practice is not always necessary when working interactively. If lines of code are being Output Inline, then the working directory is automatically the directory which contains the .Rmd file. 2.6 Packages Packages are key to using R. The community generated packages are a large part of Rs success, and it is extremely rare to perform an analysis without using at least some packages. Once installed, packages must be loaded before they are used, so again, since your environment is initialized with nothing during knitting, these must be included in your RMarkdown file. #install.packages(&quot;ggplot2&quot;) library(ggplot2) Here we load the ggplot2 package, which should be installed interactively before knitting the file. The install command is included for reference, but commented out. It could be left uncommented, but then the package would re-install every time you knit your document. #install.packages(&quot;rmarkdown&quot;) Note that rmarkdown is actually a package in R! If R never prompts you to install rmarkdown and its associated packages when first creating an RMarkdown document, use the above command to install them manually. 2.7 Plotting The following generates a boring plot, which displays the skin cancer mortality plot(Mort ~ Lat, data = example_data) This next plot, uses data from the package ggplot2 to create a more interesting plot. Notice it is huge in the resulting document, since we have modified some chunk options in the RMarkdown file to manipulate its size. plot(Mort ~ Lat, data = example_data, xlab = &quot;Latitude&quot;, ylab = &quot;Skin Cancer Mortality Rate&quot;, main = &quot;Skin Cancer Mortality vs. State Latitude&quot;, pch = 19, cex = 1.5, col = &quot;deepskyblue&quot;) But you can also notice that the labels and the plots becomes disporportional when the figure size is set too small. This can be resolved using a scalling option such as out.width = '40%, but enlarge the original figure size: 2.8 Chunk Options We have already seen chunk options fig.height, fig.width, and out.width which modified the size of plots from a particular chunk. There are many chunk options, but we will discuss some others which are frequently used including; eval, echo, message, and warning. If you noticed, the plot above was displayed without showing the code. install.packages(&quot;rmarkdown&quot;) ?log View(mpg) Using eval = FALSE the above chunk displays the code, but it is not run. Weve already discussed not wanting install code to run. The ? code pulls up documentation of a function. This will spawn a browser window when knitting, or potentially crash during knitting. Similarly, using View() is an issue with RMarkdown. Inside RStudio, this would pull up a window which displays the data. However, when knitting, R runs in the background and RStudio is not modifying the View() function. This, on OSX especially, usually causes knitting to fail. ## [1] &quot;Hello World!&quot; Above, we see output, but no code! This is done using echo = FALSE, which is often useful. x = 1:10 y = 1:10 summary(lm(y ~ x)) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.661e-16 -1.157e-16 4.273e-17 2.153e-16 4.167e-16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.123e-15 2.458e-16 4.571e+00 0.00182 ** ## x 1.000e+00 3.961e-17 2.525e+16 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.598e-16 on 8 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 6.374e+32 on 1 and 8 DF, p-value: &lt; 2.2e-16 The above code produces a warning, for reasons we will discuss later. Sometimes, in final reports, it is nice to hide these, which we have done here. message = FALSE and warning = FALSE can be used to do so. Messages are often created when loading packages to give the user information about the effects of loading the package. These should be suppressed in final reports. Be careful about suppressing these messages and warnings too early in an analysis as you could potentially miss important information! 2.9 Adding Math with LaTeX Another benefit of RMarkdown is the ability to add Latex for mathematics typesetting. Like R code, there are two ways we can include Latex; displaystyle and inline. Note that use of LaTeX is somewhat dependent on the resulting file format. For example, it cannot be used at all with .docx. To use it with .pdf you must have LaTeX installed on your machine. With .html the LaTeX is not actually rendered during knitting, but actually rendered in your browser using MathJax. 2.9.1 Displaystyle LaTeX Displaystyle is used for larger equations which appear centered on their own line. This is done by putting $$ before and after the mathematical equation. \\[ \\widehat \\sigma = \\sqrt{\\frac{1}{n - 1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\] 2.9.2 Inline LaTex We could mix LaTeX commands in the middle of exposition, for example: \\(t = 2\\). We could actually mix R with Latex as well! For example: \\(\\bar{x} = 4.7364838\\). 2.10 Output Options At the beginning of the document, there is a code which describes some metadata and settings of the document. For this file, that code is: title: &quot;RMarkdown Template&quot; author: &quot;Your Name&quot; date: &quot;Aug 26, 2018&quot; output: html_document: toc: yes This describes the output format as html, defines the theme, and toc tells R to automatically create a Table of Contents based on the headers and sub-headers you have defined using #. You can remove this line if thats not what you needed. You can edit this yourself, or click the settings button at the top of the document and select Output Options.... Here you can explore other themes and syntax highlighting options, as well as many additional options. Using this method will automatically modify this information in the document. 2.11 Try It! Be sure to play with this document! Change it. Break it. Fix it. The best way to learn RMarkdown (or really almost anything) is to try, fail, then find out what you did wrong. RStudio has provided a number of beginner tutorials which have been greatly improved recently and detail many of the specifics potentially not covered in this document. RMarkdown is continually improving, and this document covers only the very basics. "],["basics-of-probability-and-statistics.html", "Chapter 3 Basics of Probability and Statistics 3.1 Random Number Generation 3.2 Summary Statistics and Data Visualization", " Chapter 3 Basics of Probability and Statistics 3.1 Random Number Generation Random number generation is important for statistical simulation. R provides random number generators for many commonly used distributions, such as binomial (rbinom), normal (rnorm), t (rt) etc. The syntax is usually a letter r followed by the name of the distribution. # generate 10 independent Bernoulli random variables as a vector rbinom(n=10, size = 1, prob = 0.5) ## [1] 1 1 1 0 1 1 1 0 1 0 # 4 independent random standard normal variables rnorm(n=4) ## [1] 0.06927116 -0.21617874 -1.69964766 -0.85785491 Setting the seed before generating random numbers will allow us to replicate the results when necessary. # after setting the seed, the two runs will generate exactly the same &quot;random&quot; numbers set.seed(1) rnorm(n=4, mean = 1, sd = 2) ## [1] -0.2529076 1.3672866 -0.6712572 4.1905616 set.seed(1) rnorm(n=4, mean = 1, sd = 2) ## [1] -0.2529076 1.3672866 -0.6712572 4.1905616 Some more complicated distributions require additional packages. For example, the MASS package can be used to generate the multivariate normal distribution. One needs to specify a vector of means and an invertable covariance matrix. library(MASS) P = 4 V &lt;- 0.5^abs(outer(1:P, 1:P, &quot;-&quot;)) mvrnorm(3, mu=1:P, Sigma=V) ## [,1] [,2] [,3] [,4] ## [1,] 2.3075926 0.6273378 2.875879 3.515928 ## [2,] 1.8215099 3.1535552 2.918031 4.528882 ## [3,] 0.2549359 1.6922485 2.920198 3.623822 3.2 Summary Statistics and Data Visualization Statistical functions that provide a summary of the data. x = rnorm(n=100, mean = 1, sd = 2) y = rnorm(n=100, mean = 2, sd = 1) sum(x) ## [1] 123.9597 mean(x) ## [1] 1.239597 var(x) ## [1] 3.12596 median(x) ## [1] 1.116457 quantile(x, c(0.25, 0.5, 0.75)) ## 25% 50% 75% ## 0.0115149 1.1164572 2.3955519 cor(x, y) ## [1] 0.05988467 For discrete data, we can use the table function. library(ElemStatLearn) data(SAheart) table(SAheart$famhist) ## ## Absent Present ## 270 192 table(SAheart[, c(&quot;famhist&quot;,&quot;chd&quot;)]) ## chd ## famhist 0 1 ## Absent 206 64 ## Present 96 96 Fishers exact test and the Chi-square test are tests of independence between two nominal variables. # We can test the association between family history (famhist) and # the indicator of coronary heart disease (chd) # using Fisher&#39;s Exact fisher.test(table(SAheart[, c(&quot;famhist&quot;,&quot;chd&quot;)])) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(SAheart[, c(&quot;famhist&quot;, &quot;chd&quot;)]) ## p-value = 6.658e-09 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.119573 4.891341 ## sample estimates: ## odds ratio ## 3.209996 # or the Chi-square test chisq.test(table(SAheart[, c(&quot;famhist&quot;,&quot;chd&quot;)])) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: table(SAheart[, c(&quot;famhist&quot;, &quot;chd&quot;)]) ## X-squared = 33.123, df = 1, p-value = 8.653e-09 For continuous variables, data visualization can be very helpful. There are many different ways to customize a plot, such as changing the color, shape, label, etc. For more advanced features, the R package ggplot2 is a very popular choice. # We use the birthrate data introduced earlier for this example birthrate = read.csv(&quot;data/birthrate.csv&quot;) plot(birthrate, pch = 19, col = &quot;darkorange&quot;, ylab = &quot;Birth Rate&quot;, main = &quot;U.S. birth rate (1917 - 2003, per 10000)&quot;) Correlations and correlation plots can be used to summarize more variables. However, be careful that factors may not be supported by this feature and could cause errors. # load the package with loading message suppressed suppressMessages(library(PerformanceAnalytics)) chart.Correlation(SAheart[, c(1:3)], histogram=TRUE, pch=&quot;+&quot;) 3-dimensional plot is also an alternative to visualize data. We demonstrate an example using the plot3D package and the scatter3D function. The observations are colored by the outcome class (chd). The package rgl can allow for an interactive plot with rotating and zooming. library(plot3D) scatter3D(SAheart$ldl, SAheart$age, log(1+SAheart$tobacco), xlab = &quot;LDL&quot;, ylab = &quot;Age&quot;, zlab = &quot;Tobacco&quot;, pch = 18, bty = &quot;u&quot;, col.var = SAheart$chd, col = ifelse(SAheart$chd == 1, &quot;darkorange&quot;, &quot;deepskyblue&quot;), colkey = FALSE) "],["modeling-basics.html", "Chapter 4 Modeling Basics 4.1 Fitting Linear Regression 4.2 Model Diagnostics 4.3 Variable Transformations and Interactions 4.4 Model Selection 4.5 Prediction", " Chapter 4 Modeling Basics 4.1 Fitting Linear Regression A simple linear regression assumes the underlying model \\[Y = \\beta_0 + {\\boldsymbol\\beta}^\\text{T} X + \\epsilon,\\] where \\(\\beta_0\\) is an intercept and \\(\\boldsymbol\\beta\\) is a vector of coefficients corresponds to each covariate. With observed data, we can estimate the regression coefficients. Lets use a classical dataset, the Boston Housing data (Harrison Jr and Rubinfeld 1978) from the MASS package. The goal of this dataset is to model the median house value (medv) using other predictors. library(MASS) data(Boston) # Fit a linear regression using all variables fit = lm(medv ~ ., data = Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ ., data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.595 -2.730 -0.518 1.777 26.199 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.646e+01 5.103e+00 7.144 3.28e-12 *** ## crim -1.080e-01 3.286e-02 -3.287 0.001087 ** ## zn 4.642e-02 1.373e-02 3.382 0.000778 *** ## indus 2.056e-02 6.150e-02 0.334 0.738288 ## chas 2.687e+00 8.616e-01 3.118 0.001925 ** ## nox -1.777e+01 3.820e+00 -4.651 4.25e-06 *** ## rm 3.810e+00 4.179e-01 9.116 &lt; 2e-16 *** ## age 6.922e-04 1.321e-02 0.052 0.958229 ## dis -1.476e+00 1.995e-01 -7.398 6.01e-13 *** ## rad 3.060e-01 6.635e-02 4.613 5.07e-06 *** ## tax -1.233e-02 3.760e-03 -3.280 0.001112 ** ## ptratio -9.527e-01 1.308e-01 -7.283 1.31e-12 *** ## black 9.312e-03 2.686e-03 3.467 0.000573 *** ## lstat -5.248e-01 5.072e-02 -10.347 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.745 on 492 degrees of freedom ## Multiple R-squared: 0.7406, Adjusted R-squared: 0.7338 ## F-statistic: 108.1 on 13 and 492 DF, p-value: &lt; 2.2e-16 The output can be overwhelming for beginners. Here, by specifying the model with medv ~ ., we are using all variables in this data as predictors, except medv itself. And by default, an intercept term is also included. However, we could also specify particular variables as predictors. For example, if per capita crime rate by town (crim), the average number of rooms (rm) are used to predict the price, and the weighted mean of distances to five Boston employment centres (dis), along with an intercept term, we specify the following fit = lm(medv ~ crim + rm + dis, data = Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ crim + rm + dis, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21.247 -2.930 -0.572 2.390 39.072 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -29.45838 2.60010 -11.330 &lt; 2e-16 *** ## crim -0.25405 0.03532 -7.193 2.32e-12 *** ## rm 8.34257 0.40870 20.413 &lt; 2e-16 *** ## dis 0.12627 0.14382 0.878 0.38 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.238 on 502 degrees of freedom ## Multiple R-squared: 0.5427, Adjusted R-squared: 0.5399 ## F-statistic: 198.6 on 3 and 502 DF, p-value: &lt; 2.2e-16 To read the output from a linear model, we usually pay attention to several key information, such as the coefficient and the p-value for each variable, and the overall model fitting F statistic and its p-value, which is almost 0 in this case. 4.2 Model Diagnostics To further evaluate this model fitting, we may plot the residuals (for assessing the normality) and the Cooks distance for identifying potential influence observations. # setup the parameters for plotting 4 figures together, in a 2 by 2 structure par(mfrow = c(2, 2)) plot(fit) R also provides several functions for obtaining metrics related to unusual observations that may help this process. resid() provides the residual for each observation hatvalues() gives the leverage of each observation rstudent() give the studentized residual for each observation cooks.distance() calculates the influence of each observation head(resid(fit), n = 10) ## 1 2 3 4 5 6 7 8 ## -1.908839 -3.129506 3.596769 3.719837 5.286113 3.757775 1.523165 4.353400 ## 9 10 ## -1.732947 -2.519590 head(hatvalues(fit), n = 10) ## 1 2 3 4 5 6 ## 0.002547887 0.002692042 0.005408817 0.005604599 0.006415502 0.004272416 ## 7 8 9 10 ## 0.004088718 0.004346169 0.007117245 0.006403949 head(rstudent(fit), n = 10) ## 1 2 3 4 5 6 7 ## -0.3061026 -0.5019650 0.5777474 0.5975886 0.8498652 0.6032833 0.2444363 ## 8 9 10 ## 0.6990195 -0.2785307 -0.4048546 head(cooks.distance(fit), n = 10) ## 1 2 3 4 5 6 ## 5.994417e-05 1.702892e-04 4.544127e-04 5.038330e-04 1.166558e-03 3.909005e-04 ## 7 8 9 10 ## 6.144012e-05 5.337764e-04 1.392832e-04 2.645454e-04 4.3 Variable Transformations and Interactions It appears that the residuals are not normally distributed because the QQ plot deviates from the diagonal line quite a lot. Sometimes variable transformations can be used to deal with this issue, but that may not fix it completely. Plotting can be useful for detecting ill-distributed variables and suggest potential transformations. For example, we may use the correlation plot to visualize them library(PerformanceAnalytics) chart.Correlation(Boston[, c(&quot;medv&quot;, &quot;crim&quot;, &quot;rm&quot;, &quot;dis&quot;)], histogram=TRUE, pch=&quot;+&quot;) It looks like both crim and dis have heavy tail on the right hand side and could benefit from a log or a power transformation. variable transformations can be easily specified within the lm() function. fit = lm(medv ~ log(crim) + rm + I(dis^0.5), data = Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ log(crim) + rm + I(dis^0.5), data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.767 -3.506 -0.589 2.501 40.035 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -25.1474 2.8674 -8.770 &lt; 2e-16 *** ## log(crim) -1.5033 0.1864 -8.067 5.36e-15 *** ## rm 8.0520 0.4096 19.656 &lt; 2e-16 *** ## I(dis^0.5) -2.1805 0.7644 -2.853 0.00451 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.155 on 502 degrees of freedom ## Multiple R-squared: 0.5548, Adjusted R-squared: 0.5521 ## F-statistic: 208.5 on 3 and 502 DF, p-value: &lt; 2.2e-16 Another approach is to consider polynomial transformations of the outcome variable, known as the Box-Cox transformation. # explore the Box-Cox transformation trans = boxcox(medv ~ log(crim) + rm + I(dis^0.5), data = Boston) # obtain the best power for performing the polynomial trans$x[which.max(trans$y)] ## [1] 0.2626263 # refit the model fit = lm(I(medv^0.2626263) ~ log(crim) + rm + I(dis^0.5), data = Boston) One can again reevaluate the model fitting results and repeat the process if necessary. However, keep in mind that this is could be a tedious process that may not end with a satisfactory solution. To further improve the model fitting we may also consider iterations and higher order terms such as fit = lm(medv ~ log(crim) + rm + rm*log(crim) + I(rm^2) + I(dis^0.5) + as.factor(chas)*rm, data = Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ log(crim) + rm + rm * log(crim) + I(rm^2) + ## I(dis^0.5) + as.factor(chas) * rm, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.476 -2.917 -0.486 2.414 35.630 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 72.0986 12.4270 5.802 1.17e-08 *** ## log(crim) 3.2224 1.1677 2.760 0.0060 ** ## rm -23.0419 3.9298 -5.863 8.27e-09 *** ## I(rm^2) 2.3857 0.3072 7.766 4.66e-14 *** ## I(dis^0.5) -1.0768 0.6729 -1.600 0.1102 ## as.factor(chas)1 14.6814 7.4142 1.980 0.0482 * ## log(crim):rm -0.7620 0.1845 -4.129 4.27e-05 *** ## rm:as.factor(chas)1 -1.6178 1.1357 -1.424 0.1549 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.321 on 498 degrees of freedom ## Multiple R-squared: 0.6699, Adjusted R-squared: 0.6653 ## F-statistic: 144.4 on 7 and 498 DF, p-value: &lt; 2.2e-16 4.4 Model Selection Suppose we have two candidate nested models, and we want to test if adding a set of new variables is significant in terms of predicting the outcome, this is essentially an F test. We can utilize the anova() function: fit = lm(medv ~ crim + rm + dis, data = Boston) fit2 = lm(medv ~ crim + rm + dis + chas + nox, data = Boston) anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: medv ~ crim + rm + dis ## Model 2: medv ~ crim + rm + dis + chas + nox ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 502 19536 ## 2 500 17457 2 2079.2 29.776 6.057e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It appears that adding the two additional variables chas and nox is significant. Selecting variables/models is a central topic in statistics. We could consider some classical tools such as the Akaike information criterion (Akaike 1998) or the Bayesian information criterion (Schwarz and others 1978). Incorporating the stepwise selection algorithm, we may find the best AIC model: # fit a full model that contains all variables full.model = lm(medv ~ ., data = Boston) # select the best AIC model by stepwise regression stepAIC = step(full.model, trace=0, direction=&quot;both&quot;) # the best set of variables being selected attr(stepAIC$terms, &quot;term.labels&quot;) ## [1] &quot;crim&quot; &quot;zn&quot; &quot;chas&quot; &quot;nox&quot; &quot;rm&quot; &quot;dis&quot; &quot;rad&quot; ## [8] &quot;tax&quot; &quot;ptratio&quot; &quot;black&quot; &quot;lstat&quot; 4.5 Prediction The predict() function is an extremely versatile function, for, prediction. When used on the result of a model fit using lm() it will, by default, return predictions for each of the data points used to fit the model. # the fitted value from a model fitting yhat1 = fit$fitted.values # predict on a set of testing data yhat2 = predict(fit) # they are the same all(yhat1 == yhat2) ## [1] FALSE We could also specify new data, which should be a data frame or tibble with the same column names as the predictors. new_obs = data.frame(crim = 0.3, rm = 6, dis = 5) predict(fit, newdata = new_obs) ## 1 ## 21.15216 We can also obtain the confidence interval for the mean response value of this new observation predict(fit, newdata = new_obs, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 21.15216 20.44473 21.8596 Lastly, we can alter the level using the level argument. Here we report a prediction interval instead of a confidence interval. predict(fit, newdata = new_obs, interval = &quot;prediction&quot;, level = 0.99) ## fit lwr upr ## 1 21.15216 4.995294 37.30903 References "],["optimization.html", "Chapter 5 Optimization", " Chapter 5 Optimization TODO: "],["k-means-clustering.html", "Chapter 6 K-means Clustering 6.1 Basic Concepts 6.2 Example 1: iris data 6.3 Example 2: clustering of image pixels", " Chapter 6 K-means Clustering 6.1 Basic Concepts The \\(k\\)-means clustering algorithm attemps to solve the following optimization problem: \\[ \\underset{C, \\, \\{m_k\\}_{k=1}^K}\\min \\sum_{k=1}^K \\sum_{C(i) = k} \\lVert x_i - m_k \\rVert^2, \\] where \\(C(\\cdot): \\{1, \\ldots, n\\} \\rightarrow \\{1, \\ldots, K\\}\\) is a cluster assignment function, and \\(m_k\\)s are the cluster means. To solve this problem, \\(k\\)-means uses an iterative approach that updates \\(C(\\cdot)\\) and \\(m_k\\)s alternatively. Suppose we have a set of six observations. We first randomly assign them into two clusters (initiate a random \\(C\\) function). Based on this cluster assignment, we can calculate the corresponding cluster mean \\(m_k\\)s. Then we will assign each observation to the closest cluster mean. In this example, only the blue point on the top will be moved to a new cluster. Then the cluster means can then be recalculated. When there is nothing to move anymore, the algorithm stops. Keep in mind that we started with a random cluster assignment, and this objective function is not convex. Hence we may obtain different results if started with different values. The solution is to try different starting points and use the best final results. This can be tuned using the nstart parameter in the kmeans() function. # some random data set.seed(1) mat = matrix(rnorm(1000), 50, 20) # if we use only one starting point kmeans(mat, centers = 3, nstart = 1)$tot.withinss ## [1] 885.8913 # if we use multiple starting point and pick the best one kmeans(mat, centers = 3, nstart = 100)$tot.withinss ## [1] 883.8241 6.2 Example 1: iris data We use the classical iris data as an example. This dataset contains three different classes, but the goal here is to learn the clusters without knowing the class labels. # plot the original data using two varaibles head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa library(ggplot2) ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() The last two variables in the iris data carry more information on separating the three classes. Hence we will only use the Petal.Length and Petal.Width. library(colorspace) par(mar = c(3, 2, 4, 2), xpd = TRUE) MASS::parcoord(iris[, -5], col = rainbow_hcl(3)[iris$Species], var.label = TRUE, lwd = 2) legend(x = 1.2, y = 1.3, cex = 1, legend = as.character(levels(iris$Species)), fill = rainbow_hcl(3), horiz = TRUE) Lets perfrom the \\(k\\)-means clustering set.seed(1) # k mean clustering iris.kmean &lt;- kmeans(iris[, 3:4], centers = 3, nstart = 20) # the center of each class iris.kmean$centers ## Petal.Length Petal.Width ## 1 1.462000 0.246000 ## 2 5.595833 2.037500 ## 3 4.269231 1.342308 # the within cluster variation iris.kmean$withinss ## [1] 2.02200 16.29167 13.05769 # the between cluster variation iris.kmean$betweenss ## [1] 519.524 # plot the fitted clusters vs. the truth iris.kmean$cluster &lt;- as.factor(iris.kmean$cluster) ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + # true cluster geom_point(alpha = 0.3, size = 3.5) + scale_color_manual(values = c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;)) + geom_point(col = c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;)[iris.kmean$cluster]) # fitted cluster 6.3 Example 2: clustering of image pixels Lets first load and plot an image of Leo. library(jpeg) img&lt;-readJPEG(&quot;images/leo.jpg&quot;) # generate a blank image par(mar=rep(0.2, 4)) plot(c(0, 400), c(0, 500), xaxt = &#39;n&#39;, yaxt = &#39;n&#39;, bty = &#39;n&#39;, pch = &#39;&#39;, ylab = &#39;&#39;, xlab = &#39;&#39;) rasterImage(img, 0, 0, 400, 500) For a jpg file, each pixel is stored as a vector with 3 elements  representing red, green and blue intensities. However, by the way, that this objective img being constructed, it is stored as a 3d array. The first two dimensions are the height and width of the figure. We need to vectorize them and treat each pixel as an observation. dim(img) ## [1] 500 400 3 # this apply function applies vecterization to each layer (r/g/b) of the image. img_expand = apply(img, 3, c) # and now we have the desired data matrix dim(img_expand) ## [1] 200000 3 Before performing the \\(k\\)-mean clustering, lets have a quick peek at the data in a 3d view. Since there are too many observations, we randomly sample a few. library(scatterplot3d) set.seed(1) sub_pixels = sample(1:nrow(img_expand), 1000) sub_img_expand = img_expand[sub_pixels, ] scatterplot3d(sub_img_expand, pch = 19, xlab = &quot;red&quot;, ylab = &quot;green&quot;, zlab = &quot;blue&quot;, color = rgb(sub_img_expand[,1], sub_img_expand[,2], sub_img_expand[,3])) The next step is to perform the \\(k\\)-mean and obtain the cluster label. For example, lets try 5 clusters. kmeanfit &lt;- kmeans(img_expand, 5) # to produce the new graph, we simply replicate the cluster mean for all observations in the same cluster new_img_expand = kmeanfit$centers[kmeanfit$cluster, ] # now we need to convert this back to the array that can be plotted as an image. # this is a lazy way to do it, but get the job done new_img = img new_img[, , 1] = matrix(new_img_expand[,1], 500, 400) new_img[, , 2] = matrix(new_img_expand[,2], 500, 400) new_img[, , 3] = matrix(new_img_expand[,3], 500, 400) # plot the new image par(mar=rep(0.2, 4)) plot(c(0, 400), c(0, 500), xaxt = &#39;n&#39;, yaxt = &#39;n&#39;, bty = &#39;n&#39;, pch = &#39;&#39;, ylab = &#39;&#39;, xlab = &#39;&#39;) rasterImage(new_img, 0, 0, 400, 500) With this technique, we can easily reproduce results with different \\(k\\) values. Apparently, as \\(k\\) increases, we get better resolution. \\(k = 30\\) seems to recover the original image fairly well. ## Warning: did not converge in 10 iterations "],["hierarchical-clustering.html", "Chapter 7 Hierarchical Clustering 7.1 Basic Concepts 7.2 Example 1: iris data 7.3 Example 2: RNA Expression Data", " Chapter 7 Hierarchical Clustering 7.1 Basic Concepts Suppose we have a set of six observations: The goal is to progressively group them together until there is only one group. During this entire process, only the pair-wise distances are used to determine which observations are grouped. Hence, the R function hclust() will take a distance matrix and perform the clustering. A distance matrix is an \\(n \\times n\\) matrix. As a default, we can use the Euclidean distance. # the Euclidean distance can be computed using dist() as.matrix(dist(x)) ## 1 2 3 4 5 6 ## 1 0.000000 1.2294164 1.7864196 1.1971565 1.4246185 1.5698349 ## 2 1.229416 0.0000000 2.3996575 0.8727261 1.9243764 2.2708670 ## 3 1.786420 2.3996575 0.0000000 2.8586738 0.4782442 0.2448835 ## 4 1.197156 0.8727261 2.8586738 0.0000000 2.4219048 2.6741260 ## 5 1.424618 1.9243764 0.4782442 2.4219048 0.0000000 0.4204479 ## 6 1.569835 2.2708670 0.2448835 2.6741260 0.4204479 0.0000000 We then use this distance matrix in the hierarchical clustering algorithm. # the Euclidean distance can be computed using dist() as.matrix(dist(x)) ## 1 2 3 4 5 6 ## 1 0.000000 1.2294164 1.7864196 1.1971565 1.4246185 1.5698349 ## 2 1.229416 0.0000000 2.3996575 0.8727261 1.9243764 2.2708670 ## 3 1.786420 2.3996575 0.0000000 2.8586738 0.4782442 0.2448835 ## 4 1.197156 0.8727261 2.8586738 0.0000000 2.4219048 2.6741260 ## 5 1.424618 1.9243764 0.4782442 2.4219048 0.0000000 0.4204479 ## 6 1.569835 2.2708670 0.2448835 2.6741260 0.4204479 0.0000000 # pass the distance matrix to hclust() # we use a complete link function hcfit &lt;- hclust(dist(x), method = &quot;complete&quot;) par(mar=c(2, 2, 2, 1)) plot(hcfit) Hence, the merging process is exactly the same as we demonstrated previous. The height of each split represents how separated the two subsets are. Selecting the number of clusters is still a tricky problem. Usually, we pick a cutoff where the height of the next split is short. Hence, the above example fits well with two clusters. 7.2 Example 1: iris data For this example, we use all variables in the distance calculation and still use the default complete linkage. iris_hc &lt;- hclust(dist(iris[, 3:4])) plot(iris_hc) This does not seem to perform very well, considering that we know the true number of classes is three. Hence, lets try some other linkage functions. iris_hc &lt;- hclust(dist(iris[, 3:4]), method = &quot;average&quot;) plot(iris_hc, hang = -1) This looks better. Now we can also consider using other approaches to plot this result. For example, the ape package provides some interesting choices. library(ape) plot(as.phylo(iris_hc), type = &quot;unrooted&quot;, cex = 0.6, no.margin = TRUE) We can also add the true class colors to the plot. This plot is motivated by the dendextend package vignettes. Of course in a realistic situation, we wouldnt know what the true class is. 7.3 Example 2: RNA Expression Data We use a tissue gene expression dataset from the tissuesGeneExpression library, available from bioconductor. I prepared the data to include only 100 genes. You can download the data from the course website. If we simply plot the data using a heatmap. By default, a heatmap uses red to denote higher values, and yellow for lower values. Note that we first plot the data without organizing the columns or rows. The data is also standardized based on columns (genes). load(&quot;data/tissue.Rda&quot;) dim(expression) ## [1] 189 100 table(tissue) ## tissue ## cerebellum colon endometrium hippocampus kidney liver placenta ## 38 34 15 31 39 26 6 head(expression[, 1:3]) ## 211298_s_at 203540_at 211357_s_at ## GSM11805.CEL.gz 7.710426 5.856596 12.618471 ## GSM11814.CEL.gz 4.741010 5.813841 5.116707 ## GSM11823.CEL.gz 11.730652 5.986338 13.206078 ## GSM11830.CEL.gz 5.061337 6.316815 9.780614 ## GSM12067.CEL.gz 4.955245 6.561705 8.589003 ## GSM12075.CEL.gz 10.469501 5.880740 13.050554 heatmap(scale(expression), Rowv = NA, Colv = NA) hierarchical clustering may help us discover interesting patterns. If we reorganize the columns and rows based on the clusters, then it may reveal underlying subclass of issues, or subgroup of genes. heatmap(scale(expression)) Note that there are many other R packages that produce more interesting plots. For example, you can try the heatmaply package. "],["principle-component-analysis.html", "Chapter 8 Principle Component Analysis 8.1 Basic Concepts 8.2 Example 1: iris Data 8.3 Example 2: Handwritten Digits", " Chapter 8 Principle Component Analysis 8.1 Basic Concepts The goal of PCA is to find a direction of data that displays the largest variance. A nice demonstration of this search of direction is provided at this r-bloggers: Suppose we have a data matrix with \\(n\\) observations and \\(p\\) variables. Principle Component Analysis (PCA) is always done by centering the variables, i.e., subtract column means from each column of the \\(n \\times p\\) data matrix. par(mfrow=c(1,2)) # generate some random data from a 2-dimensional normal distribution. library(MASS) n = 1000 Sigma = matrix(c(0.5, -0.65, -0.65, 1), 2, 2) x = mvrnorm(n, c(1, 2), Sigma) par(mar=c(2, 2, 2, 0.3)) plot(x, main = &quot;Before Centering&quot;, xlim = c(-5, 5), ylim= c(-5, 5), pch = 19, cex = 0.5) abline(h = 0, col = &quot;red&quot;) abline(v = 0, col = &quot;red&quot;) par(mar=c(2, 2, 2, 0.3)) x = scale(x, scale = FALSE) plot(x, main = &quot;After Centering&quot;, xlim = c(-5, 5), ylim= c(-5, 5), pch = 19, cex = 0.5) abline(h = 0, col = &quot;red&quot;) abline(v = 0, col = &quot;red&quot;) For our two-dimensional case, we are trying to find a line (direction) on this plain, such that if all points are projected onto this line, their coordinates have the largest variance, compared with any other line. par(mar=c(2, 2, 0.3, 0.3)) plot(x, xlim = c(-3.5, 3.5), ylim= c(-3.5, 3.5), pch = 19, cex = 0.5) abline(h = 0, col = &quot;red&quot;) abline(v = 0, col = &quot;red&quot;) # using pca pc1 = princomp(x)$loadings[,1] abline(a = 0, b = pc1[2]/pc1[1], col = &quot;deepskyblue&quot;, lwd = 4) We can then take the residuals (after projecting onto this line), and find the largest variance direction of the residuals ## Comp.1 Comp.2 ## 1.2151280 0.2348692 We usually visualize the data in these two directions instead of the original covariates. Note that the coordinates on the PCs can be obtained using either the scores in the fitted object of princomp, or simply multiply the original data matrix by the loadings. pcafit &lt;- princomp(x) # the new coordinates on PC&#39;s head(pcafit$scores) ## Comp.1 Comp.2 ## [1,] -0.01434101 0.21350868 ## [2,] 0.62899684 -0.51439210 ## [3,] -0.29229969 -0.06649302 ## [4,] 0.87280600 0.22098440 ## [5,] -0.75355573 -0.22465367 ## [6,] -2.90820693 0.15821305 # direct calculation based on projection head(x %*% pcafit$loadings) ## Comp.1 Comp.2 ## [1,] -0.01434101 0.21350868 ## [2,] 0.62899684 -0.51439210 ## [3,] -0.29229969 -0.06649302 ## [4,] 0.87280600 0.22098440 ## [5,] -0.75355573 -0.22465367 ## [6,] -2.90820693 0.15821305 # visualize the data on the PCs # Note that the both axies are scaled par(mar=c(4, 4.2, 0.3, 0.3)) plot(pcafit$scores[,1], pcafit$scores[,2], xlab = &quot;First PC&quot;, ylab = &quot;Second PC&quot;, pch = 19, cex.lab = 1.5) abline(h = 0, col = &quot;deepskyblue&quot;, lwd = 4) abline(v = 0, col = &quot;darkorange&quot;, lwd = 4) Note that there are many different functions in R that performs PCA. princomp and prcomp are the most popular ones. 8.1.1 Note: Scaling You should always center the variables when performing PCA, however, whether to use scaling (force each variable to have a standard deviation of 1) depends on the particular application. When you have variables that are extremely disproportionate, e.g., age vs. RNA expression, scaling should be used. This is to prevent some variables from dominating the PC loadings due to their large scales. When all the variables are of the similar type, e.g., color intensities of pixels in a figure, it is better to use the original scale. This is because the variables with larger variations may carry more signal. Scaling may lose that information. 8.2 Example 1: iris Data We use the iris data again. All four variables are considered in this analysis. We plot the first and second PC directions. iris_pc &lt;- prcomp(iris[, 1:4]) library(ggplot2) ggplot(data = data.frame(iris_pc$x), aes(x=PC1, y=PC2)) + geom_point(color=c(&quot;chartreuse4&quot;, &quot;darkorange&quot;, &quot;deepskyblue&quot;)[iris$Species], size = 3) One may be interested in plotting all pair-wise direction to see if lower PCs provide useful information. pairs(iris_pc$x, col=c(&quot;chartreuse4&quot;, &quot;darkorange&quot;, &quot;deepskyblue&quot;)[iris$Species], pch = 19) However, usually, the lower PCs are less informative. This can also be speculated from the eigenvalue plot, which shows how influential each PC is. plot(iris_pc, type = &quot;l&quot;, pch = 19, main = &quot;Iris PCA eigen-values&quot;) Feature contributions to the PC can be accessed through the magnitude of the loadings. This table shows that Petal.Length is the most influential variable on the first PC, with loading \\(\\approx 0.8567\\). iris_pc$rotation ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.36138659 -0.65658877 0.58202985 0.3154872 ## Sepal.Width -0.08452251 -0.73016143 -0.59791083 -0.3197231 ## Petal.Length 0.85667061 0.17337266 -0.07623608 -0.4798390 ## Petal.Width 0.35828920 0.07548102 -0.54583143 0.7536574 We can further visualize this on a plot. This can be helpful when the number of variables is large. features = row.names(iris_pc$rotation) ggplot(data = data.frame(iris_pc$rotation), aes(x=PC1, y=PC2, label=features,color=features)) + geom_point(size = 3) + geom_text(size=3) 8.3 Example 2: Handwritten Digits The handwritten zip code digits data contains 7291 training data and 2007 testing data. Each image is a \\(16 \\times 16\\)-pixel gray-scale image. Hence they are converted to a vector of 256 variables. library(ElemStatLearn) # Handwritten Digit Recognition Data # the first column is the true digit dim(zip.train) ## [1] 7291 257 Here is a sample of some images: Lets do a simpler task, using just three letters: 1, 4 and 8. zip.sub = zip.train[zip.train[,1] %in% c(1,4,8), -1] zip.sub.truth = as.factor(zip.train[zip.train[,1] %in% c(1,4,8), 1]) dim(zip.sub) ## [1] 2199 256 zip_pc = prcomp(zip.sub) plot(zip_pc, type = &quot;l&quot;, pch = 19, main = &quot;Digits 1, 4, and 8 PCA eigen-values&quot;) The eigenvalue results suggest that the first two principal components are much more influential than the rest. A pair-wise PC plot of the first four PCs may further confirm that speculation. pairs(zip_pc$x[, 1:4], col=c(&quot;chartreuse4&quot;, &quot;darkorange&quot;, &quot;deepskyblue&quot;)[zip.sub.truth], pch = 19) Lets look at the first two PCs more closely. Even without knowing the true class (no colors) we can still vaguely see 3 clusters. library(ggplot2) ggplot(data = data.frame(zip_pc$x), aes(x=PC1, y=PC2)) + geom_point(size = 2) Finally, lets briefly look at the results of PCA for all 10 different digits. Of course, more PCs are needed for this task. You can also plot other PCs to get more information. library(colorspace) zip_pc &lt;- prcomp(zip.train) plot(zip_pc, type = &quot;l&quot;, pch = 19, main = &quot;All Digits PCA eigen-values&quot;) ggplot(data = data.frame(prcomp(zip.train)$x), aes(x=PC1, y=PC2)) + geom_point(color = rainbow_hcl(10)[zip.train[,1]+1], size = 1) "],["linear-regression-and-model-selection.html", "Chapter 9 Linear Regression and Model Selection 9.1 Basic Concepts 9.2 Model Selection Criteria and Algorithm", " Chapter 9 Linear Regression and Model Selection 9.1 Basic Concepts Suppose we collect a set of observations with design matrix \\(\\mathbf{X}\\) and outcome \\(\\mathbf{y}\\), linear regression estimate the coefficients through \\[ \\widehat{\\boldsymbol \\beta} = \\underset{\\boldsymbol \\beta}{\\arg\\min} \\big( \\mathbf y - \\mathbf{X} \\boldsymbol \\beta \\big)^\\text{T} \\big( \\mathbf y - \\mathbf{X} \\boldsymbol \\beta \\big) \\] This can be viewed as either a covex optimization problem or projections on the \\(n\\) dimentional vector space. 9.1.1 Linear regression as an optimization # generate data for a simple linear regression set.seed(20) n = 100 x &lt;- cbind(1, rnorm(n)) y &lt;- x %*% c(1, 0.5) + rnorm(n) # calculate the residual sum of squares for a grid of beta values rss &lt;- function(b, x, y) sum((y - x %*% b)^2) b1 &lt;- b2 &lt;- seq(0, 2, length= 20) z = matrix(apply(expand.grid(b1, b2), 1, rss, x, y), 20, 20) # 3d plot for RSS par(mar = c(1,1,3,1)) persp(b1, b2, z, xlab = &quot;beta 1&quot;, ylab = &quot;beta 2&quot;, zlab = &quot;RSS&quot;, main=&quot;Residual Sum of Squares&quot;, col = &quot;springgreen&quot;, shade = 0.6, theta = 30, phi = 5) # The solution can be solved by any optimization algorithm optim(c(0, 0), rss, x = x, y = y)$par ## [1] 1.088813 0.679870 9.1.2 Linear regression as projections Another view is through projections in vector space. Consider each column of \\(\\mathbf{X}\\) as a vector, and project \\(\\mathbf{y}\\) onto the column space of \\(\\mathbf{X}\\). The project is \\[ \\widehat{\\mathbf{y}} = \\mathbf{X} (\\mathbf{X}^\\text{T} \\mathbf{X})^{-1}\\mathbf{X}^\\text{T} \\mathbf{y} \\doteq {\\mathbf{H}} \\mathbf{y}, \\] where \\(\\mathbf{H}\\) is a projection matrix. And the residuals are simply \\[ \\widehat{\\mathbf{e}} = \\mathbf{y} - \\widehat{\\mathbf{y}} = (\\mathbf{I} - \\mathbf{H}) \\mathbf{y} \\] When the number of variables is large, inverting \\(\\mathbf{X}^\\text{T} \\mathbf{X}\\) is expansive. The R function lm() does not calculate the inverse directly. Instead, QR decomposition can be used. You can try a larger \\(n\\) and \\(p\\) to see a significant difference. This is only for demonstration. They are not required for our course. # generate 100 observations with 3 variables set.seed(1) n = 1000 p = 500 x = matrix(rnorm(n*p), n, p) X = cbind(1, x) # the design matrix, including 1 as the first column # define the true beta, the first entry is the intercept b = as.matrix(c(1, 1, 0.5, rep(0, p-2))) # generate training y with Gaussian errors y = X %*% b + rnorm(n) # fit a linear regression model lm.fit = lm(y ~ x) # look at the coefficients beta hat head(lm.fit$coef) ## (Intercept) x1 x2 x3 x4 x5 ## 1.016479750 1.026143517 0.496792668 -0.017272409 0.005193304 0.034639107 # using normal equations by inverting the X&#39;X matrix: b = (X&#39;X)^-1 X&#39;y # however, this is very slow # check ?solve system.time({beta_hat = solve(t(X) %*% X) %*% t(X) %*% y}) ## user system elapsed ## 0.31 0.00 0.32 head(beta_hat) ## [,1] ## [1,] 1.016479750 ## [2,] 1.026143517 ## [3,] 0.496792668 ## [4,] -0.017272409 ## [5,] 0.005193304 ## [6,] 0.034639107 # you can avoid the inversion by specifying the linear equation system X&#39;X b = X&#39;y system.time({beta_hat = solve(t(X) %*% X, t(X) %*% y)}) ## user system elapsed ## 0.14 0.00 0.14 # A better approach is to use QR decomposition or the Cholesky decomposition # The following codes are not necessarily efficient, they are only for demonstration purpose # QR decomposition # direct calling the qr.coef function system.time({beta_hat = qr.coef(qr(X), y)}) ## user system elapsed ## 0.12 0.00 0.13 # or system.time({beta_hat = qr.solve(t(X) %*% X, t(X) %*% y)}) ## user system elapsed ## 0.16 0.00 0.16 # if you want to see what Q and R are QR = qr(X) Q = qr.Q(QR) R = qr.R(QR) # get inverse of R, you can check R %*% R_inv yourself # the backsolve/forwardsolve functions can be used to solve AX = b for upper/lower triangular matrix A # ?backsolve R_inv = backsolve(R, diag(p+1), upper.tri = TRUE, transpose = FALSE) beta_hat = R_inv %*% t(Q) %*% y # Cholesky Decomposition # the chol function gives upper triangular matrix # crossprod(X) = X&#39;X system.time({ R = chol(crossprod(X)) w = backsolve(R, t(X) %*% y, upper.tri = TRUE, transpose = TRUE) beta_hat = backsolve(R, w, upper.tri = TRUE, transpose = FALSE) }) ## user system elapsed ## 0.13 0.00 0.12 # or equivalently R = t(chol(crossprod(X))) w = forwardsolve(R, t(X) %*% y, upper.tri = FALSE, transpose = FALSE) beta_hat = forwardsolve(R, w, upper.tri = FALSE, transpose = TRUE) # the transpose = TRUE means that we are solving for R&#39;b = w instead of Rb = w 9.2 Model Selection Criteria and Algorithm 9.2.1 Example: diabetes dataset We use the diabetes dataset from the lars package as a demonstration of model selection. library(lars) data(diabetes) diab = data.frame(cbind(diabetes$x, &quot;Y&quot; = diabetes$y)) # A Brief Description of the Diabetes Data (Efron et al, 2004): # Ten baseline variables: age, sex, body mass index, average blood pressure, and six blood serum # measurements were obtained for each of n = 442 diabetes patients, as well as # the response of interest, a quantitative measure of disease progression one year after baseline lmfit=lm(Y~., data=diab) # When we use normal distribution likelihood for the errors, there are 12 parameters # The function AIC() directly calculates the AIC score from a lm() fitted model n = nrow(diab) p = 11 # ?AIC AIC(lmfit) # a build-in function for calculating AIC using -2log likelihood ## [1] 4795.985 n*log(sum(residuals(lmfit)^2/n)) + n + n*log(2*pi) + 2 + 2*p ## [1] 4795.985 # In many standard R packages, the AIC is calculated by removing some constants from the likelihood # We will use this value as the default ?extractAIC extractAIC(lmfit) # AIC for the full model ## [1] 11.000 3539.643 RSS = sum(residuals(lmfit)^2) n*log(RSS/n) + 2*p ## [1] 3539.643 # so the BIC for the full model is extractAIC(lmfit, k = log(n)) ## [1] 11.000 3584.648 n*log(RSS/n) + log(n)*p ## [1] 3584.648 # if we want to calculate Cp, use the formula RSS + 2*p*summary(lmfit)$sigma^2 ## [1] 1328502 # however, the scale of this is usually very large, we may consider the following version RSS/summary(lmfit)$sigma^2 + 2*p - n ## [1] 11 The step() function can be used to select the best model based on specified model selection criteria. # Model selection: stepwise algorithm # ?step # this function shows every step during the model selection step(lmfit, direction=&quot;both&quot;, k = 2) # k = 2 (AIC) is default; ## Start: AIC=3539.64 ## Y ~ age + sex + bmi + map + tc + ldl + hdl + tch + ltg + glu ## ## Df Sum of Sq RSS AIC ## - age 1 82 1264066 3537.7 ## - hdl 1 663 1264646 3537.9 ## - glu 1 3080 1267064 3538.7 ## - tch 1 3526 1267509 3538.9 ## &lt;none&gt; 1263983 3539.6 ## - ldl 1 5799 1269782 3539.7 ## - tc 1 10600 1274583 3541.3 ## - sex 1 45000 1308983 3553.1 ## - ltg 1 56015 1319998 3556.8 ## - map 1 72103 1336086 3562.2 ## - bmi 1 179028 1443011 3596.2 ## ## Step: AIC=3537.67 ## Y ~ sex + bmi + map + tc + ldl + hdl + tch + ltg + glu ## ## Df Sum of Sq RSS AIC ## - hdl 1 646 1264712 3535.9 ## - glu 1 3001 1267067 3536.7 ## - tch 1 3543 1267608 3536.9 ## &lt;none&gt; 1264066 3537.7 ## - ldl 1 5751 1269817 3537.7 ## - tc 1 10569 1274635 3539.4 ## + age 1 82 1263983 3539.6 ## - sex 1 45831 1309896 3551.4 ## - ltg 1 55963 1320029 3554.8 ## - map 1 73850 1337915 3560.8 ## - bmi 1 179079 1443144 3594.2 ## ## Step: AIC=3535.9 ## Y ~ sex + bmi + map + tc + ldl + tch + ltg + glu ## ## Df Sum of Sq RSS AIC ## - glu 1 3093 1267805 3535.0 ## - tch 1 3247 1267959 3535.0 ## &lt;none&gt; 1264712 3535.9 ## - ldl 1 7505 1272217 3536.5 ## + hdl 1 646 1264066 3537.7 ## + age 1 66 1264646 3537.9 ## - tc 1 26840 1291552 3543.2 ## - sex 1 46382 1311094 3549.8 ## - map 1 73536 1338248 3558.9 ## - ltg 1 97509 1362221 3566.7 ## - bmi 1 178537 1443249 3592.3 ## ## Step: AIC=3534.98 ## Y ~ sex + bmi + map + tc + ldl + tch + ltg ## ## Df Sum of Sq RSS AIC ## - tch 1 3686 1271491 3534.3 ## &lt;none&gt; 1267805 3535.0 ## - ldl 1 7472 1275277 3535.6 ## + glu 1 3093 1264712 3535.9 ## + hdl 1 738 1267067 3536.7 ## + age 1 0 1267805 3537.0 ## - tc 1 26378 1294183 3542.1 ## - sex 1 44686 1312491 3548.3 ## - map 1 82154 1349959 3560.7 ## - ltg 1 102520 1370325 3567.3 ## - bmi 1 189970 1457775 3594.7 ## ## Step: AIC=3534.26 ## Y ~ sex + bmi + map + tc + ldl + ltg ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1271491 3534.3 ## + tch 1 3686 1267805 3535.0 ## + glu 1 3532 1267959 3535.0 ## + hdl 1 395 1271097 3536.1 ## + age 1 11 1271480 3536.3 ## - ldl 1 39378 1310869 3545.7 ## - sex 1 41858 1313349 3546.6 ## - tc 1 65237 1336728 3554.4 ## - map 1 79627 1351119 3559.1 ## - bmi 1 190586 1462077 3594.0 ## - ltg 1 294094 1565585 3624.2 ## ## Call: ## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab) ## ## Coefficients: ## (Intercept) sex bmi map tc ldl ltg ## 152.1 -226.5 529.9 327.2 -757.9 538.6 804.2 step(lmfit, direction=&quot;backward&quot;, trace=0) # trace=0 will not print intermediate results ## ## Call: ## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab) ## ## Coefficients: ## (Intercept) sex bmi map tc ldl ltg ## 152.1 -226.5 529.9 327.2 -757.9 538.6 804.2 step(lm(Y~1, data=diab), scope=list(upper=lmfit, lower=~1), direction=&quot;forward&quot;, trace=0) ## ## Call: ## lm(formula = Y ~ bmi + ltg + map + tc + sex + ldl, data = diab) ## ## Coefficients: ## (Intercept) bmi ltg map tc sex ldl ## 152.1 529.9 804.2 327.2 -757.9 -226.5 538.6 step(lmfit, direction=&quot;both&quot;, k=log(n), trace=0) # BIC (the default value for k=2, which corresponds to AIC) ## ## Call: ## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab) ## ## Coefficients: ## (Intercept) sex bmi map tc ldl ltg ## 152.1 -226.5 529.9 327.2 -757.9 538.6 804.2 The leaps package will calculate the best model of each model size. Then we can add the penalties to the model fitting result and conclude the best model. ########################################################################## # Best subset model selection (Cp, AIC, and BIC): leaps ########################################################################## library(leaps) # performs an exhaustive search over models, and gives back the best model # (with low RSS) of each size. # the default maximum model size is nvmax=8 RSSleaps=regsubsets(as.matrix(diab[,-11]),diab[,11]) summary(RSSleaps, matrix=T) ## Subset selection object ## 10 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## sex FALSE FALSE ## bmi FALSE FALSE ## map FALSE FALSE ## tc FALSE FALSE ## ldl FALSE FALSE ## hdl FALSE FALSE ## tch FALSE FALSE ## ltg FALSE FALSE ## glu FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: exhaustive ## age sex bmi map tc ldl hdl tch ltg glu ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; RSSleaps=regsubsets(as.matrix(diab[,-11]),diab[,11], nvmax=10) summary(RSSleaps,matrix=T) ## Subset selection object ## 10 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## sex FALSE FALSE ## bmi FALSE FALSE ## map FALSE FALSE ## tc FALSE FALSE ## ldl FALSE FALSE ## hdl FALSE FALSE ## tch FALSE FALSE ## ltg FALSE FALSE ## glu FALSE FALSE ## 1 subsets of each size up to 10 ## Selection Algorithm: exhaustive ## age sex bmi map tc ldl hdl tch ltg glu ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 9 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; sumleaps=summary(RSSleaps,matrix=T) names(sumleaps) # components returned by summary(RSSleaps) ## [1] &quot;which&quot; &quot;rsq&quot; &quot;rss&quot; &quot;adjr2&quot; &quot;cp&quot; &quot;bic&quot; &quot;outmat&quot; &quot;obj&quot; sumleaps$which ## (Intercept) age sex bmi map tc ldl hdl tch ltg glu ## 1 TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## 2 TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## 3 TRUE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE ## 4 TRUE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE TRUE FALSE ## 5 TRUE FALSE TRUE TRUE TRUE FALSE FALSE TRUE FALSE TRUE FALSE ## 6 TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE TRUE FALSE ## 7 TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE FALSE ## 8 TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE ## 9 TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## 10 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE msize=apply(sumleaps$which,1,sum) n=dim(diab)[1] p=dim(diab)[2] Cp = sumleaps$rss/(summary(lmfit)$sigma^2) + 2*msize - n; AIC = n*log(sumleaps$rss/n) + 2*msize; BIC = n*log(sumleaps$rss/n) + msize*log(n); cbind(Cp, sumleaps$cp) ## Cp ## 1 148.352561 148.352561 ## 2 47.072229 47.072229 ## 3 30.663634 30.663634 ## 4 21.998461 21.998461 ## 5 9.148045 9.148045 ## 6 5.560162 5.560162 ## 7 6.303221 6.303221 ## 8 7.248522 7.248522 ## 9 9.028080 9.028080 ## 10 11.000000 11.000000 cbind(BIC, sumleaps$bic) # It seems regsubsets uses a formula for BIC different from the one we used. ## BIC ## 1 3665.879 -174.1108 ## 2 3586.331 -253.6592 ## 3 3575.249 -264.7407 ## 4 3571.077 -268.9126 ## 5 3562.469 -277.5210 ## 6 3562.900 -277.0899 ## 7 3567.708 -272.2819 ## 8 3572.720 -267.2702 ## 9 3578.585 -261.4049 ## 10 3584.648 -255.3424 BIC-sumleaps$bic # But the two just differ by a constant, so won&#39;t affect the model selection result. ## 1 2 3 4 5 6 7 8 9 10 ## 3839.99 3839.99 3839.99 3839.99 3839.99 3839.99 3839.99 3839.99 3839.99 3839.99 n*log(sum((diab[,11] - mean(diab[,11]))^2/n)) # the difference is the score of an intercept model ## [1] 3839.99 # Rescale Cp, AIC, BIC to (0,1). inrange &lt;- function(x) { (x - min(x)) / (max(x) - min(x)) } Cp = sumleaps$cp; Cp = inrange(Cp); BIC = sumleaps$bic; BIC = inrange(BIC); AIC = n*log(sumleaps$rss/n) + 2*msize; AIC = inrange(AIC); plot(range(msize), c(0, 1.1), type=&quot;n&quot;, xlab=&quot;Model Size (with Intercept)&quot;, ylab=&quot;Model Selection Criteria&quot;) points(msize, Cp, col=&quot;red&quot;, type=&quot;b&quot;) points(msize, AIC, col=&quot;blue&quot;, type=&quot;b&quot;) points(msize, BIC, col=&quot;black&quot;, type=&quot;b&quot;) legend(&quot;topright&quot;, lty=rep(1,3), col=c(&quot;red&quot;, &quot;blue&quot;, &quot;black&quot;), legend=c(&quot;Cp&quot;, &quot;AIC&quot;, &quot;BIC&quot;)) "],["ridge-regression.html", "Chapter 10 Ridge Regression 10.1 Basic Concepts", " Chapter 10 Ridge Regression 10.1 Basic Concepts Ridge regression solves the following \\(\\ell_2\\) penalized linear model \\[\\widehat {\\boldsymbol\\beta}^{\\,\\text{ridge}} = \\underset{{\\boldsymbol\\beta}}{\\arg\\min} \\,\\, \\lVert \\mathbf y- \\mathbf X{\\boldsymbol\\beta}\\rVert^2 + \\lambda \\lVert {\\boldsymbol\\beta}\\rVert^2\\] The solution can be obtained through \\[{\\boldsymbol\\beta}= \\big(\\mathbf X^\\text{T}\\mathbf X+ \\lambda \\mathbf I\\big)^{-1} \\mathbf X^\\text{T}\\mathbf y\\] 10.1.1 Correlated Variables and Convexity Ridge regression has many advantages. Most notably, it can address highly correlated variables. library(MASS) set.seed(3) n = 30 # create highly correlated variables and a linear model X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2)) y = rnorm(n, mean = X[,1] + X[,2]) # compare parameter estimates summary(lm(y~X-1))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## X1 0.5101168 1.034108 0.4932918 0.6256534 ## X2 1.2377013 1.009635 1.2258902 0.2304509 # note that the true parameters are all 1&#39;s lm.ridge(y~X-1, lambda=5) ## X1 X2 ## 0.7911361 0.8289581 We can note that the variance of both \\(\\beta_1\\) and \\(\\beta_2\\) are extremely large. This can also be visualized through an optimization point of view. The objective function for an OLS estimator is demonstrated in the following. beta1 &lt;- seq(-1, 2, 0.005) beta2 &lt;- seq(0, 3, 0.005) allbeta &lt;- data.matrix(expand.grid(beta1, beta2)) rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y), length(beta1), length(beta2)) # quantile levels for drawing contour quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75) # plot the contour contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) box() # the truth points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) As an alternative, if we add a ridge regression penalty, the contour is forced to be more convex due to the added eigenvalues. Here is a plot of the Ridge \\(\\ell_2\\) penalty. pen &lt;- matrix(apply(allbeta, 1, function(b) 3*b %*% b), length(beta1), length(beta2)) contour(beta1, beta2, pen, levels = quantile(pen, quanlvl)) points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) box() Hence, by adding this to the OLS objective function, the solution is more stable. This may be interpreted in several different ways such as: 1) the objective function is more convex; 2) the variance of the estimator is smaller. However, this causes some bias too. Choosing the tuning parameter is again a balance of the bias-variance trade-off. par(mfrow=c(1, 2)) # adding a L2 penalty to the objective function rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + 3*b %*% b, X, y), length(beta1), length(beta2)) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) box() # adding a larger penalty rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + 20*b %*% b, X, y), length(beta1), length(beta2)) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) box() 10.1.2 Example 1: The Prostate Cancer Data We use the prostate cancer data prostate from the ElemStatLearn package. The dataset contains 8 explanatory variables and one outcome lpsa, the log prostate-specific antigen value. library(ElemStatLearn) head(prostate) ## lcavol lweight age lbph svi lcp gleason pgg45 lpsa train ## 1 -0.5798185 2.769459 50 -1.386294 0 -1.386294 6 0 -0.4307829 TRUE ## 2 -0.9942523 3.319626 58 -1.386294 0 -1.386294 6 0 -0.1625189 TRUE ## 3 -0.5108256 2.691243 74 -1.386294 0 -1.386294 7 20 -0.1625189 TRUE ## 4 -1.2039728 3.282789 58 -1.386294 0 -1.386294 6 0 -0.1625189 TRUE ## 5 0.7514161 3.432373 62 -1.386294 0 -1.386294 6 0 0.3715636 TRUE ## 6 -1.0498221 3.228826 50 -1.386294 0 -1.386294 6 0 0.7654678 TRUE We fit a ridge regression on a grid of \\(\\lambda\\) values. For each \\(\\lambda\\), the coefficients of all variables are recorded. The left plot shows how these coefficients change as a function of \\(\\lambda\\) We can easily see that as \\(\\lambda\\) becomes larger, the coefficients are shrunken towards 0. To select the best \\(\\lambda\\) value, we use the GCV (generalized cross-validation) criteria. The right plot shows how GCV changes as a function of \\(\\lambda\\). Becareful that the coefficients of the fitted objects fit$coef are scaled by the standard deviation of the covariates. If you need the original scale, use coef(fit). fit = lm.ridge(lpsa~., prostate[, -10], lambda=seq(0,100,by=0.1)) par(mfrow=c(1,2)) matplot(coef(fit)[, -1], type = &quot;l&quot;, xlab = &quot;Lambda&quot;, ylab = &quot;Coefficients&quot;) text(rep(50, 8), coef(fit)[1,-1], colnames(prostate)[1:8]) title(&quot;Prostate Cancer Data: Ridge Coefficients&quot;) # use GCV to select the best lambda plot(fit$lambda[1:500], fit$GCV[1:500], type = &quot;l&quot;, col = &quot;darkorange&quot;, ylab = &quot;GCV&quot;, xlab = &quot;Lambda&quot;, lwd = 3) title(&quot;Prostate Cancer Data: GCV&quot;) We select the best \\(\\lambda\\) that produces the smallest GCV. fit$lambda[which.min(fit$GCV)] ## [1] 6.7 round(coef(fit)[which.min(fit$GCV), ], 4) ## lcavol lweight age lbph svi lcp gleason pgg45 ## 0.0185 0.4957 0.6053 -0.0170 0.0864 0.6893 -0.0427 0.0632 0.0035 An alternative approach of selecting the best \\(\\lambda\\) is using cross-validation. This can be done using the glmnet package. Note that for ridge regression, we need to specify alpha = 0. library(glmnet) set.seed(3) fit2 = cv.glmnet(data.matrix(prostate[, 1:8]), prostate$lpsa, nfolds = 10, alpha = 0) coef(fit2, s = &quot;lambda.min&quot;) ## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 0.011566730 ## lcavol 0.492211875 ## lweight 0.604155671 ## age -0.016727236 ## lbph 0.085820464 ## svi 0.685477645 ## lcp -0.039717080 ## gleason 0.063806235 ## pgg45 0.003411982 plot(fit2$glmnet.fit, &quot;lambda&quot;) "],["lasso-regression.html", "Chapter 11 Lasso Regression 11.1 Basic Concepts", " Chapter 11 Lasso Regression 11.1 Basic Concepts Lasso regression solves the following \\(\\ell_1\\) penalized linear model \\[\\widehat {\\boldsymbol\\beta}^{\\,\\text{lasso}} = \\underset{{\\boldsymbol\\beta}}{\\arg\\min} \\,\\, \\lVert \\mathbf y- \\mathbf X{\\boldsymbol\\beta}\\rVert^2 + \\lambda \\lVert {\\boldsymbol\\beta}\\rVert_1\\] We cannot obtain an analytical solution in a general case. However, for a special case with orthogonal design, i.e., \\(\\mathbf X^\\text{T}\\mathbf X= bI\\), we can see that the Lasso solution is essentially applying a soft-thresholding function to each parameter in the OLS solution. 11.1.1 Variable Selection Property Lasso regression has a variable selection property, which may shrink some coefficients to exactly 0 if the effect of that variable is small. library(MASS) set.seed(1) n = 100 # create highly correlated variables and a linear model X = mvrnorm(n, c(0, 0), matrix(c(1, -0.5, -0.5, 1), 2,2)) y = rnorm(n, mean = 0.1*X[,1] + 0.5*X[,2]) # compare parameter estimates summary(lm(y~X-1))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## X1 0.1403512 0.1279464 1.096953 2.753501e-01 ## X2 0.5686526 0.1272897 4.467390 2.124799e-05 We can see that the optimal solution is at around (0.140, 0.569), which are both nonzero. beta1 &lt;- seq(-0.5, 0.75, 0.005) beta2 &lt;- seq(-0.25, 1, 0.005) allbeta &lt;- data.matrix(expand.grid(beta1, beta2)) # the OLS objective function contour rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2)/n, X, y), length(beta1), length(beta2)) # quantile levels for drawing contour quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) box() # the truth points(0.1, 0.5, pch = 19, col = &quot;red&quot;, cex = 2) points(0.1403512, 0.5686526, pch = 4, col = &quot;red&quot;, cex = 2) abline(h = 0, col = &quot;deepskyblue&quot;) abline(v = 0, col = &quot;deepskyblue&quot;) As an alternative, if we add a Lasso \\(\\ell_1\\) penalty, the contour will be changed. The following plot is the contour of the penalty. pen &lt;- matrix(apply(allbeta, 1, function(b) 0.2*sum(abs(b))), length(beta1), length(beta2)) contour(beta1, beta2, pen, levels = quantile(pen, quanlvl)) points(0.1, 0.5, pch = 19, col = &quot;red&quot;, cex = 2) box() abline(h = 0, col = &quot;deepskyblue&quot;) abline(v = 0, col = &quot;deepskyblue&quot;) In addition, since the Lasso penalty is not smooth, the overall objective function will have nondifferenciable points along the axies. We can see that if a sufficiently large penalty is applied, the solution is forced to shrink some parameters to 0. This is again a bias-variance trade-off. par(mfrow=c(1, 2)) # adding a L2 penalty to the objective function rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2)/n + 0.2*sum(abs(b)), X, y), length(beta1), length(beta2)) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(0.1, 0.5, pch = 19, col = &quot;red&quot;, cex = 2) abline(h = 0, col = &quot;deepskyblue&quot;) abline(v = 0, col = &quot;deepskyblue&quot;) box() # adding a larger penalty rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2)/n + 0.5*sum(abs(b)), X, y), length(beta1), length(beta2)) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(0.1, 0.5, pch = 19, col = &quot;red&quot;, cex = 2) abline(h = 0, col = &quot;deepskyblue&quot;) abline(v = 0, col = &quot;deepskyblue&quot;) box() 11.1.2 Example 1: The Prostate Cancer Data We use the prostate cancer data prostate from the ElemStatLearn package. The dataset contains 8 explainatory variables and one outcome lpsa, the log prostate-specific antigen value. library(ElemStatLearn) head(prostate) ## lcavol lweight age lbph svi lcp gleason pgg45 lpsa train ## 1 -0.5798185 2.769459 50 -1.386294 0 -1.386294 6 0 -0.4307829 TRUE ## 2 -0.9942523 3.319626 58 -1.386294 0 -1.386294 6 0 -0.1625189 TRUE ## 3 -0.5108256 2.691243 74 -1.386294 0 -1.386294 7 20 -0.1625189 TRUE ## 4 -1.2039728 3.282789 58 -1.386294 0 -1.386294 6 0 -0.1625189 TRUE ## 5 0.7514161 3.432373 62 -1.386294 0 -1.386294 6 0 0.3715636 TRUE ## 6 -1.0498221 3.228826 50 -1.386294 0 -1.386294 6 0 0.7654678 TRUE We fit the model using the glmnet package. The tuning parameter need to be selected using cross-validation with the cv.glmnet function. library(glmnet) set.seed(3) fit2 = cv.glmnet(data.matrix(prostate[, 1:8]), prostate$lpsa, nfolds = 10) We can obtain the estimated coefficients from the best \\(\\lambda\\) value. There are usually two options, lambda.min and lambda.1se. The first one is the value that minimizes the cross-validataion error, the second one is slightly more conservative, which gives larger penalty value with more shrinkage. coef(fit2, s = &quot;lambda.min&quot;) ## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 0.1537694862 ## lcavol 0.5071477800 ## lweight 0.5455934489 ## age -0.0084065349 ## lbph 0.0618168145 ## svi 0.5899942922 ## lcp . ## gleason 0.0009732886 ## pgg45 0.0023140828 coef(fit2, s = &quot;lambda.1se&quot;) ## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 0.6435469 ## lcavol 0.4553889 ## lweight 0.3142829 ## age . ## lbph . ## svi 0.3674270 ## lcp . ## gleason . ## pgg45 . The left plots demonstrates how \\(\\lambda\\) changes the cross-validation error. There are two vertical lines, which represents lambda.min and lambda.1se respectively. The right plot shows how \\(\\lambda\\) changes the parameter values. par(mfrow = c(1, 2)) plot(fit2) plot(fit2$glmnet.fit, &quot;lambda&quot;) Some other packages can perform the same analysis, for example, the lars package. "],["splines.html", "Chapter 12 Splines 12.1 Basic Concepts", " Chapter 12 Splines What? 12.1 Basic Concepts "],["smoothing-splines.html", "Chapter 13 Smoothing Splines 13.1 Basic Concepts", " Chapter 13 Smoothing Splines What? 13.1 Basic Concepts "],["kernel-regression.html", "Chapter 14 Kernel Regression 14.1 Basic Concepts", " Chapter 14 Kernel Regression What? 14.1 Basic Concepts what? "],["kernel-density-estimation.html", "Chapter 15 Kernel Density Estimation 15.1 Basic Concepts", " Chapter 15 Kernel Density Estimation What? 15.1 Basic Concepts what? "],["references.html", "Chapter 16 References", " Chapter 16 References aaa1 "]]
