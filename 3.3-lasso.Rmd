
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.align='center')
options(width = 1000)
```

# Lasso Regression

\newcommand{\bbeta}[0]{{\boldsymbol\beta}}
\newcommand{\bX}[0]{\mathbf X}
\newcommand{\bI}[0]{\mathbf I}
\newcommand{\by}[0]{\mathbf y}
\newcommand{\T}[0]{\text{T}}

## Basic Concepts

Lasso regression solves the following $\ell_1$ penalized linear model 

$$\widehat \bbeta^{\,\text{lasso}} = \underset{\bbeta}{\arg\min} \,\, \lVert \by - \bX \bbeta \rVert^2 + \lambda \lVert \bbeta \rVert_1$$

We cannot obtain an analytical solution in a general case. However, for a special case with orthogonal design, i.e., $\bX^\T \bX = bI$, we can see that the Lasso solution is essentially applying a soft-thresholding function to each parameter in the OLS solution. 

### Variable Selection Property

Lasso regression has a variable selection property, which may shrink some coefficients to exactly 0 if the effect of that variable is small. 

```{r}
    library(MASS)
    set.seed(1)
    n = 100
    
    # create highly correlated variables and a linear model
    X = mvrnorm(n, c(0, 0), matrix(c(1, -0.5, -0.5, 1), 2,2))
    y = rnorm(n, mean = 0.1*X[,1] + 0.5*X[,2])
    
    # compare parameter estimates
    summary(lm(y~X-1))$coef
```

We can see that the optimal solution is at around `(0.140, 0.569)`, which are both nonzero. 

```{r fig.width=8, fig.height=8, out.width = '45%'}
    beta1 <- seq(-0.5, 0.75, 0.005)
    beta2 <- seq(-0.25, 1, 0.005)
    allbeta <- data.matrix(expand.grid(beta1, beta2))
    
    # the OLS objective function contour
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2)/n, X, y), 
                  length(beta1), length(beta2))
    
    # quantile levels for drawing contour
    quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)
    
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
    box()
    
    # the truth
    points(0.1, 0.5, pch = 19, col = "red", cex = 2)
    points(0.1403512, 0.5686526, pch = 4, col = "red", cex = 2)
    abline(h = 0, col = "deepskyblue")
    abline(v = 0, col = "deepskyblue")
```

As an alternative, if we add a Lasso $\ell_1$ penalty, the contour will be changed. The following plot is the contour of the penalty. 

```{r fig.width=8, fig.height=8, out.width = '45%'}
    pen <- matrix(apply(allbeta, 1, function(b) 0.2*sum(abs(b))),
                  length(beta1), length(beta2))
    
    contour(beta1, beta2, pen, levels = quantile(pen, quanlvl))
    points(0.1, 0.5, pch = 19, col = "red", cex = 2)
    box()
    abline(h = 0, col = "deepskyblue")
    abline(v = 0, col = "deepskyblue")
```

In addition, since the Lasso penalty is not smooth, the overall objective function will have nondifferenciable points along the axies. We can see that if a sufficiently large penalty is applied, the solution is forced to shrink some parameters to 0. This is again a bias-variance trade-off.

```{r fig.width=16, fig.height=8, out.width = '90%'}
    par(mfrow=c(1, 2)) 

    # adding a L2 penalty to the objective function
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2)/n + 0.2*sum(abs(b)), X, y),
                  length(beta1), length(beta2))
    
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
    points(0.1, 0.5, pch = 19, col = "red", cex = 2)
    abline(h = 0, col = "deepskyblue")
    abline(v = 0, col = "deepskyblue")
    box()
    
    # adding a larger penalty
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2)/n + 0.5*sum(abs(b)), X, y),
                  length(beta1), length(beta2))
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
    points(0.1, 0.5, pch = 19, col = "red", cex = 2)
    abline(h = 0, col = "deepskyblue")
    abline(v = 0, col = "deepskyblue")
    box()
```

### Example 1: The Prostate Cancer Data

We use the prostate cancer data `prostate` from the `ElemStatLearn` package. The dataset contains 8 explainatory variables and one outcome `lpsa`, the log prostate-specific antigen value. 

```{r}
    library(ElemStatLearn)
    head(prostate)
```

We fit the model using the `glmnet` package. The tuning parameter need to be selected using cross-validation with the `cv.glmnet` function. 

```{r fig.width=8, fig.height=8, out.width = '45%'}
    library(glmnet)
    set.seed(3)
    fit2 = cv.glmnet(data.matrix(prostate[, 1:8]), prostate$lpsa, nfolds = 10)
```

We can obtain the estimated coefficients from the best $\lambda$ value. There are usually two options, `lambda.min` and `lambda.1se`. The first one is the value that minimizes the cross-validataion error, the second one is slightly more conservative, which gives larger penalty value with more shrinkage. 

```{r}
    coef(fit2, s = "lambda.min")
    coef(fit2, s = "lambda.1se")
```

The left plots demonstrates how $\lambda$ changes the cross-validation error. There are two vertical lines, which represents `lambda.min` and `lambda.1se` respectively. The right plot shows how $\lambda$ changes the parameter values. 

```{r fig.width=16, fig.height=8, out.width = '90%'}
    par(mfrow = c(1, 2))
    plot(fit2)
    plot(fit2$glmnet.fit, "lambda")
```

Some other packages can perform the same analysis, for example, the `lars` package. 

