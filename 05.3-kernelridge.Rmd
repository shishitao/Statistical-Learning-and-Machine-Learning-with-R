\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bK{\mathbf{K}}
\def\bP{\mathbf{P}}
\def\bQ{\mathbf{Q}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Kernel Ridge Regression

With our understandings of the RKHS and the representer theorem, we can now say that for any regression function models, if we want the solution to be more flexible, we may solve it within a RKHS. For example, consider the following regression problem:

$$\widehat f = \underset{f \in \cH}{\arg\min} \,\, \frac{1}{n} \sum_{i=1}^n \Big(y_i - \widehat f(x_i) \Big)^2 + \lambda \lVert f \rVert_\cH^2$$
Since we know that the solution has to take the form 

$$\widehat f = \sum_{i=1}^n \alpha_i K(x_i, \cdot),$$
we can instead solve the problem as a ridge regression type of problem:

$$\widehat f = \underset{f \in \cH}{\arg\min} \,\, \frac{1}{n} \big\lVert \by - \bK \balpha \big\rVert^2 + \lambda \lVert f \rVert_\cH^2,$$
where $\bK$ is an $n \times n$ matrix with $K(x_i, x_j)$ at its $(i,j)$th element. With some simple calculation, we also have 

\begin{align}
\lVert f \rVert_\cH^2 =& \langle f, f \rangle \nonumber \\
=& \langle \sum_{i=1}^n \alpha_i K(x_i, \cdot), \sum_{j=1}^n \alpha_j K(x_j, \cdot) \rangle \nonumber \\
=& \sum_{i, j} \alpha_i \alpha_j \big\langle K(x_i, \cdot), K(x_j, \cdot) \big\rangle \nonumber \\
=& \sum_{i, j} \alpha_i \alpha_j K(x_i, x_j) \nonumber \\
=& \balpha^\T \bK \balpha
\end{align}

Hence, the problem becomes 

$$\widehat f = \underset{f \in \cH}{\arg\min} \,\, \frac{1}{n} \big\lVert \by - \bK \balpha \big\rVert^2 + \lambda \balpha^\T \bK \balpha.$$
By taking the derivative with respect to $\balpha$, we have (note that $\bK$ is symmetric),

\begin{align}
-\frac{1}{n} \bK^\T (\by - \bK \balpha) + \lambda \bK \balpha \overset{\text{set}}{=} \mathbf{0} \nonumber \\
\bK (- \by + \bK \balpha + n\lambda \balpha) = \mathbf{0}.
\end{align}
This implies 

$$ \balpha = (\bK + n\lambda \bI)^{-1} \by.$$
and we obtained the solution. 

## Example: Linear Kernel and Ridge Regression

When $K(\bx_i, \bx_j) = \bx_i^\T \bx_j$, we also have $\bK = \bX \bX^\T$. We should expect this to match the original ridge regression since this is essentially a linear regression. First, plug this into our previous result, we have 

$$ \balpha = (\bX \bX^\T + n\lambda \bI)^{-1} \by.$$
and the fitted value is 

$$ \widehat{\by} = \bK \balpha = \bX \bX^\T (\bX \bX^\T + n\lambda \bI)^{-1} \by$$
Using a matrix identity $(\bP \bQ + \bI)^{-1}\bP = \bP (\bQ \bP + \bI)^{-1}$, and let $\bQ = \bX = \bP^\T$, we have 

$$ \widehat{\by} = \bK \balpha = \bX (\bX^\T \bX + n\lambda \bI)^{-1} \bX^\T \by$$
and 

$$ \widehat{\by} = \bX \underbrace{\big[ \bX^\T \balpha \big]}_{\bbeta} = \bX \underbrace{\big[ (\bX^\T \bX + n\lambda \bI)^{-1} \bX^\T \by \big]}_{\bbeta}$$


which is simply the Ridge regression solution, and also the corresponding linear regression solution $\widehat{\bbeta} = \bX^\T \widehat{\balpha}$. This makes the penalty term $\balpha^\T \bK \balpha = \balpha^\T \bX \bX^\T \balpha = \bbeta^\T \bbeta$, which maps every thing back to the ridge regression form. 

## Example: Alternative View

This example is motivated from an alternative derivation provided by Prof. Max Welling on his kernel ridge regression lecture note. This understanding matches the SVM primal to dual derivation, but is performed on a linear regression. We can then again switch things to the kernel version (through kernel trick). 

Consider a linear regression

$$\underset{\bbeta}{\text{minimize}} \,\, \frac{1}{n} \lVert \by - \bX \bbeta \rVert^2 + \lambda \lVert \bbeta \rVert^2$$

Introduce a new set of variables 

$$z_i = y_i - \bx_i^\T \bbeta,$$
for $i = 1, \ldots, n$. Then The original problem becomes 

\begin{align}
\underset{\bbeta, \bz}{\text{minimize}} \quad & \frac{1}{2n\lambda} \lVert \bz \rVert^2 + \frac{1}{2} \lVert \bbeta \rVert^2 \nonumber \\
\text{subj. to} \quad & z_i = y_i - \bx_i^\T \bbeta, \,\, i = 1, \ldots, n.
\end{align}

If we use the same strategy from the SVM derivation, we have the Lagrangian

$${\cal L} = \frac{1}{2n\lambda} \lVert \bz \rVert^2 + \frac{1}{2} \lVert \bbeta \rVert^2 + \sum_{i=1}^n \alpha_i (y_i - \bx_i^\T \bbeta - z_i)$$
with $\alpha_i \in \mathbb{R}$. Switching from primal to dual, by taking derivative w.r.t. $\bbeta$ and $\bz$, we have 

\begin{align}
\frac{\partial \cal L}{\partial z_i} =&\, \frac{1}{n\lambda}z_i - \alpha_i = 0, \quad \text{for} \,\, i = 1, \ldots, n, \nonumber \\
\text{and}\,\, \frac{\partial \cal L}{\partial \bbeta} =&\, \bbeta - \sum_{i=1}^n \alpha_i \bx_i = \mathbf{0}
\end{align}

Hence, we have, the estimated $\widehat{\bbeta}$ is $\sum_{i=1}^n \alpha_i \bx_i$ that matches our previous understanding. Also, if we view this as a linear kernel solution, the predicted value of at $\bx$ is 

\begin{align}
f(\bx) =& \,\, \bx^\T \bbeta \nonumber \\
=& \sum_{i=1}^n \alpha_i \bx^\T \bx_i \nonumber \\
=& \sum_{i=1}^n \alpha_i K(\bx, \bx_i).
\end{align}

Now, to complete our dual solution, we plugin these results, and have 

\begin{align}
\underset{\balpha}{\max} \underset{\bz, \bbeta}{\min} {\cal L} =& \frac{n\lambda}{2} \balpha^\T \balpha + \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j x_i^\T x_j + \sum_{j} \alpha_j \big(y_j - x_j^\T \sum_i \alpha_i \bx_i - n\lambda \alpha_i \big) \nonumber \\
 =& - \frac{n\lambda}{2} \balpha^\T \balpha - \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j \langle x_i, x_j \rangle + \sum_{i} \alpha_i y_i \nonumber \\
=& - \frac{n\lambda}{2} \balpha^\T \balpha - \frac{1}{2} \balpha^\T \bK \balpha + \balpha^\T \by
\end{align}

By again taking derivative w.r.t. $\alpha$, we have

$$ - n\lambda \bI \balpha - \bK \balpha + \by = \mathbf{0},$$
and the solution is the same as what we had before 

$$\balpha = (\bK + n\lambda \bI)^{-1} \by$$
