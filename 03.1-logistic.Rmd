```{r, echo=FALSE, results='asis'}
  # import macros
  cat("<div style='display:none;'>")
  cat(paste(scan("mathjax_header.html", what = character(), sep = "\n", quiet = TRUE), collapse = "\n"))
  cat("</div>")
```

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# (PART) Linear Classification Models {-}

# Logistic Regression

## Modeling Binary Outcomes

To model binary outcomes using a logistic regression, we will use the 0/1 coding of $Y$. We need to set its connection with covariates. Recall in a linear regression, the outcome is continuous, and we set 

$$Y = \beta_0 + \beta_1 X + \epsilon$$
However, this does not work for classification since $Y$ can only be 0 or 1. Hence we turn to consider modeling the probability $P(Y = 1 | X = \bx)$. Hence, $Y$ is a Bernoulli random variable given $X$, and this is modeled by a function of $X$: 

$$ P(Y = 1 | X = \bx) = \frac{\exp(\bx^\T \bbeta)}{1 + \exp(\bx^\T \bbeta)}$$
Note that although $\bx^\T \bbeta$ may ranges from 0 to infinity as $X$ changes, the probability will still be bounded between 0 and 1. This is an example of __Generalized Linear Models__. The relationship is still represented using a linear function of $\bx$, $\bx^\T \bbeta$. This is called a __logit link__ function (a function to connect the conditional expectation of $Y$ with $\bbeta^\T \bx$):

$$\eta(a) = \frac{\exp(a)}{1 + \exp(a)}$$
Hence, $P(Y = 1 | X = \bx) = \eta(\bx^\T \bbeta)$. We can reversely solve this and get 

\begin{aligned}
P(Y = 1 | X = \bx) = \eta(\bx^\T \bbeta) &= \frac{\exp(\bx^\T \bbeta)}{1 + \exp(\bx^\T \bbeta)}\\
1 - \eta(\bx^\T \bbeta) &= \frac{1}{1 + \exp(\bx^\T \bbeta)} \\
\text{Odds} = \frac{\eta(\bx^\T \bbeta)}{1-\eta(\bx^\T \bbeta)} &= \exp(\bx^\T \bbeta)\\
\log(\text{Odds}) = \bx^\T \bbeta
\end{aligned}

Hence, the parameters in a logistic regression is explained as __log odds__. Let's look at a concrete example. 

## Example: Cleveland Clinic Heart Disease Data

We use use the [Cleveland clinic heart disease dataset](https://www.kaggle.com/aavigan/cleveland-clinic-heart-disease-dataset). The goal is to model and predict a class label of whether the patient has a hearth disease or not. This is indicated by whether the `num` variable is $0$ (no presence) or $>0$ (presence). 

```{r}
  heart = read.csv("data/processed_cleveland.csv")
  heart$Y = as.factor(heart$num > 0)
  table(heart$Y)
```

Let's model the probability of heart disease using the `Age` variable. This can be done using the `glm()` function, which stands for the Generalized Linear Model. The syntax of `glm()` is almost the same as a linear model. Note that it is important to use `family = binomial` to specify the logistic regression. 

```{r}
  logistic.fit <- glm(Y~age, data = heart, family = binomial)
  summary(logistic.fit)
```

The result is similar to a linear regression, with some differences. The parameter estimate of age is 0.05199. It is positive, meaning that increasing age would increase the change of having heart disease. However, this does not mean that 1 year older would increase the change by 0.05. Since, by our previous formula, the probably is not directly expressed as $\bx^\T \bbeta$. 

This calculation can be realized when predicting a new target point. Let's consider a new subject with `Age = 55`. What is the predicted probability of heart disease? Based on our formula, we have 

$$\beta_0 + \beta_1 X = -3.00591 + 0.05199 \times 55 = -0.14646$$
And the estimated probability is 

$$ P(Y = 1 | X) = \frac{\exp(\beta_0 + \beta_1 X)}{1 + \exp(\beta_0 + \beta_1 X)} = \frac{\exp(-0.14646)}{1 + \exp(-0.14646)} = 0.4634503$$
Hence, the estimated probability for this subject is 46.3%. This can be done using R code. Please note that if you want to predict the probability, you need to specify `type = "response"`. Otherwise, only $\beta_0 + \beta_1 X$ is provided.

```{r}
  testdata = data.frame("age" = 55)
  predict(logistic.fit, newdata = testdata)
  predict(logistic.fit, newdata = testdata, type = "response")
```

If we need to make a 0/1 decision about this subject, a natural idea is to see if the predicted probability is greater than 0.5. In this case, we would predict this subject as 0.

## Interpretation of the Parameters

Recall that $\bx^\T \bbeta$ is the log odds, we can further interpret the effect of a single variable. Let's define the following two, with an arbitrary age value $a$:

  * A subject with `age` $= a$
  * A subject with `age` $= a + 1$

Then, if we look at the __odds ratio__ corresponding to these two target points, we have 

\begin{aligned}
\text{Odds Ratio} &= \frac{\text{Odds in Group 2}}{\text{Odds in Group 1}}\\
&= \frac{\exp(\beta_0 + \beta_1 (a+1))}{\exp(\beta_0 + \beta_1 a)}\\
&= \frac{\exp(\beta_0 + \beta_1 a) \times \exp(\beta_1)}{\exp(\beta_0 + \beta_1 a)}\\
&= \exp(\beta_1)
\end{aligned}

Taking $\log$ on both sides, we have 

$$\log(\text{Odds Ratio}) = \beta_1$$

Hence, the odds ratio between these two subjects (__they differ only with one unit of `age`__) can be directly interpreted as the exponential of the parameter of `age`. After taking the log, we can also say that 

> The parameter $\beta$ of a varaible in a logistic regression represents the __log of odds ratio__ associated with one-unit increase of this variable. 

Please note that we usually do not be explicit about what this odds ratio is about (what two subject we are comparing). Because the interpretation of the parameter does not change regardless of the value $a$, as long as the two subjects differ in one unit. 

And also note that this conclusion is regardless of the values of other covaraites. When we have a multivariate model, as long as all other covariates are held the same, the previous derivation will remain the same. 

## Solving a Logistic Regression 

The logistic regression is solved by maximizing the log-likelihood function. Note that the log-likelihood is given by 

$$\ell(\bbeta) = \sum_{i=1}^n \log \, p(y_i | x_i, \bbeta).$$
Using the probabilities of Bernoulli distribution, we have 

\begin{align}
\ell(\bbeta) =& \sum_{i=1}^n \log \left\{ \eta(\bx_i)^{y_i} [1-\eta(\bx_i)]^{1-y_i} \right\}\\
    =& \sum_{i=1}^n y_i \log \frac{\eta(\bx_i)}{1-\eta(\bx_i)} + \log [1-\eta(\bx_i)] \\
    =& \sum_{i=1}^n y_i \bx_i^\T \bbeta - \log [ 1 + \exp(\bx_i^\T \bbeta)]
\end{align}

Since this objective function is relatively simple, we can use Newton's method to update. The gradient is given by 

$$\frac{\partial \ell(\bbeta)}{\partial \bbeta} =~ \sum_{i=1}^n y_i \bx_i^\T - \sum_{i=1}^n \frac{\exp(\bx_i^\T \bbeta) \bx_i^\T}{1 + \exp(\bx_i^\T \bbeta)},$$

and the Hessian matrix is given by 

$$\frac{\partial^2 \ell(\bbeta)}{\partial \bbeta \partial \bbeta^\T} =~ - \sum_{i=1}^n \bx_i \bx_i^\T \eta(\bx_i) [1- \eta(\bx_i)].$$
This leads to the update 

$$\bbeta^{\,\text{new}} = \bbeta^{\,\text{old}} - \left[\frac{\partial^2 \ell(\bbeta)}{\partial \bbeta \partial \bbeta^\T}\right]^{-1} \frac{\partial \ell(\bbeta)}{\partial \bbeta}$$


## Example: South Africa Heart Data

We use the South Africa heart data as a demonstration. The goal is to estimate the probability of `chd`, the indicator of coronary heart disease. 

```{r}
    library(ElemStatLearn)
    data(SAheart)
    
    heart = SAheart
    heart$famhist = as.numeric(heart$famhist)-1
    n = nrow(heart)
    p = ncol(heart)
    
    heart.full = glm(chd~., data=heart, family=binomial)
    round(summary(heart.full)$coef, dig=3)
    
    # fitted value 
    yhat = (heart.full$fitted.values>0.5)
    table(yhat, SAheart$chd)
```

Based on what we learned in class, we can solve this problem ourselves using numerical optimization. Here we will demonstrate an approach that uses general solver `optim()`. First, write the objective function of the logistic regression, for any value of $\boldsymbol \beta$, $\mathbf{X}$ and $\mathbf{y}$.

```{r}
    # the negative log-likelihood function of logistic regression 
    my.loglik <- function(b, x, y)
    {
        bm = as.matrix(b)
        xb =  x %*% bm
        # this returns the negative loglikelihood
        return(sum(y*xb) - sum(log(1 + exp(xb))))
    }

    # Gradient
    my.gradient <- function(b, x, y)
    {
        bm = as.matrix(b) 
        expxb =  exp(x %*% bm)
        return(t(x) %*% (y - expxb/(1+expxb)))
    }
```

Let's check the result of this function for some arbitrary $\boldsymbol \beta$ value.  

```{r}
    # prepare the data matrix, I am adding a column of 1 for intercept
    
    x = as.matrix(cbind("intercept" = 1, heart[, 1:9]))
    y = as.matrix(heart[,10])
    
    # check my function
    b = rep(0, ncol(x))
    my.loglik(b, x, y) # scalar
    
    # check the optimal value and the likelihood
    my.loglik(heart.full$coefficients, x, y)
```

Then we optimize this objective function 

```{r}
    # Use a general solver to get the optimal value
    # Note that we are doing maximization instead of minimization, 
    # we need to specify "fnscale" = -1
    optim(b, fn = my.loglik, gr = my.gradient, 
          method = "BFGS", x = x, y = y, control = list("fnscale" = -1))
```

This matches our `glm()` solution. Now, if we do not have a general solver, we should consider using the Newton-Raphson. You need to write a function to calculate the Hessian matrix and proceed with an optimization update.

```{r, echo = FALSE, results = 'hide'}
    # Hessian
    my.hessian <- function(b, x, y)
    {
    	bm = as.matrix(b) 
    	expxb =  exp(x %*% bm)
    	x1 = sweep(x, 1, expxb/(1+expxb)^2, "*")
    	return(-t(x) %*% x1)
    }

    # check my functions to make sure the dimensions match
    b = rep(0, ncol(x))
    my.loglik(b, x, y) # scalar
    my.gradient(b, x, y) # p by 1 matrix
    my.hessian(b, x, y) # p by p matrix

    my.logistic <- function(b, x, y, tol = 1e-10, maxitr = 30, gr, hess, verbose = FALSE)
    {
        b_new = b
        
        for (j in 1:maxitr) # turns out you don't really need many iterations
        {
        	b_old = b_new
        	b_new = b_old - solve(hess(b_old, x, y)) %*% gr(b_old, x, y)
        	
        	if (verbose)
        	{
        	    cat(paste("at iteration ", j,", current beta is \n", sep = ""))
        	    cat(round(b_new, 3))
        	    cat("\n")
        	}    
        	if (sum(abs(b_old - b_new)) < 1e-10) break;
        }
        return(b_new)
    }
```

```{r}
    # my Newton-Raphson method
    # set up an initial value
    # this is sometimes crucial...
    
    b = rep(0, ncol(x))
    
    mybeta = my.logistic(b, x, y, tol = 1e-10, maxitr = 20, 
                         gr = my.gradient, hess = my.hessian, verbose = TRUE)
    
    # the parameter value
    mybeta
    # get the standard error estimation 
    mysd = sqrt(diag(solve(-my.hessian(mybeta, x, y))))    
```

With this solution, I can then get the standard errors and the p-value. You can check them with the `glm()` function solution. 

```{r}
    # my summary matrix
    round(data.frame("beta" = mybeta, "sd" = mysd, "z" = mybeta/mysd, 
    	  "pvalue" = 2*(1-pnorm(abs(mybeta/mysd)))), dig=5)
	  
    # check that with the glm fitting 
    round(summary(heart.full)$coef, dig=5)
```

## Penalized Logistic Regression

Similar to a linear regression, we can also apply penalties to a logistic regression to address collinearity problems or select variables in a high-dimensional setting. For example, if we use the Lasso penalty, the objective function is 

$$\sum_{i=1}^n \log \, p(y_i | x_i, \bbeta) + \lambda |\bbeta|_1$$
This can be done using the `glmnet` package. Specifying `family = "binomial"` will ensure that a logistic regression is used, even your `y` is not a factor (but as numerical 0/1). 

```{r}
  library(glmnet)
  lasso.fit = cv.glmnet(x = data.matrix(SAheart[, 1:9]), y = SAheart[,10], 
                        nfold = 10, family = "binomial")
  
  plot(lasso.fit)
```

The procedure is essentially the same as in a linear regression. And we could obtain the estimated parameters by selecting the best $\lambda$ value. 

```{r}
  coef(lasso.fit, s = "lambda.min")
```


