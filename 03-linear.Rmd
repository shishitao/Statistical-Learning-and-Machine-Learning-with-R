\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\br{\mathbf{r}}
\def\bzero{\mathbf{0}}
\def\bbeta{\boldsymbol \beta}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  knitr::opts_chunk$set(fig.dim = c(8, 6), out.width = "60%", fig.align = 'center')
```

# Linear Regression

This chapter severs several purposes. First, we will review some basic knowledge of linear regression. This includes the concept of vector space, projection, which leads to estimating parameters of a linear regression. Most of these knowledge are covered in the prerequisite so you shouldn't find these concepts too difficult to understand. Secondly, we will mainly use the `lm()` function as an example to demonstrate some features of `R`. This includes extracting results, visualizations, handling categorical variables, prediction and model selection. These concepts will be useful for other models. 

## Example: real estate data 

This [Real Estate data](https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set) [@yeh2018building] is provided on the [UCI machine learning repository](https://archive.ics.uci.edu/ml/index.php). The goal of this dataset is to predict the unit house price based on six different covariates: 

  * `date`: The transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)
  * `age`: The house age (unit: year)
  * `distance`: The distance to the nearest MRT station (unit: meter)
  * `stores`: The number of convenience stores in the living circle on foot (integer)
  * `latitude`: Latitude (unit: degree)
  * `longitude`: Longitude (unit: degree)
  * `price`: House price of unit area

```{r}
    realestate = read.csv("data/realestate.csv", row.names = 1)

    library(DT)
    datatable(realestate, filter = "top", rownames = FALSE,
              options = list(pageLength = 8))
    
    dim(realestate)
```

## Notation and Basic Properties

We usually denote the observed covariates data as the design matrix $\mathbf{X}$, with dimension $n \times p$. Hence in this case, the dimension of $\mathbf{X}$ is $414 \times 7$. The $j$th variable is simply the $j$th column of this matrix, which is denoted as $\mathbf{x}_j$. The outcome $\mathbf{y}$ (`price`) is a vector of length $414$. Please note that we usually use a "bold" symbol to represent a vector, while for a single element (scalar), such as the $j$th variable of subject $i$, we use $x_{ij}$. $\bX$

A linear regression concerns modeling the relationship (in matrix form)

$$\by_{n \times 1} = \bX_{n \times p} \bbeta_{p \times 1} + \bepsilon_{n \times 1}$$
And we know that the solution is obtained by minimizing the residual sum of squares (RSS):

$$ 
\begin{align}
\widehat{\bbeta} &= \underset{\bbeta}{\argmin} \sum_{i=1}^n \left(y_i - x_i^\T \bbeta \right)^2 \\
&= \underset{\bbeta}{\argmin} \big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)^\T \big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)
\end{align}
$$
Classic solution can be obtained by taking the derivative of RSS w.r.t $\bbeta$ and set it to zero. This leads to the well known normal equation: 

$$
\begin{align}
    \frac{\partial \text{RSS}}{\partial \bbeta} &= -2 \bX^\T (\by - \bX \bbeta) \doteq 0 \\
    \Longrightarrow \quad \bX^\T \by &= \bX^\T \bX \bbeta
\end{align}
$$
Assuming that $\bX$ is full rank, then $\bX^\T \bX$ is invertible. Then, we have 

$$
\widehat{\bbeta} = (\bX^\T \bX)^{-1}\bX^\T \by
$$
Some additional concepts are frequently used. The fitted values $\widehat{\by}$ are essentially the prediction of the original $n$ training data points:

$$ 
\begin{align}
\widehat{\by} =& \bX \bbeta\\
=& \underbrace{\bX (\bX^\T \bX)^{-1}\bX^\T}_{\bH} \by \\
\doteq& \bH_{n \times n} \by 
\end{align}
$$
where $\bH$ is called the "hat" matrix. It is a projection matrix that projects any vector ($\by$ in our case) onto the column space of $\bX$. A project matrix enjoys two properties 

  * Symmetric: $\bH^\T = \bH$
  * Idempotent $\bH\bH = \bH$

The residuals $\br$ can also be obtained using the hat matrix:

$$ \br = \by - \widehat{\by} = (\bI - \bH) \by$$
From the properties of a projection matrix, we also know that $\br$ should be orthogonal to any vector from the column space of $\bX$. Hence, 

$$\bX^\T \br = \mathbf{0}_{p \times 1}$$

The residuals is also used to estimate the error variance:

$$\widehat\sigma^2 = \frac{1}{n-p} \sum_{i=1}^n r_i^2 = \frac{\text{RSS}}{n-p}$$
When the data are indeed generated from a linear model, and with suitable conditions on the design matrix and random errors $\bepsilon$, we can conclude that $\widehat{\bbeta}$ is an __unbiased__ estimator of $\bbeta$. Its variance-covariance matrix satisfies

$$
\begin{align}
    \Var(\widehat{\bbeta}) &= \Var\big( (\bX^\T \bX)^{-1}\bX^\T \by \big) \nonumber \\
    &= \Var\big( (\bX^\T \bX)^{-1}\bX^\T (\bX \bbeta + \bepsilon) \big) \nonumber \\
    &= \Var\big( (\bX^\T \bX)^{-1}\bX^\T \bepsilon) \big) \nonumber \\
    &= (\bX^\T \bX)^{-1}\bX^\T \bX (\bX^\T \bX)^{-1} \bI \sigma^2 \nonumber \\
    &= (\bX^\T \bX)^{-1}\sigma^2
\end{align}
$$
All of the above mentioned results are already implemented in R through the `lm()` function to fit a linear regression. 

## Using the `lm()` Function

Let's consider a simple regression that uses `age` and `distance` to model `price`. We will save the fitted object as `lm.fit`

```{r}
    lm.fit = lm(price ~ age + distance, data = realestate)
```

This syntax contains three components:

  * `data = ` specifies the dataset
  * The outcome variable should be on the left hand side of `~` 
  * The covariates should be on the right hand side of `~`
    
To look at the detailed model fitting results, use the `summary()` function 

```{r}
    lm.summary = summary(lm.fit)
    lm.summary
```

This shows that both `age` and `distance` are highly significant for predicting the price. In fact, this fitted object (`lm.fit`) and the summary object (`lm.summary`) are both saved as a list. This is pretty common to handle an output object with many things involved. We may peek into this object to see what are provided using a `$` after the object. 

<center>
![](images/reactive.png){width=40%}
</center>

The `str()` function can also display all the items in a list. 

```{r eval=FALSE}
    str(lm.summary)
```

Usually, printing out the summary is sufficient. However, further details can be useful for other purposes. For example, if we interested in the residual vs. fits plot, we may use 

```{r}
    plot(lm.fit$fitted.values, lm.fit$residuals, 
         xlab = "Fitted Values", ylab = "Residuals",
         col = "darkorange", pch = 19, cex = 0.5)
```

It seems that the error variance is not constant (as a function of the fitted values), hence additional techniques may be required to handle this issue. However, that is beyond the scope of this book. 

### Adding Covariates

It is pretty simple if we want to include additional variables. This is usually done by connecting them with the `+` sign on the right hand side of `~`. R also provide convenient ways to include interactions and higher order terms. The following code with the interaction term between `age` and `distance`, and a squared term of `distance` should be self-explanatory. 

```{r}
    lm.fit2 = lm(price ~ age + distance + age*distance + I(distance^2), data = realestate)
    summary(lm.fit2)
```

If you choose to include all covariates presented in the data, then simply use `.` on the right hand side of `~`. However, you should always be careful when doing this because some dataset would contain meaningless variables such as subject ID. 

```{r eval=FALSE}
    lm.fit3 = lm(price ~ ., data = realestate)
```

### Categorical Variables

The store variable has several different values. We can see that it has 11 different values. One strategy is to model this as a continuous variable. However, we may also consider to discretize it. For example, we may create a new variable, say `store.cat`, defined as follows

```{r}
  table(realestate$stores)

  # define a new factor variable
  realestate$store.cat = as.factor((realestate$stores > 0) + (realestate$stores > 4))
  table(realestate$store.cat)
  levels(realestate$store.cat) = c("None", "Several", "Many")
  head(realestate$store.cat)
```

This variable is defined as a factor, which is often used for categorical variables. Since this variable has three different categories, if we include it in the linear regression, it will introduce two additional variables (using the third as the reference):

```{r}
    lm.fit3 = lm(price ~ age + distance + store.cat, data = realestate)
    summary(lm.fit3)
```

There are usually two types of categorical variables:

  * Ordinal: the numbers representing each category is ordered, e.g., how many stores in the neighborhood. Oftentimes nominal data can be treated as a continuous variable.
  * Nominal: they are not ordered and can be represented using either numbers or letters, e.g., ethnic group. 
  
The above example is treating `store.cat` as a nominal variable, and the `lm()` function is using dummy variables for each category. This should be our default approach to handle nominal variables. 

## Model Selection Criteria

We will use the `diabetes` dataset from the `lars` package as a demonstration of model selection. Ten baseline variables include age, sex, body mass index, average blood pressure, and six blood serum measurements. These measurements were obtained for each of n = 442 diabetes patients, as well as the outcome of interest, a quantitative measure of disease progression one year after baseline. More details are available in @efron2004least. Our goal is to select a linear model, preferably with a small number of variables, that can predict the outcome. To select the best model, commonly used strategies include Marrow's $C_p$, AIC (Akaike information criterion) and BIC (Bayesian information criterion). Further derivations will be provide at a later section. 

```{r}
    # load the diabetes data
    library(lars)
    data(diabetes)
    diab = data.frame(cbind(diabetes$x, "Y" = diabetes$y))

    # fit linear regression with all covariates
    lm.fit = lm(Y~., data=diab)
```

The idea of model selection is to apply some penalty on the number of parameters used in the model. In general, they are usually in the form of 

$$\text{Goodness-of-Fit} + \text{Complexity Penality}$$

### Using Marrows' $C_p$

For example, the Marrows' $C_p$ criterion minimize the following quantity (a derivation is provided at Section \@ref(mallows-cp)): 

$$\text{RSS} + 2 p \widehat\sigma_{\text{full}}^2$$
Note that the $\sigma_{\text{full}}^2$ refers to the residual variance estimation based on the full model, i.e., will all variables. Hence, this formula cannot be used when $p > n$ because you would not be able to obtain a valid estimation of $\sigma_{\text{full}}^2$. Nonetheless, we can calculate this quantity with the diabetes dataset

```{r}
    # number of variables (including intercept)
    p = 11
    n = nrow(diab)
      
    # obtain residual sum of squares
    RSS = sum(residuals(lm.fit)^2)
    
    # use the formula directly to calculate the Cp criterion 
    Cp = RSS + 2*p*summary(lm.fit)$sigma^2
    Cp
```

We can compare this with another sub-model, say, with just `age` and `glu`:

```{r}
    lm.fit_sub = lm(Y~ age + glu, data=diab)
  
    # obtain residual sum of squares
    RSS_sub = sum(residuals(lm.fit_sub)^2)
    
    # use the formula directly to calculate the Cp criterion 
    Cp_sub = RSS + 2*3*summary(lm.fit)$sigma^2
    Cp_sub
```

Comparing this with the previous one, the sub-model is better. 

### Using AIC and BIC

Calculating the AIC and BIC criteria in `R` is a lot simpler, with the existing functions. The AIC score is given by 

$$-2 \text{Log-likelihood} + 2 p,$$
while the BIC score is given by 

$$-2 \text{Log-likelihood} + \log(n) p,$$

The AIC score can be done using the `AIC()` function. We can match this result by writing out the normal density function and plug in the estimated parameters. Note that this requires one additional parameter, which is the variance. Hence the total number of parameters is 12. We can calculate this with our own code:

```{r}
    # ?AIC
    # a build-in function for calculating AIC using -2log likelihood
    AIC(lm.fit) 

    # Match the result
    n*log(RSS/n) + n + n*log(2*pi) + (2 + 2*p)
```

Alternatively, the `extractAIC()` function can calculate both AIC and BIC. However, note that the `n + n*log(2*pi)` part in the above code does not change regardless of how many parameters we use. Hence, this quantify does not affect the comparison between different models. Then we can safely remove this part and only focus on the essential ones. 

```{r}
    # ?extractAIC
    # AIC for the full model
    extractAIC(lm.fit)
    n*log(RSS/n) + 2*p

    # BIC for the full model
    extractAIC(lm.fit, k = log(n))
    n*log(RSS/n) + log(n)*p
```

Now, we can compare AIC or BIC using of two different models and select whichever one that gives a smaller value. For example the AIC of the previous sub-model is 

```{r}
    # AIC for the sub-model
    extractAIC(lm.fit_sub)
```





The `step()` function can be used to select the best model based on specified model selection criteria. 

```{r}
    # Model selection: stepwise algorithm 
    # ?step
    
    # this function shows every step during the model selection 
    step(lm.fit, direction="both", k = 2)    # k = 2 (AIC) is default; 
    
    step(lm.fit, direction="backward", trace=0) # trace=0 will not print intermediate results
    step(lm(Y~1, data=diab), scope=list(upper=lm.fit, lower=~1), direction="forward", trace=0)
    
    step(lm.fit, direction="both", k=log(n), trace=0)  # BIC (the default value for k=2, which corresponds to AIC)
```

The `leaps` package will calculate the best model of each model size. Then we can add the penalties to the model fitting result and conclude the best model. 

```{r}
    ##########################################################################
    # Best subset model selection (Cp, AIC, and BIC): leaps 
    ##########################################################################
    library(leaps)
    
    # performs an exhaustive search over models, and gives back the best model 
    # (with low RSS) of each size.
    # the default maximum model size is nvmax=8
    
    RSSleaps=regsubsets(as.matrix(diab[,-11]),diab[,11])
    summary(RSSleaps, matrix=T)
    
    RSSleaps=regsubsets(as.matrix(diab[,-11]),diab[,11], nvmax=10)
    summary(RSSleaps,matrix=T)
    
    sumleaps=summary(RSSleaps,matrix=T)
    names(sumleaps)  # components returned by summary(RSSleaps)
    
    sumleaps$which
    msize=apply(sumleaps$which,1,sum)
    n=dim(diab)[1]
    p=dim(diab)[2]
    Cp = sumleaps$rss/(summary(lm.fit)$sigma^2) + 2*msize - n;
    AIC = n*log(sumleaps$rss/n) + 2*msize;
    BIC = n*log(sumleaps$rss/n) + msize*log(n);
    
    cbind(Cp, sumleaps$cp)
    cbind(BIC, sumleaps$bic)  # It seems regsubsets uses a formula for BIC different from the one we used. 
    BIC-sumleaps$bic  # But the two just differ by a constant, so won't affect the model selection result. 
    n*log(sum((diab[,11] - mean(diab[,11]))^2/n)) # the difference is the score of an intercept model
    
    # Rescale Cp, AIC, BIC to (0,1).
    inrange <- function(x) { (x - min(x)) / (max(x) - min(x)) }
    
    Cp = sumleaps$cp; Cp = inrange(Cp);
    BIC = sumleaps$bic; BIC = inrange(BIC);
    AIC = n*log(sumleaps$rss/n) + 2*msize; AIC = inrange(AIC);
    
    plot(range(msize), c(0, 1.1), type="n", 
         xlab="Model Size (with Intercept)", ylab="Model Selection Criteria")
    points(msize, Cp, col="red", type="b")
    points(msize, AIC, col="blue", type="b")
    points(msize, BIC, col="black", type="b")
    legend("topright", lty=rep(1,3), col=c("red", "blue", "black"), legend=c("Cp", "AIC", "BIC"))
```


##  Mallows' $C_p$ Derivation {#mallows-cp}