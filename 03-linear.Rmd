

# Linear Regression

This chapter severs several purposes. First, we will review some basic knowledge of linear regression. This includes the concept of vector space, projection, which leads to estimating parameters of a linear regression. Most of these knowledge are covered in the prerequisite so you shouldn't find these concepts too difficult to understand. Secondly, we will mainly use the `lm()` function as an example to demonstrate some features of `R`. This includes extracting results, visualizations, handling categorical variables, prediction and model selection. These concepts will be useful for other models. 

## Example: real estate data 

This [Real Estate data](https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set) is provided on the [UCI machine learning repository](https://archive.ics.uci.edu/ml/index.php). The goal of this dataset is to predict the unit house price based on six different covariates: 

  * `date`: The transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)
  * `age`: The house age (unit: year)
  * `distance`: The distance to the nearest MRT station (unit: meter)
  * `stores`: The number of convenience stores in the living circle on foot (integer)
  * `latitude`: Latitude (unit: degree)
  * `longitude`: Longitude (unit: degree)
  * `price`: House price of unit area

```{r}
    realestate = read.csv("data/realestate.csv", row.names = 1)

    library(DT)
    datatable(realestate, filter = "top", rownames = FALSE,
              options = list(pageLength = 8))
    
    dim(realestate)
```

## Notation and Basic Properties

We usually denote the observed covariates data as the design matrix $\mathbf{X}$, with dimension $n \times p$. Hence in this case, the dimension of $\mathbf{X}$ is $414 \times 7$. The $j$th variable is simply the $j$th column of this matrix, which is denoted as $\mathbf{x}_j$. The outcome $\mathbf{y}$ (`price`) is a vector of length $414$. Please note that we usually use a "bold" symbol to represent a vector, while for a single element (scalar), such as the $j$th variable of subject $i$, we use $x_{ij}$. $\bX$

A linear regression concerns modeling the relationship (in matrix form)

$$\by_{n \times 1} = \bX_{n \times p} \bbeta_{p \times 1} + \bepsilon_{n \times 1}$$
And we know that the solution is obtained by minimizing the residual sum of squares (RSS):

$$ 
\begin{align}
\widehat{\bbeta} &= \underset{\bbeta}{\argmin} \sum_{i=1}^n \left(y_i - x_i^\T \bbeta \right)^2 \\
&= \underset{\bbeta}{\argmin} \big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)^\T \big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)
\end{align}
$$
Classic solution can be obtained by taking the derivative of RSS w.r.t $\bbeta$ and set it to zero. This leads to the well known normal equation: 

$$
\begin{align}
    \frac{\partial \text{RSS}}{\partial \bbeta} &= -2 \bX^\T (\by - \bX \bbeta) \doteq 0 \\
    \Longrightarrow \quad \bX^\T \by &= \bX^\T \bX \bbeta
\end{align}
$$
Assuming that $\bX$ is full rank, then $\bX^\T \bX$ is invertible. Then, we have 

$$
\widehat{\bbeta} = (\bX^\T \bX)^{-1}\bX^\T \by
$$
Some additional concepts are frequently used. The fitted values $\widehat \by$ are essentially the prediction of the original $n$ training data points:

$$ 
\begin{align}
\widehat{\by} =& \bX \bbeta\\
=& \underbrace{\bX (\bX^\T \bX)^{-1}\bX^\T}_{\bH} \by \\
\doteq& \bH_{n \times n} \by 
\end{align}
$$
where $\bH$ is called the "hat" matrix. It is a projection matrix that projects any vector ($\by$ in our case) onto the column space of $\bX$. A project matrix enjoys two properties 

  * Symmetric: $\bH^\T = \bH$
  * Idempotent $\bH\bH = \bH$

The residuals $\br$ can also be obtained using the hat matrix:

$$ \br = \by - \widehat \by = (\bI - \bH) \by$$
From the properties of a projection matrix, we also know that $\br$ should be orthogonal to any vector from the column space of $\bX$. Hence, 

$$\bX^\T \br = \mathbf{0}_{p \times 1}$$

The residuals is also used to estimate the error variance:

$$\widehat\sigma^2 = \frac{1}{n-p} \sum_{i=1}^n r_i^2 = \frac{\text{RSS}}{n-p}$$
When the data are indeed generated from a linear model, and with suitable conditions on the design matrix and random errors $\bepsilon$, we can conclude that $\widehat \bbeta$ is an __unbiased__ estimator of $\bbeta$. Its variance-covariance matrix satisfies

$$
\begin{align}
    \Var(\widehat \bbeta) &= \Var\big( (\bX^\T \bX)^{-1}\bX^\T \by \big) \nonumber \\
    &= \Var\big( (\bX^\T \bX)^{-1}\bX^\T (\bX \bbeta + \bepsilon) \big) \nonumber \\
    &= \Var\big( (\bX^\T \bX)^{-1}\bX^\T \bepsilon) \big) \nonumber \\
    &= (\bX^\T \bX)^{-1}\bX^\T \bX (\bX^\T \bX)^{-1} \bI \sigma^2 \nonumber \\
    &= (\bX^\T \bX)^{-1}\sigma^2
\end{align}
$$
All of the above mentioned results are already implemented in R through the `lm()` function to fit a linear regression. 
## Using the `lm()` Function

Let's consider a simple regression model that uses `age` and `distance` to explain `price`. We will save the fitted object as `realestat.lmfit`

```{r}
    realestat.lmfit = lm(price ~ age + distance, data = realestate)
```

This example contains three major components:

  * `data = ` specifies the dataset
  * The outcome variable should be on the left hand side of `~` 
  * The covariates should be on the right hand side of `~`
    
To look at the detailed model fitting results, use the `summary()` function 

```{r}
    summary(realestat.lmfit)
```




This can be viewed as either a convex optimization problem or projections on the $n$ dimensional vector space. 


### Linear regression as an optimization

```{r fig.width=5, fig.height=5}
    # generate data for a simple linear regression 
    set.seed(20)
    n = 100
    x <- cbind(1, rnorm(n))
    y <- x %*% c(1, 0.5) + rnorm(n)
    
    # calculate the residual sum of squares for a grid of beta values
    rss <- function(b, x, y) sum((y - x %*% b)^2)
    b1 <- b2 <- seq(0, 2, length= 20)
    z = matrix(apply(expand.grid(b1, b2), 1, rss, x, y), 20, 20)
    
    # 3d plot for RSS
    par(mar = c(1,1,3,1))
    persp(b1, b2, z, xlab = "beta 1", ylab = "beta 2", zlab = "RSS",
          main="Residual Sum of Squares", col = "springgreen", shade = 0.6,
          theta = 30, phi = 5)
    
    # The solution can be solved by any optimization algorithm 
    optim(c(0, 0), rss, x = x, y = y)$par
```

### Linear regression as projections {#linear-reg}

Another view is through projections in vector space. Consider each column of $\mathbf{X}$ as a vector, and project $\mathbf{y}$ onto the column space of $\mathbf{X}$. The project is 

$$ \widehat{\mathbf{y}} = \mathbf{X} (\mathbf{X}^\text{T} \mathbf{X})^{-1}\mathbf{X}^\text{T} \mathbf{y} \doteq {\mathbf{H}} \mathbf{y}, $$
where $\mathbf{H}$ is a projection matrix. And the residuals are simply 

$$ \widehat{\mathbf{e}} = \mathbf{y} - \widehat{\mathbf{y}} = (\mathbf{I} - \mathbf{H}) \mathbf{y} $$
When the number of variables is large, inverting $\mathbf{X}^\text{T} \mathbf{X}$ is expansive. The `R` function `lm()` does not calculate the inverse directly. Instead, QR decomposition can be used. You can try a larger $n$ and $p$ to see a significant difference. This is only for demonstration. They are not required for our course. 

```{r}
    # generate 100 observations with 3 variables
    set.seed(1)
    n = 1000
    p = 500
    x = matrix(rnorm(n*p), n, p)
    X = cbind(1, x) # the design matrix, including 1 as the first column
    
    # define the true beta, the first entry is the intercept
    b = as.matrix(c(1, 1, 0.5, rep(0, p-2))) 
    
    # generate training y with Gaussian errors
    y = X %*% b + rnorm(n)
    
    # fit a linear regression model 
    lm.fit = lm(y ~ x)
    
    # look at the coefficients beta hat
    head(lm.fit$coef)
    
    # using normal equations by inverting the X'X matrix: b = (X'X)^-1 X'y 
    # however, this is very slow
    # check ?solve
    system.time({beta_hat = solve(t(X) %*% X) %*% t(X) %*% y})
    head(beta_hat)
    
    # you can avoid the inversion by specifying the linear equation system X'X b = X'y 
    system.time({beta_hat = solve(t(X) %*% X, t(X) %*% y)})    
    
    # A better approach is to use QR decomposition or the Cholesky decomposition 
    # The following codes are not necessarily efficient, they are only for demonstration purpose
    
    # QR decomposition
    # direct calling the qr.coef function
    system.time({beta_hat = qr.coef(qr(X), y)})
    
    # or 
    system.time({beta_hat = qr.solve(t(X) %*% X, t(X) %*% y)})
    
    # if you want to see what Q and R are
    QR = qr(X)
    Q = qr.Q(QR)
    R = qr.R(QR)
    
    # get inverse of R, you can check R %*% R_inv yourself
    # the backsolve/forwardsolve functions can be used to solve AX = b for upper/lower triangular matrix A 
    # ?backsolve
    R_inv = backsolve(R, diag(p+1), upper.tri = TRUE, transpose = FALSE)
    beta_hat = R_inv %*% t(Q) %*% y
    
    # Cholesky Decomposition 
    
    # the chol function gives upper triangular matrix
    # crossprod(X) = X'X
    system.time({
    R = chol(crossprod(X))
    w = backsolve(R, t(X) %*% y, upper.tri = TRUE, transpose = TRUE)
    beta_hat = backsolve(R, w, upper.tri = TRUE, transpose = FALSE)
    })
    
    # or equivalently 
    R = t(chol(crossprod(X)))
    w = forwardsolve(R, t(X) %*% y, upper.tri = FALSE, transpose = FALSE)
    beta_hat = forwardsolve(R, w, upper.tri = FALSE, transpose = TRUE) # the transpose = TRUE means that we are solving for R'b = w instead of Rb = w 
```

## Model Selection Criteria and Algorithm

### Example: `diabetes` dataset

We use the `diabetes` dataset from the `lars` package as a demonstration of model selection. 

```{r}
    library(lars)
    data(diabetes)
    diab = data.frame(cbind(diabetes$x, "Y" = diabetes$y))
    
    # A Brief Description of the Diabetes Data (Efron et al, 2004):
    # Ten baseline variables: age, sex, body mass index, average blood pressure, and six blood serum
    # measurements were obtained for each of n = 442 diabetes patients, as well as
    # the response of interest, a quantitative measure of disease progression one year after baseline 
    
    lmfit=lm(Y~., data=diab)
    
    # When we use normal distribution likelihood for the errors, there are 12 parameters
    # The function AIC() directly calculates the AIC score from a lm() fitted model 
    n = nrow(diab)
    p = 11

    # ?AIC
    AIC(lmfit) # a build-in function for calculating AIC using -2log likelihood
    n*log(sum(residuals(lmfit)^2/n)) + n + n*log(2*pi) + 2 + 2*p

    # In many standard R packages, the AIC is calculated by removing some constants from the likelihood 
    # We will use this value as the default
    
    # ?extractAIC
    extractAIC(lmfit) # AIC for the full model
    RSS = sum(residuals(lmfit)^2)
    n*log(RSS/n) + 2*p

    # so the BIC for the full model is 
    extractAIC(lmfit, k = log(n))
    n*log(RSS/n) + log(n)*p
    
    # if we want to calculate Cp, use the formula
    RSS + 2*p*summary(lmfit)$sigma^2
    
    # however, the scale of this is usually very large, we may consider the following version
    RSS/summary(lmfit)$sigma^2 + 2*p - n
```

The `step()` function can be used to select the best model based on specified model selection criteria. 

```{r}
    # Model selection: stepwise algorithm 
    # ?step
    
    # this function shows every step during the model selection 
    step(lmfit, direction="both", k = 2)    # k = 2 (AIC) is default; 
    
    step(lmfit, direction="backward", trace=0) # trace=0 will not print intermediate results
    step(lm(Y~1, data=diab), scope=list(upper=lmfit, lower=~1), direction="forward", trace=0)
    
    step(lmfit, direction="both", k=log(n), trace=0)  # BIC (the default value for k=2, which corresponds to AIC)
```

The `leaps` package will calculate the best model of each model size. Then we can add the penalties to the model fitting result and conclude the best model. 

```{r}
    ##########################################################################
    # Best subset model selection (Cp, AIC, and BIC): leaps 
    ##########################################################################
    library(leaps)
    
    # performs an exhaustive search over models, and gives back the best model 
    # (with low RSS) of each size.
    # the default maximum model size is nvmax=8
    
    RSSleaps=regsubsets(as.matrix(diab[,-11]),diab[,11])
    summary(RSSleaps, matrix=T)
    
    RSSleaps=regsubsets(as.matrix(diab[,-11]),diab[,11], nvmax=10)
    summary(RSSleaps,matrix=T)
    
    sumleaps=summary(RSSleaps,matrix=T)
    names(sumleaps)  # components returned by summary(RSSleaps)
    
    sumleaps$which
    msize=apply(sumleaps$which,1,sum)
    n=dim(diab)[1]
    p=dim(diab)[2]
    Cp = sumleaps$rss/(summary(lmfit)$sigma^2) + 2*msize - n;
    AIC = n*log(sumleaps$rss/n) + 2*msize;
    BIC = n*log(sumleaps$rss/n) + msize*log(n);
    
    cbind(Cp, sumleaps$cp)
    cbind(BIC, sumleaps$bic)  # It seems regsubsets uses a formula for BIC different from the one we used. 
    BIC-sumleaps$bic  # But the two just differ by a constant, so won't affect the model selection result. 
    n*log(sum((diab[,11] - mean(diab[,11]))^2/n)) # the difference is the score of an intercept model
    
    # Rescale Cp, AIC, BIC to (0,1).
    inrange <- function(x) { (x - min(x)) / (max(x) - min(x)) }
    
    Cp = sumleaps$cp; Cp = inrange(Cp);
    BIC = sumleaps$bic; BIC = inrange(BIC);
    AIC = n*log(sumleaps$rss/n) + 2*msize; AIC = inrange(AIC);
    
    plot(range(msize), c(0, 1.1), type="n", 
         xlab="Model Size (with Intercept)", ylab="Model Selection Criteria")
    points(msize, Cp, col="red", type="b")
    points(msize, AIC, col="blue", type="b")
    points(msize, BIC, col="black", type="b")
    legend("topright", lty=rep(1,3), col=c("red", "blue", "black"), legend=c("Cp", "AIC", "BIC"))
```
