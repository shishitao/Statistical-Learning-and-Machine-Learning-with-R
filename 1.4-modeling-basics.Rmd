# Modeling Basics

```{r, collapse=TRUE, echo=FALSE}
    library(ElemStatLearn)
    data(SAheart)
```

## Fitting Linear Regression

A simple linear regression assumes the underlying model $$Y = \beta_0 + {\boldsymbol\beta}^\text{T} X + \epsilon,$$ where $\beta_0$ is an intercept and $\boldsymbol\beta$ is a vector of coefficients corresponds to each covariate. With observed data, we can estimate the regression coefficients. Let's use a classical dataset, the Boston Housing data [@harrison1978hedonic] from the `MASS` package. The goal of this dataset is to model the median house value (`medv`) using other predictors. 
```{r}
    library(MASS)
    data(Boston)
    # Fit a linear regression using all variables
    fit = lm(medv ~ ., data = Boston)
    summary(fit)
```

The output can be overwhelming for beginners. Here, by specifying the model with `medv ~ .`, we are using all variables in this data as predictors, except `medv` itself. And by default, an intercept term is also included. However, we could also specify particular variables as predictors. For example, if per capita crime rate by town (`crim`), the average number of rooms (`rm`) are used to predict the price, and the weighted mean of distances to five Boston employment centres (`dis`), along with an intercept term, we specify the following

```{r}
    fit = lm(medv ~ crim + rm + dis, data = Boston)
    summary(fit)
```

To read the output from a linear model, we usually pay attention to several key information, such as the coefficient and the p-value for each variable, and the overall model fitting F statistic and its p-value, which is almost 0 in this case. 

## Model Diagnostics 

To further evaluate this model fitting, we may plot the residuals (for assessing the normality) and the Cook's distance for identifying potential influence observations. 

```{r, fig.width=8, fig.height=8}
    # setup the parameters for plotting 4 figures together, in a 2 by 2 structure
    par(mfrow = c(2, 2))
    plot(fit)
```

R also provides several functions for obtaining metrics related to unusual observations that may help this process.

- `resid()` provides the residual for each observation
- `hatvalues()` gives the leverage of each observation
- `rstudent()` give the studentized residual for each observation
- `cooks.distance()` calculates the influence of each observation

```{r}
head(resid(fit), n = 10)
head(hatvalues(fit), n = 10)
head(rstudent(fit), n = 10)
head(cooks.distance(fit), n = 10)
```

## Variable Transformations and Interactions

It appears that the residuals are not normally distributed because the QQ plot deviates from the diagonal line quite a lot. Sometimes variable transformations can be used to deal with this issue, but that may not fix it completely. Plotting can be useful for detecting ill-distributed variables and suggest potential transformations. For example, we may use the correlation plot to visualize them

```{r, fig.height = 6, fig.width = 6, message = FALSE, warning = FALSE}
    library(PerformanceAnalytics)
    chart.Correlation(Boston[, c("medv", "crim", "rm", "dis")], histogram=TRUE, pch="+")
```

It looks like both `crim` and `dis` have heavy tail on the right hand side and could benefit from a log or a power transformation. variable transformations can be easily specified within the `lm()` function.

```{r}
    fit = lm(medv ~ log(crim) + rm + I(dis^0.5), data = Boston)
    summary(fit)
```

Another approach is to consider polynomial transformations of the outcome variable, known as the Box-Cox transformation.

```{r}
    # explore the Box-Cox transformation
    trans = boxcox(medv ~ log(crim) + rm + I(dis^0.5), data = Boston)
    # obtain the best power for performing the polynomial
    trans$x[which.max(trans$y)]
    # refit the model
    fit = lm(I(medv^0.2626263) ~ log(crim) + rm + I(dis^0.5), data = Boston)
```

One can again reevaluate the model fitting results and repeat the process if necessary. However, keep in mind that this is could be a tedious process that may not end with a satisfactory solution. 

To further improve the model fitting we may also consider iterations and higher order terms such as 

```{r}
    fit = lm(medv ~ log(crim) + rm + rm*log(crim) + I(rm^2) + 
             I(dis^0.5) + as.factor(chas)*rm, data = Boston)
    summary(fit)
```

## Model Selection
    
Suppose we have two candidate nested models, and we want to test if adding a set of new variables is significant in terms of predicting the outcome, this is essentially an F test. We can utilize the `anova()` function:    

```{r}
    fit = lm(medv ~ crim + rm + dis, data = Boston)
    fit2 = lm(medv ~ crim + rm + dis + chas + nox, data = Boston)
    anova(fit, fit2)
```

It appears that adding the two additional variables `chas` and `nox` is significant. Selecting variables/models is a central topic in statistics. We could consider some classical tools such as the Akaike information criterion [@akaike1998information] or the Bayesian information criterion [@schwarz1978estimating]. Incorporating the stepwise selection algorithm, we may find the best AIC model:

```{r}
    # fit a full model that contains all variables
    full.model = lm(medv ~ ., data = Boston)
    # select the best AIC model by stepwise regression
    stepAIC = step(full.model, trace=0, direction="both")
    # the best set of variables being selected
    attr(stepAIC$terms, "term.labels")
```

## Prediction

The `predict()` function is an extremely versatile function, for, prediction. When used on the result of a model fit using `lm()` it will, by default, return predictions for each of the data points used to fit the model.

```{r}
    # the fitted value from a model fitting
    yhat1 = fit$fitted.values
    # predict on a set of testing data
    yhat2 = predict(fit)
    # they are the same
    all(yhat1 == yhat2)
```

We could also specify new data, which should be a data frame or tibble with the same column names as the predictors.

```{r}
    new_obs = data.frame(crim = 0.3, rm = 6, dis = 5)
    predict(fit, newdata = new_obs)
```

We can also obtain the confidence interval for the mean response value of this new observation
```{r}
    predict(fit, newdata = new_obs, interval = "confidence")
```

Lastly, we can alter the level using the `level` argument. Here we report a prediction interval instead of a confidence interval.

```{r}
    predict(fit, newdata = new_obs, interval = "prediction", level = 0.99)
```
