--- 
title: "Statistical Learning and Machine Learning with R"
author: "[Ruoqing Zhu, PhD](https://sites.google.com/site/teazrq/)"
date: "`r Sys.Date()`"
github-repo: teazrq/SMLR
site: bookdown::bookdown_site
url: 'https://teazrq.github.io/SMLR/'
knit: "bookdown::render_book"
documentclass: book
bibliography: [book.bib]
biblio-style: apalike
favicon: "favicon.ico"
link-citations: yes
linkcolor: cyan
urlcolor: cyan
description: "A textbook for STAT 542 and 432 at UIUC"
---

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6), 
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
```

# Preface {-}

Welcome to *Statistical Learning and Machine Learning with R*! I started this project during the summer of 2018 when I was preparing for the Stat 432 course. At that time, our faculty member [Dr. David Dalpiaz](https://daviddalpiaz.com/teaching.html), had decided to move to The Ohio State University (although he moved back to UIUC later on). David introduced to me this awesome way of publishing website on GitHub, which is a very efficient approach for developing courses. Since I have also taught Stat 542 (Statistical Learning) for several years, I figured it could be beneficial to integrate what I have to this [existing book](https://daviddalpiaz.github.io/r4sl/) by David and use it as the R material for both courses. For Stat 542, the main focus is to learn the numerical optimization behind these learning algorithms, and also be familiar with the theoretical background. As you can tell, I am not being very creative on the name, so `SMLR` it is. You can find the source file of this book on my [GitHub](https://teazrq.github.io/SMLR/).

## Target Audience {-}

This book can be suitable for students ranging from advanced undergraduate to first/second year Ph.D students who have prior knowledge in statistics. Although a student at the masters level will likely benefit most from the material. Previous experience with both basic mathematics (mainly linear algebra), statistical modeling (such as linear regressions) and R are assumed.

## What's Covered? {-}

This book currently covers the following topics:

  1. Basic Knowledge
      * R, R Studio and R Markdown
      * Linear regression and linear algebra
      * Numerical optimization
  2. Penalized linear models and model selection
  3. Nonlinear and Nonparametric Models
      * Spline
      * K-nearest neighbor
      * Kernel smoothing
  4. Classification models
      * Logistic regression
      * Discriminant analysis
  5. Machine Learning Models
      * Support vector machine
      * Kernel ridge regression
      * Tree models
      * Random forest
      * Boosting
  6. Unsupervised Learning
      * K-means
      * Hierarchical clustering
      * PCA
      * self-organizing map
      * Spectral clustering

The goal of this book is to introduce not only how to run some of the popular statistical learning models in `R`, know the algorithms and programming techniques for solving these models and also understand some of the fundamental statistical theory behind them. For example, for graduate students, these topics will be discuss in more detail:

  * Optimization
    + Lagrangian
    + Primal vs. dual
  * EM and MM algorithm
  * Bias-variance trade-off in
    + Linear regression
    + KNN
    + Kernel density estimation
  * Kernel Trick and RKHS
  * Representer Theorem
    + SVM
    + Spline

For each section, the difficulty will gradually increase from an undergraduate level to a graduate level. 

It will be served as a supplement to [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) [@james2013introduction] for [STAT 432 - Basics of Statistical Learning](https://go.illinois.edu/stat432) and to [The Elements of 
Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/ElemStatLearn/) [@hastie2001elements] for [STAT 542 - Statistical Learning](https://go.illinois.edu/stat542) at the [University of Illinois at Urbana-Champaign](http://illinois.edu/).

**This book is under active development**. Hence, you may encounter errors ranging from typos to broken code, to poorly explained topics. If you do, please let me know! Simply send an email and I will make the changes as soon as possible (`rqzhu AT illinois DOT edu`). Or, if you know `R Markdown` and are familiar with GitHub, [make a pull request and fix an issue yourself](https://github.com/teazrq/SLWR)! These contributions will be acknowledged. 

## Acknowledgements {-}

The initial contents  are derived from Dr. David Dalpiaz's book. My STAT 542 course materials are also inspired by [Dr. Feng Liang](https://stat.illinois.edu/directory/profile/liangf) and [Dr. John Marden](https://stat.illinois.edu/directory/profile/jimarden) who developed earlier versions of this course. And I also incorporated many online resources, which I cannot put into a comprehensive list. If you think I missed some references, please let me know. 

## License {-}

![This work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).](images/cc.png){width=15%}


<!--chapter:end:index.Rmd-->

# (PART) Basics Knowledge {-}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6), 
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
```

# R and RStudio

## Installing R and RStudio

The first step is to download and install [R](https://www.r-project.org/) and [RStudio](https://www.rstudio.com/products/rstudio/download/#download). Most steps should be self-explanatory. You can also find many online guides for step-by-step instruction, such as [this YouTube video](https://www.youtube.com/watch?v=cX532N_XLIs&t=19s/). However, be aware that some details may have been changed over the years. 

After installing both, open your RStudio, you should see four panes, which can be seen below: 

  * Source pane on top-left where you write code in to files
  * Console on bottom-left where the code is inputted into R
  * Environment (and other tabs) on top-right where you can see current variables and objects you defined
  * File (and other tabs) on bottom-right which is essentially a file borrower

<center>
![](images/RStudio.png){width=100%}
</center>

We will mainly use the left two panes. You can either directly input code into the console to run for results, or edit your code in a file and run them in chunks or as a whole. 

## Resources and Guides  {#r-basic}

There are many online resources for how to use R, RStudio. For example, David Dalpiaz's other online book [Applied Statistics with R](http://daviddalpiaz.github.io/appliedstats/) contains an introduction to using them. There are also other online documentation such as 

* [Install R and RStudio](https://www.youtube.com/watch?v=cX532N_XLIs&t=19s/)
* [R tutorial](http://www.r-tutor.com/r-introduction)
* [Data in R Play-list (video)](https://www.youtube.com/playlist?list=PLBgxzZMu3GpPojVSoriMTWQCUno_3hjNi)
* [R and RStudio Play-list (video)](https://www.youtube.com/playlist?list=PLBgxzZMu3GpMjYhX7jLm5B9gEV7AOOJ5w)

It is worth to mention that once you become an advanced user, and possibly a developer of R packages using `C/C++` (add-on of R for performing specific tasks), and you also happen to use Windows like I do, you will have to install [Rtools](https://cran.r-project.org/bin/windows/Rtools/) that contains the gcc compilers. This is also needed if you want to install any R package from a "source" (`.tar.gz`) file instead of using the so-called "binaries" (`.zip` files). 

## Basic Mathematical Operations

Basic R calculations and operations should be self-explanatory. Try to type-in the following commands into your R console and start to explore yourself. Lines with a `#` in the front are comments, which will not be executed. Lines with `##` in the front are outputs you should expect. 

```{r, collapse=TRUE}
    # Basic mathematical operations
    1 + 3
    1 - 3
    1 * 3
    1 / 3
    3^5
    4^(-1/2)
    pi
    
    # some math functions
    sqrt(4)
    exp(1)
    log(3)
    log2(16)
    log(15, base = 3)
    factorial(5)
    sin(pi)
```

If you want to see more information about a particular function or operator in R, the easiest way is to get the reference document. Put a question mark in front of a function name:

```{r, collapse=TRUE, eval=FALSE}
    # In a default R console window, this will open up a web browser.
    # In RStudio, this will be displayed at the ‘Help’ window at the bottom-right penal (Help tab). 
    ?log10
    ?cos
```


## Data Objects

Data objects can be a complicated topic for people who never used R before. The most common data objects are `vector`, `matrix`, `list`, and `data.frame`. They are defined using a specific syntax. To define a vector, we use `c` followed by `()`, where the elements within the parenthesis are separated using comma. You can save the vector and name as something else. For example

```{r}
    # creating a vector
    c(1,2,3,4)
    c("a", "b", "c")
    
    # define a new vector object, called `x`
    x = c(1,1,1,0,0,0)
```

After defining this object `x`, it should also appear on your top-right environment pane. To access elements in an object, we use the `[]` operator, like a `C` programming reference style.

```{r}
    # getting the second element in x
    x[2]
  
    # getting the second to the fourth element in x
    x[2:4]
```

Similarly, we can create and access elements in a matrix:

```{r}
    # create a matrix by providing all of its elements
    # the elements are filled to the matrix by column
    matrix(c(1,2,3,4), 2, 2)
  
    # create a matrix by column-bind vectors
    y = c(1,0,1,0,1,0)
    cbind(x, y)
  
    # access elements in a matrix
    # Note that in R, upper and lower cases are treated as two different objects
    X = matrix(c(1:16), 4, 4)
    X
    X[2, 3]
    X[1, ]
    
    # getting a sub-matrix of X
    X[1:2, 3:4]
```  

Mathematical operations on vectors and matrices are, by default, element-wise. For matrix multiplications, you should use `%*%`.

```{r}  
    # adding two vectors
    (x + y)^2
  
    # getting the length of a vector
    length(x)
    
    # matrix multiplication
    X %*% X
    
    # getting the dimension of a matrix
    dim(X)
    
    # A warning will be issued when R detects something wrong
    # Results may still be produced however
    y + c(1,2,3,4)
```

`list()` creates a list of objects (of any type). However, some operators cannot be directly applied to a list in a similar way as to vectors or matrices. Model fitting results in R are usually stored as a list. For example, the `lm()` function, which will be introduced later.

```{r}  
    # creating a list
    x = list(c(1,2), "hello", matrix(c(1,2,3,4), 2, 2))
  
    # accessing its elements using double brackets `[[]]` 
    x[[1]]
```

`data.frame()` creates a list of vectors of equal length, and display them as a matrix-like object, in which each vector is a column of the matrix. It is mainly used for storing data. This will be our most frequently used data object for analysis. For example, in the famous `iris` data, the first four columns are numerical variables, while the last column is a categorical variable with three levels: `setosa`, `versicolor`, and `virginica`:

```{r}  
    # The iris data is included with base R, so we can use them directly
    # This will create a copy of the data into your environment
    data(iris)
  
    # the head function peeks the first several rows of the dataset 
    head(iris, n = 3)
    
    # each column usually contains a column (variable) name 
    colnames(iris)
    
    # data frame can be called by each individual column, which will be a vector
    # iris$Species
    iris$Species[2:4]
    
    # the summary function can be used to view summary statistics of all variables
    summary(iris)
```

`factor` is a special type of vector. It is frequently used to store a categorical variable with more than two categories. The last column of the iris data is a factor. You need to be a little bit careful when dealing with factor variables when during modeling since some functions do not take care of them automatically or they do it in a different way than you thought. For example, changing a factor variable into numerical ones will ignore any potential relationship among different categories. 

```{r}
    levels(iris$Species)
    as.numeric(iris$Species)
```

## Readin and save data 

Data can be imported from a variety of sources. More commonly, a dataset can be stored in `.txt` and `.csv` files. Such data reading methods require specific structures in the source file: the first row should contain column names, and there should be equal number of elements in each row. Hence you should always check your file before reading them in. 

```{r}
    # read-in data
    birthrate = read.csv("data/birthrate.csv")
    head(birthrate)
    
    # to see how many observations (rows) and variables (columns) in a dataset
    dim(birthrate)
```

R data can also be saved into other formats. The more efficient way, assuming that you are going to load these file back to R in the future, is to save them as `.RData` file. Usually, for a larger dataset, this reduces the time spend on reading the data. 

```{r, eval = FALSE}
    # saving a object to .RData file
    save(birthrate, file = "mydata.RData")
  
    # you can specify multiple objects to be saved into the same file
    save(birthrate, iris, file = "mydata.RData")
    
    # load the data again back to your environment
    load("mydata.RData")
    
    # alternatively, you can also save data to a .csv file
    write.csv(birthrate, file = "mydata.csv")
    
    # you can notice that this .csv file contains an extra column of "ID number", without a column name
    # Hence, when you read this file back into R, you should specify `row.names = 1` to indicate that.
    # Otherwise this will produce an error
    read.csv(file = "mydata.csv", row.names = 1)
```

## Using and defining functions

We have already used many functions. You can also define your own functions, and even build them into packages (more on this later) for other people to use. This is the main advantage of R. For example, let's consider writing a function that returns the minimum and maximum of a vector. Suppose we already know the `min()` and `max()` functions. 

```{r}
    myrange <- function(x) # x is the argument that your function takes in
    {
      return(c(min(x), max(x))) # return a vector that contains two elements
    }
  
    x = 1:10
    myrange(x)
    
    # R already has this function
    range(x)
```


## Distribution and random numbers

Three distributions that are most frequently used in this course are Bernoulli, Gaussian (normal), and $t$ distributions. Bernoulli distributions can be used to describe binary variables, while Gaussian distribution is often used to describe continuous ones. The following code generates some random variables

```{r echo=FALSE}
    set.seed(2) # this will not show
```

```{r}
    # read the documentation of rbinom() using ?rbinom
    x = rbinom(100, 1, 0.4)
    table(x)
```

However, this result cannot be replicated by others, since the next time we run this code, the random numbers will be different. Hence it is important to set and keep the random seed when a random algorithm is involved. The following code will always generate the same result

```{r}
    set.seed(1)
    x = rbinom(100, 1, 0.4)
    y = rnorm(100) # by default, this is mean 0 and variance 1
    
    table(x)
    hist(y)
    boxplot(y ~ x)
```

## Using packages and other resources

Packages are written and contributed to R by individuals. They provide additional features (functions or data) that serve particular needs. For example, the `ggplot2` package is developed by the RStudio team that provides nice features to plot data. We will have more examples of this later on, but first, let's install and load the package so that we can use these features. More details will be provided in the data visualization section. 

```{r, eval = FALSE}
    # to install a package
    install.packages("ggplot2")
```

```{r}
    # to load the package
    library(ggplot2)
  
    # use the ggplot() function to produce a plot
    # Sepal.Length is the horizontal axis
    # Sepal.Width is the vertical axis
    # Species labels are used as color
    ggplot(iris, aes(Sepal.Length, Sepal.Width, colour = Species)) + 
      geom_point()  
```

You may also noticed that in our previous examples, all tables only displayed the first several rows. One may be interested in looking at the entire dataset, however, it would take too much space to display the whole table. Here is a package that would allow you to display it in a compact window. It also provides searching and sorting tools. You can integrate this into your R Markdown reports. 

```{r}
    library(DT)
    datatable(iris, filter = "top", rownames = FALSE,
              options = list(pageLength = 5))
```

Often times, you may want to perform a new task and you don't know what function can be used to achieve that. Google Search or Stack Overflow are probably your best friends. I used to encounter this problem: I have a list of objects, and each of them is a vector. I then need to extract the first element of all these vectors. However, doing this using a for-loop can be slow, and I am also interested in a cleaner code. So I found [this post](https://stackoverflow.com/questions/44176908/r-list-get-first-item-of-each-element), which provided a simple answer:

```{r}
    # create the list
    a = list(c(1,1,1), c(2,2,2), c(3,3,3))
    
    # extract the first element in each vector of the list
    sapply(a, "[[", 1)
```

## Practice questions

  1. Attach a new numerical column to the `iris` data, as the product of `Petal.Length` and `Petal.Width` and name the column as `Petal.Prod`.
```{r class.source = NULL, eval = FALSE}
  iris = cbind(iris, "Petal.Prod" = iris$Petal.Length*iris$Petal.Width)
  head(iris)
```

  2. Attach a new numerical column to the `iris` data, with value 1 if the observation is `setosa`, 2 for `versicolor` and 3 for `virginica`, and name the column as `Species.Num`.
```{r class.source = NULL, eval = FALSE}
  iris = cbind(iris, "Species.Num" = as.numeric(iris$Species))
  head(iris)
```

  3. Change `Species.Num` to a factor variable such that it takes value "Type1" if the observation is `setosa` and "NA" otherwise.
```{r class.source = NULL, eval = FALSE}
  iris$Species.Num = as.factor(ifelse(iris$Species == "setosa", "Type1", "NA"))
  head(iris)
```

  4. Define a function that takes in a numerical vector, and output the mean of that vector. Do this without using the `mean()` and `sum()` function.
```{r class.source = NULL, eval = FALSE}
  mymean <- function(x)
  {
    sum = 0
    for (i in 1:length(x))
      sum = sum + x[i]
    
    return(sum = sum / length(x))
  }
  
  x = 1:10
  mymean(x)
  
  mean(x)
```  

<!--chapter:end:01.1-r-basics.Rmd-->

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6), 
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
```

# RMarkdown

## Basics and Resources

R Markdown is a built-in feature of RStudio. It integrates plain text with chunks of R code in to a single file, which is extremely useful when constructing class notes or building a website. A `.rmd` file can be compiled into nice-looking `.html`, `.pdf`, and `.docx` file. For example, this entire guide is created using R Markdown. With RStudio, you can install R Markdown from R console using the following code. Note that this should be automatically done the first time you create and compile a `.rmd` file in RStudio.

```{r eval = FALSE}
    # Install R Markdown from CRAN
    install.packages("rmarkdown")
```

Again there are many online guides for R Markdown, and these may not be the best ones. 

* [R Markdown: The Definitive Guide](https://bookdown.org/yihui/rmarkdown/){target='_blank'}
* [R Markdown Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf){target='_blank'}
* [R Markdown Play-list (video)](https://www.youtube.com/playlist?list=PLBgxzZMu3GpNgd07DwmS-2odHtMO6MWGH){target='_blank'}

To get started, create an R Markdown template file by clicking `File` -> `New File` -> `R Markdown...`

<center>
![](images/Create.png){width=70%}
</center>

You can then `Knit` the template file and start to explore its features.

<center>
![](images/Knit.png){width=70%}
</center>

Please note that this guide is provided in the `.html` format. However, your homework report should be in `.pdf` format. This can be done by selecting the `Knit to PDF` option from the Knit button. Again there are many online guides, and these may not be the best ones. 

* [R Markdown Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)
* [R Markdown Play-list (video)](https://www.youtube.com/playlist?list=PLBgxzZMu3GpNgd07DwmS-2odHtMO6MWGH)

## Formatting Text

Formatting text is easy. Bold can be done using `**` or `__` before and after the text. Italics can be done using `*` or `_` before and after the text. For example, **This is bold.** *This is italics.* and __*this is bold italics*__. `This text appears as monospaced.`

- Unordered list element 1.
- Unordered list element 2.
- Unordered list element 3.

1. Ordered list element 1.
2. Ordered list element 2.
3. Ordered list element 3.

We could mix lists and links. Note that a link can be constructed in the format `[display text](http link)`. If colors are desired, we can customize it using, for example, `[\textcolor{blue}{display text}](http link)`. But this only works in `.pdf` format. For `.html`, use `<span style="color: red;">text</span>`.

- A default link: [RMarkdown Documentation](http://rmarkdown.rstudio.com/){target='_blank'}
- colored link 1: [\textcolor{blue}{RMarkdown Cheatsheet}](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf){target='_blank'} (Not shown because it only works in PDF)
- colored link 2: [<span style="color: red;">Table Generator</span>](http://www.tablesgenerator.com/markdown_tables){target='_blank'} (only works in HTML)

Tables are sometimes tricky using Markdown. See the above link for a helpful Markdown table generator. Note that you can also adjust the alignment by using a `:` sign. 

| A  | B  | C  |
|:--:|:---|:---|
| 1  | 2  | 3  |
| Middle | Left | Right |


## Adding `R` Code

So far we have only used Markdown to create the text part. This is useful by itself, but the real power of RMarkdown comes when we add `R`. There are two ways we can do this. We can use `R` code chunks, or run `R` inline.

### `R` Chunks

The following is an example of an `R` code chunk. Start the chunk with ` ```{r} ` and end with ` ``` `:

` ```{r}`

$\quad$ `set.seed(123)`
  
$\quad$ ` rnorm(5)`

` ``` `

This generates five random observations from the standard normal distribution. We also set the seed so that the results can be later on replicated. The result looks like the following 

```{r}
    set.seed(123)
    rnorm(5)
```

```{r}
    # define function
    get_sd = function(x, biased = FALSE) {
      n = length(x) - 1 * !biased
      sqrt((1 / n) * sum((x - mean(x)) ^ 2))
    }
    
    # generate random sample data
    set.seed(42)
    (test_sample = rnorm(n = 10, mean = 2, sd = 5))
    
    # run function on generated data
    get_sd(test_sample)
```

There is a lot going on here. In the `.Rmd` file, notice the syntax that creates and ends the chunk. Also note that `example_chunk` is the chunk name. Everything between the start and end syntax must be valid `R` code. Chunk names are not necessary, but can become useful as your documents grow in size.

In this example, we define a function, generate some random data in a reproducible manner, displayed the data, then ran our function.

### Inline `R`

`R` can also be run in the middle of the exposition. For example, the mean of the data we generated is `r mean(test_sample)`.

## Importing Data

When using RMarkdown, any time you *knit* your document to its final form, say `.html`, a number of programs run in the background. Your current `R` environment seen in RStudio will be reset. Any objects you created while working interactively inside RStudio will be ignored. Essentially a new `R` session will be spawned in the background and the code in your document is run there from start to finish. For this reason, things such as importing data must be explicitly coded into your document.

```{r message = FALSE, warning = FALSE}
    library(readr)
    example_data = read_table("data/skincancer.txt")
```

The above loads the online file. In many cases, you will load a file that is locally stored in your own computer. In that case, you can either specify the full file path, or simply use, for example `read_csv("filename.csv")` if that file is stored at your **working directory**. The  **working directory** will usually be the directory that contains your `.Rmd` file. You are recommended to reference data in this manner. Note that we use the newer `read_csv()` from the `readr` package instead of the default `read.csv()`.

## Working Directory

Whenever `R` code is run, there is always a current working directory. This allows for relative references to external files, in addition to absolute references. Since the working directory when knitting a file is always the directory that contains the `.Rmd` file, it can be helpful to set the working directory inside RStudio to match while working interactively.

To do so, select `Session > Set Working Directory > To Source File Location` while editing a `.Rmd` file. This will set the working directory to the path that contains the `.Rmd`. You can also use `getwd()` and `setwd()` to manipulate your working directory programmatically. These should only be used interactively. Using them inside an RMarkdown document would likely result in lessened reproducibility.

**As of recent RStudio updates, this practice is not always necessary when working interactively.** If lines of code are being "Output Inline," then the working directory is automatically the directory which contains the `.Rmd` file.

## Plotting

The following generates a simple plot, which displays the skin cancer mortality. By default, the figure is aligned on the left, with size 3 by 5 inches. 

```{r fig.align = 'left', fig.height = 3, fig.width = 5, out.width = "50%"}
  plot(Mort ~ Lat, data = example_data)
```

In our R introduction, we used `ggplot2` to create a more interesting plot. You may also polish a plot with basic functions. Notice it is *huge* in the resulting document, since we have modified some *chunk options* (`fig.height = 6, fig.width = 8`) in the RMarkdown file to manipulate its size.

```{r fig.align = 'left', fig.height = 6, fig.width = 8, out.width = "50%"}
    plot(Mort ~ Lat, data = example_data,
         xlab = "Latitude",
         ylab = "Skin Cancer Mortality Rate",
         main = "Skin Cancer Mortality vs. State Latitude",
         pch  = 19,
         cex  = 1.5,
         col  = "deepskyblue")
```

But you can also notice that the labels and the plots becomes disproportional when the figure size is set too small. This can be resolved using a scaling option such as `out.width = '60%`, but enlarge the original figure size. We also align the figure at the center using ` fig.align = 'center' `

```{r echo = FALSE, fig.height = 6, fig.width = 8, out.width = "60%", fig.align = 'center'}
    plot(Mort ~ Lat, data = example_data,
         xlab = "Latitude",
         ylab = "Skin Cancer Mortality Rate",
         main = "Skin Cancer Mortality vs. State Latitude",
         pch  = 19,
         cex  = 1.5,
         col  = "deepskyblue")
```

## Chunk Options

We have already seen chunk options `fig.height`, `fig.width`, and `out.width` which modified the size of plots from a particular chunk. There are many [chunk options](http://yihui.name/knitr/options/), but we will discuss some others which are frequently used including; `eval`, `echo`, `message`, and `warning`. If you noticed, the plot above was displayed without showing the code.

```{r eval = FALSE}
    install.packages("rmarkdown")
    ?log
    View(mpg)
```

Using `eval = FALSE` the above chunk displays the code, but it is not run. We've already discussed not wanting install code to run. The `?` code pulls up documentation of a function. This will spawn a browser window when knitting, or potentially crash during knitting. Similarly, using `View()` is an issue with RMarkdown. Inside RStudio, this would pull up a window which displays the data. However, when knitting, `R` runs in the background and RStudio is not modifying the `View()` function. This, on OSX especially, usually causes knitting to fail.

```{r echo = FALSE}
  print("Hello World!")
```

Above, we see output, but no code! This is done using `echo = FALSE`, which is often useful.

```{r message = FALSE, warning = FALSE}
    x = 1:10
    y = 1:10
    summary(lm(y ~ x))
```

The above code produces a warning, for reasons we will discuss later. Sometimes, in final reports, it is nice to hide these, which we have done here. `message = FALSE` and `warning = FALSE` can be used to do so. Messages are often created when loading packages to give the user information about the effects of loading the package. These should be suppressed in final reports. Be careful about suppressing these messages and warnings too early in an analysis as you could potentially miss important information!

## Adding Math with LaTeX

Another benefit of RMarkdown is the ability to add [Latex for mathematics typesetting](https://www.latex-project.org/about/). Like `R` code, there are two ways we can include Latex; displaystyle and inline.

Note that use of LaTeX is somewhat dependent on the resulting file format. For example, it cannot be used at all with `.docx`. To use it with `.pdf` you must have LaTeX installed on your machine.

With `.html` the LaTeX is not actually rendered during knitting, but actually rendered in your browser using MathJax.

### Displaystyle LaTeX

Displaystyle is used for larger equations which appear centered on their own line. This is done by putting `$$` before and after the mathematical equation.

$$
\widehat \sigma = \sqrt{\frac{1}{n - 1}\sum_{i=1}^{n}(x_i - \bar{x})^2}
$$

### Inline LaTex

We could mix LaTeX commands in the middle of exposition, for example: $t = 2$. We could actually mix `R` with Latex as well! For example: $\bar{x} = `r mean(test_sample)`$.

## Output Options

At the beginning of the document, there is a code which describes some metadata and settings of the document. The default code looks like 

```{r eval = FALSE}
    title: "R Notebook"
    output: html_notebook
```

You can easily add your name and date to it, and add a Table of Contents, using `toc: yes`. Note that the following code would specify the theme of an `html` file. 

```{r eval = FALSE}
    title: "My RMarkdown Template"
    author: "Your Name"
    date: "Aug 26, 2021"
    output:
      html_document: 
        toc: yes
```

You can edit this yourself, or click the settings button at the top of the document and select `Output Options...`. Here you can explore other themes and syntax highlighting options, as well as many additional options. Using this method will automatically modify this information in the document.

## Try It!

Be sure to play with this document! Change it. Break it. Fix it. The best way to learn RMarkdown (or really almost anything) is to try, fail, then find out what you did wrong.

RStudio has provided a number of [beginner tutorials](http://rmarkdown.rstudio.com/lesson-1.html) which have been greatly improved recently and detail many of the specifics potentially not covered in this document. RMarkdown is continually improving, and this document covers only the very basics.

<!--chapter:end:01.2-rmd-basics.Rmd-->

\def\cD{\cal{D}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bC{\mathbf{C}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bzero{\mathbf{0}}
\def\bbeta{\boldsymbol \beta}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6), 
                        fig.width = 6, fig.width = 6,
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
```

# Linear Algebra Basics

You should already be familiar with some basic linear algebra concepts such as matrix and vector multiplications. Here we review some basic concepts and properties that will be used in this course. For the most part, they are used in deriving linear regression results. 

## Definition

We usually use $\bX$ to denote an $n \times p$ dimensional design matrix, where $n$ is the number of observations and $p$ is the number of variables. The columns of $\bx$ are denoted as $\bx_1, \ldots, \bx_p$:

$$
\bX = \begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1p}\\
x_{21} & x_{22} & \cdots & x_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
x_{m1} & x_{n2} & \cdots & x_{np}\\
\end{pmatrix} = 
\begin{pmatrix}
\bx_1 & \bx_2 & \cdots & \bx_p\\
\end{pmatrix}
$$
The __column space__, $\cal{C}(\bX)$ of $\bX$ is the set of all linear combinations of $\bx_1, \bx_2, \ldots, \bx_p$, i.e., 

$$c_1 \bx_1 + c_2 \bx_2 + \cdots c_p \bx_p.$$
This is also called the __span__ of these vectors, $\text{span}(\bx_1, \bx_2, \ldots, \bx_p)$. Its orthogonal space is 

$$\{\bv: \bX^\T \bv = 0\}.$$
A projection matrix $\bP$ is a matrix such that 





<!--chapter:end:01.3-linearalgebra.Rmd-->

\def\cD{\cal{D}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bzero{\mathbf{0}}
\def\bbeta{\boldsymbol \beta}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6), 
                        fig.width = 6, fig.width = 6,
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
```

# Optimization Basics

Optimization is heavily involved in statistics and machine learning. Almost all methods introduced in this book can be viewed as some form of optimization. It would be good to have some prior knowledge of it so that later chapters can use these concepts without difficulties. Especially, one should be familiar with concepts such as constrains, gradient methods, and be able to implement them using existing R functions. Since optimization is such a broad topic, we refer readers to @boyd2004convex and @nocedal2006numerical for more further reading. 

We will use a slightly different set of notations in this Chapter so that we are consistent with the literature. This means that for the most part, we will use $x$ as our parameter of interest and optimize a function $f(x)$. This is in contrast to optimizing $\theta$ in a statistical model $f_\theta(x)$ where $x$ is the observed data. However, in the example of linear regression, we may again switch back to the regular notation of $x^\text{T} \bbeta$. These transitions will only happen under clear context and should not create ambiguity.

## Basic Concept

We usually consider a convex optimization problem (non-convex problems are a bit too involving although we will also see some examples of that), meaning that we optimize (minimize) a convex function in a convex domain. A __*convex function*__ $f(\bx)$ maps some subset $C \in \mathbb{R}^p$ into $\mathbb{R}^p$, but enjoys the property that 

$$ f(t \bx_1 + (1 - t) \bx_2) \leq t f(\bx_1) + ( 1- t) f(\bx_2), $$
for all $t \in [0, 1]$ and any two points $\bx_1$, $\bx_2$ in the domain of $f$. 

<center>
![An example of convex function, [from wikipedia](https://en.wikipedia.org/wiki/Convex_function)](images/ConvexFunction.png){width=55%}
</center>

Note that if you have a concave function (the bowl faces downwards) then $-f(\bx)$ would be convex. Examples of convex functions:

  * Univariate functions: $x^2$, $\exp(x)$, $-log(x)$ 
  * Affine map: $a^\T \bx + b$ is both convex and concave
  * A quadratic function $\frac{1}{2}\bx^\T \bA \bx + b^\T \bx + c$, if $\bA$ is positive semidefinite
  * All $p$ norms are convex, following the Triangle inequality and properties of a norm.
  * A sin function is neither convex or concave

On the other hand, a __*convex set*__ $C$ means that if we have two points $x_1$ and $x_2$ in $C$, the line segment joining these two points has to lie within $C$, i.e., 

$$\bx_1, \bx_2 \in C \quad \Longrightarrow \quad t \bx_1 + (1 - t) \bx_2 \in C,$$
for all $t \in [0, 1]$.

<center>
![An example of convex set](images/ConvexSet.png){width=55%}
</center>

Examples of convex set include

  * Real line: $\mathbb{R}$
  * Norm ball: $\{ \bx: \lVert \bx \rVert \leq r \}$
  * Hyperplane: $\{ \bx: a^\T \bx = b \}$

Consider a simple optimization problem:

$$ \text{minimize} \quad f(x_1, x_2) = x_1^2 + x_2^2$$

Clearly, $f(x_1, x_2)$ is a convex function, and we know that the solution of this problem is $x_1 = x_2 = 0$. However, the problem might be a bit more complicated if we restrict that in a certain (convex) region, for example, 

\begin{align}
&\underset{x_1, x_2}{\text{minimize}} & \quad f(x_1, x_2) &= x_1^2 + x_2^2 \\
&\text{subject to} & x_1 + x_2 &\leq -1 \\
& & x_1 + x_2 &> -2
\end{align}

Here the convex set $C = \{x_1, x_2 \in \mathbb{R}: x_1 + x_2 \leq -1 \,\, \text{and} \,\, x_1 + x_2 > -2\}$. And our problem looks like the following, which attains it minimum at $(-0.5, -0.5)$.

```{r message=FALSE, echo = FALSE, out.width = "75%"}
  library(plotly)

  # generate the surface
  x1 = seq(-1.5, 1, 0.01)
  x2 = seq(-1.5, 1, 0.01)

  y = matrix(NA, length(x1), length(x2))
  
  for (i in 1:length(x1))
  for (j in 1:length(x2))
      if (x1[i] + x2[j] < -1 & x1[i] + x2[j] > -2)
          y[i, j] = x1[i]^2 + x2[j]^2
  
  # plot the surface
  plot_ly(x = x1, y = x2) %>% 
      layout(plot_bgcolor='rgb(254, 247, 234)') %>% 
      layout(paper_bgcolor='transparent') %>% 
      add_surface(z = y, 
                  colorscale = 'Viridis') %>% 
      layout(scene = list(xaxis = list(title = "X1"), 
                          yaxis = list(title = "X2"),
                          zaxis = list(title = expression(f(x1, x2))))) %>%
      add_markers(data = data.frame("x" = -0.5, "y" = -0.5, "z" = 2*0.5^2), 
              x = ~x, y = ~y, z = ~z, 
              marker = list(size = 6, color = "red", symbol = 104))
```

In general, we will be dealing with a problem in the form of 

\begin{align}
&\underset{\bx}{\text{minimize}} & \quad f(\bx) \\
&\text{subject to} & g_i(\bx) & \leq 0, \, i = 1,\ldots, m \\
& & h_j(\bx) &= 0, \, j = 1,\ldots, k
\end{align}

where $g_i(\bx)$s are a set of inequality constrains, and $h_j(\bx)$s are equality constrains. There are established result showing what type of constrains would lead to a convex set, but let's assuming for now that we will be dealing a well behaved problem. We shall see in later chapters that many models such as, Lasso, Ridge and support vector machines can all be formulated into this form. 

## Global vs. Local Optima {#global_local}

Although we would like to deal with convex optimization problems, non-convex problems appears more and more frequently. For example, deep learning models are almost always non-convex except overly simplified ones. However, __for convex optimization problems, a local minimum is also a global minimum__, i.e., a $x^\ast$ such that for any $x$ in the feasible set, $f(x^\ast) \leq f(x)$. This can be achieved by a variety of descent algorithms, to be introduced. However, for non-convex problems, we may still be interested in a local minimum, which satisfies that for any $x$ in a **neighboring set of $x^\ast$**, $f(x^\ast) \leq f(x)$. The comparison of these two cases can be demonstrated in the following plots. Again, a descent algorithm can help us find a local minimum, except for some very special cases, such as a saddle point. However, we will not discuss these issues in this book. 

```{r echo = FALSE, fig.dim = c(12, 6), out.width = "75%"}
  par(mfrow=c(1,2))
  par(mar=c(2,2,2,2))
  
  # a convex case 
  x = seq(-2, 2, 0.01)
  plot(x, x^2, type = "l", col = "deepskyblue", lwd = 1.5)
  points(0, 0, col = "red", pch = 19, cex = 2.5)
  
  # non-convex case
  x = seq(-4, 2, 0.01)
  plot(x, x^4 + 2*x^3 - 5*x^2, type = "l", col = "deepskyblue", lwd = 1.5)
  points(-2.5, (-2.5)^4 + 2*(-2.5)^3 - 5 *(-2.5)^2, col = "red", pch = 19, cex = 2.5)
  points(0, 0, col = "darkorange", pch = 19, cex = 1.5)
  points(1, -2, col = "red", pch = 19, cex = 1.5)
```

## Example: Linear Regression using `optim()`

Although completely not necessary, we may also view linear regression as an optimization problem. This is of course an unconstrained problem, meaning that $C \in \mathbb{R}^p$. Such problems can be solved using the `optim()` function. Also, let's temporarily switch back to the $\bbeta$ notation of parameters. Hence, if we observe a set of observations $\{\bx_i, y_i\}_{i = 1}^n$, our optimization problem is to minimize the objection function, i.e., residual sum of squares (RSS):

\begin{align}
\underset{\bbeta}{\text{minimize}} \quad f(\bbeta) = \frac{1}{n} \sum_i (y_i - \bx_i^\T \bbeta)^2 \\
\end{align}

We generate 200 random observations, and also write a function to calculate the RSS for any given $\bbeta$ values. The objective function looks like the following:

```{r}
    # generate data from a simple linear model 
    set.seed(20)
    n = 200
    x <- cbind(1, rnorm(n))
    y <- x %*% c(0.5, 1) + rnorm(n)
    
    # calculate the residual sum of squares for a grid of beta values
    rss <- function(b, trainx, trainy) sum((trainy - trainx %*% b)^2)
```

```{r message=FALSE, echo = FALSE, out.width = "80%"}
    # generate grid of beta for plot
    b0 <- b1 <- seq(0, 2, length = 20)
    z = matrix(apply(expand.grid(b0, b1), 1, rss, x, y), 20, 20)
    
    bestpoint = data.frame("x" = 0.5, "y" = 1, "z" = rss(c(0.5, 1), x, y))
    
    # 3d plot of RSS using `plotly`
    library(plotly)
    plot_ly(x = b0, y = b1) %>% 
        layout(plot_bgcolor='rgb(254, 247, 234)') %>% 
        layout(paper_bgcolor='transparent') %>% 
        add_surface(z = t(z), 
                    colorscale = 'Viridis') %>% 
        layout(scene = list(xaxis = list(title = "beta0"), 
               yaxis = list(title = "beta1"),
               zaxis = list(title = "RSS"))) %>% 
        add_markers(data = bestpoint, 
                  x = ~x, y = ~y, z = ~z, 
                  marker = list(size = 6, color = "red", symbol = 104))
```

Now the question is how to solve this problem. The `optim()` function uses the following syntax: 

```{r}
    # The solution can be solved by any optimization algorithm 
    lm.optim <- optim(par = c(2, 2), fn = rss, trainx = x, trainy = y)
```

  * The `par` argument specifies an initial value, in this case, $\beta_0 = \beta_1 = 2$
  * The `fn` argument specifies the name of an `R` function that can calculate the objective function. Please note that the first argument in this function has be the parameter being optimized, i.e, $\bbeta$. Also, it must be a vector, not a matrix or other types. 
  * The arguments `trainx = x`, `trainy = y` specifies any additional arguments that the objective function `fn`, i.e., `rss` needs. It behaves the same as if you are supplying this to `rss`.
  
```{r}
    lm.optim
```    

The result shows that the estimated parameters (`$par`) are 0.453 and 0.924, with a functional value 203.562. The convergence code is 0, meaning that the algorithm converged. The parameter estimates are almost the same as `lm()`, with small numerical errors. 

```{r}
    # The solution form lm()
    summary(lm(y ~ x - 1))$coefficients
```

What we will be introducing in the following are some basic approaches to solve such a numerical problem. We will start with unconstrained problems, then introduce constrained problems. 

## First and Second Order Properties

These properties are usually applied to unconstrained optimization problems. They are essentially just describing the landscape around a point $\bx^\ast$ such that it becomes the local optimizer. Since we generally concerns a convex problem, a local solution is also the global solution. However, these properties are still generally applied when solving a non-convex problem. Note that these statements are multi-dimensional. 

__First-Order Necessary Conditions__: If $f$ is continuously differentiable in an open neighborhood of local minimum $\bx^\ast$, then $\nabla f(\bx^\ast) = \mathbf{0}$. 

When we have a point $\bx^\ast$ with $\nabla f(\bx^\ast) = \mathbf{0}$, we call $\bx^\ast$ a __stationary point__. This is only a necessary condition, but not sufficient. Since example, $f(x) = x^3$ has zero derivative at $x = 0$, but this is not an optimizer. The figure in \@ref(global_local) also contains such a point. TO further strengthen this, we have 

__Second-order Necessary Conditions__: If $f$ is twice continuously differentiable in an open neighborhood of local minimum $\bx^\ast$, then $\nabla f(\bx^\ast) = \mathbf{0}$ and $\nabla^2 f(\bx^\ast)$ is positive semi-definite. 

This does rule out some cases, with a higher cost ($f$ needs to be twice continuously differentiable). But requiring positive semi-definite would not ensure everything. The same example $f(x) = x^3$ still satisfies this, but its not a local minimum. A positive definite $\nabla^2 f(\bx^\ast)$ would be sufficient:

__Second-order Sufficient Conditions__: $f$ is twice continuously differentiable in an open neighborhood of $\bx^\ast$. If $\nabla f(\bx^\ast) = \mathbf{0}$ and $\nabla^2 f(\bx^\ast)$ is positive definite, i.e.,
$$
\nabla^2 f(\bx) = \left(\frac{\partial^2 f(\bx)}{\partial x_i \partial x_j}\right) = \bH(\bx) \succeq 0
$$

then $\bx^\ast$ is a strict local minimizer of $f$. Here $\bH(\bx)$ is called the __Hessian matrix__, which will be frequently used in second-order methods. 

## Algorithm 

Most optimization algorithms follow the same idea: starting from a point $\bx^{(0)}$ (which is usually specified by the user) and move to a new point $\bx^{(1)}$ that improves the objective function value. Repeatedly performing this to get a sequence of points $\bx^{(0)}, \bx^{(1)}, \ldots$ until the certain stopping criterion is reached. 

A __stopping criterion__ could be 

  * Using the gradient of the objective function: $\lVert \nabla f(\bx^{(k)}) \rVert < \epsilon$
  * Using the (relative) change of distance: $\lVert \bx^{(k)} - \bx^{(k-1)} \rVert / \lVert \bx^{(k-1)}\rVert< \epsilon$ or $\lVert \bx^{(k)} - \bx^{(k-1)} \rVert < \epsilon$
  * Using the (relative) change of functional value: $| f(\bx^{(k)}) - f(\bx^{(k-1)})| < \epsilon$ or $| f(\bx^{(k)}) - f(\bx^{(k-1)})| / |f(\bx^{(k)})| < \epsilon$
  * Stop at a pre-specified number of iterations.

Most algorithms differ in terms of how to move from the current point $\bx^{(k)}$ to the next, better target point $\bx^{(k+1)}$. This may depend on the smoothness or structure of $f$, constrains on the domain, computational complexity, memory limitation, and many others. 

## Second-order Methods 

### Newton's Method

Now, let's discuss several specific methods. One of the oldest one is __Newton's method__. This is motivated form a quadratic approximation (essentially Taylor expansion) at a current point $\bx$, 

$$f(\bx^\ast) \approx f(\bx) + \nabla f(\bx)^\T (\bx^\ast - \bx) + \frac{1}{2} (\bx^\ast - \bx)^\T \bH(\bx) (\bx^\ast - \bx)$$
Our goal is to find a new stationary point $\bx^\ast$ such that $\nabla f(\bx^\ast) = 0$. By taking derivative of the above equation on both sides, with respect to $\bx^\ast$, we need 

$$0 = \nabla f(\bx^\ast) = 0 + \nabla f(\bx) + (\bx^\ast - \bx)^\T \bH(\bx)$$
which leads to 

$$\bx^\ast = \bx -  \bH(\bx)^{-1} \nabla f(\bx).$$

Hence, if we are currently at a point $\bx^{(k)}$, we need to calculate the gradient $\nabla f(\bx^{(k)})$ and Hessian $\bH(\bx)$ at this point, then move to the new point using $\bx^{(k+1)} = \bx^{(k)} -  \bH(\bx^{(k)})^{-1} \nabla f(\bx^{(k)})$. Some properties and things to concern regarding Newton's method:

  * Newton's method is scale invariant, meaning that you do not need to worry about the step size. It is automatically taken care of by the Hessian matrix. However, in practice, the local approximation may not be accurate, which makes the new point $\bx^{(k+1)}$ behaves differently than what we expect. Hence, it might still be safe to introduce a smaller step size $\delta \in (0, 1)$ and move with  
  $$\bx^{(k+1)} = \bx^{(k)} -  \delta \, \bH(\bx^{(k)})^{-1} \nabla f(\bx^{(k)})$$
  * There are also alternatives to take care of the step size. For example, __line search__ is frequently used, which will try to find the optimal $\delta$ that minimizes the function
  $$f(\bx^{(k)} + \delta \bv)$$
  where the direction $\bv$ in this case is $\bv = \bH(\bx^{(k)})^{-1} \nabla f(\bx^{(k)})$. It is also popular to use __backtracking line search__, which reduces the computational cost. The idea is to start with a large $\delta$ and gradually reduces it by a certain proportion if the new point doesn't significantly improves, i.e., 
  $$f(\bx^{(k)} + \delta \bv) > f(\bx^{(k)}) - \frac{1}{2} \delta \nabla f(\bx^{(k)})^\T \bv$$
  Note that when the direction $\bv$ is $\bH(\bx^{(k)})^{-1} \nabla f(\bx^{(k)})$, $\nabla f(\bx^{(k)})^\T \bv$ is essentially the norm defined by the Hessian matrix.
  * When you do not have the explicit formula of Hessian and even the gradient, you may __numerically approximate the derivative__ using the definition. For example, we could use 
  $$ \frac{f(\bx^{(k)} + \epsilon) - f(\bx^{(k)})}{\epsilon} $$
  with $\epsilon$ small enough, e.g., $10^{-5}$. However, this is very costly for the Hessian matrix if the number of variables is large. 
  
### Quasi-Newton Methods

Since the idea of Newton's method is to solve a vector $\bv$ such that 

$$\bH(\bx^{(k)}) \bv = - \nabla f(\bx^{(k)}), $$
If $\bH$ is difficult to compute, we may use some matrix to substitute it. For example, if we simplify use the identity matrix $\bI$, then this reduces to a first-order method to be introduced later. However, if we can get a good approximation, we can still solve this linear system and get to a better point. Then the question is how to obtain such matrix in a computationally inexpensive way. The Broyden–Fletcher–Goldfarb–Shanno (BFSG) algorithm is such an approach by iteratively updating its (inverse) estimation. The algorithm proceed as follows:

  1) Start with $x^{(0)}$ and a positive definite matrix, e.g., $\bB^{(0)} = \bI$
  2) For $k = 0, 1, 2, \ldots$, 
      * Search a updating direction by solving the linear system $\bB^{(k)} \bp_k = - \nabla f(\bx^{(k)})$
      * Perform line search in the direction of $\bv_k$ and obtain the next point $\bx^{(k+1)} = \bx^{(k)} + \delta \bp_k$
      * Update the approximation by 
      $$ \bB^{(k+1)} = \bB^{(k)} + \frac{\by_k^\T \by_{k}}{ \by_{k}^\T \bs_{k} } -  \frac{\bB^{(k)}\bs_{k}\bs_{k}^\T {\bB^{(k)}}^\T }{\bs_{k}^\T \bB^{(k)} \bs_{k} }, $$
      where $\by_k = \nabla f(\bx^{(k+1)}) - \nabla f(\bx^{(k)})$ and $\bs_{k} = \bx^{(k+1)} - \bx^{(k)}$.

The BFGS is performing a rank-two update by assuming that 
$$ \bB^{(k+1)} = \bB^{(k)} + a \bu \bu^\T + b \bv \bv^\T,$$
Alternatives of such type of methods include the symmetric rank-one and Davidon-Fletcher-Powell (DFP) updates. 

## First-order Methods 

### Gradient Descent

When simply using $\bH = \bI$, we update 
$$\bx^{(k+1)} = \bx^{(k)} - \delta \nabla f(\bx^{(k)}).$$ 
However, it is then crucial to figure out the step size $\delta$. A step size too large may not even converge at all, however, a step size too small will take too many iterations to converge. Alternatively, line search could be used. 

### Gradient Descent Example: Linear Regression

We use linear regression as an example. The objective function for linear regression is:

$$ \ell(\boldsymbol \beta) = \frac{1}{2n}||\mathbf{y} - \mathbf{X} \boldsymbol \beta ||^2 $$
with solution is

$$\widehat{\boldsymbol \beta} = \left(\mathbf{X}^\text{T}\mathbf{X}\right)^{-1} \mathbf{X}^\text{T} \mathbf{y} $$

```{r fig.dim = c(6, 6), out.width='45%', message= FALSE}
  par(mfrow=c(1,1))
  library(MASS)
  set.seed(3)
  n = 200
  
  # create some data with linear model
  X = mvrnorm(n, c(0, 0), matrix(c(1,0.7, 0.7, 1), 2,2))
  y = rnorm(n, mean = 2*X[,1] + X[,2])
  
  beta1 <- seq(-1, 4, 0.005)
  beta2 <- seq(-1, 4, 0.005)
  allbeta <- data.matrix(expand.grid(beta1, beta2))
  rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y), 
                length(beta1), length(beta2))
  
  # quantile levels for drawing contour
  quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)
  
  # plot the contour
  contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
  box()
  
  # the truth
  b = solve(t(X) %*% X) %*% t(X) %*% y
  points(b[1], b[2], pch = 19, col = "blue", cex = 2)
```

We use an optimization approach to solve this problem. By taking the derivative with respect to $\boldsymbol \beta$, we have the gradient

$$
\begin{align}
\frac{\partial \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta} = -\frac{1}{n} \sum_{i=1}^n (y_i - x_i^\text{T} \boldsymbol \beta) x_i.
\end{align}
$$
To perform the optimization, we will first set an initial beta value, say $\boldsymbol \beta = \mathbf{0}$ for all entries, then proceed with the update

$$ \boldsymbol \beta^\text{new} = \boldsymbol \beta^\text{old} - \frac{\partial \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta} \times \delta.$$

Let's set $\delta = 0.2$ for now. The following function performs gradient descent.  

```{r fig.dim = c(6, 6), out.width='45%'}
  # gradient descent function, which also record the path
  mylm_g <- function(x, y, 
                     b0 = rep(0, ncol(x)), # initial value
                     delta = 0.2, # step size
                     epsilon = 1e-6, #stopping rule
                     maxitr = 5000) # maximum iterations
  {
    if (!is.matrix(x)) stop("x must be a matrix")
    if (!is.vector(y)) stop("y must be a vector")
    if (nrow(x) != length(y)) stop("number of observations different")
    
    # initialize beta values
    allb = matrix(b0, 1, length(b0))

    # iterative update
    for (k in 1:maxitr)
    {
      # the new beta value
      b1 = b0 + t(x) %*% (y - x %*% b0) * delta / length(y)      

      # record the new beta
      allb = rbind(allb, as.vector(b1))
      
      # stopping rule
      if (max(abs(b0 - b1)) < epsilon)
        break;
      
      # reset beta0
      b0 = b1
    }

    if (k == maxitr) cat("maximum iteration reached\n")
    return(list("allb" = allb, "beta" = b1))
  }

  # fit the model 
  mybeta = mylm_g(X, y, b0 = c(0, 1))
  
  par(bg="transparent")
  contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
  points(mybeta$allb[,1], mybeta$allb[,2], type = "b", col = "red", pch = 19)
  points(b[1], b[2], pch = 19, col = "blue", cex = 1.5)
  box()
```

The descent path is very smooth because we choose a very small step size. However, if the step size is too large, we may observe unstable results or even unable to converge. For example, if We set $\delta = 1$ or $\delta = 1.5$.

```{r fig.dim = c(12, 6), out.width = "90%"}
  par(mfrow=c(1,2))
  par(mar=c(2,2,2,2))

  # fit the model with a larger step size
  mybeta = mylm_g(X, y, b0 = c(0, 1), delta = 1)

  contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
  points(mybeta$allb[,1], mybeta$allb[,2], type = "b", col = "red", pch = 19)
  points(b[1], b[2], pch = 19, col = "blue", cex = 1.5)
  box()
  
  # and even larger
  mybeta = mylm_g(X, y, b0 = c(0, 1), delta = 1.5, maxitr = 6)
  
  contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
  points(mybeta$allb[,1], mybeta$allb[,2], type = "b", col = "red", pch = 19)
  points(b[1], b[2], pch = 19, col = "blue", cex = 1.5)
  box()
```

## Coordinate Descent {#coordinate}

Instead of updating all parameters at a time, we can also update one parameter each time. The __Gauss-Seidel style__ coordinate descent algorithm at the $k$th iteration will sequentially update all $p$ parameters:

\begin{align}
    x_1^{(k+1)} &= \underset{\color{OrangeRed}{x_1}}{\arg\min} \quad f(\color{OrangeRed}{x_1}, x_2^{(k)}, \ldots, x_p^{(k)}) \nonumber \\
    x_2^{(k+1)} &= \underset{\color{OrangeRed}{x_2}}{\arg\min} \quad f(x_1^{\color{DodgerBlue}{(k+1)}}, \color{OrangeRed}{\bx_2}, \ldots, x_p^{(k)}) \nonumber \\
    \cdots &\nonumber \\
    x_p^{(k+1)} &= \underset{\color{OrangeRed}{x_p}}{\arg\min} \quad f(x_1^{\color{DodgerBlue}{(k+1)}}, x_2^{\color{DodgerBlue}{(k+1)}}, \ldots, \color{OrangeRed}{x_p}) \nonumber \\
\end{align}

Note that after updating one coordinate, the new parameter value is used for updating the next coordinate. After we complete this loop, all $j$ are updated to their new values, and we proceed to the next step. 

Another type of update is the __Jacobi style__, which can be performed in parallel at the $k$th iteration:

\begin{align}
    x_1^{(k+1)} &= \underset{\color{OrangeRed}{x_1}}{\arg\min} \quad f(\color{OrangeRed}{x_1}, x_2^{(k)}, \ldots, x_p^{(k)}) \nonumber \\
    x_2^{(k+1)} &= \underset{\color{OrangeRed}{x_2}}{\arg\min} \quad f(x_1^{(k+1)}, \color{OrangeRed}{\bx_2}, \ldots, x_p^{(k)}) \nonumber \\
    \cdots &\nonumber \\
    x_p^{(k+1)} &= \underset{\color{OrangeRed}{x_p}}{\arg\min} \quad f(x_1^{(k+1)}, x_2^{(k+1)}, \ldots, \color{OrangeRed}{x_p}) \nonumber \\
\end{align}

For differentiable convex functions $f$, we can ensure that if all parameters are optimized then the entire problem is also optimized. If $f$ is not differentiable, we may have trouble (see the example on [wiki](https://en.wikipedia.org/wiki/Coordinate_descent)). However, there are also cases where coordinate descent would still guarantee a convergence, e.g., a sperable case:

$$f(\bx) = g(\bx) + \sum_{j=1}^p h_j(x_j)$$
This is the Lasso formulation which will be discussed in later section. 

### Coordinate Descent Example: Linear Regression

Coordinate descent for linear regression is not really necessary. However, we will still use this as an example. Note that the update for a single parameter is 

$$
\underset{\boldsymbol \beta_j}{\text{argmin}} \frac{1}{2n} ||\by - X_j \beta_j - \bX_{(-j)} \bbeta_{(-j)} ||^2
$$

where $\mathbf{X}_{(-j)}$ is the data matrix without the $j$th column. Note that when updating $\beta_j$ coordinate-wise, we can first calculate the residual defined as $\mathbf{r} = \mathbf{y} - \mathbf{X}_{(-j)} \boldsymbol \beta_{(-j)}$ which does not depend on $\beta_j$, and optimize the rest of the formula for $\beta_j$. This is essentially the same as performing a one-dimensional regression by regressing $\mathbf{r}$ on $X_j$ and obtain the update. 
$$
\beta_j = \frac{X_j^T \mathbf{r}}{X_j^T X_j}
$$
The coordinate descent usually does not involve choosing a step size. Note that the following function is __NOT__ efficient because there are a lot of wasted calculations. It is only for demonstration purpose. Here we use the Gauss-Seidel style update.

```{r}
  # gradient descent function, which also record the path
  mylm_c <- function(x, y, b0 = rep(0, ncol(x)), epsilon = 1e-6, maxitr = 5000)
  {
    if (!is.matrix(x)) stop("x must be a matrix")
    if (!is.vector(y)) stop("y must be a vector")
    if (nrow(x) != length(y)) stop("number of observations different")
    
    # initialize beta values
    allb = matrix(b0, 1, length(b0))
    
    # iterative update
    for (k in 1:maxitr)
    {
      # initiate a vector for new beta
      b1 = b0
      
      for (j in 1:ncol(x))
      {
        # calculate the residual
        r = y - x[, -j, drop = FALSE] %*% b1[-j]
        
        # update jth coordinate
        b1[j] = t(r) %*% x[,j] / (t(x[,j, drop = FALSE]) %*% x[,j])
        
        # record the update
        allb = rbind(allb, as.vector(b1))
      }

      if (max(abs(b0 - b1)) < epsilon)
        break;
      
      # reset beta0
      b0 = b1
    }

    if (k == maxitr) cat("maximum iteration reached\n")
    return(list("allb" = allb, "beta" = b1))
  }

  # fit the model 
  mybeta = mylm_c(X, y, b0 = c(0, 3))

  par(mfrow=c(1,1))
  contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
  points(mybeta$allb[,1], mybeta$allb[,2], type = "b", col = "red", pch = 19)
  points(b[1], b[2], pch = 19, col = "blue", cex = 1.5)
  box()
```

## Stocastic Gradient Descent

The main advantage of using Stochastic Gradient Descent (SGD) is its computational speed. Calculating the gradient using all observations can be costly. Instead, we consider update the parameter __based on a single observation__. Hence, the gradient is defined as 

$$
\frac{\partial \ell_i(\boldsymbol \beta)}{\partial \boldsymbol \beta} = - (y_i - x_i^\text{T} \boldsymbol \beta) x_i.
$$
Compared with using all observations, this is $1/n$ of the cost. However, because this is rater not accurate for each iteration, but can still converge in the long run. There is a decay rate involved in SGD step size. If the step size does not decreases to 0, the algorithm cannot converge. However, it also has to sum up to infinite to allow us to go as far as we can. For example, a choice could be $\delta_k = 1/k$, hence $\sum \delta_k = \infty$ and $\sum \delta_k^2 < \infty$. 

```{r fig.width=6, fig.height=6, out.width = '45%', fig.align='center'}
  # gradient descent function, which also record the path
  mylm_sgd <- function(x, y, b0 = rep(0, ncol(x)), delta = 0.05, maxitr = 10)
  {
    if (!is.matrix(x)) stop("x must be a matrix")
    if (!is.vector(y)) stop("y must be a vector")
    if (nrow(x) != length(y)) stop("number of observations different")
    
    # initialize beta values
    allb = matrix(b0, 1, length(b0))
    
    # iterative update
    for (k in 1:maxitr)
    {
      # going through all samples
      for (i in sample(1:nrow(x)))
      {
        # update based on the gradient of a single subject
        b0 = b0 + (y[i] - sum(x[i, ] * b0)) * x[i, ] * delta

        # record the update
        allb = rbind(allb, as.vector(b0))
        
        # learning rate decay
        delta = delta * 1/k
      }
    }
    
    return(list("allb" = allb, "beta" = b0))
  }

  # fit the model 
  mybeta = mylm_sgd(X, y, b0 = c(0, 1), maxitr = 3)
  
  contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
  points(mybeta$allb[,1], mybeta$allb[,2], type = "b", col = "red", pch = 19)
  points(b[1], b[2], pch = 19, col = "blue", cex = 1.5)
  box()
```

### Mini-batch Stocastic Gradient Descent

Instead of using just one observation, we could also consider splitting the data into several small "batches" and use one batch of sample to calculate the gradient at each iteration. 

```{r fig.width=6, fig.height=6, out.width = '45%', fig.align='center'}
  # gradient descent function, which also record the path
  mylm_sgd_mb <- function(x, y, b0 = rep(0, ncol(x)), delta = 0.3, maxitr = 20)
  {
    if (!is.matrix(x)) stop("x must be a matrix")
    if (!is.vector(y)) stop("y must be a vector")
    if (nrow(x) != length(y)) stop("number of observations different")
    
    # initiate batches with 10 observations each
    batch = sample(rep(1:floor(nrow(x)/10), length.out = nrow(x)))
  
    # initialize beta values
    allb = matrix(b0, 1, length(b0))
    
    # iterative update
    for (k in 1:maxitr)
    {
      for (i in 1:max(batch)) # loop through batches
      {
        # update based on the gradient of a single subject
        b0 = b0 + t(x[batch==i, ]) %*% (y[batch==i] - x[batch==i, ] %*% b0) * 
          delta / sum(batch==i)
        
        # record the update
        allb = rbind(allb, as.vector(b0))
        
        # learning rate decay
        delta = delta * 1/k
      }
    }
    
    return(list("allb" = allb, "beta" = b0))
  }

  # fit the model 
  mybeta = mylm_sgd_mb(X, y, b0 = c(0, 1), maxitr = 3)

  contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
  points(mybeta$allb[,1], mybeta$allb[,2], type = "b", col = "red", pch = 19)
  points(b[1], b[2], pch = 19, col = "blue", cex = 1.5)
  box()
```

You may further play around with these tuning parameters to see how sensitive the optimization is to them. A stopping rule can be difficult to determine, hence in practice, early stop is also used. 

## Lagrangian Multiplier for Constrained Problems

Constrained optimization problems appear very frequently. Both Lasso and Ridge regressions can be viewed as constrained problems, while support vector machines (SVM) is another example, which will be introduced later on. Let's investigate this using a toy example. Suppose we have an optimization problem

$$\text{minimize} \quad f(x, y) = x^2 + y^2$$
$$\text{subj. to} \quad g(x, y) = xy - 4 = 0$$

```{r fig.width=6, fig.height=6, out.width = '45%', fig.align='center'}
  x <- seq(-5, 5, 0.05)
  y <- seq(-5, 5, 0.05)
  mygrid <- data.matrix(expand.grid(x, y))
  f <- matrix(mygrid[,1]^2 + mygrid[,2]^2, length(x), length(y))

  f2 <- matrix(mygrid[,1]*mygrid[,2], length(x), length(y))
  
  # plot the contour
  par(mar=c(2,2,2,2))
  contour(x, y, f, levels = c(0.2, 1, 2, 4, 8, 16))
  contour(x, y, f2, levels = 4, add = TRUE, col = "blue", lwd = 2)
  box()

  lines(seq(1, 3, 0.01), 4- seq(1, 3, 0.01), type = "l", col = "darkorange", lwd = 3)    
  points(2, 2, col = "red", pch = 19, cex = 2)
  points(-2, -2, col = "red", pch = 19, cex = 2)
```

The problem itself is very simple. We know that the optimizer is the red dot. But an interesting point of view is to look at the level curves of the objective function. As it is growing (expanding), there is one point (the red dot) at which level curve barely touches the constrain curve (blue line). This should be the optimizer. But this also implies that the tangent line (orange line) of this leveling curve must coincide with the tangent line of the constraint. Noticing that the tangent line can be obtained by taking the derivative of the function, this observation implies that gradients of the two functions (the objective function and the constraint function) must be a multiple of the other. Hence,  

$$ 
\begin{align}
& \bigtriangledown f = \lambda \bigtriangledown g \\
\\
\Longrightarrow \qquad &  \begin{cases}
    2x = \lambda y & \text{by taking derivative w.r.t.} \,\, x\\
    2y = \lambda x & \text{by taking derivative w.r.t.} \,\, y\\
    xy - 4 = 0 & \text{the constraint itself}
  \end{cases}
\end{align}
$$

The three equations put together is very easy to solve. We have $x = y = 0$ or $\lambda = \pm 2$ based on the first two equations. The first one is not feasible based on the constraint. The second solution leads to two feasible solutions: $x = y = 2$ or $x = y = -2$. Hence, we now know that there are two solutions. 

Now, looking back at the equation $\bigtriangledown f = \lambda \bigtriangledown g$, this is simply the derivative of the **Lagrangian function** defined as

$${\cal L}(x, y, \lambda) = f(x, y) - \lambda g(x, y),$$
while solving for the solution of the constrained problem becomes finding the stationary point of the Lagrangian. Be aware that in some cases, the solution you found can be maximizers instead of minimizers. Hence, its necessary to compare all of them and see which one is smaller. 

<!--chapter:end:01.4-optimization.Rmd-->

\def\cD{\cal{D}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\br{\mathbf{r}}
\def\be{\mathbf{e}}
\def\bzero{\mathbf{0}}
\def\bbeta{\boldsymbol \beta}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6), 
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
```

# (PART) Linear and Penalized Linear Models {-}

# Linear Regression and Model Selection

This chapter severs several purposes. First, we will review some basic knowledge of linear regression. This includes the concept of vector space, projection, which leads to estimating parameters of a linear regression. Most of these knowledge are covered in the prerequisite so you shouldn't find these concepts too difficult to understand. Secondly, we will mainly use the `lm()` function as an example to demonstrate some features of `R`. This includes extracting results, visualizations, handling categorical variables, prediction and model selection. These concepts will be useful for other models. Finally, we will introduce several model selection criteria and  algorithms to perform model selection. 

## Example: real estate data 

This [Real Estate data](https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set) [@yeh2018building] is provided on the [UCI machine learning repository](https://archive.ics.uci.edu/ml/index.php). The goal of this dataset is to predict the unit house price based on six different covariates: 

  * `date`: The transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)
  * `age`: The house age (unit: year)
  * `distance`: The distance to the nearest MRT station (unit: meter)
  * `stores`: The number of convenience stores in the living circle on foot (integer)
  * `latitude`: Latitude (unit: degree)
  * `longitude`: Longitude (unit: degree)
  * `price`: House price of unit area

```{r}
    realestate = read.csv("data/realestate.csv", row.names = 1)

    library(DT)
    datatable(realestate, filter = "top", rownames = FALSE,
              options = list(pageLength = 8))
    
    dim(realestate)
```

## Notation and Basic Properties

We usually denote the observed covariates data as the design matrix $\mathbf{X}$, with dimension $n \times p$. Hence in this case, the dimension of $\mathbf{X}$ is $414 \times 7$. The $j$th variable is simply the $j$th column of this matrix, which is denoted as $\mathbf{x}_j$. The outcome $\mathbf{y}$ (`price`) is a vector of length $414$. Please note that we usually use a "bold" symbol to represent a vector, while for a single element (scalar), such as the $j$th variable of subject $i$, we use $x_{ij}$.

A linear regression concerns modeling the relationship (in matrix form)

$$\by_{n \times 1} = \bX_{n \times p} \bbeta_{p \times 1} + \bepsilon_{n \times 1}$$
And we know that the solution is obtained by minimizing the residual sum of squares (RSS):

$$ 
\begin{align}
\widehat{\bbeta} &= \underset{\bbeta}{\argmin} \sum_{i=1}^n \left(y_i - x_i^\T \bbeta \right)^2 \\
&= \underset{\bbeta}{\argmin} \big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)^\T \big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)
\end{align}
$$
Classic solution can be obtained by taking the derivative of RSS w.r.t $\bbeta$ and set it to zero. This leads to the well known normal equation: 

$$
\begin{align}
    \frac{\partial \text{RSS}}{\partial \bbeta} &= -2 \bX^\T (\by - \bX \bbeta) \doteq 0 \\
    \Longrightarrow \quad \bX^\T \by &= \bX^\T \bX \bbeta
\end{align}
$$
Assuming that $\bX$ is full rank, then $\bX^\T \bX$ is invertible. Then, we have 

$$
\widehat{\bbeta} = (\bX^\T \bX)^{-1}\bX^\T \by
$$
Some additional concepts are frequently used. The fitted values $\widehat{\by}$ are essentially the prediction of the original $n$ training data points:

$$ 
\begin{align}
\widehat{\by} =& \bX \bbeta\\
=& \underbrace{\bX (\bX^\T \bX)^{-1}\bX^\T}_{\bH} \by \\
\doteq& \bH_{n \times n} \by 
\end{align}
$$
where $\bH$ is called the "hat" matrix. It is a projection matrix that projects any vector ($\by$ in our case) onto the column space of $\bX$. A project matrix enjoys two properties 

  * Symmetric: $\bH^\T = \bH$
  * Idempotent $\bH\bH = \bH$

The residuals $\br$ can also be obtained using the hat matrix:

$$ \br = \by - \widehat{\by} = (\bI - \bH) \by$$
From the properties of a projection matrix, we also know that $\br$ should be orthogonal to any vector from the column space of $\bX$. Hence, 

$$\bX^\T \br = \mathbf{0}_{p \times 1}$$

The residuals is also used to estimate the error variance:

$$\widehat\sigma^2 = \frac{1}{n-p} \sum_{i=1}^n r_i^2 = \frac{\text{RSS}}{n-p}$$
When the data are indeed generated from a linear model, and with suitable conditions on the design matrix and random errors $\bepsilon$, we can conclude that $\widehat{\bbeta}$ is an __unbiased__ estimator of $\bbeta$. Its variance-covariance matrix satisfies

$$
\begin{align}
    \Var(\widehat{\bbeta}) &= \Var\big( (\bX^\T \bX)^{-1}\bX^\T \by \big) \nonumber \\
    &= \Var\big( (\bX^\T \bX)^{-1}\bX^\T (\bX \bbeta + \bepsilon) \big) \nonumber \\
    &= \Var\big( (\bX^\T \bX)^{-1}\bX^\T \bepsilon) \big) \nonumber \\
    &= (\bX^\T \bX)^{-1}\bX^\T \bX (\bX^\T \bX)^{-1} \bI \sigma^2 \nonumber \\
    &= (\bX^\T \bX)^{-1}\sigma^2
\end{align}
$$
All of the above mentioned results are already implemented in R through the `lm()` function to fit a linear regression. 

## Using the `lm()` Function

Let's consider a simple regression that uses `age` and `distance` to model `price`. We will save the fitted object as `lm.fit`

```{r}
    lm.fit = lm(price ~ age + distance, data = realestate)
```

This syntax contains three components:

  * `data = ` specifies the dataset
  * The outcome variable should be on the left hand side of `~` 
  * The covariates should be on the right hand side of `~`
    
To look at the detailed model fitting results, use the `summary()` function 

```{r}
    lm.summary = summary(lm.fit)
    lm.summary
```

This shows that both `age` and `distance` are highly significant for predicting the price. In fact, this fitted object (`lm.fit`) and the summary object (`lm.summary`) are both saved as a list. This is pretty common to handle an output object with many things involved. We may peek into this object to see what are provided using a `$` after the object. 

<center>
![](images/reactive.png){width=40%}
</center>

The `str()` function can also display all the items in a list. 

```{r eval=FALSE}
    str(lm.summary)
```

Usually, printing out the summary is sufficient. However, further details can be useful for other purposes. For example, if we interested in the residual vs. fits plot, we may use 

```{r}
    plot(lm.fit$fitted.values, lm.fit$residuals, 
         xlab = "Fitted Values", ylab = "Residuals",
         col = "darkorange", pch = 19, cex = 0.5)
```

It seems that the error variance is not constant (as a function of the fitted values), hence additional techniques may be required to handle this issue. However, that is beyond the scope of this book. 

### Adding Covariates

It is pretty simple if we want to include additional variables. This is usually done by connecting them with the `+` sign on the right hand side of `~`. R also provide convenient ways to include interactions and higher order terms. The following code with the interaction term between `age` and `distance`, and a squared term of `distance` should be self-explanatory. 

```{r}
    lm.fit2 = lm(price ~ age + distance + age*distance + I(distance^2), data = realestate)
    summary(lm.fit2)
```

If you choose to include all covariates presented in the data, then simply use `.` on the right hand side of `~`. However, you should always be careful when doing this because some dataset would contain meaningless variables such as subject ID. 

```{r eval=FALSE}
    lm.fit3 = lm(price ~ ., data = realestate)
```

### Categorical Variables

The `store` variable has several different values. We can see that it has 11 different values. One strategy is to model this as a continuous variable. However, we may also consider to discretize it. For example, we may create a new variable, say `store.cat`, defined as follows

```{r}
  table(realestate$stores)

  # define a new factor variable
  realestate$store.cat = as.factor((realestate$stores > 0) + (realestate$stores > 4))
  table(realestate$store.cat)
  levels(realestate$store.cat) = c("None", "Several", "Many")
  head(realestate$store.cat)
```

This variable is defined as a factor, which is often used for categorical variables. Since this variable has three different categories, if we include it in the linear regression, it will introduce two additional variables (using the third as the reference):

```{r}
    lm.fit3 = lm(price ~ age + distance + store.cat, data = realestate)
    summary(lm.fit3)
```

There are usually two types of categorical variables:

  * Ordinal: the numbers representing each category is ordered, e.g., how many stores in the neighborhood. Oftentimes nominal data can be treated as a continuous variable.
  * Nominal: they are not ordered and can be represented using either numbers or letters, e.g., ethnic group. 
  
The above example is treating `store.cat` as a nominal variable, and the `lm()` function is using dummy variables for each category. This should be our default approach to handle nominal variables. 

## Model Selection Criteria

We will use the `diabetes` dataset from the `lars` package as a demonstration of model selection. Ten baseline variables include age, sex, body mass index, average blood pressure, and six blood serum measurements. These measurements were obtained for each of n = 442 diabetes patients, as well as the outcome of interest, a quantitative measure of disease progression one year after baseline. More details are available in @efron2004least. Our goal is to select a linear model, preferably with a small number of variables, that can predict the outcome. To select the best model, commonly used strategies include Marrow's $C_p$, AIC (Akaike information criterion) and BIC (Bayesian information criterion). Further derivations will be provide at a later section. 

```{r}
    # load the diabetes data
    library(lars)
    data(diabetes)
    diab = data.frame(cbind(diabetes$x, "Y" = diabetes$y))

    # fit linear regression with all covariates
    lm.fit = lm(Y~., data=diab)
```

The idea of model selection is to apply some penalty on the number of parameters used in the model. In general, they are usually in the form of 

$$\text{Goodness-of-Fit} + \text{Complexity Penality}$$

### Using Marrows' $C_p$

For example, the Marrows' $C_p$ criterion minimize the following quantity (a derivation is provided at Section \@ref(marrows-cp)): 

$$\text{RSS} + 2 p \widehat\sigma_{\text{full}}^2$$
Note that the $\sigma_{\text{full}}^2$ refers to the residual variance estimation based on the full model, i.e., will all variables. Hence, this formula cannot be used when $p > n$ because you would not be able to obtain a valid estimation of $\sigma_{\text{full}}^2$. Nonetheless, we can calculate this quantity with the diabetes dataset

```{r}
    # number of variables (including intercept)
    p = 11
    n = nrow(diab)
      
    # obtain residual sum of squares
    RSS = sum(residuals(lm.fit)^2)
    
    # use the formula directly to calculate the Cp criterion 
    Cp = RSS + 2*p*summary(lm.fit)$sigma^2
    Cp
```

We can compare this with another sub-model, say, with just `age` and `glu`:

```{r}
    lm.fit_sub = lm(Y~ age + glu, data=diab)
  
    # obtain residual sum of squares
    RSS_sub = sum(residuals(lm.fit_sub)^2)
    
    # use the formula directly to calculate the Cp criterion 
    Cp_sub = RSS_sub + 2*3*summary(lm.fit)$sigma^2
    Cp_sub
```

Comparing this with the previous one, the full model is better. 

### Using AIC and BIC

Calculating the AIC and BIC criteria in `R` is a lot simpler, with the existing functions. The AIC score is given by 

$$-2 \text{Log-likelihood} + 2 p,$$
while the BIC score is given by 

$$-2 \text{Log-likelihood} + \log(n) p,$$

Interestingly, when assuming that the error distribution is Gaussian, the log-likelihood part is just a function of the RSS. In general, AIC performs similarly to $C_p$, while BIC tend to select a much smaller set due to the larger penalty. Theoretically, both AIC and $C_p$ are interested in the prediction error, regardless of whether the model is specified correctly, while BIC is interested in selecting the true set of variables, while assuming that the true model is being considered. 

The AIC score can be done using the `AIC()` function. We can match this result by writing out the normal density function and plug in the estimated parameters. Note that this requires one additional parameter, which is the variance. Hence the total number of parameters is 12. We can calculate this with our own code:

```{r}
    # ?AIC
    # a build-in function for calculating AIC using -2log likelihood
    AIC(lm.fit) 

    # Match the result
    n*log(RSS/n) + n + n*log(2*pi) + 2 + 2*p
```

Alternatively, the `extractAIC()` function can calculate both AIC and BIC. However, note that the `n + n*log(2*pi) + 2` part in the above code does not change regardless of how many parameters we use. Hence, this quantify does not affect the comparison between different models. Then we can safely remove this part and only focus on the essential ones. 

```{r}
    # ?extractAIC
    # AIC for the full model
    extractAIC(lm.fit)
    n*log(RSS/n) + 2*p

    # BIC for the full model
    extractAIC(lm.fit, k = log(n))
    n*log(RSS/n) + log(n)*p
```

Now, we can compare AIC or BIC using of two different models and select whichever one that gives a smaller value. For example the AIC of the previous sub-model is 

```{r}
    # AIC for the sub-model
    extractAIC(lm.fit_sub)
```

## Model Selection Algorithms

In previous examples, we have to manually fit two models and calculate their respective selection criteria and compare them. This is a rather tedious process if we have many variables and a huge number of combinations to consider. To automatically compare different models and select the best one, there are two common computational approaches: best subset regression and step-wise regression. As their name suggest, the best subset selection will exhaust all possible combination of variables, while the step-wise regression would adjust the model by adding or subtracting one variable at a time to reach the best model. 

### Best Subset Selection with `leaps`

Since the penalty is only affected by the number of variables, we may first choose the best model with the smallest RSS for each model size, and then compare across these models by attaching the penalty terms of their corresponding sizes. The `leaps` package can be used to calculate the best model of each model size. It essentially performs an exhaustive search, however, still utilizing some tricks to skip some really bad models. Note that the `leaps` package uses the data matrix directly, instead of specifying a formula.  

```{r}
    library(leaps)
    
    # The package specifies the X matrix and outcome y vector
    RSSleaps = regsubsets(x = as.matrix(diab[, -11]), y = diab[, 11])
    summary(RSSleaps, matrix=T)
```

The results is summarized in a matrix, with each row representing a model size. The `"*"` sign indicates that the variable is include in the model for the corresponding size. Hence, there should be only one of such in the first row, two in the second row, etc. 
    
By default, the algorithm would only consider models up to size 8. This is controlled by the argument `nvmax`. If we want to consider larger model sizes, then set this to a larger number. However, be careful that this many drastically increase the computational cost. 

```{r}    
    # Consider maximum of 10 variables
    RSSleaps = regsubsets(x = as.matrix(diab[, -11]), y = diab[, 11], nvmax = 10)
    summary(RSSleaps,matrix=T)
    
    # Obtain the matrix that indicates the variables
    sumleaps = summary(RSSleaps, matrix = T)
    
    # This object includes the RSS results, which is needed to calculate the scores
    sumleaps$rss
    
    # This matrix indicates whether a variable is in the best model(s)
    sumleaps$which
    
    # The package automatically produces the Cp statistic
    sumleaps$cp
```

We can calculate different model selection criteria with the best models of each size. The model fitting result already produces the $C_p$ and BIC results. However, please note that both quantities are modified slightly. For the $C_p$ statistics, the quantity is divided by the estimated error variance, and also adjust for the sample size. For the BIC, the difference is a constant regardless of the model size. Hence these difference do will not affect the model selection result because the modification is the same regardless of the number of variables. 
```{r}
    modelsize=apply(sumleaps$which,1,sum)
    
    Cp = sumleaps$rss/(summary(lm.fit)$sigma^2) + 2*modelsize - n;
    AIC = n*log(sumleaps$rss/n) + 2*modelsize;
    BIC = n*log(sumleaps$rss/n) + modelsize*log(n);
    
    # Comparing the Cp scores 
    cbind("Our Cp" = Cp, "leaps Cp" = sumleaps$cp) 
    
    # Comparing the BIC results. The difference is a constant, 
    # which is the score of an intercept model
    cbind("Our BIC" = BIC, "leaps BIC" = sumleaps$bic, 
          "Difference" = BIC-sumleaps$bic, 
          "Intercept Score" = n*log(sum((diab[,11] - mean(diab[,11]))^2/n)))
```

Finally, we may select the best model, using any of the criteria. The following code would produced a plot to visualize it. We can see that BIC selects 6 variables, while both AIC and $C_p$ selects 7. 

```{r}
    # Rescale Cp, AIC and BIC to (0,1).
    inrange <- function(x) { (x - min(x)) / (max(x) - min(x)) }
    
    Cp = inrange(Cp)
    BIC = inrange(BIC)
    AIC = inrange(AIC)

    plot(range(modelsize), c(0, 0.4), type="n", 
         xlab="Model Size (with Intercept)", 
         ylab="Model Selection Criteria", cex.lab = 1.5)

    points(modelsize, Cp, col = "green4", type = "b", pch = 19)
    points(modelsize, AIC, col = "orange", type = "b", pch = 19)
    points(modelsize, BIC, col = "purple", type = "b", pch = 19)
    legend("topright", legend=c("Cp", "AIC", "BIC"),
           col=c("green4", "orange", "purple"), 
           lty = rep(1, 3), pch = 19, cex = 1.7)
```

### Step-wise regression using `step()`

The idea of step-wise regression is very simple: we start with a certain model (e.g. the intercept or the full mode), and add or subtract one variable at a time by making the best decision to improve the model selection score. The `step()` function implements this procedure. The following example starts with the full model and uses AIC as the selection criteria (default of the function). After removing several variables, the model ends up with six predictors.  

```{r}
    # k = 2 (AIC) is default; 
    step(lm.fit, direction="both", k = 2)
```

We can also use different settings, such as which model to start with, which is the minimum/maximum model, and do we allow to adding/subtracting. 

```{r}
    # use BIC (k = log(n))instead of AIC
    # trace = 0 will suppress the output of intermediate steps 
    step(lm.fit, direction="both", k = log(n), trace=0)

    # Start with an intercept model, and use forward selection (adding only)
    step(lm(Y~1, data=diab), scope=list(upper=lm.fit, lower=~1), 
         direction="forward", trace=0)
```

We can see that these results are slightly different from the best subset selection. So which is better? Of course the best subset selection is better because it considers all possible candidates, which step-wise regression may stuck at a sub-optimal model, while adding and subtracting any variable do not benefit further. Hence, the results of step-wise regression may be unstable. On the other hand, best subset selection not really feasible for high-dimensional problems because of the computational cost. 

## Derivation of Marrows' $C_p$ {#marrows-cp}

Suppose we have a set of training data $\cD_n = \{x_i, \color{DodgerBlue}{y_i}\}_{i=1}^n$ and a set of testing data, with the same covariates $\cD_n^\ast = \{x_i, \color{OrangeRed}{y_i^\ast}\}_{i=1}^n$. Hence, this is an __in-sample prediction__ problem. However, the $\color{OrangeRed}{y_i^\ast}$s are newly observed. Assuming that the data are generated from a linear model, i.e., in vector form,

\def\rby{\color{OrangeRed}{\by^\ast}}
\def\rbe{\color{OrangeRed}{\be^\ast}}
\def\rbbeta{\color{OrangeRed}{\widehat{\bbeta}}}

\def\bby{\color{DodgerBlue}{\by}}
\def\bbe{\color{DodgerBlue}{\be}}
\def\bbbeta{\color{DodgerBlue}{\widehat{\bbeta}}}


$$\bby = \bmu + \bbe = \bX \bbeta + \bbe,$$
and
$$\rby = \bmu + \rbe = \bX \bbeta + \rbe,$$
where the error terms are i.i.d with mean 0 and variance $\sigma^2$. We want to know what is the best model that predicts $\rby$. Let's look at the testing error first:

\begin{align}
\E[\color{OrangeRed}{\text{Testing Error}}] =& ~\E \lVert \rby - \bX \bbbeta \rVert^2 \\
=& ~\E \lVert (\rby - \bX \bbeta) + (\bX \bbeta - \bX \bbbeta) \rVert^2 \\
=& ~\E \lVert \rbe \rVert^2 + \E \lVert \bX (\bbbeta - \bbeta) \rVert^2 \\
=& ~\color{OrangeRed}{n \sigma^2} + \E \big[ \Trace \big( (\bbbeta - \bbeta)^\T \bX^\T \bX (\bbbeta - \bbeta) \big) \big] \\
=& ~\color{OrangeRed}{n \sigma^2} + \Trace\big(\bX^\T \bX \Cov(\bbbeta)\big) \\
=& ~\color{OrangeRed}{n \sigma^2} + \color{DodgerBlue}{p \sigma^2}.
\end{align}

In the above, we used properties

  * $\Trace(ABC) = \Trace(CBA)$
  * $\E[\Trace(A)] = \Trace(\E[A])$

On the other hand, the training error is 

\begin{align}
\E[\color{DodgerBlue}{\text{Training Error}}] =& ~\E \lVert \by - \bby \rVert^2 \\
=& ~\E \lVert (\bI - \bH)(\bX \bbeta + \bbe) \rVert^2 \\
=& ~\E \lVert (\bI - \bH)\bbe \rVert^2 \\
=& ~\E [\Trace(\bbe^\T(\bI - \bH)^\T (\bI - \bH) \bbe)]\\
=& ~\Trace((\bI - \bH)^\T (\bI - \bH) \Cov(\bbe)]\\
=& ~\color{DodgerBlue}{(n - p) \sigma^2}.
\end{align}

In the above, we further used properties 

  * $\bH$ and $\bI - \bH$ are projection matrices
  * $\bH \bX = \bX$

If we contrast the two results above, the difference between the training and testing errors is $2 p \sigma^2$. Hence, if we can obtain a valid estimation of $\sigma^2$, then the training error plus $2 p \widehat{\sigma}^2$ is a good approximation of the testing error, which we want to minimize. And that is exactly what Marrows' $C_p$ does. 

We can also generalize this result to the case when the underlying model is not a linear model. Assume that

$$\bby = f(\bX) + \bbe = \bmu + \bbe,$$
and
$$\rby = f(\bX) + \rbe = \bmu + \rbe.$$
In this case, a linear model would not estimate $\bmu$. Instead, it is only capable to produce the best linear approximation of $\bmu$ using the columns in $\bX$, which is $\bH\bmu$, the projection of $\bmu$ on the column space of $\bX$. In general, $\bH \bmu \neq \bmu$, and the remaining part $\bmu - \bH \bmu$ is called __bias__. This is a new concept that will appear frequently in this book. Selection variables will essentially trade between bias and variance of a model. The following derivation shows this phenomenon:

\begin{align}
\E[\color{OrangeRed}{\text{Testing Error}}] =& ~\E \lVert \rby - \bX \bbbeta \rVert^2 \\
=& ~\E \lVert \rby - \bH \bby \rVert^2 \\
=& ~\E \lVert (\rby - \bmu) + (\bmu - \bH \bmu) + (\bH \bmu - \bH \bby) \rVert^2 \\
=& ~\E \lVert \rby - \bmu \rVert^2 + \E \lVert \bmu - \bH \bmu \rVert^2 + \E \lVert \bH \bmu - \bH \bby  \rVert^2 \\
=& ~\E \lVert \rbe \rVert^2 + \E \lVert \bmu - \bH \bmu \rVert^2 + \E \lVert \bH \bbe  \rVert^2 \\
=& ~\color{OrangeRed}{n \sigma^2} + \text{Bias}^2 + \color{DodgerBlue}{p \sigma^2},
\end{align}

while the training error is 

\begin{align}
\E[\color{DodgerBlue}{\text{Training Error}}] =& ~\E \lVert \bby - \bX \bbbeta \rVert^2 \\
=& ~\E \lVert \bby - \bH \bby \rVert^2 \\
=& ~\E \lVert (\bI - \bH)(\bmu + \bbe) \rVert^2 \\
=& ~\E \lVert (\bI - \bH)\bmu \rVert^2 + \E \lVert (\bI - \bH)\bbe \rVert^2\\
=& ~\text{Bias}^2 + \color{DodgerBlue}{(n - p) \sigma^2}.
\end{align}

We can notice again that the difference is $2p\sigma^2$. Note that this is regardless of whether the linear model is correct or not. 


<!--chapter:end:02.1-linear.Rmd-->

\def\cD{\cal{D}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bzero{\mathbf{0}}
\def\bbeta{\boldsymbol \beta}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
```

# Ridge Regression

Ridge regression was proposed by @hoerl1970ridge, but is also a special case of Tikhonov regularization. The essential idea is very simple: Knowing that the ordinary least squares (OLS) solution is not unique in an ill-posed problem, i.e., $\bX^\T \bX$ is not invertible, a ridge regression adds a ridge (diagonal matrix) on $\bX^\T \bX$:

$$\widehat{\bbeta}^\text{ridge} = (\bX^\T \bX + n \lambda \bI)^{-1} \bX^\T \by,$$
It provides a solution of linear regression when multicollinearity happens, especially when the number of variables is larger than the sample size. Alternatively, this is also the solution of a regularized least square estimator. We add an $\ell_2$ penalty to the residual sum of squares, i.e., 

$$
\begin{align}
\widehat{\bbeta}^\text{ridge} =& \argmin_{\bbeta} (\by - \bX \bbeta)^\T (\by - \bX \bbeta) + n \lambda \lVert\bbeta\rVert_2^2\\
=& \argmin_{\bbeta} \frac{1}{n} \sum_{i=1}^n (y_i - x_i^\T \bbeta)^2 + \lambda \sum_{j=1}^p \beta_j^2,
\end{align}
$$

for some penalty $\lambda > 0$. Another approach that leads to the ridge regression is a constraint on the $\ell_2$  norm of the parameters, which will be introduced in the next Chapter. Ridge regression is used extensively in genetic analyses to address "small-$n$-large-$p$" problems. We will start with a motivation example and then discuss the bias-variance trade-off issue. 

## Motivation: Correlated Variables and Convexity

Ridge regression has many advantages. Most notably, it can address highly correlated variables. From an optimization point of view, having highly correlated variables means that the objective function ($\ell_2$ loss) becomes "flat" along certain directions in the parameter domain. This can be seen from the following example, where the true parameters are both $1$ while the estimated parameters concludes almost all effects to the first variable. You can change different seed to observe the variability of these parameter estimates and notice that they are quite large. Instead, if we fit a ridge regression, the parameter estimates are relatively stable.  

```{r}
  library(MASS)
  set.seed(2)
  n = 30
  
  # create highly correlated variables and a linear model
  X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
  y = rnorm(n, mean = X[,1] + X[,2])
  
  # compare parameter estimates
  summary(lm(y~X-1))$coef
  
  # note that the true parameters are all 1's
  # Be careful that the `lambda` parameter in lm.ridge is our (n*lambda)
  lm.ridge(y~X-1, lambda=5)
```

The variance of both $\beta_1$ and $\beta_2$ are quite large. This is expected because we know from linear regression that the variance of $\widehat{\bbeta}$ is $\sigma^2 (\bX^\T \bX)^{-1}$. However, since the columns of $\bX$ are highly correlated, the smallest eigenvalue of $\bX^\T \bX$ is close to 0, making the largest eigenvalue of $(\bX^\T \bX)^{-1}$ very large. This can also be interpreted through an optimization point of view. The objective function for an OLS estimator is demonstrated in the following.

```{r}
  beta1 <- seq(0, 3, 0.005)
  beta2 <- seq(-1, 2, 0.005)
  allbeta <- data.matrix(expand.grid(beta1, beta2))
  rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y), 
                length(beta1), length(beta2))
  
  # quantile levels for drawing contour
  quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)
  
  # plot the contour
  contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
  box()
  
  # the truth
  points(1, 1, pch = 19, col = "red", cex = 2)
  
  # the data 
  betahat <- coef(lm(y~X-1))
  points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 2)
```

Over many simulation runs, the solution lies around the line of $\beta_1 + \beta_2 = 2$.

```{r}
  # the truth
  plot(NA, NA, xlim = c(-1, 3), ylim = c(-1, 3))
  points(1, 1, pch = 19, col = "red", cex = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    betahat <- solve(t(X) %*% X) %*% t(X) %*% y
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
  }
```

## Ridge Penalty and the Reduced Variation

If we add a ridge regression penalty, the contour is forced to be more convex due to the added eigenvalues on $\bX^\bX$, making the eignvalues of $(\bX^\bX)^{-1}$ smaller. Here is a plot of the Ridge $\ell_2$ penalty.

```{r echo = FALSE}
  pen <- matrix(apply(allbeta, 1, function(b) 3*b %*% b),
                length(beta1), length(beta2))
  
  contour(beta1, beta2, pen, levels = quantile(pen, quanlvl))
  points(1, 1, pch = 19, col = "red", cex = 2)
  box()
```

Hence, by adding this to the OLS objective function, the solution is more stable. This may be interpreted in several different ways such as: 1) the objective function is more convex; 2) the variance of the estimator is smaller. However, this causes some bias too. Choosing the tuning parameter is a balance of the bias-variance trade-off, which will be discussed in the following. 

```{r fig.dim = c(12, 6), out.width = '90%'}
    par(mfrow=c(1, 2))

    # adding a L2 penalty to the objective function
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + b %*% b, X, y),
                  length(beta1), length(beta2))
    
    # the ridge solution
    bh = solve(t(X) %*% X + diag(2)) %*% t(X) %*% y
    
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
    points(1, 1, pch = 19, col = "red", cex = 2)
    points(bh[1], bh[2], pch = 19, col = "blue", cex = 2)
    box()
    
    # adding a larger penalty
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + 10*b %*% b, X, y),
                  length(beta1), length(beta2))
    
    bh = solve(t(X) %*% X + 10*diag(2)) %*% t(X) %*% y
    
    # the ridge solution
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl))
    points(1, 1, pch = 19, col = "red", cex = 2)
    points(bh[1], bh[2], pch = 19, col = "blue", cex = 2)
    box()
```

We can check the ridge solution over many simulation runs 

```{r fig.dim=c(6, 6), out.width = '45%'}
  par(mfrow=c(1, 1))

  # the truth
  plot(NA, NA, xlim = c(-1, 3), ylim = c(-1, 3))
  points(1, 1, pch = 19, col = "red", cex = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    # betahat <- solve(t(X) %*% X + 2*diag(2)) %*% t(X) %*% y
    betahat <- lm.ridge(y ~ X - 1, lambda = 2)$coef
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
  }
```

This effect is gradually changing as we increase the penalty level. The following simulation shows how the variation of $\bbeta$ changes. We show this with two penalty values, and see how the estimated parameters are away from the truth. 

```{r out.width = "80%", fig.dim=c(12, 6)}
  par(mfrow = c(1, 2))

  # small penalty
  plot(NA, NA, xlim = c(-1, 3), ylim = c(-1, 3))
  points(1, 1, pch = 19, col = "red", cex = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    betahat <- lm.ridge(y ~ X - 1, lambda = 2)$coef
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
  }
  
  # large penalty
  plot(NA, NA, xlim = c(-1, 3), ylim = c(-1, 3))
  points(1, 1, pch = 19, col = "red", cex = 2) 
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    # betahat <- solve(t(X) %*% X + 30*diag(2)) %*% t(X) %*% y
    betahat <- lm.ridge(y ~ X - 1, lambda = 30)$coef
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
  }
```


## Bias and Variance of Ridge Regression

We can set a relationship between Ridge and OLS, assuming that the OLS estimator exist. 

\begin{align}
\widehat{\bbeta}^\text{ridge} =& (\bX^\T \bX + n\lambda \bI)^{-1} \bX^\T \by \\
=& (\bX^\T \bX + n\lambda \bI)^{-1} (\bX^\T \bX) \color{OrangeRed}{(\bX^\T \bX)^{-1} \bX^\T \by}\\
=& (\bX^\T \bX + n\lambda \bI)^{-1} (\bX^\T \bX) \color{OrangeRed}{\widehat{\bbeta}^\text{ols}}
\end{align}

This leads to a biased estimator (since the OLS estimator is unbiased) if we use any nonzero $\lambda$. 

  * As $\lambda \rightarrow 0$, the ridge solution is eventually the same as OLS
  * As $\lambda \rightarrow \infty$, $\widehat{\bbeta}^\text{ridge} \rightarrow 0$

It can be easier to analyze a case with $\bX^\T \bX = n \bI$, i.e, with standardized and orthogonal columns in $\bX$. Note that in this case, each $\beta_j^{\text{ols}}$ is just the projection of $\by$ onto $\bx_j$, the $j$th column of the design matrix. We also have 

\begin{align}
\widehat{\bbeta}^\text{ridge} =& (\bX^\T \bX + n\lambda \bI)^{-1} (\bX^\T \bX) \widehat{\bbeta}^\text{ols}\\
=& (\bI + \lambda \bI)^{-1}\widehat{\bbeta}^\text{ols}\\
=& (1 + \lambda)^{-1} \widehat{\bbeta}^\text{ols}\\

\Longrightarrow \beta_j^{\text{ridge}} =& \frac{1}{1 + \lambda} \beta_j^\text{ols}
\end{align}

Then in this case, the bias and variance of the ridge estimator can be explicitly expressed: 

  * $\text{Bias}(\beta_j^{\text{ridge}}) = \frac{-\lambda}{1 + \lambda} \beta_j^\text{ols}$ (not zero)
  * $\text{Var}(\beta_j^{\text{ridge}}) = \frac{1}{(1 + \lambda)^2} \text{Var}(\beta_j^\text{ols})$ (reduced from OLS)

Of course, we can ask the question: is it worth it? We could proceed with a simple analysis of the MSE of $\beta$ (dropping $j$):

\begin{align}
\text{MSE}(\beta) &= \E(\widehat{\beta} - \beta)^2 \\
&= \E[\widehat{\beta} - \E(\widehat{\beta})]^2 + \E[\widehat{\beta} - \beta]^2 \\
&= \E[\widehat{\beta} - \E(\widehat{\beta})]^2 + 0 + [\E(\widehat{\beta}) - \beta]^2 \\
&= \Var(\widehat{\beta}) + \text{Bias}^2.
\end{align}

This bias-variance breakdown formula will appear multiple times. Now, plug-in the results developed earlier based on the orthogonal design matrix, and investigate the derivative of the MSE of the Ridge estimator, we have 

\begin{align}
\frac{\partial \text{MSE}(\widehat{\beta}^\text{ridge})}{ \partial \lambda} =& \frac{\partial}{\partial \lambda} \left[ \frac{1}{(1+\lambda)^2} \Var(\widehat{\beta}^\text{ols}) + \frac{\lambda^2}{(1 + \lambda)^2} \beta^2 \right] \\
=& \frac{2}{(1+\lambda)^3} \left[ \lambda \beta^2 - \Var(\widehat{\beta}^\text{ols}) \right]
\end{align}

Note that when the derivative is negative, increasing $\lambda$ would decrease the MSE. This implies that we can reduce the MSE by choosing a small $\lambda$. Of course the situation is much more involving when the columns in $\bX$ are not orthogonal. However, the following analysis helps to understand a non-orthogonal case. It is essentially re-organizing the columns of $\bX$ into its principle components so that they are still orthogonal. 

Let's first take a singular value decomposition (SVD) of $\bX$, with $\bX = \bU \bD \bV^\T$, then the columns in $\bU$ form an orthonormal basis and columns in $\bU \bD$ are the __principal components__ and $\bV$ defines the principle directions. In addition, we have $n \widehat{\boldsymbol \Sigma} = \bX^\T \bX = \bV \bD^2 \bV^\T$. Assuming that $p < n$, and $\bX$ has full column ranks, then the Ridge estimator fitted $\by$ value can be decomposed as

\begin{align}
\widehat{\by}^\text{ridge} =& \bX \widehat{\beta}^\text{ridge} \\ 
=& \bX (\bX^\T \bX + n \lambda)^{-1} \bX^\T \by \\
=& \bU \bD \bV^\T ( \bV \bD^2 \bV^\T + n \lambda \bV \bV^\T)^{-1} \bV \bD \bU^\T \by \\
=& \bU \bD^2 (n \lambda + \bD^2)^{-1} \bU^\T \by \\
=& \sum_{j = 1}^p \bu_j \left( \frac{d_j^2}{n \lambda + d_j^2} \bu_j^\T \by \right),
\end{align}

where $d_j$ is the $j$th eigenvalue of the PCA. Hence, the Ridge regression fitted value can be understood as 

  * Perform PCA of $\bX$
  * Project $\by$ onto the PCs
  * Shrink the projection $\bu_j^\T \by$ by the factor $d_j^2 / (n \lambda + d_j^2)$
  * Reassemble the PCs using all the shrunken length

Hence, the bias-variance notion can be understood as the trade-off on these derived directions $\bu_j$ and their corresponding parameters $\bu_j^\T \by$.

## Degrees of Freedom

We know that for a linear model, the degrees of freedom (DF) is simply the number of parameters used. There is a formal definition, using 

\begin{align}
\text{DF}(\widehat{f}) =& \frac{1}{\sigma^2} \Cov(\widehat{y}_i, y_i)\\
=& \frac{1}{\sigma^2} \Trace[\Cov(\widehat{\by}, \by)]
\end{align}

We can check that for a linear regression (assuming the intercept is already included in $\bX$), the DF is 

\begin{align}
\frac{1}{\sigma^2} \Trace[\Cov(\widehat{\by}^\text{ols}, \by)] =& \frac{1}{\sigma^2} \Trace[\Cov(\bX(\bX^\T\bX)^{-1} \bX^\T \by, \by)] \\
=& \Trace(\bX(\bX^\T\bX)^{-1} \bX^\T) \\
=& \Trace(\bI_{p\times p})\\
=& p
\end{align}

For the Ridge regression, we can perform the same analysis on ridge regression. 

\begin{align}
\frac{1}{\sigma^2} \Trace[\Cov(\widehat{\by}^\text{ridge}, \by)] =& \frac{1}{\sigma^2} \Trace[\Cov(\bX(\bX^\T\bX + n \lambda \bI)^{-1} \bX^\T \by, \by)] \\
=& \Trace(\bX(\bX^\T\bX)^{-1} \bX^\T) \\
=& \Trace(\bU \bD \bV^\T ( \bV \bD^2 \bV^\T + n \lambda \bV \bV^\T)^{-1} \bV \bD \bU^\T)\\
=& \sum_{j = 1}^p \frac{d_j^2}{d_j^2 + n\lambda}
\end{align}

Note that this is smaller than $p$ as long as $\lambda \neq 0$. This implies that the Ridge regression does not use the full potential of all $p$ variables, since there is a risk of over-fitting. 

## Using the `lm.ridge()` function

We have seen how the `lm.ridge()` can be used to fit a Ridge regression. However, keep in mind that the `lambda` parameter used in the function actually specifies the $n \lambda$ entirely we used in our notation. However, regardless, our goal is mainly to tune this parameter to achieve a good balance of bias-variance trade off. However, the difficulty here is to evaluate the performance without knowing the truth. Let's first use a simulated example, in which we do know the truth and then introduce the cross-validation approach for real data where we do not know the truth. 

We use the prostate cancer data `prostate` from the `ElemStatLearn` package. The dataset contains 8 explanatory variables and one outcome `lpsa`, the log prostate-specific antigen value. 

```{r}
  # ElemStatLearn is currently archived, install a previous version
  # library(devtools)
  # install_version("ElemStatLearn", version = "2015.6.26", repos = "http://cran.r-project.org")
  library(ElemStatLearn)
  head(prostate)
```

### Scaling Issue

We can use `lm.ridge()` with a fixed $\lambda$ value, as we have shown in the previous example. Its syntax is again similar to the `lm()` function, with an additional argument `lambda`. We can also compare that with our own code. 

```{r}
  # lm.ridge function from the MASS package
  lm.ridge(lpsa ~., data = prostate[, 1:9], lambda = 1)

  # using our own code
  X = cbind(1, data.matrix(prostate[, 1:8]))
  y = prostate[, 9]
  solve(t(X) %*% X + diag(9)) %*% t(X) %*% y
```
However, they look different. This is because ridge regression has a scaling issue: it would shrink parameters differently if the corresponding covariates have different scales. This can be seen from our previous development of the SVD analysis. Since the shrinkage is the same for all $d_j$s, it would apply a larger shrinkage for small $d_j$. A commonly used approach to deal with the scaling issue is to __standardize  all covariates__ such that they are treated the same way. In addition, we will also __center both $\bX$ and $\by$__ before performing the ridge regression. An interesting consequence of centering is that we do not need the intercept anymore, since $\bX \bbeta = \mathbf{0}$ for all $\bbeta$. One last point is that when performing scaling, `lm.ridge()` use the $n$ factor instead of $n-1$ when calculating the standard deviation. Hence, incorporating all these, we have 

```{r}
  # perform centering and scaling
  X = scale(data.matrix(prostate[, 1:8]), center = TRUE, scale = TRUE)

  # use n instead of (n-1) for standardization
  n = nrow(X)
  X = X * sqrt(n / (n-1))
  
  # center y but not scaling
  y = scale(prostate[, 9], center = TRUE, scale = FALSE)
  
  # getting the estimated parameter
  mybeta = solve(t(X) %*% X + diag(8)) %*% t(X) %*% y
  ridge.fit = lm.ridge(lpsa ~., data = prostate[, 1:9], lambda = 1)
  
  # note that $coef obtains the coefficients internally from lm.ridge
  # however coef() would transform these back to the original scale version
  cbind(mybeta, ridge.fit$coef)
```

### Multiple $\lambda$ values

Since we now face the problem of bias-variance trade-off, we can fit the model with multiple $\lambda$ values and select the best. This can be done using the following code. 

```{r}
  library(MASS)
  fit = lm.ridge(lpsa~.,  data = prostate[, -10], lambda = seq(0, 100, by=0.2))
```

For each $\lambda$, the coefficients of all variables are recorded. The plot shows how these coefficients change as a function of $\lambda$. We can easily see that as $\lambda$ becomes larger, the coefficients are shrunken towards 0. This is consistent with our understanding of the bias. On the very left hand size of the plot, the value of each parameter corresponds to the OLS result since no penalty is applied. Be careful that the coefficients of the fitted objects `fit$coef` are scaled by the standard deviation of the covariates. If you need the original scale, make sure to use `coef(fit)`.

```{r echo = FALSE}
  par(mfrow=c(1,1))
```

```{r fig.dim=c(6, 6), out.width = '45%'}
    matplot(coef(fit)[, -1], type = "l", xlab = "Lambda", ylab = "Coefficients")
    text(rep(50, 8), coef(fit)[1,-1], colnames(prostate)[1:8])
    title("Prostate Cancer Data: Ridge Coefficients")
```

To select the best $\lambda$ value, there can be several different methods. We will discuss two approaches among them: $k$-fold cross-validation and generalized cross-validation (GCV).

## Cross-validation

Cross-validation (CV) is a technique to evaluate the performance of a model on an independent set of data. The essential idea is to separate out a subset of the data and do not use that part during the training, while using it for testing. We can then rotate to or sample a different subset as the testing data. Different cross-validation methods differs on the mechanisms of generating such testing data. __$K$-fold cross-validation__ is probably the the most popular among them. The method works in the following steps:

  1. Randomly split the data into $K$ equal portions 
  2. For each $k$ in $1, \ldots, K$: use the $k$th portion as the testing data and the rest as training data, obtain the testing error
  3. Average all $K$ testing errors

Here is a graphical demonstration of a $10$-fold CV:

<center>
![](images/kfoldcv.png){width=80%}
</center>

There are also many other cross-validation procedures, for example, the __Monte Carlo cross-validation__ randomly splits the data into training and testing (instead of fix $K$ portions) each time and repeat the process as many times as we like. The benefit of such procedure is that if this is repeated enough times, the estimated testing error becomes fairly stable, and not affected much by the random mechanism. On the other hand, we can also repeat the entire $K$-fold CV process many times, then average the errors. This is also trying to reduced the influence of randomness. 

## Leave-one-out cross-validation

Regarding the randomness, the leave-one-out cross-validation is completely nonrandom. It is essentially the $k$-fold CV approach, but with $k$ equal to $n$, the sample size. A standard approach would require to re-fit the model $n$ times, however, some linear algebra can show that there is an equivalent form using the "Hat" matrix when fitting a linear regression:

\begin{align}
\text{CV}(n) =& \frac{1}{n}\sum_{i=1}^n (y_i - \widehat{y}_{[i]})^2\\
=& \frac{1}{n} \sum_{i=1}^n \left( \frac{y_i - \widehat{y}_i}{1 - \bH_{ii}} \right)^2,
\end{align}
where $\widehat{y}_{i}$ is the fitted value using the whole dataset, but $\widehat{y}_{[i]}$ is the prediction of $i$th observation using the data without it when fitting the model. And $\bH_{ii}$ is the $i$th diagonal element of the hat matrix $\bH = \bX(\bX^\T \bX)^{-1} \bX$. The proof is essentially an application of the [Sherman–Morrison–Woodbury (SMW)](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula) formula, which is also used when deriving the rank-one update of a quasi-Newton optimization approach. 

```{proof}
Denote $\bX_{[i]}$ and $\by_{[i]}$ the data derived from $\bX$ and $\by$, but with the $i$ observation ($x_i$, $y_i$) removed. We then have the properties that

$$\bX_{[i]}^\T \bX_{[i]} = \bX^\T \bX - x_i x_i^\T, $$

and 

$$\bH_{ii} = x_i^\T (\bX^\T \bX)^{-1} x_i.$$

By the SMW formula, we have 

$$(\bX_{[i]}^\T \bX_{[i]})^{-1} = (\bX^\T \bX)^{-1} + \frac{(\bX^\T \bX)^{-1}x_i x_i^\T (\bX^\T \bX)^{-1}}{ 1 - \bH_{ii}}, $$

Further notice that 

$$\bX_{[i]}^\T \by_{[i]} = \bX^\T \by - x_i y_i, $$

we can then reconstruct the fitted parameter when observation $i$ is removed:
  
\begin{align}
\widehat{\bbeta}_{[i]} =& (\bX_{[i]}^\T \bX_{[i]})^{-1} \bX_{[i]}^\T \by_{[i]} \\
=& \left[ (\bX^\T \bX)^{-1} + \frac{(\bX^\T \bX)^{-1}x_i x_i^\T (\bX^\T \bX)^{-1}}{ 1 - \bH_{ii}} \right] (\bX^\T \by - x_i y_i)\\
=& (\bX^\T \bX)^{-1} \bX^\T \by + \left[ - (\bX^\T \bX)^{-1} x_i y_i +  \frac{(\bX^\T \bX)^{-1}x_i x_i^\T (\bX^\T \bX)^{-1}}{ 1 - \bH_{ii}} (\bX^\T \by - x_i y_i) \right] \\
=& \widehat{\bbeta} - \frac{(\bX^\T \bX)^{-1} x_i}{1 - \bH_{ii}} \left[ y_i (1 - \bH_{ii}) - x_i^\T \widehat{\bbeta} + \bH_{ii} y_i \right]\\
=& \widehat{\bbeta} - \frac{(\bX^\T \bX)^{-1} x_i}{1 - \bH_{ii}} \left( y_i - x_i^\T \widehat{\bbeta} \right)
\end{align}

Then the error of the $i$th obervation from the leave-one-out model is 

\begin{align}
y _i - \widehat{y}_{[i]} =& y _i - x_i^\T \widehat{\bbeta}_{[i]} \\
=& y _i - x_i^\T \left[ \widehat{\bbeta} - \frac{(\bX^\T \bX)^{-1} x_i}{1 - \bH_{ii}} \left( y_i - x_i^\T \widehat{\bbeta} \right)  \right]\\
=& y _i - x_i^\T \widehat{\bbeta} + \frac{x_i^\T (\bX^\T \bX)^{-1} x_i}{1 - \bH_{ii}} \left( y_i - x_i^\T \widehat{\bbeta} \right)\\
=& y _i - x_i^\T \widehat{\bbeta} + \frac{\bH_{ii}}{1 - \bH_{ii}} \left( y_i - x_i^\T \widehat{\bbeta} \right)\\
=& \frac{y_i - x_i^\T \widehat{\bbeta}}{1 - \bH_{ii}}
\end{align}

This completes the proof. 
```

### Generalized cross-validation 
 
The generalized cross-validation (GCV, @golub1979generalized) is a modified version of the leave-one-out CV:

$$\text{GCV}(\lambda) = \frac{\sum_{i=1}^n (y_i - x_i^\T \widehat{\bbeta}^\text{ridge}_\lambda)}{(n - \Trace(\bS_\lambda))}$$
where $\bS_\lambda$ is the hat matrix corresponding to the ridge regression:

$$\bS_\lambda = \bX (\bX^\T \bX + \lambda \bI)^{-1} \bX^\T$$

The following plot shows how GCV value changes as a function of $\lambda$. 

```{r}
    # use GCV to select the best lambda
    plot(fit$lambda[1:500], fit$GCV[1:500], type = "l", col = "darkorange", 
         ylab = "GCV", xlab = "Lambda", lwd = 3)
    title("Prostate Cancer Data: GCV")
```

We can select the best $\lambda$ that produces the smallest GCV. 

```{r}
    fit$lambda[which.min(fit$GCV)]
    round(coef(fit)[which.min(fit$GCV), ], 4)
```

## The `glmnet` package

The `glmnet` package implements the $k$-fold cross-validation. To perform a ridge regression with cross-validation, we need to use the `cv.glmnet()` function with $alpha = 0$. Here, the $\alpha$ is a parameter that controls the $\ell_2$ and $\ell_1$ (Lasso) penalties. In addition, the lambda values are also automatically selected, on the log-scale. 

```{r}
  library(glmnet)
  set.seed(3)
  fit2 = cv.glmnet(data.matrix(prostate[, 1:8]), prostate$lpsa, nfolds = 10, alpha = 0)
  plot(fit2$glmnet.fit, "lambda")
```

It is useful to plot the cross-validation error against the $\lambda$ values , then select the corresponding $\lambda$ with the smallest error. The corresponding coefficient values can be obtained using the `s = "lambda.min"` option in the `coef()` function. However, this can still be subject to over-fitting, and sometimes practitioners use `s = "lambda.1se"` to select a slightly heavier penalized version based on the variations observed from different folds. 

```{r}
  plot(fit2)
  coef(fit2, s = "lambda.min")
  coef(fit2, s = "lambda.1se")
```

### Scaling Issue

The `glmnet` package would using the same strategies for scaling: center and standardize $\bX$ and center $\by$. A slight difference is that it considers using $1/(2n)$ as the normalizing factor of the residual sum of squares, but also uses $\lambda/2 \lVert \bbeta\rVert_2^2$ as the penalty. This does not affect our formulation since the $1/2$ cancels out. However, it would slightly affect the Lasso formulation introduced in the next Chapter since the $\ell_1$ penalty does not apply this $1/2$ factor. Nonetheless, we can check the (nearly) equivalence between `lm.ridge` and `glmnet()`:

```{r fig.dim = c(12, 6), out.width = "70%"}
  n = 100
  p = 5

  X <- as.matrix(scale(matrix(rnorm(n*p), n, p)))
  y <- as.matrix(scale(X[, 1] + X[,2]*0.5 + rnorm(n, sd = 0.5)))
 
  lam = 10^seq(-1, 3, 0.1)
 
  fit1 <- lm.ridge(y ~ X, lambda = lam)
  fit2 <- glmnet(X, y, alpha = 0, lambda = lam / nrow(X))
 
  # the estimated parameters
  par(mfrow=c(1, 2))
  matplot(apply(coef(fit1), 2, rev), type = "l", main = "lm.ridge")
  matplot(t(as.matrix(coef(fit2))), type = "l", main = "glmnet")
  
  # Check differences
  max(abs(apply(coef(fit1), 2, rev) - t(as.matrix(coef(fit2)))))
```




<!--chapter:end:02.2-ridge.Rmd-->

\def\cD{\cal{D}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bzero{\mathbf{0}}
\def\bbeta{\boldsymbol \beta}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
```

# Lasso

Lasso [@tibshirani1996regression] is among the most popular machine learning models. Different from the Ridge regression, its adds $\ell_1$ penalty on the fitted parameters:

\begin{align}
\widehat{\bbeta}^\text{lasso} =& \argmin_{\bbeta} (\by - \bX \bbeta)^\T (\by - \bX \bbeta) + n \lambda \lVert\bbeta\rVert_1\\
=& \argmin_{\bbeta} \frac{1}{n} \sum_{i=1}^n (y_i - x_i^\T \bbeta)^2 + \lambda \sum_{i=1}^p |\beta_j|,
\end{align}

The main advantage of adding such a penalty is that small $\widehat{\beta}_j$ values can be __shrunk to zero__. This may prevents over-fitting and also improve the interpretability especially when the number of variables is large. We will analyze the Lasso starting with a single variable case, and then discuss the application of coordinate descent algorithm to obtain the solution. 

## One-Variable Lasso and Shrinkage

To illustrate how Lasso shrink a parameter estimate to zero, let's consider an orthogonal design matrix case, i.e., $\bX^\T \bX = n \bI$, which will eventually reduce to a one-variable problem. Note that the intercept term is not essential because we can always pre-center the observed data $x_i$ and $y_i$s so that they can be recovered after this one variable problem. Our objective function is 

$$\frac{1}{n}\lVert \by - \bX \bbeta \rVert^2 + \lambda \lVert\bbeta\rVert_1$$
We are going to relate the solution the OLS solution, which exists in this case because $\bX^\T \bX$ is invertible. Hence, we have 

\begin{align}
&\frac{1}{n}\lVert \by - \bX \bbeta \rVert^2 + \lambda \lVert\bbeta\rVert_1\\
=&\frac{1}{n}\lVert \by - \color{OrangeRed}{\bX \widehat{\bbeta}^\text{ols} + \bX \widehat{\bbeta}^\text{ols}} - \bX \bbeta \rVert^2 + \lambda \lVert\bbeta\rVert_1\\
=&\frac{1}{n}\lVert \by - \bX \widehat{\bbeta}^\text{ols} \rVert^2 + \frac{1}{n} \lVert \bX \widehat{\bbeta}^\text{ols} - \bX \bbeta \rVert^2 + \lambda \lVert\bbeta\rVert_1
\end{align}

The cross-term is zero because the OLS residual term is orthogonal to the columns of $\bX$:

\begin{align}
&2(\by - \bX \widehat{\bbeta}^\text{ols})^\T (\bX \widehat{\bbeta}^\text{ols} - \bX \bbeta )\\
=& 2\br^\T \bX (\widehat{\bbeta}^\text{ols} - \bbeta )\\
=& 0
\end{align}

Then we just need to optimize the part that involves $\bbeta$:

\begin{align}
&\underset{\bbeta}{\argmin} \frac{1}{n}\lVert \by - \bX \widehat{\bbeta}^\text{ols} \rVert^2 + \frac{1}{n} \lVert \bX \widehat{\bbeta}^\text{ols} - \bX \bbeta \rVert^2 + \lambda \lVert\bbeta\rVert_1\\
=&\underset{\bbeta}{\argmin} \frac{1}{n} \lVert \bX \widehat{\bbeta}^\text{ols} - \bX \bbeta \rVert^2 + \lambda \lVert\bbeta\rVert_1\\
=&\underset{\bbeta}{\argmin} \frac{1}{n} (\widehat{\bbeta}^\text{ols} - \bbeta )^\T \bX^\T \bX (\widehat{\bbeta}^\text{ols} - \bbeta )  + \lambda \lVert\bbeta\rVert_1\\
=&\underset{\bbeta}{\argmin} \frac{1}{n} (\widehat{\bbeta}^\text{ols} - \bbeta )^\T n \bI (\widehat{\bbeta}^\text{ols} - \bbeta )  + \lambda \lVert\bbeta\rVert_1\\
=&\underset{\bbeta}{\argmin} \sum_{j = 1}^p (\widehat{\bbeta}^\text{ols}_j - \bbeta_j )^2 + \lambda \sum_j |\bbeta_j|\\
\end{align}

This is a separable problem meaning that we can solve each $\beta_j$ independently since they do not interfere each other. Then the univariate problem is 

$$\underset{\beta}{\argmin} \,\, (\beta - a)^2 + \lambda |\beta|$$
We learned that to solve for an optimizer, we can set the gradient to be zero. However, the function is not everywhere differentiable. Still, we can separate this into two cases: $\beta > 0$ and $\beta < 0$. For the positive side, we have 

\begin{align}
0 =& \frac{\partial}{\partial \beta} \,\, (\beta - a)^2 + \lambda |\beta| = 2 (\beta - a) + \lambda \\
\Longrightarrow \quad \beta =&\, a - \lambda/2
\end{align}

However, this will maintain positive only when $\beta$ is greater than $a - \lambda/2$. The negative size is similar. And whenever $\beta$ falls in between, it will be shrunk to zero. Overall, for our previous univariate optimization problem, the solution is  

\begin{align}
\hat\beta_j^\text{lasso} &=
        \begin{cases}
        \hat\beta_j^\text{ols} - \lambda/2 & \text{if} \quad \hat\beta_j^\text{ols} > \lambda/2 \\
        0 & \text{if} \quad |\hat\beta_j^\text{ols}| < \lambda/2 \\
        \hat\beta_j^\text{ols} + \lambda/2 & \text{if} \quad \hat\beta_j^\text{ols} < -\lambda/2 \\
        \end{cases}\\
        &= \text{sign}(\hat\beta_j^\text{ols}) \left(|\hat\beta_j^\text{ols}| - \lambda/2 \right)_+ \\
        &\doteq \text{SoftTH}(\hat\beta_j^\text{ols}, \lambda)
\end{align}

This is called a __soft-thresholding function__. This implies that when $\lambda$ is large enough, the estimated $\beta$ parameter of Lasso will be shrunk towards zero. The following animated figure demonstrates how adding an $\ell_1$ penalty can change the optimizer. The objective function is $0.5 + (\beta - 1)^2$ and based on our previous analysis, once the penalty is larger than 2, the optimizer would stay at 0. 

```{r fig.dim = c(8, 6), out.width = "100%", fig.align = 'center', message=FALSE, warning=FALSE, echo=FALSE}
  library(gganimate)
  library(plotly)
  
  b = seq(-0.5, 2, 0.01)
  fb = 0.5 + (b - 1)^2
  alllambda = seq(0, 2.5, 0.1)
  onevarlasso = data.frame()
  
  for (i in 1:length(alllambda))
  {
    lambdadata = rbind(data.frame("b" = b, "value" = fb, "Function" = "Loss", 
                             "Lambda" = alllambda[i], "bnum" = 1:length(b)), 
                       data.frame("b" = b, "value" = abs(b*alllambda[i]), "Function" = "Penalty", 
                             "Lambda" = alllambda[i], "bnum" = 1:length(b)),
                       data.frame("b" = b, "value" = fb + abs(b*alllambda[i]), "Function" = "Loss + Penalty", 
                             "Lambda" = alllambda[i], "bnum" = 1:length(b)))
    
    onevarlasso = rbind(onevarlasso, lambdadata)
  }
  
  
  p <- ggplot(data.frame(onevarlasso), aes(x = b, y = value, color = Function)) +
    geom_line(aes(frame = Lambda)) +
    scale_x_continuous(name = "Beta", limits = c(-0.5, 2)) +
    scale_y_continuous(name = "Function Value", limits = c(0, 4)) +
    scale_color_manual(values=c("#ff8c00", "#000000", "#00bfff")) +
    theme(
      panel.background = element_rect(fill = "transparent"), # bg of the panel
      plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
      legend.background = element_rect(fill = "transparent"), # get rid of legend bg
      legend.box.background = element_rect(fill = "transparent") # get rid of legend panel bg
    )
  fig <- ggplotly(p)
  
  fig
```

## Constrained Optimization View

Of course in a multivariate case, this is much more complicated since one variable may affect the optimizer of another. A commonly used alternative interpretation of the Lasso problem is the constrained optimization formulation:

\begin{align}
\min_{\bbeta} \,\,& \lVert \by - \bX \bbeta \rVert^2\\
\text{subject to} \,\, & \lVert\bbeta\rVert_1 \leq t
\end{align}

We can see from the left penal of the following figure that, the Lasso penalty imposes a constraint with the rhombus, i.e., the solution has to stay within the shaded area. The objective function is shown with the contour, and once the contained area is sufficiently small, some $\beta$ parameter will be shrunk to exactly zero. On the other hand, the Ridge regression also has a similar interpretation. However, since the constrained areas is a circle, it will never for the estimated parameters to be zero. 

<center>
![From online sources](images/lassoridge2.png){width=100%} Figure from online sources.
</center>

## The Solution Path

We are interested in getting the fitted model with a given $\lambda$ value, however, for selecting the tuning parameter, it would be much more stable to obtain the solution on a sequence of $\lambda$ values. The corresponding $\bbeta$ parameter estimates are called the solution path, i.e., the path how parameter changes as $\lambda$ changes. We have seen an example of this with the Ridge regression. For Lasso, the the solution path has an interpretation as the __forward-stagewise__ regression. This is different than the forward stepwise model we introduced before. A forward stagewise regression works in the following way:

  * Start with the Null model (intercept) and choose the best variable out of all $p$, such that when its parameter grows by a small magnitude $\epsilon$ (either positive or negative), the RSS reduces the most. Grow the parameter estimate of this variable by $\epsilon$ and repeat. 

The stage-wise regression solution has been shown to give the same solution path as the Lasso, if we start with a sufficiently large $\lambda$, and gradually reduces it towards zero. This can be done with the least angle regression (`lars`) package. Note that the `lars` package introduces another computationally more efficient approach to obtain the same solution, but we will not discuss it in details. We comparing the two approaches (stagewise and stepwise) using the `prostate` data from the `ElemStatLearn` package.

```{r echo=FALSE}
  par(mar=c(4,4,4,2))
  par(mfrow=c(1,2))
```

```{r message=FALSE, fig.dim = c(12, 6), out.width = "90%"}
  library(lars)
  library(ElemStatLearn)
  data(prostate)
  
  lars.fit = lars(x = data.matrix(prostate[, 1:8]), y = prostate$lpsa, 
                  type = "forward.stagewise")
  plot(lars.fit)
  
  lars.fit = lars(x = data.matrix(prostate[, 1:8]), y = prostate$lpsa, 
                  type = "stepwise")
  plot(lars.fit)
```

At each vertical line, a new variable enters the model by growing its parameter out of zero. You can relate this to our previous animated graph where as $\lambda$ decreases, the parameter estimate eventually comes out of zero. However, they may change their grow rate as a new variable comes. This is due to the covariance structure. 

## Path-wise Coordinate Descent

The coordinate descent algorithm [@friedman2010regularization] is probably the most efficient way to solve the Lasso solution up to now. The idea shares similarities with the stage-wise regression. However, with some careful analysis, we can obtain coordinate updates exactly, instead of moving a small step size. And this is done on a decreasing grid of $\lambda$ values. A pseudo algorithm proceed in the following way:

  1) Start with a $\lambda$ value sufficiently large such that all parameter estimates are zero. 
  2) Reduce $\lambda$ by a fraction, e.g., 0.05, and perform coordinate descent updates:
      i) For $j = 1, \ldots p$, update $\beta_j$ using a one-variable penalized formulation.
      ii) Repeat i) until convergence. 
  3) Record the corresponding $\widehat{\bbeta}^\text{lasso}_\lambda$.
  4) Repeat steps 2) and 3) until $\lambda$ is sufficiently small or there are already $n$ nonzero parameters entered into the model. Output $\widehat{\bbeta}^\text{lasso}_\lambda$ for all $\lambda$ values.
  
The crucial step is then figuring out the explicit formula of the coordinate update. Recall that in a coordinate descent algorithm of OLS at Section \@ref(coordinate), we update $\beta_j$ using 

$$
\underset{\boldsymbol \beta_j}{\text{argmin}} \,\, \frac{1}{n} ||\by - X_j \beta_j - \bX_{(-j)} \bbeta_{(-j)} ||^2
$$
Since this is a one-variable OLS problem, the solution is 

$$
\beta_j = \frac{X_j^T \mathbf{r}}{X_j^T X_j}
$$

with $\br = \by - \bX_{(-j)} \bbeta_{(-j)}$. Now, adding the penalty $|\beta_j|$, we essentially reduces back to the previous example of the single variable lasso problem, where we have the OLS solution. Hence, all we need to do is to apply the soft-thresholding function. The the Lasso coordinate update becomes 

$$\beta_j^\text{new} = \text{SoftTH}\left(\frac{X_j^T \mathbf{r}}{X_j^T X_j}, \lambda\right) $$
Incorporate this into the previous algorithm, we can obtain the entire solution path of a Lasso problem. This algorithm is implemented in the `glmnet` package. We will show an example of it. 

## Using the `glmnet` package

We still use the prostate cancer data `prostate` data. The dataset contains 8 explanatory variables and one outcome `lpsa`, the log prostate-specific antigen value. We fit the model using the `glmnet` package. The tuning parameter can be selected using cross-validation with the `cv.glmnet` function. You can specify `nfolds` for the number of folds in the cross-validation. The default is 10. For Lasso, we should use `alpha = 1`, while `alpha = 0` is for Ridge. However, it is the default value that you do not need to specify. 

```{r}
    library(glmnet)
    set.seed(3)
    fit2 = cv.glmnet(data.matrix(prostate[, 1:8]), prostate$lpsa, nfolds = 10, alpha = 1)
```

The left plot demonstrates how $\lambda$ changes the cross-validation error. There are two vertical lines, which represents `lambda.min` and `lambda.1se` respectively. The right plot shows how $\lambda$ changes the parameter values, with each line representing a variable. The x-axis in the figure is in terms of $\log(\lambda)$, hence their is a larger penalty to the right. Note that the `glmnet` package uses $1/(2n)$ in the loss function instead of $1/n$, hence the corresponding soft-thresholding function would reduce the magnitude of $\lambda$ by $\lambda$ instead of half of it. Moreover, the package will perform scaling before the model fitting, which essentially changes the corresponding one-variable OLS solution. The solution on the original scale will be retrieved once the entire solution path is finished. However, we usually do not need to worry about these computationally issues in practice. The main advantage of Lasso is shown here that the model can be sparse, with some parameter estimates shrunk to exactly 0.  

```{r fig.dim = c(12, 6), out.width = '90%'}
    par(mfrow = c(1, 2))
    plot(fit2)
    plot(fit2$glmnet.fit, "lambda")
```

We can obtain the estimated coefficients from the best $\lambda$ value. Similar to the ridge regression example, there are two popular options, `lambda.min` and `lambda.1se`. The first one is the value that minimizes the cross-validation error, the second one is slightly more conservative, which gives larger penalty value with more shrinkage. You can notice that `lambda.min` contains more nonzero parameters. 

```{r}
    coef(fit2, s = "lambda.min")
    coef(fit2, s = "lambda.1se")
```

Prediction can be done using the `predict()` function. 

```{r}
    pred = predict(fit2, data.matrix(prostate[, 1:8]), s = "lambda.min")
    # training error
    mean((pred - prostate$lpsa)^2)
```
## Elastic-Net

Lasso may suffer in the case where two variables are strongly correlated. The situation is similar to OLS, however, in Lasso, it would only select one out of the two, instead of letting both parameter estimates to be large. This is not preferred in some practical situations such as genetic studies because expressions of genes from the same pathway may have large correlation, but biologist want to identify all of them instead of just one. The Ridge penalty may help in this case because it naturally considers the correlation structure. Hence the __Elastic-Net__ [@zou2005regularization] penalty has been proposed to address this issue: the data contains many correlated variables and we want to select them together if they are important for prediction. The `glmnet` package uses the following definition of an Elastic-Net penalty, which is a mixture of $\ell_1$ and $\ell_2$ penalties:

$$\lambda \left[ (1 - \alpha)/2 \lVert \bbeta\rVert_2^2 + \alpha |\bbeta|_1 \right],$$
which involves two tuning parameters. However, in practice, it is very common to simply use $\alpha = 0.5$.




<!--chapter:end:02.3-lasso.Rmd-->

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bF{\mathbf{F}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{amsmath}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
```

# (PART) Nonparametric Models {-}

# Spline

## From Linear to Nonlinear

In previous chapters, we mainly focused linear models. Modeling nonlinear trends can still be done with linear model by introducing higher-order terms, or nonlinear transformations. For example, $x^2$, $\log(x)$ are all very commonly used approaches to model nonlinear effects. There is another class of approaches that is more flexible with nice theoretical properties, the splines. In this chapter, we mainly focus on univariate regression problems. 

## A Motivating Example and Polynomials

We use the U.S. birth rate data as an example. The data records birth rates from 1917 to 2003. The birth rate trend is obviously very nonlinear. 

```{r}
    birthrates= read.csv("data/birthrate2.csv", row.names = 1)
    head(birthrates)
    par(mar = c(4,4,1,1))
    plot(birthrates, pch = 19, col = "darkorange")
```

It might be interesting to fit a linear regression with high order polynomials to approximate this curve. This can be carried out using the `poly()` function, which calculates all polynomials up to a certain power. Please note that this is a more stable method compared with writing out the powers such as `I(Year^2)`, `I(Year^3)` etc because the `Year` variable is very large, and is numerically unstable. 

```{r fig.dim = c(12, 6), out.width = '80%'}
    par(mfrow=c(1,2))

    par(mar = c(2,3,2,0))
    lmfit <- lm(Birthrate ~ poly(Year, 3), data = birthrates)
    plot(birthrates, pch = 19, col = "darkorange")
    lines(birthrates$Year, lmfit$fitted.values, lty = 1, col = "deepskyblue", lwd = 2)
    title("degree = 3")
    
    par(mar = c(2,3,2,0))
    lmfit <- lm(Birthrate ~ poly(Year, 5), data = birthrates)
    plot(birthrates, pch = 19, col = "darkorange")
    lines(birthrates$Year, lmfit$fitted.values, lty = 1, col = "deepskyblue", lwd = 2)
    title("degree = 5")
```

These fittings do not seem to perform very well. How about we take a different approach to model the curve locally. Well, we know there is an approach that works in a similar way -- $k$NN. But we will try something new. Let's first divide the year range into several non-overlapping intervals, say, every 10 years. Then we will estimate the regression coefficients within each interval by averaging the observations, just like $k$NN. The only difference is that for prediction, we do not recalculate the neighbors anymore, just check the intervals. 

```{r fig.dim = c(6, 6), out.width = "45%"}
    par(mfrow=c(1,1))
    par(mar = c(2,2,2,0))

    mybasis = matrix(NA, nrow(birthrates), 8)
    for (l in 1:8)
        mybasis[, l] = birthrates$Year*(birthrates$Year >= 1917 + l*10)
        
    lmfit <- lm(birthrates$Birthrate ~ ., data = data.frame(mybasis))
    plot(birthrates, pch = 19, col = "darkorange")
    lines(birthrates$Year, lmfit$fitted.values, lty = 1, type = "s", col = "deepskyblue", lwd = 2)
    title("Histgram Regression")
```

The method is called a histogram regression. Suppose the interval that contains a given testing point $x$ is $\phi(x)$, then, we are fitting a model with 

$$\widehat{f}(x) = \frac{\sum_{i=1}^n Y_i \,\, I\{X_i \in \phi(x)\} }{ \sum_{i=1}^n I\{X_i \in \phi(x)\}}$$

You may know the word histogram from the plotting the density of a set of observations. Yes, these two are actually motivated by the same philosophy. We will discuss the connection later on. For the purpose of fitting a regression function, the histogram regression does not seem to perform ideally since there will be jumps at the edge of an interval. Hence we need a more flexible framework. 

## Piecewise Polynomials

Instead of fitting constant functions within each interval (between two knots), we may consider fitting a line. Consider a simpler case, where we just use 3 knots at 1938, 1960, 1978, which gives 4 intervals. 

```{r fig.dim = c(12, 6), out.width = '80%'}
    par(mfrow=c(1,2))

    myknots = c(1936, 1960, 1978)
    bounds = c(1917, myknots, 2003)  
    
    # piecewise constant
    mybasis = cbind("x_1" = (birthrates$Year < myknots[1]), 
				    "x_2" = (birthrates$Year >= myknots[1])*(birthrates$Year < myknots[2]), 
				    "x_3" = (birthrates$Year >= myknots[2])*(birthrates$Year < myknots[3]),
				    "x_4" = (birthrates$Year >= myknots[3]))
        
    lmfit <- lm(birthrates$Birthrate ~ . -1, data = data.frame(mybasis))
    par(mar = c(2,3,2,0))    
    plot(birthrates, pch = 19, col = "darkorange")
    abline(v = myknots, lty = 2)
    title("Piecewise constant")
    
    for (k in 1:4)
        points(c(bounds[k], bounds[k+1]), rep(lmfit$coefficients[k], 2), type = "l", 
               lty = 1, col = "deepskyblue", lwd = 4)
    
    # piecewise linear
    mybasis = cbind("x_1" = (birthrates$Year < myknots[1]), 
				    "x_2" = (birthrates$Year >= myknots[1])*(birthrates$Year < myknots[2]), 
				    "x_3" = (birthrates$Year >= myknots[2])*(birthrates$Year < myknots[3]),
				    "x_4" = (birthrates$Year >= myknots[3]),
                    "x_11" = birthrates$Year*(birthrates$Year < myknots[1]), 
				    "x_21" = birthrates$Year*(birthrates$Year >= myknots[1])*(birthrates$Year < myknots[2]), 
				    "x_31" = birthrates$Year*(birthrates$Year >= myknots[2])*(birthrates$Year < myknots[3]),
				    "x_41" = birthrates$Year*(birthrates$Year >= myknots[3]))
        
    lmfit <- lm(birthrates$Birthrate ~ .-1, data = data.frame(mybasis))
    par(mar = c(2,3,2,0))  
    plot(birthrates, pch = 19, col = "darkorange")
    abline(v = myknots, lty = 2)
    title("Piecewise linear")
    
    for (k in 1:4)
        points(c(bounds[k], bounds[k+1]), 
               lmfit$coefficients[k] + c(bounds[k], bounds[k+1])*lmfit$coefficients[k+4], 
               type = "l", lty = 1, col = "deepskyblue", lwd = 4)
```

## Splines

However, these functions are not continuous. Hence we use a trick to construct continuous basis:

```{r fig.dim=c(6, 6), out.width = '45%'}
    par(mfrow=c(1,1))
    pos <- function(x) x*(x>0)
    mybasis = cbind("int" = 1, "x_1" = birthrates$Year, 
				    "x_2" = pos(birthrates$Year - myknots[1]), 
				    "x_3" = pos(birthrates$Year - myknots[2]),
				    "x_4" = pos(birthrates$Year - myknots[3]))
    
    par(mar = c(2,2,2,0))
    matplot(birthrates$Year, mybasis[, -1], type = "l", lty = 1, 
            yaxt = 'n', ylim = c(0, 50), lwd = 2)
    title("Spline Basis Functions")
```

With this definition, any fitted model will be 

* Continuous everywhere
* Linear everywhere except the knots
* Has a different slot for each region

The resulted model is called a spline. 

```{r fig.width=5.5, fig.height=4, out.width = '45%'}
    lmfit <- lm(birthrates$Birthrate ~ .-1, data = data.frame(mybasis))
    par(mar = c(2,3,2,0))  
    plot(birthrates, pch = 19, col = "darkorange")
    lines(birthrates$Year, lmfit$fitted.values, lty = 1, col = "deepskyblue", lwd = 4)
    abline(v = myknots, lty = 2)
    title("Linear Spline")
```

Of course, writing this out explicitly is very tedious, hence we have the `bs` function in the `splines` package to help us. 

```{r fig.width=5.5, fig.height=4, out.width = '45%'}
    par(mar = c(2,2,2,0))
    lmfit <- lm(Birthrate ~ splines::bs(Year, degree = 1, knots = myknots), data = birthrates)
    plot(birthrates, pch = 19, col = "darkorange")
    lines(birthrates$Year, lmfit$fitted.values, lty = 1, col = "deepskyblue", lwd = 4)
    title("Linear spline with the bs() function")
```

The next step is to increase the degree to account for more complicated functions. A few things we need to consider here: 

* How many knots should be used 
* Where to place the knots
* What is the degree of functions in each region

For example, we consider this setting 

```{r fig.width=5.5, fig.height=4, out.width = '45%'}
    par(mar = c(2,2,2,0))
    lmfit <- lm(Birthrate ~ splines::bs(Year, degree = 3, knots = myknots), data = birthrates)
    plot(birthrates, pch = 19, col = "darkorange")
    lines(birthrates$Year, lmfit$fitted.values, lty = 1, col = "deepskyblue", lwd = 4)
    title("Cubic spline with 3 knots")
```

All of them affects the performance. In particular, the number of knots and the number of degrees in each region will determine the total number of degrees of freedom. For simplicity, we can control that using the `df` parameter. We use a total of 6 parameters, chosen by the function automatically. However, this does not seems to perform better than the knots we implemented. The choice of knots can be crucial. 

```{r}
    par(mar = c(2,2,2,0))
    lmfit <- lm(Birthrate ~ splines::bs(Year, df = 5), data = birthrates)
    plot(birthrates, pch = 19, col = "darkorange")
    lines(birthrates$Year, lmfit$fitted.values, lty = 1, col = "deepskyblue", lwd = 4)
    title("Linear spline with 6 degrees of parameters")
```

## Spline Basis

There are different ways to construct spline basis. We used two techniques previously, the regression spline and basis spline (B-spline). The B-spline has slight more advantages computationally. Here is a comparision of B-spline with different degrees. 

```{r}
    par(mfrow = c(4, 1), mar = c(0, 0, 2, 0))

    for (d in 0:3)
    {
        bs_d = splines2::bSpline(1:100, degree = d, knots = seq(10, 90, 10), intercept = TRUE)
        matplot(1:100, bs_d, type = ifelse(d == 0, "s", "l"), lty = 1, ylab = "spline", 
                xaxt = 'n', yaxt = 'n', ylim = c(-0.05, 1.05), lwd = 2)
        title(paste("degree =", d))
    }
```

## Natural Cubic Spline

Extrapolations are generally dangerous because the functions could be extream outside the range of the observed data. In linear models fit by `bs()`, extrapolations outside the boundaries will trigger a warning.  

```{r}
    par(mfrow=c(1,1))
    library(splines)
    fit.bs = lm(Birthrate ~ bs(Year, df=6), data=birthrates)
    plot(birthrates$Year, birthrates$Birthrate, ylim=c(0,280), pch = 19, 
         xlim = c(1900, 2020), xlab = "year", ylab = "rate", col = "darkorange")   
    lines(seq(1900, 2020), predict(fit.bs, data.frame("Year"= seq(1900, 2020))),
          col="deepskyblue", lty=1, lwd = 3)    
    
    fit.ns = lm(Birthrate ~ ns(Year, df=6), data=birthrates)    
    lines(seq(1900, 2020), predict(fit.ns, data.frame("Year"= seq(1900, 2020))), 
          col="darkgreen", lty=1, lwd = 3)
    legend("topright", c("Cubic B-Spline", "Natural Cubic Spline"), 
           col = c("deepskyblue", "darkgreen"), lty = 1, lwd = 3, cex = 1.2)
    title("Birth rate extrapolation")
```

Hence this motivates us to consider setting additional constrains that forces the extrapolations to be come more regular. This is done by forcing the second and third derivatives to be 0 if beyond the two extreme knots. 

```{r}
    par(mar = c(0, 2, 2, 0))
    ncs = ns(1:100, df = 6, intercept = TRUE)
    matplot(1:100, ncs, type = "l", lty = 1, ylab = "spline",
            xaxt = 'n', yaxt = 'n', lwd = 3)
    title("Natural Cubic Spline")
```

## Smoothing Spline

The motivation is to trying to solve the knots selection issue. Instead, let's start with a "horrible" idea, by putting knots at all each observed data point $(x_1, x_2, \ldots, x_n)$. Then, we can create $n$ natural cubic spline basis. However, we also know that this leads to over-fitting since there are too many parameters. Let's utilize the ridge regression idea by adding some penalties. This leads to the objective function

\begin{equation}
\underset{\bbeta}{\text{min}} \,\, \lVert \by - \bF \bbeta \rVert^2 + \lambda \bbeta^\T \Omega \bbeta, (\#eq:penalized)
\end{equation}
where $\bF$ is the matrix of all $n$ natural cubic spline basis, and $\Omega$ is some covariance matrix that takes care of some form of relationship among different basis, which we will define later, and $\lambda$ is similar to the ridge regression. The question is, will this type of regression problem provide a good solution with nice properties? 

Let's consider fitting a regression model by solving a regression function $g(x)$ with the following penalized criteria:

\begin{equation}
\frac{1}{n} \sum_{i=1}^n \big(y_i - g(x_i)\big)^2 + \lambda \int_a^b \big[g''(x) \big]^2 dx. (\#eq:roughness)
\end{equation}

This is the sum of $\ell_2$ loss and a roughness penalty that enforce certain smoothness on $g(x)$. And we shall show that the optimal $g(x)$ that minimize this objective function will take the ridge penalty form in \@ref(eq:penalized) mentioned previously. 

We will consider all absolutely continuous functions on $[a, b] = [\min(x_i), \max(x_i)]$, with finite roughness, i.e., $\int_a^b \big[g''(x) \big]^2 dx < \infty$. This is known as the second order Sobolev space. Let's first define $g(\cdot)$ as the optimal solution to Equation \@ref(eq:roughness). Since the loss part in \@ref(eq:roughness) only involves $n$ data points, we can find define a natural cubic spline (NCS) fit $\tilde{g}(\cdot)$ such that it matches with $g(\cdot)$ on all the observed data, i.e., 

$$g(x_i) = \widetilde{g}(x_i), \quad i = 1, \ldots, n.$$

Note that the roughness of NCS fit $\widetilde{g}$ is also finite, hence $\widetilde{g}$ is within the space of functions we are considering. Also, such matching on all observed data points is doable when we have $n$ basis in the natural cubic spline. In this case, the loss corresponds to $\tilde{g}$ and $g$ are identical. Hence, it only matters if the penalty part of $\tilde{g}(\cdot)$ is the same. To analyze this, we define the difference between these two functions as 

$$h(x) = g(x) - \tilde{g}(x).$$
It is then obvious that $h(x_i) = 0$ for all observed $i$. Then we have 

$$\int g''^2 dx = \int \widetilde{g}''^2 dx + \int h''^2 dx + 2 \int \widetilde{g}'' h'' dx$$
The first and second term on the right hand side are both non-negative. Hence, only the third term matters. WLOG, we assume that $x_i$'s are ordered from the smallest to the largest. Then 

\begin{align}
\int \tilde{g}'' h'' dx =& ~\tilde{g}'' h' \Big|_a^b - \int_a^b h' \tilde{g}^{(3)} dx \nonumber \\
=&~ 0 - \int_a^b h' \tilde{g}^{(3)} dx \nonumber \\
=&~ - \sum_{i=1}^{n-1} \tilde{g}^{(3)}(x_j^+) \int_{x_j}^{x_{j+1}} h' dx \quad \nonumber \\
=&~ - \sum_{i=1}^{n-1} \tilde{g}^{(3)}(x_j^+) \big(h(x_{j+1}) - h(x_j)\big) \nonumber \\
=&~ 0 
\end{align}

The second equation is because $\tilde{g}$ is a NCS and suppose to have 0 second derivative on the two boundaries $a$ and $b$. The third equation is because $\tilde{g}$ is at most $x^3$ on any regions and have constant third derivatives, which we can pull out of the integration. And the last equation is because $h(x) = 0$ on all $x_i$'s. 

Hence, this shows that the roughness penalty $$\int \widetilde{g}''^2 dx$$ of our NCS solution is no larger than the best solution $g$. Noticing that $\widetilde{g}$ is also with the space of functions we are considering, then $g$ must be our NCS solution. 

Hence, $g$ has a finite sample representation

$$\widehat g(x) = \sum_{j=1}^n \beta_j N_j(x)$$

where $N_j$'s are a set of natural cubic spline basis functions with knots at each of the unique $x$ values. Then Equation \@ref(eq:roughness) becomes 

\begin{align}
& \lVert \by - \sum_{j=1}^n \beta_j N_j(x) \rVert^2 + \int \Big( \sum_{j=1}^n \beta_j N_j''(x)\Big)^2 dx \\
=& \lVert \by - \bF\bbeta \rVert^2 + \bbeta^\T \Omega \bbeta,
\end{align}

where $\bF$ is the design matrix corresponds to the $n$ NCS basis $N_j$'s, and $\Omega$ is an $n \times n$ matrix with $\Omega_{ij} = \int N_i''(x) N_j(x) dx.$ The solution is essentially a ridge solution:

$$\widehat{\bbeta} = (\bF^\T \bF + \lambda \Omega)^{-1} \bF^\T \by.$$
and the penalty $\lambda$ can be tuned using GCV. 

## Fitting Smoothing Splines

Fitting a smoothing spline can be done by using the `smooth.spline` package. However, since the birthrate data has little variation in adjacent years, over-fitting is quite severe. The function will automatically use GCV to tune the parameter. 

```{r}
    # smoothing spline 
    fit = smooth.spline(birthrates$Year, birthrates$Birthrate)
    plot(birthrates$Year, birthrates$Birthrate, pch = 19, 
         xlab = "Year", ylab = "BirthRates", col = "darkorange")
    lines(seq(1917, 2003), predict(fit, seq(1917, 2003))$y, col="deepskyblue", lty=1, lwd = 3)
    
    # the degrees of freedom is very large
    fit$df
```

Let's look at another simulation example, where this method performs resonabaly well. 

```{r}
    set.seed(1)
    n = 100
    x = seq(0, 1, length.out = n)
    y = sin(12*(x+0.2))/(x+0.2) + rnorm(n)
    
    # fit smoothing spline
    fit = smooth.spline(x, y)
    
    # the degrees of freedom
    fit$df    
    
    # fitted model
    plot(x, y, pch = 19, xlim = c(0, 1), xlab = "x", ylab = "y", col = "darkorange")
    lines(x, sin(12*(x+0.2))/(x+0.2), col="red", lty=1, lwd = 3)    
    lines(x, predict(fit, x)$y, col="deepskyblue", lty=1, lwd = 3)
    legend("bottomright", c("Truth", "Smoothing Splines"), 
           col = c("red", "deepskyblue"), lty = 1, lwd = 3, cex = 1.2)
```

## Extending Splines to Multiple Varibles

Since all spline approaches can be transformed into some kind of linear model, if we postulate an additive structure, we can fit a multivariate model with

$$f(x) = \sum_j h_j(x_j) = \sum_j \sum_k N_{jk}(x_j) \beta_{jk}$$
where $h_j(x_j)$ is a univariate function for $x_j$ that can be approximated by splines basis $N_{jk}(\cdot), k = 1, \ldots, K$. This works for both linear regression and generalized linear regressions. For the South Africa Heart Disease data, we use the `gam()` function in the `gam` (generalized additive models) package. We compute a logistic regression model using natural splines (note `famhist` is included as a factor).  

```{r fig.dim = c(15, 15), out.width = '90%'}
    library(ElemStatLearn)
    library(gam)

    form = formula("chd ~ ns(sbp,df=4) + ns(tobacco,df=4) + 
    					  ns(ldl,df=4) + famhist + ns(obesity,df=4) + 
    					  ns(alcohol,df=4) + ns(age,df=4)")
    
    # note that we can also do 
    # m = glm(form, data=SAheart, family=binomial)
    # print(summary(m), digits=3)
    # however, the gam function provides more information 
    
    m = gam(form, data=SAheart, family=binomial)
    summary(m)
    
    par(mfrow = c(3, 3), mar = c(5, 5, 2, 0))
    plot(m, se = TRUE, residuals = TRUE, pch = 19, col = "darkorange")
```





<!--chapter:end:03.1-spline.Rmd-->

\def\cD{\cal{D}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bzero{\mathbf{0}}
\def\bbeta{\boldsymbol \beta}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# K-Neariest Neighber

$K$ nearest neighbor (KNN) is a simple nonparametric method. It can be used for both regression and classification problems. However, the idea is quite different from models we introduced before. In a linear model, we have a set of parameters $\bbeta$ and our estimated function value, for any target point $x_0$ is $x_0^\T \bbeta$. In KNN, we don't really specify these parameters. Instead, we directed estimate $f(x_0)$. This is traditionally called __nonparametric models__ in statistics. Usually these models perform a local averaging technique to estimate $f(x_0)$ using observations close to $x_0$. 

## Definition

Suppose we collect a set of observations $\{x_i, y_i\}_{i=1}^n$, the prediction at a new target point $x_0$ is

$$\widehat y = \frac{1}{k} \sum_{x_i \in N_k(x_0)} y_i,$$
where $N_k(x_0)$ defines the $k$ samples from the training data that are closest to $x_0$. As default, closeness is defined using a distance measure, such as the Euclidean distance. Here is a demonstration of the fitted regression function. 

```{r fig.dim = c(9, 6)}
    # generate training data with 2*sin(x) and random Gaussian errors
    set.seed(1)    
    x <- runif(15, 0, 2*pi)
    y <- 2*sin(x) + rnorm(length(x))
    
    # generate testing data points where we evaluate the prediction function
    test.x = seq(0, 1, 0.001)*2*pi

    # "1-nearest neighbor" regression using kknn package
    library(kknn)
    knn.fit = kknn(y ~ x, train = data.frame(x = x, y = y), 
                   test = data.frame(x = test.x),
                   k = 1, kernel = "rectangular")
    test.pred = knn.fit$fitted.values
    
    # plot the data
    par(mar=rep(2,4))
    plot(x, y, xlim = c(0, 2*pi), pch = "o", cex = 2, 
         xlab = "", ylab = "", cex.lab = 1.5)
    title(main="1-Nearest Neighbor Regression", cex.main = 1.5)
    
    # plot the true regression line
    lines(test.x, 2*sin(test.x), col = "deepskyblue", lwd = 3)
    
    # plot the fitted line
    lines(test.x, test.pred, type = "s", col = "darkorange", lwd = 3)
    legend("topright", c("Fitted line", "True function"), 
           col = c("darkorange", "deepskyblue"), lty = 1, cex = 1.5)
```

## Tuning $k$

Tuning $k$ is crucial for $k$NN. Let's observe its effect. This time, I generate 200 observations. For 1NN, the fitted regression line is very jumpy. 

```{r}
  # generate more data
  set.seed(1)
  n = 200
  x <- runif(n, 0, 2*pi)
  y <- 2*sin(x) + rnorm(length(x))

  test.y = 2*sin(test.x) + rnorm(length(test.x))
  
  # 1-nearest neighbor
  knn.fit = kknn(y ~ x, train = data.frame(x = x, y = y), 
                 test = data.frame(x = test.x),
                 k = 1, kernel = "rectangular")
  test.pred = knn.fit$fitted.values
```

```{r echo = FALSE, fig.dim = c(9, 6)}
  par(mar=rep(2,4))
  plot(x, y, pch = 19, cex = 1, 
       xlim = c(0, 2*pi), ylim = c(-4.25, 4.25))
  title(main="1-Nearest Neighbor Regression", cex.main = 1.5)
  lines(test.x, 2*sin(test.x), col = "deepskyblue", lwd = 3)
  lines(test.x, test.pred, type = "s", col = "darkorange", lwd = 3)
  legend("topright", c("Fitted line", "True function"), 
         col = c("darkorange", "deepskyblue"), lty = 1, cex = 1.5)
```

We can evaluate the prediction error of this model: 

```{r}
  # prediction error
  mean((test.pred - test.y)^2)
```

If we consider different values of $k$, we can observe the trade-off between bias and variance. 

```{r echo = FALSE, fig.dim = c(12, 6), out.width = "90%"}
  par(mfrow=c(2,3))
  par(mar=rep(2,4))
  
  for (k in c(1, 5, 10, 33, 66, 100))
  {
      knn.fit = kknn(y ~ x, train = data.frame(x = x, y = y), 
                     test = data.frame(x = test.x),
                     k = k, kernel = "rectangular")
      test.pred = knn.fit$fitted.values
      cat(paste("Prediction Error for K =", k, ":", mean((test.pred - test.y)^2)))

      plot(x, y, xlim = c(0, 2*pi), pch = 19, cex = 0.7, 
           axes=FALSE, ylim = c(-4.25, 4.25))
      title(main=paste("K =", k))
      lines(test.x, 2*sin(test.x), col = "deepskyblue", lwd = 3)
      lines(test.x, test.pred, type = "s", col = "darkorange", lwd = 3)
      box()
  }
```

As $k$ increases, we have a more stable model, i.e., smaller variance. However, the bias is also increased. As $k$ decreases, the bias decreases, but the model is less stable.

## The Bias-variance Trade-off

Formally, the prediction error (at a given target point $x_0$) can be broke down into three parts: the __irreducible error__, the __bias squared__, and the __variance__.

\begin{aligned}
\E\Big[ \big( Y - \widehat f(x_0) \big)^2 \Big] &= \E \Big[ \big( Y - f(x_0) + f(x_0) -  \E[\widehat f(x_0)] + \E[\widehat f(x_0)] - \widehat f(x_0) \big)^2 \Big] \\
&= \E \Big[ \big( Y - f(x_0) \big)^2 \Big] + \E \Big[ \big(f(x_0) - \E[\widehat f(x_0)] \big)^2 \Big] + \E\Big[ \big(E[\widehat f(x_0)] - \widehat f(x_0) \big)^2 \Big] + \text{Cross Terms}\\
&= \underbrace{\E\Big[ ( Y - f(x_0))^2 \big]}_{\text{Irreducible Error}} +
\underbrace{\Big(f(x_0) - \E[\widehat f(x_0)]\Big)^2}_{\text{Bias}^2} + 
\underbrace{\E\Big[ \big(\widehat f(x_0) - \E[\widehat f(x_0)] \big)^2 \Big]}_{\text{Variance}}
\end{aligned}

As we can see from the previous example, when $k=1$, the prediction error is about 2. This is because for all the testing points, the theoretical irreducible error is 1 (variance of the error term), the bias is almost 0 since the function is smooth, and the variance is the variance of 1 nearest neighbor, which is again 1. On the other extreme side, when $k = n$, the variance should be in the level of $1/n$, the bias is the difference between the sin function and the overall average. Overall, we can expect the trend:

  * As $k$ increases, bias increases and variance decreases
  * As $k$ decreases, bias decreases and variance increases

## KNN for Classification 

For classification, kNN is different from the regression model in term of finding neighbers. The only difference is to majority voting instead of averaging. Majority voting means that we look for the most popular class label among its neighbors. For 1NN, it is simply the class of the closest neighbor. The visualization of 1NN is a Voronoi tessellation. The plot on the left is some randomly observed data in $[0, 1]^2$, and the plot on the right is the corresponding 1NN classification model. 

```{r message=FALSE, fig.dim=c(12, 6), out.width="90%", echo = FALSE}
  # knn for classification: 
  library(class)
  par(mfrow=c(1,2))
  par(mar=rep(2,4))
  
  # generate 20 random observations, with random class 1/0
  set.seed(1)    
  x <- matrix(runif(40), 20, 2)
  g <- rbinom(20, 1, 0.5)

  # plot the data
  plot(x, col=ifelse(g==1, "darkorange", "deepskyblue"), pch = 19, 
       cex = 3, xlim= c(0, 1), ylim = c(0, 1))
  symbols(0.7, 0.7, circles = 0.12, add = TRUE, inches = FALSE)
  points(0.7, 0.7, pch = 4, lwd = 2, col = "red")

  # generate a grid for plot
  xgd1 = xgd2 = seq(0, 1, 0.01)
  gd = expand.grid(xgd1, xgd2)

  # fit a 1-nearest neighbor model and get the fitted class
  knn1 <- knn(x, gd, g, k=1)
  knn1.class <- matrix(knn1, length(xgd1), length(xgd2))

  # Voronoi tessalation plot (1NN)
  library(deldir)
  z <- deldir(x = data.frame(x = x[,1], y = x[,2], z=as.factor(g)), 
              rw = c(0, 1, 0, 1))
  w <- tile.list(z)
  
  plot(w, fillcol=ifelse(g==1, "bisque", "cadetblue1"))
  points(x, col=ifelse(g==1, "darkorange", "deepskyblue"), pch = 19, cex = 3)
  points(0.7, 0.7, pch = 4, lwd = 2, col = "red")
```

```{r echo = FALSE}
  par(mfrow=c(1,1))
```

## Example 1: An artificial data

We use artificial data from the `ElemStatLearn` package. 

```{r fig.dim=c(6, 6), out.width="45%"}
  library(ElemStatLearn)
  
  x <- mixture.example$x
  y <- mixture.example$y
  xnew <- mixture.example$xnew
  
  par(mar=rep(2,4))
  plot(x, col=ifelse(y==1, "darkorange", "deepskyblue"), 
       axes = FALSE, pch = 19)
  box()
```

The decision boundary is highly nonlinear. We can utilize the `contour()` function to demonstrate the result. 

```{r}
  # knn classification 
  k = 15
  knn.fit <- knn(x, xnew, y, k=k)
  
  px1 <- mixture.example$px1
  px2 <- mixture.example$px2
  pred <- matrix(knn.fit == "1", length(px1), length(px2))
  
  contour(px1, px2, pred, levels=0.5, labels="",axes=FALSE)
  box()
  title(paste(k, "-Nearest Neighbor", sep= ""))
  points(x, col=ifelse(y==1, "darkorange", "deepskyblue"), pch = 19)
  mesh <- expand.grid(px1, px2)
  points(mesh, pch=".", cex=1.2, col=ifelse(pred, "darkorange", "deepskyblue"))
```

We can evaluate the in-sample prediction result of this model using a confusion matrix:

```{r message=FALSE}
  # the confusion matrix
  knn.fit <- knn(x, x, y, k = 15)
  xtab = table(knn.fit, y)
  
  library(caret)
  confusionMatrix(xtab)
```

## Tuning with the `caret` Package

The `caret` package has some built-in feature that can tune some popular machine learning models using cross-validation. The cross-validation settings need to be specified using the $trainControl()$ function. 

```{r message=FALSE}
  library(caret)
  control <- trainControl(method = "cv", number = 10)
```

There are other cross-validation methods, such as `repeatedcv` the repeats the CV several times, and leave-one-out CV `LOOCV`. For more details, you can read the [documentation](https://www.rdocumentation.org/packages/caret/versions/6.0-88/topics/trainControl). We can then setup the training by specifying a grid of $k$ values, and also the CV setup. Make sure that you specify `method = "knn"` and also construct the outcome as a factor in a data frame. 

```{r echo=FALSE}  
  par(mar=c(4,4,2,2))
```

```{r}  
  set.seed(1)
  knn.cvfit <- train(y ~ ., method = "knn", 
                     data = data.frame("x" = x, "y" = as.factor(y)),
                     tuneGrid = data.frame(k = seq(1, 40, 1)),
                     trControl = control)
  
  plot(knn.cvfit$results$k, 1-knn.cvfit$results$Accuracy,
       xlab = "K", ylab = "Classification Error", type = "b",
       pch = 19, col = "darkorange")
```

Print out the fitted object, we can see that the best $k$ is 6. And there is a clear "U" shaped pattern that shows the potential bias-variance trade-off.

## Distance Measures

Closeness between two points needs to be defined based on some distance measures. By default, we use the squared Euclidean distance ($\ell_2$ norm) for continuous variables:

$$d^2(\bu, \bv) = \lVert \bu - \bv \rVert_2^2 = \sum_{j=1}^p (u_j, v_j)^2.$$
However, this measure is not scale invariant. A variable with large scale can dominate this measure. Hence, we often consider a normalized version:

$$d^2(\bu, \bv) = \sum_{j=1}^p \frac{(u_j, v_j)^2}{\sigma_j^2},$$
where $\sigma_j^2$ can be estimated using the sample variance of variable $j$. Another choice that further taking the covariance structure into consideration is the __Mahalanobis distance__:

$$d^2(\bu, \bv) = (\bu - \bv)^\T \Sigma^{-1} (\bu - \bv),$$
where $\Sigma$ is the covariance matrix, and can be estimated using the sample covariance. In the following plot, the red cross and orange cross have the same Euclidean distance to the center. However, the red cross is more of a "outlier" based on the joint distribution. The Mahalanobis distance would reflect this. 

```{r, message=FALSE}
  x=rnorm(100)
  y=1 + 0.3*x + 0.3*rnorm(100)

  library(car)
  dataEllipse(x, y, levels=c(0.6, 0.9), col = c("black", "deepskyblue"), pch = 19)
  points(1, 0.5, col = "red", pch = 4, cex = 2, lwd = 4)
  points(1, 1.5, col = "darkorange", pch = 4, cex = 3, lwd = 4)
```

For categorical variables, the Hamming distance is commonly used:

$$d(\bu, \bv) = \sum_{j=1}^p I(u_j \neq v_j).$$
It simply counts how many entries are not the same. 


## 1NN Error Bound

We can show that 1NN can achieve reasonable performance for fixed $p$, as $n \rightarrow \infty$ by showing that the 1NN error is no more than twice of the Bayes error, which is the smaller one of $P(Y = 1 | X = x_0)$ and $1 - P(Y = 1 | X = x_0)$. 

Let's denote $x_{1nn}$ the closest neighbor of $x_0$, we have $d(x_0, x_{1nn}) \rightarrow 0$ by the law of large numbers. Assuming smoothness, we have $P(Y = 1 | x_{1nn}) \rightarrow P(Y = 1 | x_0)$. Hence, the 1NN error is the chance we make a wrong prediction, which can happen in two situations. WLOG, let's assume that $P(Y = 1 | X = x_0)$ is larger than $1-P(Y = 1 | X = x_0)$, then 

\begin{align}
& \,P(Y=1|x_0)[ 1 - P(Y=1|x_{1nn})] + P(Y=1|x_0)[ 1 - P(Y=1|x_{1nn})]\\
\leq & \,[ 1 - P(Y=1|x_{1nn})] + [ 1 - P(Y=1|x_{1nn})]\\
\rightarrow & \, 2[ 1 - P(Y=1|x_0)]\\
= & \,2 \times \text{Bayes Error}\\
\end{align}

This is a crude bound, but shows that 1NN can still be a reasonable estimator when the noise is small (Bayes error small). 

## Example 2: Handwritten Digit Data

Let's consider another example using the handwritten digit data. Each observation in this data is a $16 \times 16$ pixel image. Hence, the total number of variables is $256$. At each pixel, we have the gray scale as the numerical value. 

```{r out.width="20%"}
  # Handwritten Digit Recognition Data
  library(ElemStatLearn)
  # the first column is the true digit
  dim(zip.train)
  dim(zip.test)
  
  # look at one sample
  image(zip2image(zip.train, 1), col=gray(256:0/256), zlim=c(0,1), 
        xlab="", ylab="", axes = FALSE)
```

We use 3NN to predict all samples in the testing data. The model is fairly accurate. 
    
```{r}
  # fit 3nn model and calculate the error
  knn.fit <- knn(zip.train[, 2:257], zip.test[, 2:257], zip.train[, 1], k=3)
  
  # overall prediction error
  mean(knn.fit != zip.test[,1])
  
  # the confusion matrix
  table(knn.fit, zip.test[,1])
```

## Curse of Dimensionality

Many of the practical problems we encounter today are high-dimensional. The resolution of the handwritten digit example is $16 \times 16 = 256$. Genetic studies often involves more than 25K gene expressions, etc. For a given sample size $n$, as the number of variables $p$ increases, the data becomes very sparse. Nearest neighbor methods usually do not perform very well on high-dimensional data. This is because for any given target point, there will not be enough training data that lies close enough. To see this, let's consider a $p$-dimensional hyper-cube. Suppose we have $n=1000$ observations uniformly spread out in this cube, and we are interested in predicting a target point with $k=10$ neighbors. If these neighbors are really close to the target point, then this would be a good estimation with small bias. Suppose $p=2$, then we know that if we take a cube (square) with height and width both $l = 0.1$, then there will be $1000 \times 0.1^2 = 10$ observations within the square. In general, we have the relationship 

$$l^p = \frac{k}{n}$$
Try different $p$, we have 

  * If $p = 2$, $l = 0.1$
  * If $p = 10$, $l = 0.63$
  * If $p = 100$, $l = 0.955$

This implies that if we have 100 dimensions, then the nearest 10 observations would be 0.955 away from the target point at each dimension, this is almost at the other corner of the cube. Hence there will be a very large bias. Decreasing $k$ does not help much in this situation since even the closest point can be really far away, and the model would have large variance. 

<center>
![](images/highd.png){width=30%}
</center>

However, why our model performs well in the handwritten digit example? There is possibly (approximately) a lower dimensional representation of the data so that when you evaluate the distance on the high-dimensional space, it is just as effective as working on the low dimensional space. Dimension reduction is an important topic in statistical learning and machine learning. Many methods, such as sliced inverse regression [@li1991sliced] and UMAP [@mcinnes2018umap] have been developed based on this concept.

<center>
![](images/manifold.png){width=70%}

Image from @cayton2005algorithms.
</center>

<!--chapter:end:03.2-knn.Rmd-->

\def\cD{\cal{D}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bzero{\mathbf{0}}
\def\bbeta{\boldsymbol \beta}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Kernel Smoothing

Fundamental ideas of local regression approaches are similar to $k$NN. But most approaches would address a fundamental drawback of $k$NN that the estimated function is not smooth. Having a smoothed estimation would also allow us to estimate the derivative, which is essentially used when estimating the density function. We will start with the intuition of the kernel estimator and then discuss the bias-variance trade-off using kernel density estimation as an example. 

## KNN vs. Kernel

We first compare the $K$NN method with a Gaussian kernel regression. $K$NN has jumps while Gaussian kernel regression is smooth. 

```{r echo = FALSE}
  par(mfrow = c(1, 2))
```
 
```{r fig.dim=c(12, 5), out.width = '90%', echo = FALSE}
    
    library(kknn)
    set.seed(1)
    
    # generate some data
    x <- runif(40, 0, 2*pi)
    y <- 2*sin(x) + rnorm(length(x))
    
    testx = seq(0, 2*pi, 0.01)
    
    # compare two different kernels: rectangular or Epanechnikov
    
    knn.fit = kknn(y ~ x, train = data.frame("x" = x, "y" = y),
                   test = data.frame("x" = testx),
                   k = 10, kernel = "rectangular")
    
    plot(x, y, xlim = c(0, 2*pi), cex = 1.5, xlab = "", ylab = "", cex.lab = 1.5, pch = 19)
    title(main=paste("KNN"), cex.main = 1.5)
    lines(testx, 2*sin(testx), col = "deepskyblue", lwd = 3)
    lines(testx, knn.fit$fitted.values, type = "s", col = "darkorange", lwd = 3)
    box()
    
    # ksmooth() is a built-in function in base R
    ksmooth.fit = ksmooth(x, y, bandwidth = 1, kernel = "normal", x.points = testx)
    
    plot(x, y, xlim = c(0, 2*pi), cex = 1.5, xlab = "", ylab = "", cex.lab = 1.5, pch = 19)
    title(main=paste("Gaussian kernel"), cex.main = 1.5)
    lines(testx, 2*sin(testx), col = "deepskyblue", lwd = 3)
    lines(testx, ksmooth.fit$y, type = "s", col = "darkorange", lwd = 3)
    box()
```

## Kernel Density Estimations

A natural estimator, by using the counts, is 

$$\widehat f(x) = \frac{\#\big\{x_i: x_i \in [x - \frac{h}{2}, x + \frac{h}{2}]\big\}}{h n}$$
This maybe compared with the histogram estimator 

```{r fig.dim=c(12, 5), out.width = '90%'}
    library(ggplot2)
    data(mpg)
    
    # histogram 
    hist(mpg$hwy, breaks = seq(6, 50, 2))
    
    # uniform kernel
    xgrid = seq(6, 50, 0.1)
    histden = sapply(xgrid, FUN = function(x, obs, h) sum( ((x-h/2) <= obs) * ((x+h/2) > obs))/h/length(obs), 
                     obs = mpg$hwy, h = 2)
    
    plot(xgrid, histden, type = "s")
```

This can be view in two ways. The easier interpretation is that, for each target point, we count how many observations are close-by. We can also interpret it as evenly distributing the point-mass of each observation to a close-by region with width $h$, and then stack them all. 

$$\widehat f(x) = \frac{1}{h n} \sum_i \,\underbrace{ \mathbf{1} \Big(x \in [x_i - \frac{h}{2}, x_i + \frac{h}{2}]}_{\text{uniform density centered at } x_i} \Big)$$
Here is a close-up demonstration of how those uniform density functions are stacked for all observations.

```{r echo = FALSE}
  par(mfrow = c(1, 1))
```

```{r fig.dim = c(9, 6), out.width = "55%", echo = FALSE}
    # a toy example for density estimation 
    set.seed(3)
    par(mar=rep(2, 4))
    n=5
    x <- c(rnorm(n), rnorm(n, 4, 2))
    xgrid = seq(-2, 8, 0.01)
    plot(xgrid, 0.5*dnorm(xgrid, 0, 1) + 0.5* dnorm(xgrid, 4, 2), 
         type = "l", lwd = 3, col = "deepskyblue", 
         xlab = "x", ylab = "density", ylim = c(0, 0.25))
    
    # the observed points 
    for (i in 1:length(x))
      segments(x[i], 0, x[i], 1/length(x), lty = 1, lwd = 3)
    
    # Let's try a uniform kernel, with width = 2
    bw = 1
    den = matrix(NA, length(x), length(xgrid))
    for (i in 1:length(x))
      den[i, ] = (abs(xgrid - x[i]) <= bw)/length(x)/2/bw
    
    lines(xgrid, colSums(den), type = "l", lwd = 3, col = "darkorange")
```

However, this is will lead to jumps at the end of each small uniform density. Let's consider using a smooth function instead. Naturally, we can use the Gaussian kernel function to calculate the numerator in the above equation. 

```{r fig.dim = c(9, 6), out.width = "55%", echo = FALSE}
  par(mar=rep(2, 4))
  plot(xgrid, 0.5*dnorm(xgrid, 0, 1) + 0.5* dnorm(xgrid, 4, 2), 
       type = "l", lwd = 3, col = "deepskyblue", 
       xlab = "x", ylab = "density", ylim = c(0, 0.25))
  
  # the observed points 
  for (i in 1:length(x))
    segments(x[i], 0, x[i], 1/length(x), lty = 1, lwd = 3)
  
  # Gaussian kernel, with width = 2
  bw = 0.75
  den = matrix(NA, length(x), length(xgrid))
  
  for (i in 1:length(x))
  {
      den[i, ] = exp(-0.5*(x[i] - xgrid)^2 / bw^2)/sqrt(2*pi)/bw/length(x)
      points(xgrid, den[i, ], type = "l")
  }
  
  lines(xgrid, colSums(den), type = "l", lwd = 3, col = "darkorange")
```

We apply this to the `mpg` dataset. 

```{r fig.dim = c(9, 6), out.width = "55%"}
  xgrid = seq(6, 50, 0.1)
  kernelfun <- function(x, obs, h) sum(exp(-0.5*((x-obs)/h)^2)/sqrt(2*pi)/h)
  plot(xgrid, sapply(xgrid, FUN = kernelfun, obs = mpg$hwy, h = 1.5)/length(mpg$hwy), type = "l",
       xlab = "MPG", ylab = "Estimated Density", col = "darkorange", lwd = 3)
```

The `ggplot2` packages provides some convenient features to plot the density and histogram. 

```{r message=FALSE}
  ggplot(mpg, aes(x=hwy)) + 
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
    geom_density(alpha=.2, fill="#ff8c00")
```

## Bias-variance trade-off

Let's consider estimating a density, using the Parzen estimator 

$$\widehat f(x) = \frac{1}{n} \sum_{i=1}^n K_{h} (x, x_i)$$
here, $K_h(\bu, \bv) = K(|\bu - \bv|/h)/h$ is a kernel function that satisfies 

  * $\int K(u)du = 1$ (a proper density)
  * $K(-u) = K(u)$ (symmetric)
  * $\int K(u) u^2 du \leq \infty$ (finite second moment)

Note that $h$ simply scales the covariate and adjust the density accordingly. Our goal is to estimate a target point $x$ using a set of iid data. First, we can analyze the bias:

\begin{align}
    \E\big[ \widehat f(x) \big] &= \E\left[ K\left( \frac{x - x_1}{h} \right) \Big/ h \right] \\
    &= \int_{-\infty}^\infty \frac{1}{h} K\left(\frac{x-x_1}{h}\right) f(x_1) d x_1 \\
    &= \int_{\infty}^{-\infty} \frac{1}{h} K(t) f(x - th) d (x-th) \\
    (\text{Taylor expansion}) \quad &= f(x) + \frac{h^2}{2} f''(x) \int_{-\infty}^\infty K(t) t^2 dt + o(h^2) \\
    (\text{as } ha \rightarrow 0) \quad &\rightarrow f(x)
\end{align}

Since the density is over the entire domain, we can define the integrated Bias$^2$:

\begin{align}
\text{Bias}^2 &= \int \left( E[\widehat f(x)] - f(x)\right)^2 dx \\
    &\approx \frac{h^4 \sigma_K^4}{4} \int \big[ f''(x)\big]^2 dx
\end{align}
where $\sigma_K^2 = \int_{-\infty}^\infty K(t) t^2 dt$.

On the other hand, the variance term is 

\begin{align}
  \Var\big[ \widehat f(x) \big] &= \frac{1}{n} \Var \Big[\frac{1}{h}K\big( \frac{x - x_1}{h} \big) \Big] \\
  &= \frac{1}{n} \E \bigg[ \frac{1}{h^2} K^2\big( \frac{x - x_1}{h}\big) - \E\Big[ \frac{1}{h} K\big( \frac{x - x_1}{h} \big)\Big]^2 \bigg]\\
  &= \frac{1}{n} \Big[ \int \frac{1}{h} K^2( \frac{x - x_1}{h} ) f(x_1) dx_1 + O(1) \Big] \\
  &= \frac{1}{n} \Big[ \frac{1}{h} \int K^2( u ) f(x) du + O(1) \Big] \\
  &= \frac{f(x)}{nh} \int K^2( u ) du 
\end{align}

with the integrated variance being 

$$\frac{1}{nh} \int K^2( u ) dt $$

By minimizing the asymptotic mean integrated squared error (AMISE), defined as the sum of integrated Bias$^2$ and variance, we have the optimal $h$ being 

$$h^\text{opt} = \bigg[\frac{1}{n} \frac{\int K^2(u)du}{ \sigma^2_K \int f''(u)du} \bigg]^{1/5},$$ 

and the optimal $h$ is in the order of $\cal O(n^{-4/5})$.

## Gaussian Kernel Regression

A Nadaraya-Watson kernel regression model has the following formula. Note that we use $h$ as the bandwidth instead of $h$. 

$$\widehat f(x) = \frac{\sum_i K_h(x, x_i) y_i}{\sum_i K_h(x, x_i)},$$
where $h$ is the bandwidth. At each target point $x$, training data $x_i$s that are closer to $x$ receives higher weights $K_h(x, x_i)$, hence their $y_i$ values are more influential in terms of estimating $f(x)$. For the Gaussian kernel, we use 

$$K_h(x, x_i) = \frac{1}{h\sqrt{2\pi}} \exp\left\{ -\frac{(x - x_i)^2}{2 h^2}\right\}$$

```{r fig.dim = c(12, 8), out.width = '90%', echo = FALSE}
    par(mfrow = c(2, 2))

    # generate some data
    set.seed(1)    
    x <- runif(40, 0, 2*pi)
    y <- 2*sin(x) + rnorm(length(x))
    testx = seq(0, 2*pi, 0.01)
    
    # plots for different h values
    for (x0 in c(2, 3, 4, 5))
    {
        # predicting the point at x_0
        plot(x, y, xlim = c(-0.25, 2*pi+0.25), cex = 3*dnorm(x, x0), xlab = "", ylab = "", 
             cex.lab = 1.5, pch = 19, xaxt='n', yaxt='n')
        title(main=paste("Kernel average at x =", x0), cex.main = 1.5)
        lines(testx, 2*sin(testx), col = "deepskyblue", lwd = 3)
        lines(testx, ksmooth.fit$y, type = "s", col = "darkorange", lwd = 3)
        points(x0, ksmooth.fit$y[testx == x0], col = "red", pch = 18, cex = 3)
        
        cord.x <- seq(-0.25, 2*pi+0.25, 0.01)
        cord.y <- 3*dnorm(cord.x, x0) - 3 # Gaussian density with h = 1
     
        # The Gaussian Kernel Function in the shaded area
        polygon(c(-0.25, cord.x, 2*pi+0.25),
                c(-3, cord.y, -3),
                col=rgb(0.5, 0.5, 0.5, 0.5), 
                border = rgb(0.5, 0.5, 0.5, 0.5))
    }
```

### Bias-variance Trade-off

The bandwidth $h$ is an important tuning parameter that controls the bias-variance trade-off. It behaves the same as the density estimation. By setting a large $h$, the estimator is more stable but has more bias.

```{r echo = FALSE}
  par(mfrow = c(1, 1))
```

```{r fig.dim = c(8, 6)}
  # a small bandwidth
  ksmooth.fit1 = ksmooth(x, y, bandwidth = 0.5, kernel = "normal", x.points = testx)

  # a large bandwidth
  ksmooth.fit2 = ksmooth(x, y, bandwidth = 2, kernel = "normal", x.points = testx)
  
  # plot both
  plot(x, y, xlim = c(0, 2*pi), pch = 19, xaxt='n', yaxt='n')
  lines(testx, 2*sin(testx), col = "deepskyblue", lwd = 3)
  lines(testx, ksmooth.fit1$y, type = "s", col = "darkorange", lwd = 3)
  lines(testx, ksmooth.fit2$y, type = "s", col = "red", lwd = 3)
  legend("topright", c("h = 0.5", "h = 2"), col = c("darkorange", "red"), 
         lty = 1, lwd = 2, cex = 1.5)
```

## Choice of Kernel Functions 

Other kernel functions can also be used. The most efficient kernel is the Epanechnikov kernel, which will minimize the mean integrated squared error (MISE). The efficiency is defined as 

$$ \Big(\int u^2K(u) du\Big)^\frac{1}{2}  \int K^2(u) du, $$
Different kernel functions can be visualized in the following. Most kernels are bounded within $[-h/2, h/2]$, except the Gaussian kernel. 

```{r fig.dim = c(8, 6), echo=FALSE}
    x = seq(-1.5, 1.5, 0.01)
    KerFuns = cbind(0.5*(abs(x) <= 1), 
                    (1 - abs(x))*(abs(x) <= 1), 
                    0.75*(1-x^2)*(abs(x) <= 1), 
                    35/32*(1-x^2)^3*(abs(x) <= 1), 
                    exp(-0.5*(x)^2)/sqrt(2*pi))
    
    colnames(KerFuns) = c("Uniform", "Triangular", "Epanechnikov",
                          "Triweight", "Gaussian")

    par(mar=rep(2, 4))
    matplot(x, KerFuns, type = "l", col = c(2,3,4,6, "darkorange"), 
            lty = 1, lwd = 3, ylim = c(0, 1.1))
    legend("topleft", legend = colnames(KerFuns), col = c(2,3,4,6,"darkorange"), 
           lty = 1, lwd = 3, cex = 1.6)
```

## Local Linear Regression

Local averaging will suffer severe bias at the boundaries. One solution is to use the local polynomial regression. The following examples are local linear regressions, evaluated as different target points. We are solving for a linear model weighted by the kernel weights

$$\sum_{i = 1}^n K_h(x, x_i) \big( y_i - \beta_0 - \beta_1 x_i \big)^2$$

```{r fig.dim=c(12, 8), out.width = '90%', echo=FALSE}
    # generate some data
    set.seed(1)
    n = 150
    x <- seq(0, 2*pi, length.out = n)
    y <- 2*sin(x) + rnorm(length(x))
    
    #Silverman optimal bandwidth for univariate regression
    h = 1.06*sd(x)*n^(-1/5) 

    par(mfrow = c(2, 2), mar=rep(2, 4))
        
    for (x0 in c(0, pi, 1.5*pi, 2*pi))
    {
        # Plotting the data
        plot(x, y, xlim = c(0, 2*pi), cex = 3*h*dnorm(x, x0, h), xlab = "", ylab = "", 
             cex.lab = 1.5, pch = 19, xaxt='n', yaxt='n')
        title(main=paste("Local Linear Regression at x =", round(x0, 3)), cex.main = 1.5)
        lines(x, 2*sin(x), col = "deepskyblue", lwd = 3)
        
        # kernel smoother
        ksmooth.fit = ksmooth(x, y, bandwidth = h, kernel = "normal", x.points = x)
        lines(x, ksmooth.fit$y, type = "l", col = "darkorange", lwd = 3)
          
        # local linear
        K = exp(-0.5*((x - x0)/h)^2)/sqrt(2*pi)/h
        wX = sweep(cbind(1, x), 1, sqrt(K), FUN = "*")
        wy = y*sqrt(K)
        b = solve(t(wX) %*% wX) %*% t(wX) %*% wy
    
        segments(x0 - h, b[1]+(x0-h)*b[2], x0 + h, b[1]+(x0+h)*b[2], 
                 lwd = 2, col = "red")
        points(x0, b[1]+x0*b[2], col = "red", pch = 18, cex = 3)
        
        # The Gaussian Kernel Function
        cord.x <- seq(x0-3*h, x0+3*h, 0.01)
        cord.y <- 3*h*dnorm(cord.x, x0, h) - 3
        polygon(cord.x, cord.y, col=rgb(0.5, 0.5, 0.5, 0.5), border = rgb(0.5, 0.5, 0.5, 0.5))
        legend("topright", c("training data", "kernel smoother", "local linear"), lty = c(0, 1, 1), 
               col = c(1, "darkorange", "red"), lwd = 2, pch = c(19, NA, 18), cex = 1.5)
    }
```

## Local Polynomial Regression

The following examples are local polynomial regressions, evaluated as different target points. We can easily extend the local linear model to inccorperate higher orders terms:

$$\sum_{i=1}^n K_h(x, x_i) \Big[ y_i - \beta_0(x) - \sum_{r=1}^d \beta_j(x) x_i^r \Big]^2$$

The followings are local quadratic fittings, which will further correct the bias.

```{r echo = FALSE}
  par(mfrow = c(2, 2))
```

```{r fig.dim=c(12, 8), out.width = '90%', echo = FALSE}
    # local quadratic regression
    for (x0 in c(0, pi, 1.5*pi, 2*pi))
    {
        # Plotting the data
        plot(x, y, xlim = c(-0.25, 2*pi+0.25), cex = 3*h*dnorm(x, x0, h), xlab = "", ylab = "", 
             cex.lab = 1.5, pch = 19, xaxt='n', yaxt='n')
        title(main=paste("Local Quadratic Regression at x =", round(x0, 3)), cex.main = 1.5)
        lines(x, 2*sin(x), col = "deepskyblue", lwd = 3)
        
        # kernel smoother
        ksmooth.fit = ksmooth(x, y, bandwidth = h, kernel = "normal", x.points = x)
        lines(x, ksmooth.fit$y, type = "l", col = "darkorange", lwd = 3)
          
        # local linear
        K = exp(-0.5*((x - x0)/h)^2)/sqrt(2*pi)/h
        wX = sweep(cbind(1, x, x^2), 1, sqrt(K), FUN = "*")
        wy = y*sqrt(K)
        b = solve(t(wX) %*% wX) %*% t(wX) %*% wy
    
        points(seq(x0-h, x0+h, 0.01), b[1] + b[2]*seq(x0-h, x0+h, 0.01) +
               b[3]*seq(x0-h, x0+h, 0.01)^2, col = "red", type = "l", lwd = 3)
        points(x0, b[1]+x0*b[2]+x0^2*b[3], col = "red", pch = 18, cex = 3)
        
        # The Gaussian Kernel Function
        cord.x <- seq(x0-3*h, x0+3*h, 0.01)
        cord.y <- 3*h*dnorm(cord.x, x0, h) - 3
        
        # shade the area
        polygon(c(-0.25, cord.x, 2*pi+0.25),
                c(-3, cord.y, -3),
                col=rgb(0.5, 0.5, 0.5, 0.5), 
                border = rgb(0.5, 0.5, 0.5, 0.5))
        
        legend("topright", c("training data", "kernel smoother", "local quadratic"), 
               lty = c(0, 1, 1), col = c(1, "darkorange", "red"), 
               lwd = 2, pch = c(19, NA, 18), cex = 1.5)
    }
```

## R Implementations 

Some popular `R` functions implements the local polynomial regressions: `loess`, `locfit`, `locploy`, etc. These functions automatically calculate the fitted value for each target point (essentially all the observed points). This can be used in combination with `ggplot2`. The point-wise confidence intervals are also calculated. 

```{r}
    ggplot(mpg, aes(displ, hwy)) + geom_point() +
      geom_smooth(col = "red", method = "loess", span = 0.5)
```

A toy example that compares different bandwidth. Be careful that different methods may formulat the bandwidth parameter in different ways.  

```{r echo = FALSE}
  par(mfrow = c(1, 1))
```

```{r message=FALSE}
    # local polynomial fitting using locfit and locpoly
    
    library(KernSmooth)
    library(locfit)
    
    n <- 100
    x <- runif(n,0,1)
    y <- sin(2*pi*x)+rnorm(n,0,1)
    y = y[order(x)]
    x = sort(x)
    
    plot(x, y, pch = 19)
    points(x, sin(2*pi*x), lwd = 3, type = "l", col = 1)
    lines(locpoly(x, y, bandwidth=0.15, degree=2), col=2, lwd = 3)
    lines(locfit(y~lp(x, nn = 0.3, h=0.05, deg=2)), col=4, lwd = 3)
    
    legend("topright", c("locpoly", "locfit"), col = c(2,4), lty = 1, cex = 1.5, lwd =2)
```






<!--chapter:end:03.3-kernel.Rmd-->

\def\cD{\cal{D}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bzero{\mathbf{0}}
\def\bbeta{\boldsymbol \beta}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# (PART) Classification Models {-}

# Logistic Regression

## Modeling Binary Outcomes

To model binary outcomes using a logistic regression, we will use the 0/1 coding of $Y$. We need to set its connection with covariates. Recall in a linear regression, the outcome is continuous, and we set 

$$Y = \beta_0 + \beta_1 X + \epsilon$$
However, this does not work for classification since $Y$ can only be 0 or 1. Hence we turn to consider modeling the probability $P(Y = 1 | X = \bx)$. Hence, $Y$ is a Bernoulli random variable given $X$, and this is modeled by a function of $X$: 

$$ P(Y = 1 | X = \bx) = \frac{\exp(\bx^\T \bbeta)}{1 + \exp(\bx^\T \bbeta)}$$
Note that although $\bx^\T \bbeta$ may ranges from 0 to infinity as $X$ changes, the probability will still be bounded between 0 and 1. This is an example of __Generalized Linear Models__. The relationship is still represented using a linear function of $\bx$, $\bx^\T \bbeta$. This is called a __logit link__ function (a function to connect the conditional expectation of $Y$ with $\bbeta^\T \bx$):

$$\eta(a) = \frac{\exp(a)}{1 + \exp(a)}$$
Hence, $P(Y = 1 | X = \bx) = \eta(\bx^\T \bbeta)$. We can reversely solve this and get 

\begin{aligned}
P(Y = 1 | X = \bx) = \eta(\bx^\T \bbeta) &= \frac{\exp(\bx^\T \bbeta)}{1 + \exp(\bx^\T \bbeta)}\\
1 - \eta(\bx^\T \bbeta) &= \frac{1}{1 + \exp(\bx^\T \bbeta)} \\
\text{Odds} = \frac{\eta(\bx^\T \bbeta)}{1-\eta(\bx^\T \bbeta)} &= \exp(\bx^\T \bbeta)\\
\log(\text{Odds}) = \bx^\T \bbeta
\end{aligned}

Hence, the parameters in a logistic regression is explained as __log odds__. Let's look at a concrete example. 

## Example: Cleveland Clinic Heart Disease Data

We use use the [Cleveland clinic heart disease dataset](https://www.kaggle.com/aavigan/cleveland-clinic-heart-disease-dataset). The goal is to model and predict a class label of whether the patient has a hearth disease or not. This is indicated by whether the `num` variable is $0$ (no presence) or $>0$ (presence). 

```{r}
  heart = read.csv("data/processed_cleveland.csv")
  heart$Y = as.factor(heart$num > 0)
  table(heart$Y)
```

Let's model the probability of heart disease using the `Age` variable. This can be done using the `glm()` function, which stands for the Generalized Linear Model. The syntax of `glm()` is almost the same as a linear model. Note that it is important to use `family = binomial` to specify the logistic regression. 

```{r}
  logistic.fit <- glm(Y~age, data = heart, family = binomial)
  summary(logistic.fit)
```

The result is similar to a linear regression, with some differences. The parameter estimate of age is 0.05199. It is positive, meaning that increasing age would increase the change of having heart disease. However, this does not mean that 1 year older would increase the change by 0.05. Since, by our previous formula, the probably is not directly expressed as $\bx^\T \bbeta$. 

This calculation can be realized when predicting a new target point. Let's consider a new subject with `Age = 55`. What is the predicted probability of heart disease? Based on our formula, we have 

$$\beta_0 + \beta_1 X = -3.00591 + 0.05199 \times 55 = -0.14646$$
And the estimated probability is 

$$ P(Y = 1 | X) = \frac{\exp(\beta_0 + \beta_1 X)}{1 + \exp(\beta_0 + \beta_1 X)} = \frac{\exp(-0.14646)}{1 + \exp(-0.14646)} = 0.4634503$$
Hence, the estimated probability for this subject is 46.3%. This can be done using R code. Please note that if you want to predict the probability, you need to specify `type = "response"`. Otherwise, only $\beta_0 + \beta_1 X$ is provided.

```{r}
  testdata = data.frame("age" = 55)
  predict(logistic.fit, newdata = testdata)
  predict(logistic.fit, newdata = testdata, type = "response")
```

If we need to make a 0/1 decision about this subject, a natural idea is to see if the predicted probability is greater than 0.5. In this case, we would predict this subject as 0.

## Interpretation of the Parameters

Recall that $\bx^\T \bbeta$ is the log odds, we can further interpret the effect of a single variable. Let's define the following two, with an arbitrary age value $a$:

  * A subject with `age` $= a$
  * A subject with `age` $= a + 1$

Then, if we look at the __odds ratio__ corresponding to these two target points, we have 

\begin{aligned}
\text{Odds Ratio} &= \frac{\text{Odds in Group 2}}{\text{Odds in Group 1}}\\
&= \frac{\exp(\beta_0 + \beta_1 (a+1))}{\exp(\beta_0 + \beta_1 a)}\\
&= \frac{\exp(\beta_0 + \beta_1 a) \times \exp(\beta_1)}{\exp(\beta_0 + \beta_1 a)}\\
&= \exp(\beta_1)
\end{aligned}

Taking $\log$ on both sides, we have 

$$\log(\text{Odds Ratio}) = \beta_1$$

Hence, the odds ratio between these two subjects (__they differ only with one unit of `age`__) can be directly interpreted as the exponential of the parameter of `age`. After taking the log, we can also say that 

> The parameter $\beta$ of a varaible in a logistic regression represents the __log of odds ratio__ associated with one-unit increase of this variable. 

Please note that we usually do not be explicit about what this odds ratio is about (what two subject we are comparing). Because the interpretation of the parameter does not change regardless of the value $a$, as long as the two subjects differ in one unit. 

And also note that this conclusion is regardless of the values of other covaraites. When we have a multivariate model, as long as all other covariates are held the same, the previous derivation will remain the same. 

## Solving a Logistic Regression 

The logistic regression is solved by maximizing the log-likelihood function. Note that the log-likelihood is given by 

$$\ell(\bbeta) = \sum_{i=1}^n \log \, p(y_i | x_i, \bbeta).$$
Using the probabilities of Bernoulli distribution, we have 

\begin{align}
\ell(\bbeta) =& \sum_{i=1}^n \log \left\{ \eta(\bx_i)^{y_i} [1-\eta(\bx_i)]^{1-y_i} \right\}\\
    =& \sum_{i=1}^n y_i \log \frac{\eta(\bx_i)}{1-\eta(\bx_i)} + \log [1-\eta(\bx_i)] \\
    =& \sum_{i=1}^n y_i \bx_i^\T \bbeta - \log [ 1 + \exp(\bx_i^\T \bbeta)]
\end{align}

Since this objective function is relatively simple, we can use Newton's method to update. The gradient is given by 

$$\frac{\partial \ell(\bbeta)}{\partial \bbeta} =~ \sum_{i=1}^n y_i \bx_i^\T - \sum_{i=1}^n \frac{\exp(\bx_i^\T \bbeta) \bx_i^\T}{1 + \exp(\bx_i^\T \bbeta)},$$

and the Hessian matrix is given by 

$$\frac{\partial^2 \ell(\bbeta)}{\partial \bbeta \partial \bbeta^\T} =~ - \sum_{i=1}^n \bx_i \bx_i^\T \eta(\bx_i) [1- \eta(\bx_i)].$$
This leads to the update 

$$\bbeta^{\,\text{new}} = \bbeta^{\,\text{old}} - \left[\frac{\partial^2 \ell(\bbeta)}{\partial \bbeta \partial \bbeta^\T}\right]^{-1} \frac{\partial \ell(\bbeta)}{\partial \bbeta}$$


## Example: South Africa Heart Data

We use the South Africa heart data as a demonstration. The goal is to estimate the probability of `chd`, the indicator of coronary heart disease. 

```{r}
    library(ElemStatLearn)
    data(SAheart)
    
    heart = SAheart
    heart$famhist = as.numeric(heart$famhist)-1
    n = nrow(heart)
    p = ncol(heart)
    
    heart.full = glm(chd~., data=heart, family=binomial)
    round(summary(heart.full)$coef, dig=3)
    
    # fitted value 
    yhat = (heart.full$fitted.values>0.5)
    table(yhat, SAheart$chd)
```

Based on what we learned in class, we can solve this problem ourselves using numerical optimization. Here we will demonstrate an approach that uses general solver `optim()`. First, write the objective function of the logistic regression, for any value of $\boldsymbol \beta$, $\mathbf{X}$ and $\mathbf{y}$.

```{r}
    # the negative log-likelihood function of logistic regression 
    my.loglik <- function(b, x, y)
    {
        bm = as.matrix(b)
        xb =  x %*% bm
        # this returns the negative loglikelihood
        return(sum(y*xb) - sum(log(1 + exp(xb))))
    }

    # Gradient
    my.gradient <- function(b, x, y)
    {
        bm = as.matrix(b) 
        expxb =  exp(x %*% bm)
        return(t(x) %*% (y - expxb/(1+expxb)))
    }
```

Let's check the result of this function for some arbitrary $\boldsymbol \beta$ value.  

```{r}
    # prepare the data matrix, I am adding a column of 1 for intercept
    
    x = as.matrix(cbind("intercept" = 1, heart[, 1:9]))
    y = as.matrix(heart[,10])
    
    # check my function
    b = rep(0, ncol(x))
    my.loglik(b, x, y) # scalar
    
    # check the optimal value and the likelihood
    my.loglik(heart.full$coefficients, x, y)
```

Then we optimize this objective function 

```{r}
    # Use a general solver to get the optimal value
    # Note that we are doing maximization instead of minimization, 
    # we need to specify "fnscale" = -1
    optim(b, fn = my.loglik, gr = my.gradient, 
          method = "BFGS", x = x, y = y, control = list("fnscale" = -1))
```

This matches our `glm()` solution. Now, if we do not have a general solver, we should consider using the Newton-Raphson. You need to write a function to calculate the Hessian matrix and proceed with an optimization update.

```{r, echo = FALSE, results = 'hide'}
    # Hessian
    my.hessian <- function(b, x, y)
    {
    	bm = as.matrix(b) 
    	expxb =  exp(x %*% bm)
    	x1 = sweep(x, 1, expxb/(1+expxb)^2, "*")
    	return(-t(x) %*% x1)
    }

    # check my functions to make sure the dimensions match
    b = rep(0, ncol(x))
    my.loglik(b, x, y) # scalar
    my.gradient(b, x, y) # p by 1 matrix
    my.hessian(b, x, y) # p by p matrix

    my.logistic <- function(b, x, y, tol = 1e-10, maxitr = 30, gr, hess, verbose = FALSE)
    {
        b_new = b
        
        for (j in 1:maxitr) # turns out you don't really need many iterations
        {
        	b_old = b_new
        	b_new = b_old - solve(hess(b_old, x, y)) %*% gr(b_old, x, y)
        	
        	if (verbose)
        	{
        	    cat(paste("at iteration ", j,", current beta is \n", sep = ""))
        	    cat(round(b_new, 3))
        	    cat("\n")
        	}    
        	if (sum(abs(b_old - b_new)) < 1e-10) break;
        }
        return(b_new)
    }
```

```{r}
    # my Newton-Raphson method
    # set up an initial value
    # this is sometimes crucial...
    
    b = rep(0, ncol(x))
    
    mybeta = my.logistic(b, x, y, tol = 1e-10, maxitr = 20, 
                         gr = my.gradient, hess = my.hessian, verbose = TRUE)
    
    # the parameter value
    mybeta
    # get the standard error estimation 
    mysd = sqrt(diag(solve(-my.hessian(mybeta, x, y))))    
```

With this solution, I can then get the standard errors and the p-value. You can check them with the `glm()` function solution. 

```{r}
    # my summary matrix
    round(data.frame("beta" = mybeta, "sd" = mysd, "z" = mybeta/mysd, 
    	  "pvalue" = 2*(1-pnorm(abs(mybeta/mysd)))), dig=5)
	  
    # check that with the glm fitting 
    round(summary(heart.full)$coef, dig=5)
```

## Penalized Logistic Regression

Similar to a linear regression, we can also apply penalties to a logistic regression to address collinearity problems or select variables in a high-dimensional setting. For example, if we use the Lasso penalty, the objective function is 

$$\sum_{i=1}^n \log \, p(y_i | x_i, \bbeta) + \lambda |\bbeta|_1$$
This can be done using the `glmnet` package. Specifying `family = "binomial"` will ensure that a logistic regression is used, even your `y` is not a factor (but as numerical 0/1). 

```{r}
  library(glmnet)
  lasso.fit = cv.glmnet(x = data.matrix(SAheart[, 1:9]), y = SAheart[,10], 
                        nfold = 10, family = "binomial")
  
  plot(lasso.fit)
```

The procedure is essentially the same as in a linear regression. And we could obtain the estimated parameters by selecting the best $\lambda$ value. 

```{r}
  coef(lasso.fit, s = "lambda.min")
```



<!--chapter:end:04.1-logistic.Rmd-->

\def\cD{\cal{D}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bzero{\mathbf{0}}
\def\bbeta{\boldsymbol \beta}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Discriminant Analysis

When we model the probability of $Y$ given $X$, such as using a logistic regression, the approach is often called a soft classification, meaning that we do not directly produce the class label for prediction. However, we can also view the task as finding a function, with 0/1 as the output. In this case, the function is called a __classifier__:

$$f : \mathbb{R}^p \longrightarrow \{0, 1\}$$
In this case, we can directly evaluate the prediction error, which is calculated from the __0-1 loss__:

$$L\big(f(\bx), y \big) = \begin{cases}
0 \quad \text{if} \quad y = f(\bx)\\
1 \quad \text{o.w.}
\end{cases}$$

The goal is to minimize the overall __risk__, the integrated loss:

$$\text{R}(f) = \E_{X, Y} \left[ L\big(f(X), Y\big) \right]$$
Continuing the notation from the logistic regression, with $\eta(\bx) = \Prob(Y = 1 | X = \bx)$, we can easily see the decision rule to minimize the risk is to take the dominate label for any given $\bx$, this leads to the __Bayes rule__:

\begin{align}
f_B(\bx) = \underset{f}{\arg\min} \,\, \text{R}(f) =
    \begin{cases}
    1 & \text{if} \quad \eta(\bx) \geq 1/2 \\
    0 & \text{if} \quad \eta(\bx) < 1/2. \\
    \end{cases}
\end{align}

Note that it doesn't matter when $\eta(\bx) = 1/2$ since we will make 50% mistake anyway. The risk associated with this rule is called the __Bayes risk__, which is the best risk we could achieve with a classification model with 0/1 loss. 

## Bayes Rule

The essential idea of Discriminant Analysis is to estimate the densities functions of each class, and compare the densities at any given target point to perform classification. Let's construct the Bayes rule from the Bayes prospective:

\begin{align}
  \Prob(Y = 1 | X = \bx) &= \frac{\Prob(X = \bx | Y = 1)\Prob(Y = 1)}{\Prob(X = \bx)} \\
  \Prob(Y = 0 | X = \bx) &= \frac{\Prob(X = \bx | Y = 0)\Prob(Y = 0)}{\Prob(X = \bx)}
\end{align}

Lets further define marginal probabilities (__prior__) $\pi_1 = P(Y = 1)$ and $\pi_0 = 1 - \pi_1 = P(Y = 0)$, then, denote the conditional densities of $X$ as 

\begin{align}
  f_1 = \Prob(X = \bx| Y = 1)\\
  f_0 = \Prob(X = \bx| Y = 0)\\
\end{align}

Note that the Bayes rule suggests to make the decision 1 when $\eta(\bx) \geq 1/2$, this is equivalent to $\pi_1 > \pi_0$. Utilizing the __Bayes Theorem__, we have 

\begin{align}
f_B(\bx) = \underset{f}{\arg\min} \,\, \text{R}(f) =
    \begin{cases}
    1 & \text{if} \quad \pi_1 f_1(\bx) \geq \pi_0 f_0(\bx) \\
    0 & \text{if} \quad \pi_1 f_1(\bx) < \pi_0 f_0(\bx). \\
    \end{cases}
\end{align}

This suggests that we can model the conditional density of $X$ given $Y$ instead of modeling $P(Y | X)$ to make the decision. 

## Example: Linear Discriminant Analysis (LDA)

We create two density functions that use the __same covariance matrix__: $X_1 \sim \cal{N}(\mu_1, \Sigma)$ and $X_2 \sim \cal{N}(\mu_2, \Sigma)$, with $\mu_1 = (0.5, -1)^\T$, $\mu_2 = (-0.5, 0.5)^\T$, and $\Sigma_{2\times2}$ has diagonal elements 1 and off diagonal elements 0.5. Let's first generate some observations.

```{r}
  library(mvtnorm)
  set.seed(1)
  
  # generate two sets of samples
  Sigma = matrix(c(1, 0.5, 0.5, 1), 2, 2)
  mu1 = c(0.5, -1)
  mu2 = c(-0.5, 0.5)
  
  # define prior
  p1 = 0.4 
  p2 = 0.6
    
  n = 1000
  
  Class1 = rmvnorm(n*p1, mean = mu1, sigma = Sigma)
  Class2 = rmvnorm(n*p2, mean = mu2, sigma = Sigma)

  plot(Class1, pch = 19, col = "darkorange", xlim = c(-4, 4), ylim = c(-4, 4))
  points(Class2, pch = 19, col = "deepskyblue")
```

If we know their true density functions, then the decision line is linear. 

```{r, message=FALSE, out.width = "75%", echo = FALSE}
    library(plotly)

    # generate two densities 
    x1 = seq(-2.5, 2.5, 0.1)
    x2 = seq(-2.5, 2.5, 0.1)
    data = expand.grid(x1, x2)
    
    # the density function after removing some constants
    sigma_inv = solve(Sigma)
    d1 = apply(data, 1, function(x) exp(-0.5 * t(x - mu1) %*% sigma_inv %*% (x - mu1))*p1 )
    d2 = apply(data, 1, function(x) exp(-0.5 * t(x - mu2) %*% sigma_inv %*% (x - mu2))*p2 )
    
    # plot the densities
    plot_ly(x = x1, y = x2) %>% 
            add_surface(z = matrix(d1, length(x1), length(x2)), colorscale = list(c(0,"rgb(255,112,183)"), c(1,"rgb(128,0,64)"))) %>% 
            layout(paper_bgcolor='transparent') %>% 
            add_surface(z = matrix(d2, length(x1), length(x2))) %>% 
            layout(scene = list(xaxis = list(title = "X1"), 
                                yaxis = list(title = "X2"),
                                zaxis = list(title = "Log Normal Densities")))
```

## Linear Discriminant Analysis

As we demonstrated earlier using the Bayes rule, the conditional probability can be formulated using Bayes Theorem. For this time, we will assume in generate that there are $K$ classes instead of just two. However, the notation are similar to the previous case:

\begin{align}
\Prob(Y = k | X = \bx) =&~ \frac{\Prob(X = \bx | Y = k)\Prob(Y = k)}{\Prob(X = \bx)}\\
                     =&~ \frac{\Prob(X = \bx | Y = k)\Prob(Y = k)}{\sum_{l=1}^K \Prob(X = \bx | Y = l) \Prob(Y = l)}\\
                     =&~ \frac{\pi_k f_k(\bx)}{\sum_{l=1}^K \pi_l f_l(\bx)}
\end{align}

Given any target point $\bx$, the best prediction is simply picking the one that maximizes the posterior 

$$\underset{k}{\arg\max} \,\, \pi_k f_k(x)$$
Both LDA and QDA model $f_k$ as a normal density function. Suppose we model each class density as multivariate Gaussian ${\cal N}(\bmu_k, \Sigma_k)$, and \alert{assume that the covariance matrices are the same across all $k$, i.e., $\Sigma_k = \Sigma$}. Then 

$$f_k(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp\left[ -\frac{1}{2} (\bx - \bmu_k)^\T \Sigma^{-1} (\bx - \bmu_k) \right].$$
The log-likelihood function for the conditional distribution is

\begin{align}
\log f_k(\bx) =&~ -\log \big((2\pi)^{p/2} |\Sigma|^{1/2} \big) - \frac{1}{2} (\bx - \bmu_k)^\T \Sigma^{-1} (\bx - \bmu_k) \\
    =&~ - \frac{1}{2} (\bx - \bmu_k)^\T \Sigma^{-1} (\bx - \bmu_k) + \text{Constant}
\end{align}

The __maximum a posteriori__ probability (MAP) estimate is simply 

\begin{align}
\widehat f(\bx) =& ~\underset{k}{\arg\max} \,\, \log \big( \pi_k f_k(\bx) \big) \\
    =& ~\underset{k}{\arg\max} \,\, - \frac{1}{2} (\bx - \bmu_k)^\T \Sigma^{-1} (\bx - \bmu_k) + \log(\pi_k)
\end{align}

The term $(\bx - \bmu_k)^\T \Sigma^{-1} (\bx - \bmu_k)$ is simply the \alert{Mahalanobis distance} between $x$ and the centroid $\bmu_k$ for class $k$. Hence, this is essentially classifying $x$ to the class label with the closest centroid (after adjusting the for prior). This sets a connection with the $k$NN algorithm. __A special case__ is that when $\Sigma = \bI$, i.e., only Euclidean distance is needed, and we have 

$$\underset{k}{\arg\max} \,\, - \frac{1}{2} \lVert x - \bmu_k \rVert^2 + \log(\pi_k)$$

The decision boundary of LDA, as its name suggests, is a linear function of $\bx$. To see this, let's look at the terms in the MAP. Note that anything that does not depends on the class index $k$ is irrelevant to the decision. 

\begin{align}
 & - \frac{1}{2} (\bx - \bmu_k)^\T \Sigma^{-1} (\bx - \bmu_k) + \log(\pi_k)\\
=&~ \bx^\T \Sigma^{-1} \bmu_k - \frac{1}{2}\bmu_k^\T \Sigma^{-1} \bmu_k + \log(\pi_k) \text{irrelevant terms} \\
=&~ \bx^\T \bw_k + b_k + \text{irrelevant terms}
\end{align}

Then, the decision boundary between two classes, $k$ and $l$ is 

\begin{align}
\bx^\T \bw_k + b_k &= \bx^\T \bw_l + b_l \\
\Leftrightarrow \quad \bx^\T (\bw_k - \bw_l) + (b_k - b_l) &= 0, \\
\end{align}

which is a linear function of $\bx$. The previous density plot already showed this effect. Estimating the parameters in LDA is very simple:

  * Prior probabilities: $\widehat{\pi}_k = n_k / n = n^{-1} \sum_k \mathbf{1}\{y_i = k\}$, where $n_k$ is the number of observations in class $k$.
  * Centroids: $\widehat{\bmu}_k = n_k^{-1} \sum_{i: \,y_i = k} x_i$
  * Pooled covariance matrix:
    $$\widehat \Sigma = \frac{1}{n-K} \sum_{k=1}^K \sum_{i : \, y_i = k} (\bx_i - \widehat{\bmu}_k) (\bx_i - \widehat{\bmu}_k)^\T$$

## Example: Quadratic Discriminant Analysis (QDA)

When we assume that each class has its own covariance structure, the decision boundary will not be linear anymore. Let's visualize this by creating two density functions that use different covariance matrices.

```{r out.width = "75%", echo = FALSE}
    Sigma2 = matrix(c(1, -0.5, -0.5, 1), 2, 2)
    sigma2_inv = solve(Sigma2)
    
    # the density function after removing some constants
    d1 = apply(data, 1, function(x) 1/sqrt(det(Sigma))*exp(-0.5 * t(x - mu1) %*% sigma_inv %*% (x - mu1))*p1 )
    d2 = apply(data, 1, function(x) 1/sqrt(det(Sigma2))*exp(-0.5 * t(x - mu2) %*% sigma2_inv %*% (x - mu2))*p2 )
    
    # plot the densities
    plot_ly(x = x1, y = x2) %>% 
            add_surface(z = matrix(d1, length(x1), length(x2)), 
                        colorscale = list(c(0,"rgb(107,184,214)"),c(1,"rgb(0,90,124)"))) %>% 
            layout(paper_bgcolor='transparent') %>% 
            add_surface(z = matrix(d2, length(x1), length(x2))) %>% 
            layout(scene = list(xaxis = list(title = "X1"), 
                                yaxis = list(title = "X2"),
                                zaxis = list(title = "Log Normal Densities")))  
```

```{r include = FALSE}    
    # constants 
    c1 = - 0.5 * t(mu1) %*% sigma_inv %*% mu1 + log(0.3)
    c2 = - 0.5 * t(mu2) %*% sigma_inv %*% mu2 + log(0.7)    
    
    # the discriminate function 
    d1 = apply(data, 1, function(x) t(x) %*% sigma_inv %*% mu1 + c1 )
    d2 = apply(data, 1, function(x) t(x) %*% sigma_inv %*% mu2 + c2 )   
```    
    
## Quadratic Discriminant Analysis

QDA simply abandons the assumption of the common covariance matrix. Hence, $\Sigma_k$'s are not equal. In this case, the determinant $|\Sigma_k|$ of each covariance matrix will be different. In addition, the MAP decision becomes a quadratic function of the target point $\bx$

\begin{align}
 & \underset{k}{\max} \,\, \log \big( \pi_k f_k(x) \big) \\
=& ~\underset{k}{\max} \,\, -\frac{1}{2} \log |\Sigma_k| - \frac{1}{2} (\bx - \bmu_k)^\T \Sigma_k^{-1} (\bx - \bmu_k) + \log(\pi_k) \\
=& \bx^\T \bW_k \bx + \bw_k^\T \bx + b_k
\end{align}

This leads to quadratic decision boundary between class $k$ and $l$

$$\big\{\bx: \bx^\T (\bW_k - \bW_l) \bx + \bx^\T (\bw_k - \bw_l) + (b_k - b_l) = 0\big\}.$$

The estimation procedure is also similar:

  * Prior probabilities: $\widehat{\pi}_k = n_k / n = n^{-1} \sum_k \mathbf{1}\{y_i = k\}$, where $n_k$ is the number of observations in class $k$.
  * Centroid: $\widehat{\bmu}_k = n_k^{-1} \sum_{i: \,y_i = k} \bx_i$
  * Sample covariance matrix for each class:
    $$\widehat \Sigma_k = \frac{1}{n_k-1} \sum_{i : \, y_i = k} (\bx_i - \widehat{\bmu}_k)(\bx_i - \widehat{\bmu}_k)^\T$$

## Example: the Hand Written Digit Data

We first sample 100 data from both the training and testing sets. 

```{r}
    library(ElemStatLearn)
    # a plot of some samples 
    findRows <- function(zip, n) {
        # Find n (random) rows with zip representing 0,1,2,...,9
        res <- vector(length=10, mode="list")
        names(res) <- 0:9
        ind <- zip[,1]
        for (j in 0:9) {
        res[[j+1]] <- sample( which(ind==j), n ) }
        return(res) 
    }
    
    set.seed(1)
    
    # find 100 samples for each digit for both the training and testing data
    train.id <- findRows(zip.train, 100)
    train.id = unlist(train.id)
    
    test.id <- findRows(zip.test, 100)
    test.id = unlist(test.id)
    
    X = zip.train[train.id, -1]
    Y = zip.train[train.id, 1]
    dim(X)
    
    Xtest = zip.test[test.id, -1]
    Ytest = zip.test[test.id, 1]
    dim(Xtest)
```

We can then fit LDA and QDA and predict.

```{r}
    # fit LDA
    library(MASS)
    
    dig.lda=lda(X,Y)
    Ytest.pred=predict(dig.lda, Xtest)$class
    table(Ytest, Ytest.pred)
    mean(Ytest != Ytest.pred)
```

However, QDA does not work in this case because there are too many parameters
```{r eval=FALSE}
    dig.qda = qda(X, Y) # error message
```

<!--chapter:end:04.2-da.Rmd-->

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# (PART) Machine Learning Algorithms {-}

# Support Vector Machines

Support Vector Machine (SVM) is one of the most popular classification models. The original SVM was proposed by Vladimir Vapnik and Alexey Chervonenkis in 1963. Then two important improvements was developed in the 90's: the soft margin version [@cortes1995support] and the nonlinear SVM using the kernel trick [@boser1992training]. We will start with the hard margin version, and then introduce all other techniques. 

## Maximum-margin Classifier

This is the original SVM proposed in 1963. It shares similarities with the perception algorithm, but in certain sense is a stable version. We observe the training data $\cD_n = \{\bx_i, y_i\}_{i=1}^n$, where we code $y_i$ as a binary outcome from $\{-1, 1\}$. The advantages of using this coding instead of $0/1$ will be seen later. The goal is to find a linear classification rule $f(\bx) = \beta_0 + \bx^\T \bbeta$ such that the classification rule is the sign of $f(\bx)$:

$$
\hat{y} = 
\begin{cases}
        +1, \quad \text{if} \quad f(\bx) > 0\\ 
        -1, \quad \text{if} \quad f(\bx) < 0
\end{cases}
$$
Hence, a correct classification would satisfy $y_i f(\bx_i) > 0$. Let's look at the following example of data from two classes. 

```{r}
    set.seed(1)
    n <- 6
    p <- 2
    
    # Generate positive and negative examples
    xneg <- matrix(rnorm(n*p,mean=0,sd=1),n,p)
    xpos <- matrix(rnorm(n*p,mean=3,sd=1),n,p)
    x <- rbind(xpos,xneg)
    y <- matrix(as.factor(c(rep(1,n),rep(-1,n))))
    
    # plot 
    plot(x,col=ifelse(y>0,"blue","red"), pch = 19, cex = 1.2, lwd = 2, 
         xlab = "X1", ylab = "X2", cex.lab = 1.5)
    legend("bottomright", c("Positive", "Negative"),col=c("blue", "red"),
           pch=c(19, 19), text.col=c("blue", "red"), cex = 1.5)
```

There are many linear lines that can perfectly separate the two classes. But which is better? The SVM defines this as the line that maximizes the margin, which can be seen in the following. 

We use the `e1071` package to fit the SVM. There is a cost parameter $C$, with default value 1. This parameter has a significant impact on non-separable problems. However, for this __separable case__, we should set this to be a very large value, meaning that the cost for having a wrong classification is very large. We also need to specify the `linear` kernel. 

```{r}
    library(e1071)
    svm.fit <- svm(y ~ ., data = data.frame(x, y), type='C-classification', 
                   kernel='linear', scale=FALSE, cost = 10000)
```

The following code can recover the fitted linear separation margin. Note here that the points on the margins are the ones with $\alpha_i > 0$ (will be introduced later):

  * `coefs` provides the $y_i \alpha_i$ for the support vectors
  * `SV` are the $x_i$ values correspond to the support vectors 
  * `rho` is negative $\beta_0$

```{r}
    b <- t(svm.fit$coefs) %*% svm.fit$SV
    b0 <- -svm.fit$rho
    
    # an alternative of b0 as the lecture note
    b0 <- -(max(x[y == -1, ] %*% t(b)) + min(x[y == 1, ] %*% t(b)))/2
    
    # plot on the data 
    plot(x,col=ifelse(y>0,"blue","red"), pch = 19, cex = 1.2, lwd = 2, 
         xlab = "X1", ylab = "X2", cex.lab = 1.5)
    legend("bottomleft", c("Positive","Negative"),col=c("blue","red"),
           pch=c(19, 19),text.col=c("blue","red"), cex = 1.5)
    abline(a= -b0/b[1,2], b=-b[1,1]/b[1,2], col="black", lty=1, lwd = 2)
    
    # mark the support vectors
    points(x[svm.fit$index, ], col="black", cex=3)
    
    # the two margin lines 
    abline(a= (-b0-1)/b[1,2], b=-b[1,1]/b[1,2], col="black", lty=3, lwd = 2)
    abline(a= (-b0+1)/b[1,2], b=-b[1,1]/b[1,2], col="black", lty=3, lwd = 2)
```

As we can see, the separation line is trying to have the maximum distance from both classes. This is why it is called the __Maximum-margin Classifier__. 

```{r include = FALSE, echo = FALSE}
    n = 200
    p = 2
    trainx = matrix(runif(n*p, -1, 1), n, p)

    trainx = trainx[ abs(trainx[,2] - 0.75*sin(2*pi*trainx[,1]))>0.2,  ]

    y = sign(trainx[,2] - 0.75*sin(2*pi*trainx[,1]))
    plot(trainx, col = as.factor(y), pch = 19)
    lines(trainx[,1], 0.75*sin(2*pi*trainx[,1])) 
    
    svm.fit <- svm(y ~ ., data = data.frame(trainx, y), type='C-classification', 
                   kernel='linear', scale=FALSE, cost = 10)
    
    svm.fit <- svm(y ~ ., data = data.frame(trainx, y), type='C-classification', 
                   kernel='radial', scale=FALSE, cost = 10000)    
```

## Linearly Separable SVM

In linearly SVM, $f(\bx) = \beta_0 + \bx^\T \bbeta$. When $f(\bx) = 0$, it corresponds to a hyperplane that separates the two classes:

$$\{ \bx: \beta_0 + \bx^\text{T} \boldsymbol \beta = 0 \}$$

Hence, for this separable case, all observations with $y_i = 1$ are on one side $f(\bx) > 0$, and observations with $y_i = -1$ are on the other side. 

<center>
![](images/SVMdist.png){width=40%}
</center>

First, let's calculate the __distance from any point $\bx$ to this hyperplane__. We can first find a point $\bx_0$ on the hyperplane, such that $\bx_0^\T \bbeta = - \beta_0$. By taking the difference between $\bx$ and $\bx_0$, and project this vector to the direction of $\bbeta$, we have that the distance from $\bx$ to the hyperplane is the projection of $\bx - \bx_0$ onto the normed vector $\frac{\bbeta}{\lVert \bbeta \lVert}$:

\begin{align}
& \left \langle  \frac{\bbeta}{\lVert \bbeta \lVert}, \bx - \bx_0 \right \rangle \\
=& \frac{1}{\lVert \bbeta \lVert} (\bx - \bx_0)^\T \bbeta \\
=& \frac{1}{\lVert \bbeta \lVert} (\bx^\T \bbeta + \beta_0) \\
=& \frac{1}{\lVert \bbeta \lVert} f(\bx) \\
\end{align}

Since the goal of SVM is to create the maximum margin, let's denote this as $M$. Then we want all observations to be lied on the correct side, with at least an margin $M$. This means $y_i (\bx_i^\T \bbeta + \beta_0) \geq M$. But the scale of $\bbeta$ is also playing a role in calculating the margin. Hence, we will use the normed version. Then, the linearly separable SVM is to solve this constrained optimization problem:

\begin{align}
\underset{\bbeta, \beta_0}{\text{max}} \quad & M \\
\text{subject to} \quad & \frac{1}{\lVert \bbeta \lVert} y_i(\bx^\T \bbeta + \beta_0) \geq M, \,\, i = 1, \ldots, n.
\end{align}

Note that the scale of $\bbeta$ can be arbitrary, let's set it as $\lVert \bbeta \rVert = 1/M$. The maximization becomes minimization, and its equivalent to minimizing $\frac{1}{2} \lVert \bbeta \rVert^2$. Then we have the __primal form__ of the SVM optimization problem. 

\begin{align}
\text{min} \quad & \frac{1}{2} \lVert \bbeta \rVert^2 \\
\text{subject to} \quad & y_i(\bx^\T \bbeta + \beta_0) \geq 1, \,\, i = 1, \ldots, n.
\end{align}

### From Primal to Dual 

This is a general inequality constrained optimization problem. 

\begin{align}
\text{min} \quad & g(\btheta) \\
\text{subject to} \quad & h(\btheta) \leq 0, \,\, i = 1, \ldots, n.
\end{align}

We can consider the corresponding Lagrangian (with all $\alpha_i$'s positive):

$$\cL(\btheta, \balpha) = g(\btheta) + \sum_{i = 1}^n \alpha_i h_i(\btheta)$$
Then there can be two ways to optimize this. If we maximize $\alpha_i$'s first, for any fixed $\btheta$, then for any $\btheta$ that violates the constraint, i.e., $h_i(\btheta) > 0$ for some $i$, we can always choose an extremely large $\alpha_i$ so that $\cal{L}(\btheta, \balpha)$ is infinity. Hence the solution of this __primal form__ must satisfy the constraint.

$$\underset{\btheta}{\min} \underset{\balpha \succeq 0}{\max} \cL(\btheta, \balpha)$$
On the other hand, if we minimize $\btheta$ first, then maximize for $\balpha$, we have the __dual form__:

$$\underset{\balpha \succeq 0}{\max} \underset{\btheta}{\min} \cL(\btheta, \balpha)$$
In general, the two are not the same:

$$\underbrace{\underset{\balpha \succeq 0}{\max} \underset{\btheta}{\min} \cL(\btheta, \balpha)}_{\text{duel}} \leq \underbrace{\underset{\btheta}{\min} \underset{\balpha \succeq 0}{\max} \cL(\btheta, \balpha)}_{\text{primal}}$$
But a sufficient condition is that if both $g$ and $h_i$'s are convex and also the constraints $h_i$'s are feasible. We will use this technique to solve the SVM problem. 

First, rewrite the problem as 

\begin{align}
\text{min} \quad & \frac{1}{2} \lVert \bbeta \rVert^2 \\
\text{subject to} \quad & - \{ y_i(\bx^\T \bbeta + \beta_0) - 1\} \leq 0, i = 1, \ldots, n.
\end{align}

Then the Lagrangian is 

$$\cL(\bbeta, \beta_0, \balpha) = \frac{1}{2} \lVert \bbeta \rVert^2 - \sum_{i = 1}^n \alpha_i \big\{ y_i(\bx_i^\T \bbeta + \beta_0) - 1 \big\}$$
To solve this using the dual form, we first find the optimizer of $\bbeta$ and $\beta_0$. We take derivatives with respect to them:

\begin{align}
    \bbeta - \sum_{i = 1}^n \alpha_i y_i \bx_i  =&~ 0 \quad (\nabla_\bbeta \cL = 0 ) \\
    \sum_{i = 1}^n \alpha_i y_i =&~ 0 \quad (\nabla_{\beta_0} \cL = 0 )
\end{align}

Take these solution and plug them back into the Lagrangian, we have

$$\cL(\bbeta, \beta_0, \balpha) = \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \bx_i^\T \bx_j$$
Hence, the dual optimization problem is 

\begin{align}
\underset{\balpha}{\max} \quad & \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \bx_i^\T \bx_j \nonumber \\
\text{subject to} \quad & \alpha_i \geq 0, \,\, i = 1, \ldots, n. \nonumber \\
& \sum_{i = 1}^n \alpha_i y_i = 0
\end{align}

Compared with the original primal form, this version has a trivial feasible solution with all $\alpha_i$'s being 0. One can start from this solution to search for the optimizer while maintaining within the contained region. However, the primal form is difficult since there is no apparent way to satisfy the constraint. 

After solving the dual form, we have all the $\alpha_i$ values. The ones with $\alpha_i > 0$ are called the support vectors. Based on our previous analysis, $\widehat{\bbeta} = \sum_{i = 1}^n \alpha_i y_i x_i$, and we can also obtain $\beta_0$ by calculating the midpoint of two "closest" support vectors to the separating hyperplane:

$$\widehat{\beta}_0 = - \,\, \frac{\max_{i: y_i = -1} \bx_i^\T \widehat{\bbeta} + \min_{i: y_i = 1} \bx_i^\T \widehat{\bbeta} }{2}$$
And the decision is $\text{sign}(\bx^\T \widehat{\bbeta} + \widehat{\beta}_0)$. An example has been demonstrated previously with the `e1071` package. 

## Linearly Non-separable SVM with Slack Variables

When we cannot have a perfect separation of the two classes, the original SVM cannot find a solution. Hence, a slack was introduce to incorporate such observations: 

$$y_i (\bx_i^\T \bbeta + \beta_0) \geq (1 - \xi_i)$$
for a positive $\xi$. Note that when $\xi = 0$, the observation is lying at the correct side, with enough margin. When $1 > \xi > 0$, the observation is lying at the correct side, but the margin is not sufficiently large. When $\xi > 1$, the observation is lying on the wrong side of the separation hyperplane. 

<center>
![](images/SVMslack.png){width=40%}
</center>

This new optimization problem can be formulated as 

\begin{align}
\text{min} \quad & \frac{1}{2}\lVert \bbeta \rVert^2 + C \sum_{i=1}^n \xi_i \\
\text{subject to} \quad & y_i (\bx_i^\T \bbeta + \beta_0) \geq (1 - \xi_i), \,\, i = 1, \ldots, n, \\
\text{and} \quad & \xi_i \geq 0, \,\, i = 1, \ldots, n,
\end{align}

where $C$ is a tuning parameter that controls the emphasis on the slack variable. Large $C$ will be less tolerable on having positive slacks. We can again write the Lagrangian primal $\cL(\bbeta, \beta_0, \balpha, \bxi)$ as

$$\frac{1}{2} \lVert \bbeta \rVert^2 + C \sum_{i=1}^n \xi_i - \sum_{i = 1}^n \alpha_i \big\{ y_i(x_i^\T \bbeta + \beta_0) - (1 - \xi_i) \big\} - \sum_{i = 1}^n \gamma_i \xi_i,$$
where $\alpha_i$'s and $\gamma_i$'s are all positive. We can similarly obtain the solution corresponding to $\bbeta$, $\beta_0$ and $\bxi$:

\begin{align}
\bbeta - \sum_{i = 1}^n \alpha_i y_i x_i  =&~ 0 \quad (\nabla_\bbeta \cL = 0 ) \\
\sum_{i = 1}^n \alpha_i y_i =&~ 0 \quad (\nabla_{\beta_0} \cL = 0 ) \\
C - \alpha_i - \gamma_i =&~ 0 \quad (\nabla_{\xi_i} \cL = 0 )
\end{align}

Substituting them back into the Lagrangian, we have the dual form:

\begin{align}
\underset{\balpha}{\max} \quad & \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \color{OrangeRed}{\langle \bx_i, \bx_j \rangle} \\
\text{subject to} \quad & 0 \leq \alpha_i \leq C, \,\, i = 1, \ldots, n, \\
\text{and} \quad & \sum_{i = 1}^n \alpha_i y_i = 0.
\end{align}

Here, the inner product $\langle \bx_i, \bx_j \rangle$ is nothing but $\bx_i^\T \bx_j$. The observations with $0 < \alpha_i < C$ are the \alert{support vectors} that lie on the margin. Hence, we can obtain these observations and perform the same calculations as before to obtain $\widehat{\beta}_0$. The following code generates some data for this situation and fit SVM. We use the default $C = 1$. 

```{r fig.width=8, fig.height=8, out.width = '40%'}

    set.seed(70)
    n <- 10 # number of data points for each class
    p <- 2 # dimension

    # Generate the positive and negative examples
    xneg <- matrix(rnorm(n*p,mean=0,sd=1),n,p)
    xpos <- matrix(rnorm(n*p,mean=1.5,sd=1),n,p)
    x <- rbind(xpos,xneg)
    y <- matrix(as.factor(c(rep(1,n),rep(-1,n))))

    # Visualize the data
    
    plot(x,col=ifelse(y>0,"blue","red"), pch = 19, cex = 1.2, lwd = 2, 
         xlab = "X1", ylab = "X2", cex.lab = 1.5)
    legend("topright", c("Positive","Negative"),col=c("blue","red"),
           pch=c(19, 19),text.col=c("blue","red"), cex = 1.5)

    svm.fit <- svm(y ~ ., data = data.frame(x, y), type='C-classification', 
                   kernel='linear',scale=FALSE, cost = 1)

    b <- t(svm.fit$coefs) %*% svm.fit$SV
    b0 <- -svm.fit$rho
    
    points(x[svm.fit$index, ], col="black", cex=3)     
    abline(a= -b0/b[1,2], b=-b[1,1]/b[1,2], col="black", lty=1, lwd = 2)
    
    abline(a= (-b0-1)/b[1,2], b=-b[1,1]/b[1,2], col="black", lty=3, lwd = 2)
    abline(a= (-b0+1)/b[1,2], b=-b[1,1]/b[1,2], col="black", lty=3, lwd = 2)
```

If we instead use a smaller $C$:

```{r}
    # Visualize the data
    plot(x,col=ifelse(y>0,"blue","red"), pch = 19, cex = 1.2, lwd = 2, 
         xlab = "X1", ylab = "X2", cex.lab = 1.5)
    legend("topright", c("Positive","Negative"),col=c("blue","red"),
           pch=c(19, 19),text.col=c("blue","red"), cex = 1.5)

    # fit SVM with C = 10
    svm.fit <- svm(y ~ ., data = data.frame(x, y), type='C-classification', 
                   kernel='linear',scale=FALSE, cost = 0.1)

    b <- t(svm.fit$coefs) %*% svm.fit$SV
    b0 <- -svm.fit$rho
    
    points(x[svm.fit$index, ], col="black", cex=3)     
    abline(a= -b0/b[1,2], b=-b[1,1]/b[1,2], col="black", lty=1, lwd = 2)
    
    abline(a= (-b0-1)/b[1,2], b=-b[1,1]/b[1,2], col="black", lty=3, lwd = 2)
    abline(a= (-b0+1)/b[1,2], b=-b[1,1]/b[1,2], col="black", lty=3, lwd = 2)
```

## Example: `SAheart` Data

If you want to use the `1071e` package and perform cross-validation, you could consider using the `caret` package. Make sure that you specify `method = "svmLinear2"`. The following code is using the `SAheart` as an example.

```{r}
  library(ElemStatLearn)
  data(SAheart)
  library(caret)

  cost.grid = expand.grid(cost = seq(0.01, 2, length = 20))
  train_control = trainControl(method="repeatedcv", number=10, repeats=3)
  
  svm2 <- train(as.factor(chd) ~., data = SAheart, method = "svmLinear2", 
                trControl = train_control,  
                tuneGrid = cost.grid)
  
  # see the fitted model
  svm2
```

Note that when you fit the model, there are a few things you could consider:

  * You can consider centering and scaling the covariates. This can be done during pre-processing. Or you may specify `preProcess = c("center", "scale")` in the `train()` function. 
  * You may want to start with a wider range of cost values, then narrow down to a smaller range, since SVM can be quite sensitive to tuning in some cases. 
  * There are many other SVM libraries, such as `kernlab`. This can be specified by using `method = "svmLinear"`. However, `kernlab` uses `C` as the parameter name for cost. We will show an example later. 

## Nonlinear SVM via Kernel Trick

The essential idea of kernel trick can be summarized as using the kernel function of two observations $\bx$ and $\bz$ to replace the inner product between some feature mapping of the two covariate vectors. In other words, if we want to create some nonlinear features of $\bx$, such as $x_1^2$, $\exp(x_2)$, $\sqrt{x_3}$, etc., we may in general write them as 

$$\Phi : \cX \rightarrow \cF, \,\,\, \Phi(\bx) = (\phi_1(\bx), \phi_2(\bx), \ldots ),$$
where $\cF$ has either finite or infinite dimensions. Then, we can still treat this as a linear SVM by constructing the decision rule as 

$$f(x) = \langle \Phi(\bx), \bbeta \rangle = \Phi(\bx)^\T \bbeta.$$
This is why we used the $\langle \cdot, \cdot\rangle$ operator in the previous example. Now, the kernel trick is essentially skipping the explicit calculation of $\Phi(\bx)$ by utilizing the property that 

$$K(\bx, \bz) = \langle \Phi(\bx), \Phi(\bz) \rangle$$
for some kernel function $K(\bx, \bz)$. Since $\langle \Phi(\bx), \Phi(\bz) \rangle$ is all we need in the dual form, we can simply replace it by $K(\bx, \bz)$, which gives the kernel form:

\begin{align}
\underset{\balpha}{\max} \quad & \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \color{OrangeRed}{K(\bx_i, \bx_j)} \\
\text{subject to} \quad & 0 \leq \alpha_i \leq C, \,\, i = 1, \ldots, n, \\
\text{and} \quad & \sum_{i = 1}^n \alpha_i y_i = 0.
\end{align}

One most apparent advantage of doing this is to save computational cost. This maybe understood using the following example:

  * Consider kernel function $K(\bx, \bz) = (\bx^\T \bz)^2$
  * Consider $\Phi(\bx)$ being the basis expansion that contains all second order interactions: $x_k x_l$ for $1 \leq k, l \leq p$
  
We can show that the two gives equivalent results, however, the kernel version is much faster. $K(\bx, \bz)$ takes $p+1$ operations, while $\langle \Phi(\bx_i),  \Phi(\bx_j) \rangle$ requires $3p^2$. 

\begin{align}
K(\bx, \bz) &=~ \left(\sum_{k=1}^p x_k z_k\right) \left(\sum_{l=1}^p x_l z_l\right) \\
&=~ \sum_{k=1}^p \sum_{l=1}^p x_k z_k x_l z_l \\
&=~ \sum_{k, l=1}^p (x_k x_l) (z_k z_l) \\
&=~ \langle \Phi(\bx),  \Phi(\bz) \rangle
\end{align}

Formally, this property is guaranteed by the __Mercer’s theorem__ that states: The kernel matrix $K$ is positive semi-definite if and only if the function $K(x_i ,x_j)$ is equivalent to some inner product $\langle \Phi(\bx), \Phi(\bz) \rangle$.

Besides making the calculation of nonlinear functions easier, using the kernel trick also implies that if we use a proper kernel function, then it defines a space of functions $\cH$ (reproducing kernel Hilbert space, RKHS) that can be represented in the form of $f(x) = \sum_i \alpha_i K(x, x_i)$ for some $x_i$ in $\cX$ (see the Moore–Aronszajn theorem) with a proper definition of inner product. However, this space is of infinite dimension, noticing that $i$ goes from 1 to infinity. However, as long as we search for the solution within $\cH$, and also apply a proper penalty of the estimated function $\widehat{f}(\bx)$, then our computational job will reduce to solving the $\alpha_i$'s that corresponds to the observed $n$ data points, meaning that we only need to solve the solution within a finite space. This is guaranteed by the __Representer theorem__. There are numerous articles on the RKHS. Hence, we will not focus on introducing this technique. However, we will later on use this property in the penalized formulation of SVM. 

## Example: `mixture.example` Data

We use the `mixture.example` data in the `ElemStatLearn` package. In addition, we use a different package `kernlab`. The red dotted line indicates the true decision boundary. 

```{r message=FALSE}
    library(ElemStatLearn)
    data(mixture.example)

    # redefine data
    px1 = mixture.example$px1
    px2 = mixture.example$px2
    x = mixture.example$x
    y = mixture.example$y
    
    # plot the data and true decision boundary
    prob <- mixture.example$prob
    prob.bayes <- matrix(prob, 
                         length(px1), 
                         length(px2))
    contour(px1, px2, prob.bayes, levels=0.5, lty=2, 
            labels="", xlab="x1",ylab="x2",
            main="SVM with linear kernal", col = "red", lwd = 2)
    points(x, col=ifelse(y==1, "darkorange", "deepskyblue"), pch = 19)

    # train linear SVM using the kernlab package
    library(kernlab)
    cost = 10
    svm.fit <- ksvm(x, y, type="C-svc", kernel='vanilladot', C=cost)

    # plot the SVM decision boundary
    # Extract the indices of the support vectors on the margin:
    sv.alpha<-alpha(svm.fit)[[1]][which(alpha(svm.fit)[[1]]<cost)]
    sv.index<-alphaindex(svm.fit)[[1]][which(alpha(svm.fit)[[1]]<cost)]
    sv.matrix<-x[sv.index,]
    points(sv.matrix, pch=16, col=ifelse(y[sv.index] == 1, "darkorange", "deepskyblue"), cex=1.5)

    # Plot the hyperplane and the margins:
    w <- t(cbind(coef(svm.fit)[[1]])) %*% xmatrix(svm.fit)[[1]]
    b <- - b(svm.fit)

    abline(a= -b/w[1,2], b=-w[1,1]/w[1,2], col="black", lty=1, lwd = 2)
    abline(a= (-b-1)/w[1,2], b=-w[1,1]/w[1,2], col="black", lty=3, lwd = 2)
    abline(a= (-b+1)/w[1,2], b=-w[1,1]/w[1,2], col="black", lty=3, lwd = 2)
```

Let's also try a nonlinear SVM, using the radial kernel. 

```{r}
    # fit SVM with radial kernel, with cost = 5
    dat = data.frame(y = factor(y), x)
    fit = svm(y ~ ., data = dat, scale = FALSE, kernel = "radial", cost = 5)
    
    # extract the prediction
    xgrid = expand.grid(X1 = px1, X2 = px2)
    func = predict(fit, xgrid, decision.values = TRUE)
    func = attributes(func)$decision
    
    # visualize the decision rule
    ygrid = predict(fit, xgrid)
    plot(xgrid, col = ifelse(ygrid == 1, "bisque", "cadetblue1"), 
         pch = 20, cex = 0.2, main="SVM with radial kernal")
    points(x, col=ifelse(y==1, "darkorange", "deepskyblue"), pch = 19)
    
    # our estimated function value, cut at 0
    contour(px1, px2, matrix(func, 69, 99), level = 0, add = TRUE, lwd = 2)
    
    # the true probability, cut at 0.5
    contour(px1, px2, matrix(prob, 69, 99), level = 0.5, add = TRUE, 
            col = "red", lty=2, lwd = 2)
```

You may also consider some other popular kernels. The following ones are implemented in the `e1071` package, with additional tuning parameters $\text{coef}_0$ and $\gamma$. 

  * Linear: $K(\bx, \bz) = \bx^\T \bz$
  * $d$th degree polynomial: $K(\bx, \bz) = (\text{coef}_0 + \gamma \bx^\T \bz)^d$
  * Radial basis: $K(\bx, \bz) = \exp(- \gamma \lVert \bx - \bz \lVert^2)$
  * Sigmoid: $\tanh(\gamma \bx^\T \bz + \text{coef}_0)$
  
Cross-validation can also be doing using the `caret` package. To specify the kernel, one must correctly specify the `method` parameter in the `train()` function. For this example, we use the `method = "svmRadial"` that uses the `kernlab` package to fit the model. For this choice, you need to tune just `sigma` and `C` (cost). More details are refereed to the [`caret` documentation](https://topepo.github.io/caret/train-models-by-tag.html#support-vector-machines). 

```{r}
  svm.radial <- train(y ~ ., data = dat, method = "svmRadial",
                preProcess = c("center", "scale"),
                tuneGrid = expand.grid(C = c(0.01, 0.1, 0.5, 1), sigma = c(1, 2, 3)),
                trControl = trainControl(method = "cv", number = 5))
  svm.radial
```

## SVM as a Penalized Model

Recall that in SVM, we need $y_i f(\bx_i)$ to be at least $1 - \xi_i$, this implies that we would prefer $1 - y_i f(\bx_i)$ to be negative or 0. And observation with $1 - y_i f(\bx_i)$ should be penalized. Hence, recall that the objective function of dual form in SVM is $\frac{1}{2}\lVert \bbeta \rVert^2 + C \sum_{i=1}^n \xi_i$, we may rewrite this as a new version:

$$\min \,\, \sum_{i=1}^n \big[ 1 - y_i f(\bx_i) \big]_{+} \, +\, \lambda \lVert \bbeta \rVert^2.$$
Here, we converted $1/(2C)$ to $\lambda$. And this resembles a familiar form of "Loss $+$ Penalty", where the slack variables becomes the loss and the norm of $\bbeta$ is the penalty. This particular loss function is called the __Hinge loss__, with 

$$L(y, f(\bx)) = [1 - yf(\bx)]_+ = \max(0, 1 - yf(\bx))$$
However, the Hinge loss is not differentiable. There are some other loss functions that can be used as substitute:

  * Logistic loss: 
  $$L(y, f(\bx)) = \log_2( 1 + e^{-y f(\bx)})$$
  * Modified Huber Loss: 
  $$L(y, f(\bx)) = \begin{cases}
    \max(0, 1 - yf(\bx))^2 & \text{for} \quad yf(\bx) \geq -1 \\
    -4 yf(\bx)  & \text{otherwise}  \\
    \end{cases}$$

Here is a visualization of several different loss functions. 

```{r}
  t = seq(-2, 3, 0.01)

  # different loss functions
  hinge = pmax(0, 1 - t) 
  zeroone = (t <= 0)
  logistic = log2(1 + exp(-t))
  modifiedhuber = ifelse(t >= -1, (pmax(0, 1 - t))^2, -4*t)
  
  # plot
  plot(t, zeroone, type = "l", lwd = 2, ylim = c(0, 4),
       main = "Loss Functions")
  points(t, hinge, type = "l", lwd = 2, col = "red", )
  points(t, logistic, type = "l", lty = 2, col = "darkorange", lwd = 2)
  points(t, modifiedhuber, type = "l", lty = 2, col = "deepskyblue", lwd = 2)
  legend("topright", c("Zero-one", "Hinge", "Logistic", "Modified Huber"),
         col = c(1, 2, "darkorange", "deepskyblue"), lty = c(1, 1, 2, 2), 
         lwd = 2, cex = 1.5)
```

For linear decision rules, with $f(\bx) = \beta_0 + \bx^\T \bbeta$, this should be trivial to solve. However, we also want to consider nonlinear decision functions. But the above form does not contain a kernel function to use the kernel trick. The __Representer Theorem__ [@kimeldorf1970correspondence] can help us in this case. This theorem was originally developed for in the setting of Chebyshev splines, but later on generalized. The theorem ensures that if we solve the function $f$ with regularization with respect to the norm in the RKHS induced from a kernel function $K$, then the solution must admits a finite representation of the form (although the space $\cH$ we search for the solution is infinite):

$$\widehat{f}(\bx) = \sum_{i = 1}^n \beta_i K(\bx, \bx_i).$$
This suggests that the optimization problem becomes 

$$\sum_{i=1}^n L(y_i, \bK_i^\T \bbeta) + \lambda \bbeta^\T \bK \bbeta,$$
where $\bK_{n \times n}$ is the kernel matrix with $\bK_{ij} = K(x_i, x_j)$, and $\bK_i$ is the $i$ the column of $\bK$. This is an unconstrained optimization problem that can be solved using gradient decent if $L$ is differentiable. More details will be presented in the next Chapter.

## Kernel and Feature Maps: Another Example

We give another example about the equivalence of kernel and the inner product of feature maps, which is ensured by the Mercer's Theorem [@mercer1909xvi]. Consider the Gaussian kernel $e^{-\gamma \lVert \bx - \bz \rVert}$. We can write, using Tayler expansion, 

\begin{align}
&e^{\gamma \lVert \bx - \bz \rVert} \nonumber \\
=& e^{-\gamma \lVert \bx \rVert + 2 \gamma \bx^\T \bz - \gamma \lVert \bz \rVert} \nonumber \\
=& e^{-\gamma \lVert \bx \rVert - \gamma \lVert \bz \rVert} \bigg[ 1 + \frac{2 \gamma \bx^\T \bz}{1!} + \frac{(2 \gamma \bx^\T \bz)^2}{2!} + \frac{(2 \gamma \bx^\T \bz)^3}{3!} + \cdots \bigg]
\end{align}

Note that $\bx^\T \bz$ is the inner product of all first order feature maps. We also showed previously $(\bx^\T \bz)^2$ is equivalent to the inner product of all second order feature maps ($\Phi_2(\bx)$), and $(\bx^\T \bz)^3$ would be equivalent to the third order version ($\Phi_3(\bx)$), etc.. Hence, the previous equation can be written as the inner product of feature maps in the form of

$$e^{-\gamma \lVert \bx \rVert} \bigg[ 1, \sqrt{\frac{2\gamma}{1!}} \bx^\T, \sqrt{\frac{(2\gamma)^2}{2!}} \Phi_2^\T(\bx), \sqrt{\frac{(2\gamma)^3}{3!}} \Phi_3^\T(\bx), \cdots \bigg]$$

This shows the Gaussian kernel is corresponding to all polynomials with a scaling factor of $e^{-\gamma \lVert \bx \rVert}$


<!--chapter:end:05.1-svm.Rmd-->

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Reproducing Kernel Hilbert Space

In the previous chapter of SVM, we gave an example to show that instead of using the inner product $\langle \Phi(\bx), \Phi(\bz) \rangle$ between the feature maps of $\bx$ and $\bz$, we can instead use the kernel trick $K(\bx, \bz)$ to perform the exact same calculation. And also, in the penalized kernel version, we mentioned that the decision rule can be expressed in the finite sample form of $\sum_{i = 1}^n \beta_i K(\cdot, \bx_i)$ by the Representer Theorem. All of these are based on a fundamental tool of the Reproducing Kernel Hilbert Space and we will provide some basic knowledge of it. We will also prove the Representer Theorem [@kimeldorf1970correspondence], which is very similar to the proof of smoothing spline. 

## Constructing the RKHS

Recall that in the smoothing spline example, we wanted to fit a regression model by solving for a function $f$ in a quite complicated space, the second order Sobolev space. We could not exhaust all the candidates in this space because that would be computationally untraceable. However, the results there shows that the solution has a finite representation. In general, when we solve a regression problem using a __Loss $+$ Penalty__ form, we will also enjoy that property if we search the (regression or decision) function $f$ within a RKHS. So let's first define what this space look like. 

We start with the feature space $\cal X$ of $X$, where $X$ is just the $p$ dimensional feature we often deal with. Let's say we have a sample $x_1$, then if we have a kernel function $k(\cdot, \cdot)$, we can construct a new function called $K(x_1, \cdot)$. Keep in mind that $K(x_1, \cdot)$ is a function with argument $\cdot$ and parameter $x_1$ in this case. Similarly, we can do another sample, say $x_2$ and generate a function based on that sample, called $K(x_1, \cdot)$. The following plot shows three of such functions, using red, orange and blue lines, receptively. 

```{r echo = FALSE}
  x1 = 0.1
  x2 = 0.9
  x3 = 4
  
  t = seq(-3, 8, 0.01)
  k1 = dnorm(t, mean = x1)
  k2 = dnorm(t, mean = x2)
  k3 = dnorm(t, mean = x3)
  
  plot(t, k1, col = "red", type = "l", lwd = 2, 
       ylim = c(-0.3, 1), ylab = "f(x)", xlab = "x")
  lines(t, k2, col = "darkorange", lwd = 2)
  lines(t, k3, col = "deepskyblue", lwd = 2)
  legend("topright", c("K(x1, .)", "K(x2, .)", "K(x3, .)"),
         lty = 1, lwd = 2, col = c("red", "darkorange", "deepskyblue"))
  legend("topleft", c("0.5 K(x1, .) - 0.8 K(x2, .) + 1.7 K(x3, .)"), lwd = 2) 
  
  lines(t, 0.5*k1 - 0.8*k2 + 1.7*k3 , col = "black", lwd = 2)
```

Since we can have many samples from $\cal X$, we will also have infinite such functions like $K(x, \cdot)$, and also the linear combinations of them would also be interesting to us. Let's consider a space ${\cal G}$ of all such functions

$${\cal G} = \left\{\sum_{i}^n \alpha_i K(x_i, \cdot) \mid \alpha \in \mathbb{R}, n \in \mathbb{N}, x_i \in {\cal X} \right\} $$
The black curve in the previous plot is an example of such linear combinations. We can see that the functions within ${\cal G}$ start to become more and more flexible as we consider all the linear combinations. And as one final step, we will consider the completion of this space, which leads to the RKHS. 

$$\cal H = \bar{\cal G}.$$
Completion here means that $\cal H$ will contain the limits of all Cauchy sequences of such functions in $\cal G$. 

## Properties of RKHS

This space $\cal H$ enjoys several important and useful properties. First, by the [Riesz representation theorem](https://en.wikipedia.org/wiki/Riesz_representation_theorem), we know that $\cal H$ is a __Hilbert space with the reproducing property__. For a (real valued) Hilbert space, it must satisfy 

  * symmetric: $\langle K_x, K_z \rangle = \langle K_z, K_x \rangle$
  * linear: $\langle a K_{x_1} + b K_{x_2}, K_z \rangle = a \langle K_{x_1}, K_z \rangle + b \langle K_{x_2}, K_z \rangle$
  * positive definite: $\langle K_x, K_x \rangle \geq 0$ and $\langle K_x, K_x \rangle = 0$ iff $K_x = 0$
  
Also, the reproducing property means that when we evaluate a function $f \in \cal H$ at a point $x$, it is the same as calculating the inner product between $f$ and $K_x$. Formally, 

$$f(x) = \langle f, K_x \rangle_{\cal H}$$

Now, we could simply take $f = K_z$, that means, evaluating $K_z(x)$ is 

$$K_z(x) = \langle K_z, K_x \rangle_{\cal H}$$
Note that $K_z(x) = K(z, x)$, this implies that the inner product in $\cal H$ is done by the kernel:

$$\langle K_z, K_x \rangle_{\cal H} = K(z, x)$$
For example, if we have $f(\cdot) = \sum_i \alpha_i K(x_i, \cdot)$, then evaluating $f$ at $x$ is 

\begin{align}
f(x) =& \, \langle f, K(x, \cdot) \rangle_{\cal H} \nonumber \\
=& \, \left\langle \sum_i \alpha_i K(x_i, \cdot), K(x, \cdot) \right\rangle_{\cal H} \nonumber \\
=& \, \sum_i \alpha_i \left\langle K(x_i, \cdot), K(x, \cdot) \right\rangle_{\cal H} \nonumber \\
=& \, \sum_i \alpha_i K(x_i, x)
\end{align}

The Moore–Aronszajn theorem [@aronszajn1950theory] ensures that a positive definite kernel $K(\cdot, \cdot)$ on $\cal X$ would uniquely define such a RKHS, where $K(\cdot, \cdot)$ itself is the reproducing kernel. Hence, all we need is the original $\cal X$ and a kernel function. Then the RKHS can be defined as we stated previously, with all the nice properties. Besides these, another results by Mercer interprets kernels as feature maps, which we have already see in the SVM chapter that $K(x, z)= \langle \Phi(x), \Phi(z) \rangle$. Overall, we set some relationships among these three quantities in their respective spaces:

  * original features $x$
  * feature maps $\Phi(x)$
  * functions $K(x, \cdot)$

## The Representer Theorem 

$\cal H$ is still a very large space of functions. And it is not clear if we want to find $f$ in $\cal H$ for our optimization problem, how do we computationally complete that task. It is unlikely that we can exhaust all such functions. Well, luckily, we don't need to. This is ensured by the __Representer Theorem__, which states that only a finite sample presentation is needed. 

```{theorem, name = "Representer Theorem"}
If we are given a set of data $\{x_i, y_i\}_{i=1}^n$, and we search for the best solution in $\cH$ of the optimization problem
$$\widehat f = \underset{f \in \cal H}{\arg\min} \,\, \cL(\{y_i, f(x_i)\}_{i=1}^n) + p(\| f \|_{\cH}^2 ),$$
where $\cL$ is the loss function, $p$ is a monotone penalty function, and $\cH$ is the RKHS with kernel $K$. Then the solution must take the form
$$\widehat f = \sum_{i=1}^n w_i K(\cdot, x_i)$$
```

The proof is quite simple. The logic is the same as the smoothing spline proof. 

```{proof}
We can first use the kernel $K$ associated with $\cal H$ to define a set of functions
$$K(\cdot, x_i), \, K(\cdot, x_2), \, \cdots, \, K(\cdot, x_n)$$
  
Then, suppose the solution is some function $f \in \cH$, we could find its projection on the space spaned by these functions. This means that we could write $f$ as

$$f(\cdot) = \sum_{i=1}^n \alpha_i K(\cdot, x_i) + h(\cdot)$$

for some $\alpha_i$ and $h(\cdot)$. Also, since $h(\cdot)$ is in the orthogonal space of all such $K(\cdot, x_i)$, we have, by the reproducing property,

$$ h(x_i) = \langle K(x_i, \cdot), h(\cdot) \rangle = 0$$

for all $i$. You may recall our proof in the smoothing spline for the same construction of $h$ that has $h(x_i) = 0$ for all $i$. By the reproducing property, we have, for any observations in the training data,  

\begin{align}
f(x_j) =& \langle f(\cdot), K(\cdot, x_j) \rangle \nonumber \\
=& \left\langle \sum_{i=1}^n \alpha_i K(x_i, \cdot) + h(\cdot), K(\cdot, x_j) \right\rangle \nonumber \\
=& \sum_{i=1}^n \alpha_i K(x_i, x_j) + \sum_{i=1}^n \alpha_i h(x_j) \nonumber \\
=& \sum_{i=1}^n \alpha_i K(x_i, x_j)
\end{align}

Which means that, the evaluation of $f(x_j)$ would be the same as just evaluating it on this finite represtantation. Hence the loss function would be the same regardless of whether we have $h$ or not. And also the penalty term of this finite represtantation would be better since 

\begin{align}
\lVert f \rVert^2 =& \lVert \sum_{i=1}^n \alpha_i K(\cdot, x_i) + h(\cdot) \rVert^2 \nonumber \\
=& \lVert \sum_{i=1}^n \alpha_i K(\cdot, x_i) \rVert^2 + \lVert h(\cdot) \rVert^2 \nonumber \\
\geq& \lVert \sum_{i=1}^n \alpha_i K(\cdot, x_i) \rVert^2
\end{align}

This completes the proof since this finite represetnation would be the one being prefered than $f$. 
```







<!--chapter:end:05.2-rkhs.Rmd-->

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bK{\mathbf{K}}
\def\bP{\mathbf{P}}
\def\bQ{\mathbf{Q}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Kernel Ridge Regression

With our understandings of the RKHS and the representer theorem, we can now say that for any regression function models, if we want the solution to be more flexible, we may solve it within a RKHS. For example, consider the following regression problem:

$$\widehat f = \underset{f \in \cH}{\arg\min} \,\, \frac{1}{n} \sum_{i=1}^n \Big(y_i - \widehat f(x_i) \Big)^2 + \lambda \lVert f \rVert_\cH^2$$
Since we know that the solution has to take the form 

$$\widehat f = \sum_{i=1}^n \alpha_i K(x_i, \cdot),$$
we can instead solve the problem as a ridge regression type of problem:

$$\widehat f = \underset{f \in \cH}{\arg\min} \,\, \frac{1}{n} \big\lVert \by - \bK \balpha \big\rVert^2 + \lambda \lVert f \rVert_\cH^2,$$
where $\bK$ is an $n \times n$ matrix with $K(x_i, x_j)$ at its $(i,j)$th element. With some simple calculation, we also have 

\begin{align}
\lVert f \rVert_\cH^2 =& \langle f, f \rangle \nonumber \\
=& \langle \sum_{i=1}^n \alpha_i K(x_i, \cdot), \sum_{j=1}^n \alpha_j K(x_j, \cdot) \rangle \nonumber \\
=& \sum_{i, j} \alpha_i \alpha_j \big\langle K(x_i, \cdot), K(x_j, \cdot) \big\rangle \nonumber \\
=& \sum_{i, j} \alpha_i \alpha_j K(x_i, x_j) \nonumber \\
=& \balpha^\T \bK \balpha
\end{align}

Hence, the problem becomes 

$$\widehat f = \underset{f \in \cH}{\arg\min} \,\, \frac{1}{n} \big\lVert \by - \bK \balpha \big\rVert^2 + \lambda \balpha^\T \bK \balpha.$$
By taking the derivative with respect to $\balpha$, we have (note that $\bK$ is symmetric),

\begin{align}
-\frac{1}{n} \bK^\T (\by - \bK \balpha) + \lambda \bK \balpha \overset{\text{set}}{=} \mathbf{0} \nonumber \\
\bK (- \by + \bK \balpha + n\lambda \balpha) = \mathbf{0}.
\end{align}
This implies 

$$ \balpha = (\bK + n\lambda \bI)^{-1} \by.$$
and we obtained the solution. 

## Example: Linear Kernel and Ridge Regression

When $K(\bx_i, \bx_j) = \bx_i^\T \bx_j$, we also have $\bK = \bX \bX^\T$. We should expect this to match the original ridge regression since this is essentially a linear regression. First, plug this into our previous result, we have 

$$ \balpha = (\bX \bX^\T + n\lambda \bI)^{-1} \by.$$
and the fitted value is 

$$ \widehat{\by} = \bK \balpha = \bX \bX^\T (\bX \bX^\T + n\lambda \bI)^{-1} \by$$
Using a matrix identity $(\bP \bQ + \bI)^{-1}\bP = \bP (\bQ \bP + \bI)^{-1}$, and let $\bQ = \bX = \bP^\T$, we have 

$$ \widehat{\by} = \bK \balpha = \bX (\bX^\T \bX + n\lambda \bI)^{-1} \bX^\T \by$$
and 

$$ \widehat{\by} = \bX \underbrace{\big[ \bX^\T \balpha \big]}_{\bbeta} = \bX \underbrace{\big[ (\bX^\T \bX + n\lambda \bI)^{-1} \bX^\T \by \big]}_{\bbeta}$$


which is simply the Ridge regression solution, and also the corresponding linear regression solution $\widehat{\bbeta} = \bX^\T \widehat{\balpha}$. This makes the penalty term $\balpha^\T \bK \balpha = \balpha^\T \bX \bX^\T \balpha = \bbeta^\T \bbeta$, which maps every thing back to the ridge regression form. 

## Example: Alternative View

This example is motivated from an alternative derivation provided by Prof. Max Welling on his kernel ridge regression lecture note. This understanding matches the SVM primal to dual derivation, but is performed on a linear regression. We can then again switch things to the kernel version (through kernel trick). 

Consider a linear regression

$$\underset{\bbeta}{\text{minimize}} \,\, \frac{1}{n} \lVert \by - \bX \bbeta \rVert^2 + \lambda \lVert \bbeta \rVert^2$$

Introduce a new set of variables 

$$z_i = y_i - \bx_i^\T \bbeta,$$
for $i = 1, \ldots, n$. Then The original problem becomes 

\begin{align}
\underset{\bbeta, \bz}{\text{minimize}} \quad & \frac{1}{2n\lambda} \lVert \bz \rVert^2 + \frac{1}{2} \lVert \bbeta \rVert^2 \nonumber \\
\text{subj. to} \quad & z_i = y_i - \bx_i^\T \bbeta, \,\, i = 1, \ldots, n.
\end{align}

If we use the same strategy from the SVM derivation, we have the Lagrangian

$${\cal L} = \frac{1}{2n\lambda} \lVert \bz \rVert^2 + \frac{1}{2} \lVert \bbeta \rVert^2 + \sum_{i=1}^n \alpha_i (y_i - \bx_i^\T \bbeta - z_i)$$
with $\alpha_i \in \mathbb{R}$. Switching from primal to dual, by taking derivative w.r.t. $\bbeta$ and $\bz$, we have 

\begin{align}
\frac{\partial \cal L}{\partial z_i} =&\, \frac{1}{n\lambda}z_i - \alpha_i = 0, \quad \text{for} \,\, i = 1, \ldots, n, \nonumber \\
\text{and}\,\, \frac{\partial \cal L}{\partial \bbeta} =&\, \bbeta - \sum_{i=1}^n \alpha_i \bx_i = \mathbf{0}
\end{align}

Hence, we have, the estimated $\widehat{\bbeta}$ is $\sum_{i=1}^n \alpha_i \bx_i$ that matches our previous understanding. Also, if we view this as a linear kernel solution, the predicted value of at $\bx$ is 

\begin{align}
f(\bx) =& \,\, \bx^\T \bbeta \nonumber \\
=& \sum_{i=1}^n \alpha_i \bx^\T \bx_i \nonumber \\
=& \sum_{i=1}^n \alpha_i K(\bx, \bx_i).
\end{align}

Now, to complete our dual solution, we plugin these results, and have 

\begin{align}
\underset{\balpha}{\max} \underset{\bz, \bbeta}{\min} {\cal L} =& \frac{n\lambda}{2} \balpha^\T \balpha + \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j x_i^\T x_j + \sum_{j} \alpha_j \big(y_j - x_j^\T \sum_i \alpha_i \bx_i - n\lambda \alpha_i \big) \nonumber \\
 =& - \frac{n\lambda}{2} \balpha^\T \balpha - \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j \langle x_i, x_j \rangle + \sum_{i} \alpha_i y_i \nonumber \\
=& - \frac{n\lambda}{2} \balpha^\T \balpha - \frac{1}{2} \balpha^\T \bK \balpha + \balpha^\T \by
\end{align}

By again taking derivative w.r.t. $\alpha$, we have

$$ - n\lambda \bI \balpha - \bK \balpha + \by = \mathbf{0},$$
and the solution is the same as what we had before 

$$\balpha = (\bK + n\lambda \bI)^{-1} \by$$

<!--chapter:end:05.3-kernelridge.Rmd-->

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\cT{{\cal T}}
\def\cA{{\cal A}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Classification and Regression Trees

A tree model is very simple to fit and enjoys interpretability. It is also the core component of random forest and boosting. Both trees and random forests can be used for classification and regression problems, although trees are not ideal for regressions problems due to its large bias. There are two main stream of tree models,  Classification and Regression Trees (CART, @breiman1984classification) and C4.5 [@quinlan1993c4], which is an improvement of the ID3 (Iterative Dichotomiser 3) algorithm. The main difference is to use binary or multiple splits and the criteria of the splitting rule. In fact the splitting rule criteria is probably the most essential part of a tree. 

## Example: Classification Tree

Let's generate a model with nonlinear classification rule. 

```{r}
    set.seed(1)
    n = 500
    x1 = runif(n, -1, 1)
    x2 = runif(n, -1, 1)
    y = rbinom(n, size = 1, prob = ifelse(x1^2 + x2^2 < 0.6, 0.9, 0.1))
    
    par(mar=rep(2,4))
    plot(x1, x2, col = ifelse(y == 1, "deepskyblue", "darkorange"), pch = 19)
    symbols(0, 0, circles = sqrt(0.6), add = TRUE, inches = FALSE, cex = 2)
```

A classification tree model is recursively splitting the feature space such that eventually each region is dominated by one class. We will use `rpart` as an example to fit trees, which stands for recursively partitioning. 

```{r}
    set.seed(1)
    library(rpart)
    rpart.fit = rpart(as.factor(y)~x1+x2, data = data.frame(x1, x2, y))
    
    # the tree structure    
    par(mar=rep(0.5, 4))
    plot(rpart.fit)
    text(rpart.fit)    

    # if you want to peek into the tree 
    # note that we set cp = 0.041, which is a tuning parameter
    # we will discuss this later
    rpart.fit$cptable
    prune(rpart.fit, cp = 0.041)
```

The model proceed with the following steps. Note that steps 5 and 6 may not be really beneficial (consider that we know the true model). 

```{r fig.dim = c(12, 9), out.width = '90%', echo = FALSE}
  TreeSteps <- function(i)
  {
    plot(x1, x2, col = ifelse(y == 1, "deepskyblue", "darkorange"), pch = 19, yaxt="n", xaxt = "n")
      
    # the four cuts that are performing well
    if(i > 0) lines(x = c(-1, 1), y = c(-0.6444322, -0.6444322), lwd = 2)
    if(i > 1) lines(x = c(0.6941279, 0.6941279), y = c(-0.6444322, 1), lwd = 2)
    if(i > 2) lines(x = c(-1, 0.6941279), y = c(0.7484327, 0.7484327), lwd = 2)
    if(i > 3) lines(x = c(-0.6903174, -0.6903174), y = c(-0.6444322, 0.7484327), lwd = 2)
    
    # the model will go further, but they seem to be over-fitting
    if(i > 4) lines(x = c(-0.7675897, -0.7675897), y = c(-0.6444322, 0.7484327), 
                    lwd = 3, lty = 2, col = "red")
    if(i > 5) lines(x = c(-0.6903174, 0.6941279), y = c(0.3800769, 0.3800769), 
                    lwd = 3, lty = 2, col = "red")           
  }

  par(mfrow = c(2, 3), mar=c(0.5, 0.5, 3, 0.5))
  for (i in c(1,2,3,4,5,6)) 
  {
    TreeSteps(i)
    title(paste("Tree splitting step", i))
  }
```

Alternatively, there are many other packages that can perform the same analysis. For example, the `tree` package. However, be careful that this package uses a different splitting rule by default If you want to match the result, use `split = "gini"`. Note that this plot is very crowded because it will split until pretty much only one class in each terminal node. Hence, you can imaging that there will be a tuning parameter issue. We will discuss this later. 

```{r echo = FALSE}
  par(mfrow = c(1, 1))
```

```{r}
    library(tree)
    tree.fit = tree(as.factor(y)~x1+x2, data = data.frame(x1, x2, y), split = "gini")
    plot(tree.fit)
    text(tree.fit)
```

## Splitting a Node 

In a tree model, the splitting mechanism performs in the following way, which is just comparing all possible splits on all variables. For simplicity, we will assume that a binary splitting rule is used, i.e., we split the current node into to two child nodes, and apply the procedure recursively.

  * At the current node, go through each variable to find the best cut-off point that splits the node.
  * Compare all the best cut-off points across all variable and choose the best one to split the current node and then iterate.
  
So, what error criterion should we use to compare different cut-off points? There are three of them at least:

  * Gini impurity (CART)
  * Shannon entropy (C4.5)
  * Mis-classification error

Gini impurity is used in CART, while ID3/C4.5 uses the Shannon entropy. These criteria have different effects than the mis-classifications error. They usually prefer more "pure" nodes, meaning that it is more likely to single out a set of pure class terminal node if we use Gini impurity and Shannon entropy. This is because their measures are nonlinear. 

Suppose that we have a population (or a set of observations) with $p_k$ proportion of class $k$, for $k = 1, \ldots, K$. Then, the Gini impurity is given by 

$$ \text{Gini} = \sum_{k = 1}^K p_k (1 - p_k) = 1 - \sum_{k=1}^K p_k^2.$$
The Shannon theory is defined as 

$$- \sum_{k=1}^K p_k \log(p_k).$$
And the classification error simply adds up all mis-classified portions if we predict the population into the most prevalent one:

$$ 1 - \underset{k = 1, \ldots, K}{\max} \,\, p_k$$
The following plot shows all three quantities as a function of $p$, when there are only two classes, i.e., $K = 2$. 

```{r echo = FALSE}
    gini <- function(y)
    {
    	p = table(y)/length(y)
    	sum(p*(1-p))
    }
    
    shannon <- function(y)
    {
    	p = table(y)/length(y)
    	-sum(p*log(p))
    }
    
    error <- function(y)
    {
    	p = table(y)/length(y)
    	1 - max(p)
    }
    
    score <- function(TL, TR, measure)
    {
    	nl = length(TL)
    	nr = length(TR)
    	n = nl + nr
    	f <- get(measure)
    	f(c(TL, TR)) - nl/n*f(TL) - nr/n*f(TR)
    }
    
    TL = rep(1, 3)
    TR = c(rep(1, 4), rep(0, 3))
    
    # score(TL, TR, "gini")
    # score(TL, TR, "shannon")
    # score(TL, TR, "error")
    
    x = seq(0, 1, 0.01)
    g = 2*x*(1-x)
    s = -x*log(x) - (1-x)*log(1-x)
    e = 1-pmax(x, 1-x)
    
    par(mar=c(4.2,4.2,2,2))
    plot(x, s, type = "l", lty = 1, col = 3, lwd = 2, ylim = c(0, 1), ylab = "Impurity", xlab = "p", cex.lab = 1.5)
    lines(x, g, lty = 1, col = 2, lwd = 2)
    lines(x, e, lty = 1, col = 4, lwd = 2)
    
    legend("topleft", c("Entropy", "Gini", "Error"), col = c(3,2,4), lty =1, cex = 1.2)
```

For each quantity, smaller value means that the node is more "pure", hence, there is a higher certainty when we predict a new value. The idea of splitting a node is that, we want the two resulting child node to contain less variation. In other words, we want each child node to be as "pure" as possible. Hence, the idea is to calculate this error criterion both before and after the split and see what cut-off point gives us the best reduction of error. Of course, all of these quantities will be calculated based on the sample version, instead of the truth. For example, if we use the Gini impurity to compare different splits, we use the following quantity for an __internal node__ $\cA$:

\begin{align}
\text{score}(j, c) = \text{Gini}(\cA) - \left( \frac{N_{\cA_L}}{N_{\cA}} \text{Gini}(\cA_L) + \frac{N_{\cA_R}}{N_{\cA}} \text{Gini}(\cA_R)  \right).
\end{align}

Here, $\cA_L$ (left child node) and $\cA_R$ (right child node) denote the two child nodes resulted from a potential split on the $j$th variable at a cut-off point $c$, such that 

$$\cA_L = \{\bx: \bx \in \cA, \, x_j \leq c\}$$
and 

$$\cA_R = \{\bx: \bx \in \cA, \, x_j > c\}.$$
Then $N_\cA$, $N_{\cA_L}$, $N_{\cA_R}$ are the number of observations in these nodes, respectively. The implication of this is quite intuitive: $\text{Gini}(\cA)$ calculates the uncertainty of the entire node $\cA$, while the second quantity is a summary of the uncertainty of the two potential child nodes. Hence a larger score indicates a better split, and we may choose the best index $j$ and cut-off point $c$ to proceed, 

$$\underset{j \, , \, c}{\argmax} \,\, \text{score}(j, c)$$

and then work on each child node separately using the same procedure. 

## Regression Trees 

The basic procedure for a regression tree is pretty much the same as a classification tree, except that we will use a different way to evaluate how good a potential split is. Note that the variance is a simple quantity to describe the noise within a node, we can use 

\begin{align}
\text{score}(j, c) = \text{Var}(\cA) - \left( \frac{N_{\cA_L}}{N_{\cA}} \text{Var}(\cA_L) + \frac{N_{\cA_R}}{N_{\cA}} \text{Var}(\cA_R)  \right).
\end{align}

## Predicting a Target Point 

When we have a new target point $\bx_0$ to predict, the basic strategy is to "drop it down the tree". This is simply starting from the root node and following the splitting rule to see which terminal node it ends up with. Note that a fitted tree will have a collection of terminal nodes, say, $\{\cA_1, \cA_2, \ldots, \cA_M\}$, then suppose $\bx_0$ falls into terminal node $\cA_m$, we use $\bar{y}_{\cA_m}$, the average of original training data that falls into this node, as the prediction. The final prediction can be written as 

\begin{align}
\widehat{f}(\bx_0) =& \sum_{m = 1}^M \bar{y}_{\cA_m} \mathbf{1}\{\bx_0 \in \cA_m\} \\
=& \sum_{m = 1}^M \frac{\sum_{i=1}^n y_i \mathbf{1}\{\bx_i \in \cA_m\}}{\sum_{i=1}^n \mathbf{1}\{\bx_i \in \cA_m\}} \mathbf{1}\{\bx_0 \in \cA_m\}.
\end{align}

## Tuning a Tree Model

Tree tuning is essentially about when to stop splitting. Or we could look at this reversely by first fitting a very large tree, then see if we could remove some branches of a tree to make it simpler without sacrificing much accuracy. One approach is called the __cost-complexity pruning__. This is another penalized framework that we use the accuracy as the loss function, and use the tree-size as the penalty part for complexity. Formally, if we have any tree model $\cT$, consider this can be written as 

\begin{align}
C_\alpha(\cT) =&~ \sum_{\text{all terminal nodes $t$ in $\cT$}} N_t \cdot \text{Impurity}(t) + \alpha |\cT| \nonumber \\
=&~ C(\cT) + \alpha |\cT|
\end{align}

Now, we can start with a very large tree, say, fitted until all pure terminal nodes. Call this tree as $\cT_\text{max}$. We can then exhaust all its sub-trees by pruning any branches, and calculate this $C(\cdot)$ function of the sub-tree. Then the tree that gives the smallest value will be our best tree. 

But this can be computationally too expensive. Hence, one compromise, instead of trying all possible sub-trees, is to use the __weakest-link cutting__. This means that, we cut the branch (essentially a certain split) that displays the weakest banefit towards the $C(\cdot)$ function. The procedure is the following:


  * Look at an internal node $t$ of $\cT_\text{max}$, and denote the entire branch starting from $t$ as $\cT_t$
  * Compare: remove the entire branch (collapse $\cT_t$ into a single terminal node) vs. keep $T_t$. To do this, calculate
    $$\alpha \leq \frac{C(t) - C(\cT_t)}{|T_t| - 1}$$
    Note that $|\cT_t| - 1$ is the size difference between the two trees.
  * Try all internal nodes $t$, and cut the branch $t$ that has the smallest value on the right hand side. This gives the smallest $\alpha$ value to remove some branches. Then iterate the procedure based on this reduced tree. 

Note that the $\alpha$ values will get larger as we move more branches. Hence this produces a solution path. Now this is very similar to the Lasso solution path idea, and we could use cross-validation to select the best tuning. By default, the `rpart` function uses a 10-fold cross-validation. This can be controlled using the `rpart.control()` function and specify the `xval` argument. For details, please see the [documentation](https://cran.r-project.org/web/packages/rpart/rpart.pdf). The following plot using `plotcp()` in the `rpart` package gives a visualization of the relative cross-validation error. It also produces a horizontal line (the dotted line). It suggests the lowest (plus certain variation) that we could achieve. Hence, we will select the best `cp` value ($alpha$) that is above this line. The way that this is constructed is similar to the `lambda.1se` choice in `glmnet`. 

```{r}
  # and the tuning parameter 
  plotcp(rpart.fit)  
  printcp(rpart.fit)
```
    

<!--chapter:end:05.4-tree.Rmd-->

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Random Forests

Roughly speaking, random forests [@breiman2001random] are parallelly fitted CART models with some randomness. There are several main components: 

  * Bootstrapping of data for each tree using the Bagging idea [@breiman1996bagging], and use the averaged result (for regression) or majority voting (for classification) of all trees as the prediction. 
  * At each internal node, we may not consider all variables. Instead, we consider a randomly selected `mtry` variables to search for the best split. This idea was inspired by @ho1998random.
  * For each tree, we will not perform pruning. Instead, we simply stop when the internal node contains no more than `nodesize` number of observations. 

Later on, there were various version of random forests that attempts to improve the performance, from both computational and theoretical prospective. We will introduce them later. 

## Bagging Predictors

CART models may be difficult when dealing with non-axis-aligned decision boundaries. This can be seen from the example below, in a two-dimensional case. The idea of Bagging is that we can fit many CART models, each from a Bootstrap sample, i.e., sample with replacement from the original $n$ observations. The reason that Breiman considered bootstrap samples is because it can approximate the original distribution that generates the data. But the end result is that since each tree may be slightly different from each other, when we stack them, the decision bound can be more "smooth". 

```{r}
  # generate some data 
  set.seed(2)
  n = 1000
  x1 = runif(n, -1, 1)
  x2 = runif(n, -1, 1)
  y = rbinom(n, size = 1, prob = ifelse((x1 + x2 > -0.5) & (x1 + x2 < 0.5) , 0.8, 0.2))
  xgrid = expand.grid(x1 = seq(-1, 1, 0.01), x2 = seq(-1, 1, 0.01))
```

```{r echo = FALSE} 
  par(mfrow=c(1, 2), mar=c(0.5, 0.5, 2, 0.5))
```

Let's compare the decision rule of CART and Bagging. For CART, the decision line has to be aligned to axis. For Bagging, we use a total of 200 trees, specified by `nbagg` in the `ipred` package.  

```{r fig.dim = c(12, 6), out.width = "90%"}
  # fit CART
  library(rpart)
  rpart.fit = rpart(as.factor(y)~x1+x2, data = data.frame(x1, x2, y))

  # we could fit a different tree using a bootstrap sample
  # rpart.fit = rpart(as.factor(y)~x1+x2, data = data.frame(x1, x2, y)[sample(1:n, n, replace = TRUE), ])

  pred = matrix(predict(rpart.fit, xgrid, type = "class") == 1, 201, 201)
  contour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels=0.5, labels="",axes=FALSE)
  points(x1, x2, col = ifelse(y == 1, "deepskyblue", "darkorange"), pch = 19, yaxt="n", xaxt = "n")
  points(xgrid, pch=".", cex=1.2, col=ifelse(pred, "deepskyblue", "darkorange"))
  box()    
  title("CART")
 
  # fit Bagging
  library(ipred)
  bag.fit = bagging(as.factor(y)~x1+x2, data = data.frame(x1, x2, y), nbagg = 200, ns = 400)
  pred = matrix(predict(prune(bag.fit), xgrid) == 1, 201, 201)
  contour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels=0.5, labels="",axes=FALSE)
  points(x1, x2, col = ifelse(y == 1, "deepskyblue", "darkorange"), pch = 19, yaxt="n", xaxt = "n")
  points(xgrid, pch=".", cex=1.2, col=ifelse(pred, "deepskyblue", "darkorange"))
  box()
  title("Bagging")
```

## Random Forests

Random forests are equipped with this Bootstrapping strategy, but also with other things, which are mentioned previously. They are controlled by several key parameters: 

  * `ntree`: number of trees
  * `sampsize`: how many samples to use when fitting each tree
  * `mtry`: number of randomly sampled variable to consider at each internal node
  * `nodesize`: stop splitting when the node sample size is no larger than `nodesize`
  
Using the `randomForest` package, we can fit the model. It is difficult to visualize this when `p > 2`. But we can look at the testing error. 

```{r echo = FALSE} 
    par(mfrow=c(1, 1), mar=c(0.5, 0.5, 2, 0.5))
```

```{r, fig.dim = c(6, 6), out.width = "45%"}
  # generate some data with larger p
  set.seed(2)
  n = 1000
  p = 10
  X = matrix(runif(n*p, -1, 1), n, p)
  x1 = X[, 1]
  x2 = X[, 2]
  y = rbinom(n, size = 1, prob = ifelse((x1 + x2 > -0.5) & (x1 + x2 < 0.5), 0.8, 0.2))
  xgrid = expand.grid(x1 = seq(-1, 1, 0.01), x2 = seq(-1, 1, 0.01))

  # fit random forests with a selected tuning
  library(randomForest)
  rf.fit = randomForest(X, as.factor(y), ntree = 1000, 
                        mtry = 7, nodesize = 10, sampsize = 800)
```

Instead of generating a set of testing samples labels, let's directly compare with the "true" decision rule, the Bayes rule. 
    
```{r}
  # the testing data 
  Xtest = matrix(runif(n*p, -1, 1), n, p)
  
  # the Bayes rule
  BayesRule = ifelse((Xtest[, 1] + Xtest[, 2] > -0.5) & 
                     (Xtest[, 1] + Xtest[, 2] < 0.5), 1, 0)
  
  mean( (predict(rf.fit, Xtest) == "1") == BayesRule )
```

## Effect of `mtry`

In the two dimensional setting, we probably won't see much difference by using random forests, since the only effective change is `mtry = 1`, which is not really different than `mtry = 2` (the CART choice). You can try this by yourself.
However, the difference would be significant in higher dimensional settings, in our case $p=10$. This is again an issue of bias-variance trade-off. The intuition is that, when we use a small `mtry`, and when $p$ is large, we may by chance randomly select some irrelevant variables that has nothing to do with the outcome. Then this particular split would be wasted. Missing the true variable may cause larger bias. On the other hand, when we use a large `mtry`, we will be greedy for signals since we compare many different variables and pick the best one. But this is also as the risk of over-fitting. Hence, tuning is necessary. 

Just as an example, let's try a small `mtry`:

```{r}
  rf.fit = randomForest(X, as.factor(y), ntree = 1000, 
                        mtry = 1, nodesize = 10, sampsize = 800)

  mean( (predict(rf.fit, Xtest) == "1") == BayesRule )
```

## Effect of `nodesize`

When we use a small `nodesize`, we are at the risk of over-fitting. This is similar to the 1NN example. When we use large `nodesize`, there could be under-fitting. 

## Variable Importance 

Random forests model provides a way to evaluate the importance of each variable. This can be done by specifying the `importance` argument. We usually use the `MeanDecreaseAccuracy` or `MeanDecreaseGini` column as the summary of the importance of each variable. 

```{r fig.dim=c(6,6), out.width = '45%'}
  rf.fit = randomForest(X, as.factor(y), ntree = 1000, 
                        mtry = 7, nodesize = 10, sampsize = 800,
                        importance=TRUE)

  importance(rf.fit)
```

## Kernel view of Random Forets

```{r echo = FALSE}
  # define a function to extract kernel
  rf.kernel.weights <- function(rffit, x, testx)
  {
      if (ncol(x) != length(testx))
          stop("dimention of x and test differ.")
      
      if (is.null(rffit$inbag))
          stop("the random forest fitting must contain inbag information")
      
      register = matrix(NA, nrow(x), rffit$ntree)
      
      for (i in 1:nrow(x))
          register[i, ] = attributes(predict(rffit, x[i,], node = TRUE))$nodes
      
      regi = attributes(predict(rffit, testx, node = TRUE))$nodes
      
      return(rowSums( sweep(register, 2, regi, FUN = "==")*rffit$inbag ))
  }

  plotRFKernel <- function(rffit, x, onex)
  {
      wt = rf.kernel.weights(rffit, x, onex)
      wt = wt/max(wt)
      
      contour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels=0.5, labels="",axes=FALSE)
      points(x1, x2, cex = 4*wt^(2/3), pch = 1, cex.axis=1.25, lwd = 2)
      points(x1, x2, col = ifelse(y == 1, "deepskyblue", "darkorange"), pch = 19, cex = 0.75, yaxt="n", xaxt = "n")
      points(xgrid, pch=".", cex=1.2, col=ifelse(pred, "deepskyblue", "darkorange"))
      points(onex[1], onex[2], pch = 4, col = "red", cex =4, lwd = 6)        
      box()
  }
```

I wrote a small function that will extract the kernel weights from a random forests for predicting a testing point $x$. This is essentially the counts for how many times a training data falls into the same terminal node as $x$. Since the prediction on $x$ are essentially the average of them in a weighted fashion, this is basically a kernel averaging approach. However, the kernel weights are adaptive to the true structure. 

```{r fig.dim=c(12,6), out.width = '90%'}
  # generate the 2 dimensional case
  set.seed(2)
  n = 1000
  x1 = runif(n, -1, 1)
  x2 = runif(n, -1, 1)
  y = rbinom(n, size = 1, prob = ifelse((x1 + x2 > -0.5) & (x1 + x2 < 0.5) , 0.8, 0.2))
  xgrid = expand.grid(x1 = seq(-1, 1, 0.01), x2 = seq(-1, 1, 0.01))
  
  # fit a random forest model
  rf.fit = randomForest(cbind(x1, x2), as.factor(y), ntree = 300, 
                        mtry = 1, nodesize = 20, keep.inbag = TRUE)
  pred = matrix(predict(rf.fit, xgrid) == 1, 201, 201)
  
  par(mfrow=c(1,2), mar=c(0.5, 0.5, 2, 0.5))

  # check the kernel weight at different points
  plotRFKernel(rf.fit, data.frame(cbind(x1, x2)), c(-0.1, 0.4))
  plotRFKernel(rf.fit, data.frame(cbind(x1, x2)), c(0, 0.6))
```

As contrast, here is the regular Gaussian kernel weights (after some tuning). This effect will play an important role when $p$ is large. 

```{r echo = FALSE}
  par(mfrow=c(1,1))
```

```{r fig.dim=c(6, 6), out.width = '45%'}
  # Gaussian kernel weights
  onex = c(-0.1, 0.4)
  h = 0.2
  wt = exp(-0.5*rowSums(sweep(cbind(x1, x2), 2, onex, FUN = "-")^2)/h^2)
  contour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, 
          levels=0.5, labels="",axes=FALSE)
  points(x1, x2, cex = 4*wt^(2/3), pch = 1, cex.axis=1.25, lwd = 2)
  points(x1, x2, col = ifelse(y == 1, "deepskyblue", "darkorange"), 
         pch = 19, cex = 0.75, yaxt="n", xaxt = "n")
  points(xgrid, pch=".", cex=1.2, 
         col=ifelse(pred, "deepskyblue", "darkorange"))
  points(onex[1], onex[2], pch = 4, col = "red", cex =4, lwd = 6)
  box()
```

<!--chapter:end:05.5-rf.Rmd-->

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Boosting

Boosting is another ensemble model, created in the form of 

$$F_T(x) = \sum_{t = 1}^T \alpha_t f_t(x)$$

However, it is different from random forest, in which each $f_t(x)$ is learned parallelly. These $f_t(x)$'s are called weak learners and are constructed __sequentially__, with coefficients $\alpha_t$'s to represent their weights. The most classical model, AdaBoost was proposed by @freund1997decision for classification problems, and a more statically view of this model called gradient boosting machines [@friedman2001greedy] can handle any loss function we commonly use. We will first introduce AdaBoost and then discuss gradient boosting. 

## AdaBoost

Following our common notation, we observe a set of data $\{\bx_i, y_i\}_{i=1}^n$. Similar to SVM, we code $y_i$s as $-1$ or $1$. The AdaBoost works by creating $F_T(x)$ sequentially and use $\text{sign}(F_T(x))$ as the classification rule. The algorithm is given in the following:

  * Initiate weights $w_i^{(1)} = 1/n$, for $i = 1, \ldots, n$
  * For $t = 1, \ldots, T$, do
     + Fit a classifier $f_t(x)$ to the training data with subject weights $w_i^{(t)}$'s. 
     + Compute the weighed error rate 
     $$\epsilon_t = \sum_{i=1}^n w_i^{(t)} \mathbf{1}\{y_i \neq f_t(x_i) \}$$
     + Compute 
     $$\alpha_t = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t}$$
     + Update subject weights
     $$w_i^{(t + 1)} = \frac{1}{Z_t} w_i^{(t)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\}$$
     where $Z_t$ is a normalizing constant make $w_i^{(t + 1)}$'s sum up to 1:
     $$Z_t = \sum_{i=1}^n w_i^{(t)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\}$$
  * Output the final model
  $$F_T(x) = \sum_{t = 1}^T \alpha_t f_t(x)$$
  and the decision rule is $\text{sign}(F_T(x))$.

An important mechanism in AdaBoost is the weight update step. We can notice that the weight is increased if $\exp\big\{ - \alpha_t y_i f_t(x_i) \big\}$ is larger than 1. This is simply when $y_i f_t(x_i)$ is negative, i.e., subject $i$ got mis-classified by $f_t$ at this iteration. Hence, during the next iteration $t+1$, the model $f_{(t+1)}$ will more likely to address this subject. Here, $f_t$ can be any classification model, for example, we could use a tree model. The following figures demonstrate this idea of updating weights and aggregate the learners. 

```{r include = FALSE}
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,2))
```

```{r fig.dim = c(12, 6), out.width = "90%"}
  x1 = seq(0.1, 1, 0.1)
  x2 = c(0.5, 0.3, 0.1, 0.6, 0.7,
         0.8, 0.5, 0.7, 0.8, 0.2)
  
  # the data
  y = c(1, 1, -1, -1, 1, 
        1, -1, 1, -1, -1)
  X = cbind("x1" = x1, "x2" = x2)
  xgrid = expand.grid("x1" = seq(0, 1.1, 0.01), "x2" = seq(0, 0.9, 0.01))
  
  # plot data
  plot(X[, 1], X[, 2], col = ifelse(y > 0, "deepskyblue", "darkorange"),
       pch = ifelse(y > 0, 4, 1), xlim = c(0, 1.1), lwd = 3,
       ylim = c(0, 0.9), cex = 3)
  
  # fit gbm with 3 trees
  library(gbm)
  gbm.fit = gbm(y ~., data.frame(x1, x2, y= as.numeric(y == 1)), 
                distribution="adaboost", interaction.depth = 1, 
                n.minobsinnode = 1, n.trees = 3, 
                shrinkage = 1, bag.fraction = 1)
  
  # you may peek into each tree
  pretty.gbm.tree(gbm.fit, i.tree = 1)
  
  # we can view the predicted decision rule
  plot(X[, 1], X[, 2], col = ifelse(y > 0, "deepskyblue", "darkorange"),
       pch = ifelse(y > 0, 4, 1), xlim = c(0, 1.1), lwd = 3,
       ylim = c(0, 0.9), cex = 3)
  pred = predict(gbm.fit, xgrid)
  points(xgrid, col = ifelse(pred > 0, "deepskyblue", "darkorange"), 
         cex = 0.2)
```

Here is a rundown of the algorithm. Let's initialize all weights as $1/n$. We only used trees with a single split as weak learners. The first tree is splitting at $X_1 = 0.25$. After the first split, we need to adjust the weights. 


```{r include = FALSE}
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,2))
```

```{r fig.dim = c(12, 6), out.width = "90%"}
  w1 = rep(1/10, 10)
  f1 <- function(x) ifelse(x[, 1] < 0.25, 1, -1)
  e1 = sum(w1*(f1(X) != y))
  a1 = 0.5*log((1-e1)/e1)
  
  w2 = w1*exp(- a1*y*f1(X))
  w2 = w2/sum(w2)
  
  # the first tree
  plot(X[, 1], X[, 2], col = ifelse(y > 0, "deepskyblue", "darkorange"),
       pch = ifelse(y > 0, 4, 1), xlim = c(0, 1.1), lwd = 3,
       ylim = c(0, 0.9), cex = 3)
  
  pred = f1(xgrid)
  points(xgrid, col = ifelse(pred > 0, "deepskyblue", "darkorange"), 
         cex = 0.2)
  
  # weights after the first tree
  plot(X[, 1], X[, 2], col = ifelse(y > 0, "deepskyblue", "darkorange"),
       pch = ifelse(y > 0, 4, 1), xlim = c(0, 1.1), lwd = 3,
       ylim = c(0, 0.9), cex = 30*w2)
```

We can notice that the observations got correctly classified will decrease their weights while those mis-classified will increase the weights. 

```{r fig.dim = c(12, 6), out.width = "90%"}
  f2 <- function(x) ifelse(x[, 2] > 0.65, 1, -1)
  e2 = sum(w2*(f2(X) != y))
  a2 = 0.5*log((1-e2)/e2)
  
  w3 = w2*exp(- a2*y*f2(X))
  w3 = w3/sum(w3)
  
  # the second tree
  plot(X[, 1], X[, 2], col = ifelse(y > 0, "deepskyblue", "darkorange"),
       pch = ifelse(y > 0, 4, 1), xlim = c(0, 1.1), lwd = 3,
       ylim = c(0, 0.9), cex = 30*w2)
  
  pred = f2(xgrid)
  points(xgrid, col = ifelse(pred > 0, "deepskyblue", "darkorange"), 
         cex = 0.2)
  
  # weights after the second tree
  plot(X[, 1], X[, 2], col = ifelse(y > 0, "deepskyblue", "darkorange"),
       pch = ifelse(y > 0, 4, 1), xlim = c(0, 1.1), lwd = 3,
       ylim = c(0, 0.9), cex = 30*w3)
```

And then we have the third step. Combining all three steps and their decision function, we have the final classifier 

\begin{align}
F_3(x) =& \sum_{t=1}^3 \alpha_t f_t(x) \nonumber \\
=& 0.4236 \cdot f_1(x) + 0.6496 \cdot f_2(x) + 0.9229 \cdot f_3(x)
\end{align}

```{r fig.dim = c(12, 6), out.width = "90%"}
  f3 <- function(x) ifelse(x[, 1] < 0.85, 1, -1)
  e3 = sum(w3*(f3(X) != y))
  a3 = 0.5*log((1-e3)/e3)
  
  # the third tree
  plot(X[, 1], X[, 2], col = ifelse(y > 0, "deepskyblue", "darkorange"),
       pch = ifelse(y > 0, 4, 1), xlim = c(0, 1.1), lwd = 3,
       ylim = c(0, 0.9), cex = 30*w3)
  
  pred = f3(xgrid)
  points(xgrid, col = ifelse(pred > 0, "deepskyblue", "darkorange"), 
         cex = 0.2)
  
  # the final decision rule 
  plot(X[, 1], X[, 2], col = ifelse(y > 0, "deepskyblue", "darkorange"),
       pch = ifelse(y > 0, 4, 1), xlim = c(0, 1.1), lwd = 3,
       ylim = c(0, 0.9), cex = 3)
  
  pred = a1*f1(xgrid) + a2*f2(xgrid) + a3*f3(xgrid)
  points(xgrid, col = ifelse(pred > 0, "deepskyblue", "darkorange"), 
         cex = 0.2)
  abline(v = 0.25) # f1
  abline(h = 0.65) # f2
  abline(v = 0.85) # f3
```

## Training Error of AdaBoost

There is an interesting property about the boosting algorithm that if we can always find a classifier that performs better than random guessing at each iteration $t$, then the training error will eventually converge to zero. This works by analyzing the weight after the last iteration $T$:

\begin{align}
w_i^{(T+1)} =& \frac{1}{Z_T} w_i^{(T)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\} \nonumber \\
=& \frac{1}{Z_1\cdots Z_T} w_i^{(1)} \prod_{t = 1}^T \exp\big\{ - \alpha_t y_i f_t(x_i) \big\} \nonumber \\
=& \frac{1}{Z_1\cdots Z_T} \frac{1}{n} \exp\Big\{ - y_i \sum_{t = 1}^T \alpha_t f_t(x_i) \Big\}
\end{align}

Since $\sum_{t = 1}^T \alpha_t f_t(x_i)$ is just the model at the $T$-th iteration, we can write it as $F_T(x_i)$. Noticing that they sum up to 1, we have 

$$1 = \sum_{i = 1}^n w_i^{(T+1)} = \frac{1}{Z_1\cdots Z_T} \frac{1}{n} \sum_{i = 1}^n \exp\big\{ - y_i F_T(x_i) \big\}$$
and 
$$Z_1\cdots Z_T = \frac{1}{n} \sum_{i = 1}^n \exp\big\{ - y_i F_T(x_i) \big\}$$
On the right-hand-side, this is the exponential loss after we fit the model. In fact, this quantity would bound above the 0/1 loss, since the exponential loss is $\exp[ - y f(x) ]$,

  * For correctly classified subjects, $y f(x) > 0$, and $\exp[ - y f(x) ] > 0$
  * For incorrectly classified subjects, $y f(x) < 0$ the exponential loss is larger than 1

This means that 

$$Z_1\cdots Z_T > \frac{1}{n} \sum_{i = 1}^n \mathbf{1} \big\{ y_i \neq \text{sign}(F_T(x_i)) \big\}$$
Hence, if we want the final model to have low training error, we should bound above the $Z_t$'s. Recall that $Z_t$ is used to normalize the weights, we have 

$$Z_t = \sum_i^{n} w_i^{(t)} \exp[ - \alpha_t y_i f_t(x_i) ].$$
We have two cases at this iteration, $y_i f(x_i) = 1$ for correct subjects, and $y_i f(x_i) = -1$ for the incorrect ones, hence, 
By our definition, $\epsilon_t = \sum_i w_i^{(t)} \mathbf{1} \big\{ y_i \neq f_t(x_i) \big\}$ is the proportion of weights for mis-classified samples.
\begin{align}
Z_t =& \,\,\sum_{i=1}^n w_i^{(t)} \exp[ - \alpha_t y_i f_t(x_i)] \nonumber\\
=&\,\,\sum_{y_i = f_t(x_i)} w_i^{(t)} \exp[ - \alpha_t ] +  \sum_{y_i \neq f_t(x_i)} w_i^{(t)} \exp[ \alpha_t ] \nonumber\\
=& \,\, \exp[ - \alpha_t ] \sum_{y_i = f_t(x_i)} w_i^{(t)} + \exp[ \alpha_t ] \sum_{y_i \neq f_t(x_i)} w_i^{(t)}
\end{align}

So we have 

$$ Z_t = (1 - \epsilon_t) \exp[ - \alpha_t ] + \epsilon_t \exp[ \alpha_t ].$$

If we want to minimize the product of all $Z_t$'s, we can consider minimizing each of them. Let's consider this as a function of $\alpha_t$, then by taking a derivative with respect to $\alpha_t$, we have 

$$ - (1 - \epsilon_t) \exp[ - \alpha_t ] + \epsilon_t \exp[ \alpha_t ] = 0$$
and 

$$\alpha_t = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t}.$$
Plugging this back into $Z_t$, we have 

$$Z_t = 2 \sqrt{\epsilon_t(1-\epsilon_t)}$$
Since $\epsilon_t(1-\epsilon_t)$ can only attain maximum of $1/4$, $Z_t$ must be smaller than 1. This makes the product $Z_1 \cdots Z_T$ converging to 0. If we look at this more closely, by defining $\gamma_t = \frac{1}{2} - \epsilon_t$ as the improvement from a random model (with error $1/2$), then 

\begin{align}
Z_t =& 2 \sqrt{\epsilon_t(1-\epsilon_t)} \nonumber \\
=& \sqrt{1 - 4 \gamma_t^2} \nonumber \\
\leq& \exp\big[ - 2 \gamma_t^2 \big]
\end{align}

The last equation is because by Taylor expansion, $\exp\big[ - 4 \gamma_t^2 \big] \geq 1 - 4 \gamma_t^2$. Then, we can finally put all $Z_t$'s together:

\begin{align}
\text{Training Error} =& \sum_{i = 1}^n \mathbf{1} \big\{ y_i \neq \text{sign}(F_T(x_i)) \big\} \nonumber \\
=& \sum_{i = 1}^n \exp \big[ - y_i \neq F_T(x_i) \big] \nonumber \\
=& Z_1 \cdots Z_T \nonumber \\
\leq& \exp \big[ - 2 \sum_{t=1}^T \gamma_t^2 \big],
\end{align}

which converges to 0 as long as $\sum_{t=1}^T \gamma_t^2$ accumulates up to infinite. But of course, in practice, it would increasing difficult find $f_t(x)$ that reduces the training error greatly. 

## Gradient Boosting

Let's take an alternative view of this problem, we use an additive structure to fit models

$$F_T(x) = \sum_{t = 1}^T \alpha_t f(x; \btheta_t)$$

by minimizing a loss function 

$$\underset{\{\alpha_t, \btheta_t\}_{t=1}^T}{\min} \sum_{i=1}^n L\big(y_i, F_T(x_i)\big)$$
In this framework, we may choose a loss function $L$ that is suitable for the problem, and also choose the base learner $f(x; \btheta)$ with parameter $\btheta$. Examples of this include linear function, spline, tree, etc.. While it maybe difficult to minimize over all parameters  $\{\alpha_t, \btheta_t\}_{t=1}^T$, we may consider doing this in a stage-wise fashion. The algorithm could work in the following way:

  * Set $F_0(x) = 0$
  * For $t = 1, \ldots, T$
      + Choose $(\alpha_t, \btheta_t)$ to minimize the loss
        $$\underset{\alpha, \btheta}{\min} \,\, \sum_{i=1}^n L\big(y_i, F_{t-1}(x_i) + \alpha f(x_i; \btheta)\big)$$
      + Update $F_t(x) = F_{t-1}(x) + \alpha_t f(x; \btheta_t)$
  * Output $F_T(x)$ as the final model

The previous AdaBoost example is using exponential loss function. Also, it doesn't pick an optimal $f(x; \btheta)$ at each step. We just need a model that is better than random. The step size $\alpha_t$ is optimized at each $t$ given the fitted $f(x; \btheta_t)$. 

Another example is the forward stage-wise linear regression. In this case, we fit a single variable linear model at each step $t$:

$$f(x, j) = \text{sign}\big(\text{Cor}(X_j, \br)\big) X_j$$
  * $\br$ is the residual, as $r_i = y_i - F_{t-1}(x_i)$
  * $j$ is the index that has the largest absolute correlation with $\br$

Then we give a very small step size $\alpha_t$, say, $\alpha_t = 10^{-5}$, and with sign equal to the correlation between $X_j$. In this case, $F_t(x)$ is almost equivalent to the Lasso solution path, as $t$ increases. 

We may notice that $r_i$ is in fact the negative gradient of the squared-error loss, as a function of the fitted function:

$$r_{it} = - \left[ \frac{\partial \, \big(y_i - F(x_i)\big)^2 }{\partial \, F(x_i)} \right]_{F(x_i) = F_{t-1}(x_i)}$$
and we are essentially fitting a weak leaner $f_t(x)$ to the residuals and update the fitted model $F_t(x)$. The following example shows the result of using a tree leaner as $f_t(x)$:

```{r include = FALSE}
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

```{r out.width="45%"}
  library(gbm)

  # a simple regression problem
  p = 1
  x = seq(0, 1, 0.001)
  fx <- function(x) 2*sin(3*pi*x)
  y = fx(x) + rnorm(length(x))

  plot(x, y, pch = 19, ylab = "y", col = "gray", cex = 0.5)
  # plot the true regression line
  lines(x, fx(x), lwd = 2, col = "deepskyblue")
```

We can see that the fitted model progressively approaximates the true function. 

```{r out.width="90%", fig.dim = c(12, 8)}
  # fit regression boosting
  # I use a very large shrinkage value for demonstrating the functions
  # in practice you should use 0.1 or even smaller values for stability
  gbm.fit = gbm(y~x, data = data.frame(x, y), distribution = "gaussian",
                n.trees=300, shrinkage=0.5, bag.fraction=0.8)

  # somehow, cross-validation for 1 dimensional problem creates error
  # gbm(y ~ ., data = data.frame(x, y), cv.folds = 3) # this produces an error  
  
  # plot the fitted regression function at several iterations
  par(mfrow=c(2,3))
  size=c(1,5,10,50,100,300)
  
  for(i in 1:6)
  {
    par(mar=c(2,2,3,1))
    plot(x, y, pch = 19, ylab = "y", col = "gray", cex = 0.5)
    lines(x, fx(x), lwd = 2, col = "deepskyblue")
    
    # this returns the fitted function, but not class
    Fx = predict(gbm.fit, n.trees=size[i])
    lines(x, Fx, lwd = 3, col = "darkorange")
    title(paste("# of Iterations = ", size[i]))
  }
```

This idea can be generalized to any loss function $L$. This is the __gradient boosting__ model:

  * At each iteration $t$, calculate ``pseudo-residuals'', i.e., the negative gradient for each observation
  $$g_{it} = - \left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F(x_i) = F_{t-1}(x_i)}$$
  * Fit $f_t(x, \btheta_t)$ to pseudo-residual $g_{it}$'s
  * Search for the best \alert{step length} 
  $$\alpha_t = \underset{\alpha}{\arg\min} \sum_{i=1}^n L\big(y_i, F_{t-1}(x_i) + \alpha f(x_i; \btheta_t)\big)$$
  * Update $F_t(x) = F_{t-1}(x) + \alpha_t f(x; \btheta_t)$

Hence, the only change when modeling different outcomes is to choose the loss function $L$, and derive the pseudo-residuals

  * For regression, the loss is $\frac{1}{2} (y  - f(x))^2$, and the pseudo-residual is $y_i - f(x_i)$
  * For quantile regression to model median, the loss is $|y  - f(x)|$, and the pseudo-residual is sign$(y_i - f(x_i))$ \\
  * For classification, we can use the deviance $y\log(p) + (1-y)\log(1-p)$, and express $p$ as the log-odds of a scale predictor, i.e., $f = \log(p/(1-p))$. Then the pseudo-residual is $y_i - p(x_i)$






    

<!--chapter:end:05.6-boosting.Rmd-->

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# (PART) Unsupervised Learning {-}

# K-Means

## Basic Concepts

The $k$-means clustering algorithm attempts to solve the following optimization problem: 

$$ \underset{C, \, \{m_k\}_{k=1}^K}\min \sum_{k=1}^K \sum_{C(i) = k} \lVert x_i - m_k \rVert^2, $$
where $C(\cdot): \{1, \ldots, n\} \rightarrow \{1, \ldots, K\}$ is a cluster assignment function, and $m_k$'s are the cluster means. To solve this problem, $k$-means uses an iterative approach that updates $C(\cdot)$ and $m_k$'s alternatively. Suppose we have a set of six observations. 

```{r echo = FALSE, out.width = "35%"}
    set.seed(3)
    x = replicate(2, rnorm(6))
    par(mar=rep(0.5, 4))
    plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5, xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), pch = 19)
```

We first randomly assign them into two clusters (initiate a random $C$ function). Based on this cluster assignment, we can calculate the corresponding cluster mean $m_k$'s.

```{r echo = FALSE}
  par(mfrow=c(1,2))
  par(mar=rep(0.5, 4))
```

```{r fig.dim = c(12, 6), out.width = "70%", echo = FALSE}
  set.seed(3)
  C = sample(1:2, 6, replace = TRUE)
  
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5, 
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), 
       pch = 19, col = c("darkorange", "deepskyblue")[C])
  
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5, 
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), 
       pch = 19, col = c("darkorange", "deepskyblue")[C])
  
  m1 = colMeans(x[C==1, ])
  m2 = colMeans(x[C==2, ])
  points(m1[1], m1[2], col = "darkorange", pch = 4, cex = 2, lwd = 4)
  points(m2[1], m2[2], col = "deepskyblue", pch = 4, cex = 2, lwd = 4)
```

Then we will assign each observation to the closest cluster mean. In this example, only the blue point on the top will be moved to a new cluster. Then the cluster means can then be recalculated.

```{r fig.dim = c(12, 6), out.width = "70%", echo = FALSE}
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5, 
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), 
       pch = 19, col = c("darkorange", "deepskyblue")[C])
  
  points(m1[1], m1[2], col = "darkorange", pch = 4, cex = 2, lwd = 4)
  points(m2[1], m2[2], col = "deepskyblue", pch = 4, cex = 2, lwd = 4)
  arrows(x[2, 1], x[2, 2], -0.9041313, 0.7644366, length = 0.15)
  
  C[2] = 1
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5, 
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), pch = 19, 
       col = c("darkorange", "deepskyblue")[C])
  
  m1 = colMeans(x[C==1, ])
  m2 = colMeans(x[C==2, ])
  points(m1[1], m1[2], col = "darkorange", pch = 4, cex = 2, lwd = 4)
  points(m2[1], m2[2], col = "deepskyblue", pch = 4, cex = 2, lwd = 4)
```

When there is nothing to move anymore, the algorithm stops. Keep in mind that we started with a random cluster assignment, and this objective function is not convex. Hence we may obtain different results if started with different values. The solution is to try different starting points and use the best final results. This can be tuned using the `nstart` parameter in the `kmeans()` function.

```{r}    
    # some random data
    set.seed(1)
    mat = matrix(rnorm(1000), 50, 20)
    
    # if we use only one starting point
    kmeans(mat, centers = 3, nstart = 1)$tot.withinss
    
    # if we use multiple starting point and pick the best one
    kmeans(mat, centers = 3, nstart = 100)$tot.withinss
```    

## Example 1: `iris` data

We use the classical `iris` data as an example. This dataset contains three different classes, but the goal here is to learn the clusters without knowing the class labels.

```{r}
    # plot the original data using two variables
    head(iris)
    library(ggplot2)
    ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
```

The last two variables in the `iris` data carry more information on separating the three classes. Hence we will only use the `Petal.Length` and `Petal.Width`.

```{r echo = FALSE}
  par(mfrow=c(1,1))
  par(mar=rep(0.5, 4))
```

```{r}
    library(colorspace)
    par(mar = c(3, 2, 4, 2), xpd = TRUE)
    MASS::parcoord(iris[, -5], col = rainbow_hcl(3)[iris$Species], 
                   var.label = TRUE, lwd = 2)
    legend(x = 1.2, y = 1.3, cex = 1,
       legend = as.character(levels(iris$Species)),
        fill = rainbow_hcl(3), horiz = TRUE)
```

Let's perform the $k$-means clustering

```{r}
  set.seed(1)

  # k mean clustering
  iris.kmean <- kmeans(iris[, 3:4], centers = 3, nstart = 20)
  
  # the center of each class
  iris.kmean$centers
  
  # the within cluster variation 
  iris.kmean$withinss
  
  # the between cluster variation 
  iris.kmean$betweenss
  
  # plot the fitted clusters vs. the truth
  iris.kmean$cluster <- as.factor(iris.kmean$cluster)
  
  ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + # true cluster
    geom_point(alpha = 0.3, size = 3.5) + 
    scale_color_manual(values = c('red', 'green', 'blue')) +
    geom_point(col = c("blue", "green", "red")[iris.kmean$cluster]) # fitted cluster 
```

## Example 2: clustering of image pixels

Let's first load and plot an image of Leo. 

```{r fig.dim=c(6, 9), out.width = "35%"}
    library(jpeg)
    img<-readJPEG("data/leo.jpg")
    
    # generate a blank image
    par(mar=rep(0.2, 4))
    plot(c(0, 400), c(0, 500), xaxt = 'n', yaxt = 'n', 
         bty = 'n', pch = '', ylab = '', xlab = '')

    rasterImage(img, 0, 0, 400, 500)
```

For a `jpg` file, each pixel is stored as a vector with 3 elements --- representing red, green and blue intensities. However, by the way, that this objective `img` being constructed, it is stored as a 3d array. The first two dimensions are the height and width of the figure. We need to vectorize them and treat each pixel as an observation. 

```{r}
    dim(img)
    
    # this apply function applies vecterization to each layer (r/g/b) of the image. 
    img_expand = apply(img, 3, c)

    # and now we have the desired data matrix
    dim(img_expand)
```

Before performing the $k$-mean clustering, let's have a quick peek at the data in a 3d view. Since there are too many observations, we randomly sample a few. 

```{r fig.dim= c(6, 6), out.width = "55%"}
    library(scatterplot3d)
    set.seed(1)
    sub_pixels = sample(1:nrow(img_expand), 1000)
    sub_img_expand = img_expand[sub_pixels, ]
    
    scatterplot3d(sub_img_expand, pch = 19, 
                  xlab = "red", ylab = "green", zlab = "blue", 
                  color = rgb(sub_img_expand[,1], sub_img_expand[,2],
                              sub_img_expand[,3]))
```

The next step is to perform the $k$-mean and obtain the cluster label. For example, let's try 5 clusters. 

```{r fig.dim= c(6, 9), out.width = "35%"}
  kmeanfit <- kmeans(img_expand, 5)

  # to produce the new graph, we simply replicate the cluster mean 
  # for all observations in the same cluster
  new_img_expand = kmeanfit$centers[kmeanfit$cluster, ]
  
  # now we need to convert this back to the array that can be plotted as an image. 
  # this is a lazy way to do it, but get the job done
  new_img = img
  new_img[, , 1] = matrix(new_img_expand[,1], 500, 400)
  new_img[, , 2] = matrix(new_img_expand[,2], 500, 400)
  new_img[, , 3] = matrix(new_img_expand[,3], 500, 400)

  # plot the new image
  plot(c(0, 400), c(0, 500), xaxt = 'n', yaxt = 'n', bty = 'n', 
       pch = '', ylab = '', xlab = '')

  rasterImage(new_img, 0, 0, 400, 500)
```

With this technique, we can easily reproduce results with different $k$ values. Apparently, as $k$ increases, we get better resolution. $k = 30$ seems to recover the original image fairly well. 

```{r echo = FALSE}
  pixals.clustered <- function(img, k)
  {
      img_expand = apply(img, 3, c)
      kmeanfit <- kmeans(img_expand, k)
      new_img_expand = kmeanfit$centers[kmeanfit$cluster, ]
      
      new_img = img
      for (j in 1:3)
          new_img[, , j] = matrix(new_img_expand[,j], dim(img)[1], dim(img)[2])
      
      return(new_img)
  }
```

```{r, fig.dim= c(15, 5), out.width = "100%", echo = FALSE}
  par(mfrow=c(1,5))
  par(mar=rep(0.2, 4))
  plot(c(0, 400), c(0, 540), xaxt = 'n', yaxt = 'n', bty = 'n', 
       pch = '', ylab = '', xlab = '')
  rasterImage(img, 0, 0, 400, 500)
  text(200, 530, "Orignal", col = "deepskyblue", cex = 3)
  
  for (k in c(2, 5, 10, 30))
  {
      par(mar=rep(0.2, 4))
      plot(c(0, 400), c(0, 540), xaxt = 'n', yaxt = 'n', bty = 'n', 
           pch = '', ylab = '', xlab = '')
      rasterImage(pixals.clustered(img, k), 0, 0, 400, 500)
      text(200, 530, paste("k =", k), col = "deepskyblue", cex = 3)
  }
```

<!--chapter:end:06.1-kmeans.Rmd-->

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Hierarchical Clustering

## Basic Concepts 

Suppose we have a set of six observations: 

```{r include = FALSE}
  par(mar=rep(0.3, 4))
```

```{r fig.dim = c(3, 3), out.width = "30%", echo = FALSE}
  set.seed(3)
  x = replicate(2, rnorm(6))
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5, 
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61))
  text(x[,1], x[, 2], c(1:6))
```

The goal is to progressively group them together until there is only one group. During this process, we will always choose the closest two groups (some may be individuals) to merge. 

```{r include = FALSE}
  par(mfrow=c(1,4))
  par(mar=c(0.3, 0.3, 2, 0.3))
```

```{r, fig.dim = c(10, 2.5), out.width = "100%", echo = FALSE}
  library(plotrix)

  # Step 1
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5, 
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), main = "Step 1")
  text(x[,1], x[, 2], c(1:6), lwd =2)
  draw.ellipse(0.1444561, -1.1750380, a = 0.2, b = 0.25, 
               angle = 60, border = "darkorange", lwd = 2)
  
  # Step 2
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5,
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), main = "Step 2")
  text(x[,1], x[, 2], c(1:6), lwd =2)
  draw.ellipse(0.1444561, -1.1750380, a = 0.2, b = 0.25, 
               angle = 60, border = "darkorange", lwd = 2)
  draw.circle(0.161565, -1.031619, 0.3, border = "darkorange", lwd = 2)
  
  # Step 3
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5,
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), main = "Step 3")
  text(x[,1], x[, 2], c(1:6), lwd =2)
  draw.ellipse(0.1444561, -1.1750380, a = 0.2, b = 0.25, 
               angle = 60, border = "darkorange", lwd = 2)
  draw.circle(0.161565, -1.031619, 0.3, border = "darkorange", lwd = 2)    
  draw.ellipse(-0.7223288, 1.1919895, a = 0.5, b = 0.25, 
               angle = 170, border = "darkorange", lwd = 2)

  # Step 4
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5, 
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), main = "Step 4")
  text(x[,1], x[, 2], c(1:6), lwd =2)
  draw.ellipse(0.1444561, -1.1750380, a = 0.2, b = 0.25, 
               angle = 60, border = "darkorange", lwd = 2)
  draw.circle(0.161565, -1.031619, 0.3, border = "darkorange", lwd = 2)   
  draw.ellipse(-0.7223288, 1.1919895, a = 0.5, b = 0.25, 
               angle = 170, border = "darkorange", lwd = 2)    
  draw.circle(-0.74, 0.8331322, 0.57, border = "darkorange", lwd = 2)
```

If we evaluate the distance between two observations, that would be very easy. For example, the Euclidean distance and Hamming distance can be used. But what about the distance between two groups? Suppose we have two groups of observations $G$ and $H$, then several distance metric can be considered:

  * __Complete linkage__: the furthest pair 
  $$d(G, H) = \underset{i \in G, \, j \in G}{\max} d(x_i, x_j)$$
  * __Single linkage__: the closest pair
  $$d(G, H) = \underset{i \in G, \, j \in G}{\min} d(x_i, x_j)$$
  * __Average linkage__: average distance
  $$d(G, H) = \frac{1}{n_G n_H} \sum_{i \in G} \sum_{i \in H} d(x_i, x_j)$$

The `R` function `hclust()` uses the complete linkage as default. To perform a hierarchical clustering, we need to know all the pair-wise distances, i.e., $d(x_i, x_j)$. Let's consider the Euclidean distance. 

```{r}
  # the Euclidean distance can be computed using dist()
  as.matrix(dist(x))
```

We use this distance matrix in the hierarchical clustering algorithm `hclust()`. The `plot()` function will display the merging process. This should be exactly the same as we demonstrated previously. 

```{r include = FALSE}
  par(mfrow=c(1,1))
  par(mar=rep(2, 4))
```

```{r fig.dim = c(6, 6), out.width = "45%", echo = FALSE}
  # pass the distance matrix to hclust()
  # we use a complete link function
  hcfit <- hclust(dist(x), method = "complete")
  plot(hcfit)
```

The height of each split represents how separated the two subsets are (the distance when they are merged). Selecting the number of clusters is still a tricky problem. Usually, we pick a cutoff where the height of the next split is short. Hence, the above example fits well with two clusters.  

## Example 1: `iris` data

The `iris` data contains three clusters and four variables. We use all variables in the distance calculation and use the default complete linkage. 

```{r fig.width=8, fig.height=6, out.width = '40%'}
  iris_hc <- hclust(dist(iris[, 3:4]))
  plot(iris_hc)
```

This does not seem to perform very well, considering that we know the true number of classes is three. This shows that, in practice, the detected clusters can heavily depend on the variables you use. Let's try some other linkage functions. 

```{r fig.width=8, fig.height=6, out.width = '40%'}
  iris_hc <- hclust(dist(iris[, 3:4]), method = "average")
  plot(iris_hc, hang = -1)
```

This looks better, at least more consistent with the truth. Now we can also consider using other package to plot this result. For example, the `ape` package provides some interesting choices. 

```{r fig.width=5, fig.height=4, out.width = '45%'}
  library(ape)
  plot(as.phylo(iris_hc), type = "unrooted", cex = 0.6, no.margin = TRUE)
```

We can also add the true class colors to the plot. This plot is motivated by the `dendextend` package vignettes. Of course in a realistic situation, we wouldn't know what the true class is. 

```{r, fig.width=5, fig.height=4, out.width = '60%', echo = FALSE, message=FALSE}
  library(colorspace) 
  library(dendextend)
  
  dend <- as.dendrogram(iris_hc)
  # order it the closest we can to the order of the observations:
  dend <- rotate(dend, 1:150)
  
  # Color the branches based on the clusters:
  dend <- color_branches(dend, k=3) #, groupLabels=iris_species)
  
  # Manually match the labels, as much as possible, to the real classification of the flowers:
  labels_colors(dend) <-
     rainbow_hcl(3)[sort_levels_values(
        as.numeric(iris[,5])[order.dendrogram(dend)]
     )]
  
  # We shall add the flower type to the labels:
  labels(dend) <- paste(as.character(iris[,5])[order.dendrogram(dend)],
                             "(",labels(dend),")", 
                             sep = "")
  # We hang the dendrogram a bit:
  dend <- hang.dendrogram(dend,hang_height=0.1)
  # reduce the size of the labels:
  # dend <- assign_values_to_leaves_nodePar(dend, 0.5, "lab.cex")
  dend <- set(dend, "labels_cex", 0.5)
  # And plot:
  par(mar = c(3,1,1,5))
  plot(dend, horiz =  TRUE,  nodePar = list(cex = .007))
  legend("topleft", legend = levels(iris[,5]), fill = rainbow_hcl(3))
```

## Example 2: RNA Expression Data

We use a tissue gene expression dataset from the `tissuesGeneExpression` library, available from bioconductor. I prepared the data to include only 100 genes. You can download the data from the course website. In this first step, we simply plot the data using a heatmap. By default, a heatmap uses red to denote higher values, and yellow for lower values. Note that we first plot the data without organizing the columns or rows. The data is also standardized based on columns (genes). 

```{r}
    load("data/tissue.Rda")
    dim(expression)
    table(tissue)
    head(expression[, 1:3])
    heatmap(scale(expression), Rowv = NA, Colv = NA)
```

Hierarchical clustering may help us discover interesting patterns. If we reorganize the columns and rows based on the clusters, then it may reveal underlying subclasses of issues, or subgroups of genes. 

```{r}
    heatmap(scale(expression))
```

Note that there are many other `R` packages that produce more interesting plots. For example, you can try the [heatmaply](https://cran.r-project.org/web/packages/heatmaply/vignettes/heatmaply.html) package.

<!--chapter:end:06.2-hclust.Rmd-->

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Principle Component Analysis

## Basic Concepts

Principle Component Analysis (PCA) is arguably the most commonly used approach for dimension reduction and visualization. The idea is to capture major signals of variation in a dataset. A nice demonstration of the search of direction is provided at this [r-bloggers](https://www.r-bloggers.com/principal-component-analysis-in-r/) site:

<center>
![](images/PCA2.gif)
</center>

Let's look at a two-dimensional case, we are trying to find a line (direction) on this plain, such that if all points are projected onto this line, their coordinates have the largest variance, compared with any other line. The following code is used to generate a set of observations. 

```{r}
  # generate some random data from a 2-dimensional normal distribution. 
  library(MASS)
  set.seed(1)
  
  n = 100
  Sigma = matrix(c(0.5, -0.65, -0.65, 1), 2, 2)
  x_org = mvrnorm(n, c(1, 2), Sigma)
  x = scale(x_org, scale = FALSE, center = TRUE)
```

```{r out.width = "40%"}
  plot(x, xlim = c(-3, 3), ylim= c(-3, 3), 
       pch = 19, cex = 0.75)
  abline(h = 0, col = "red")
  abline(v = 0, col = "red")
```

Let' start with finding a direction to project all the observations onto. And we want this projection to have the largest variation. Of course the direction that goes along the spread of the data would be the best choice for the purpose of large variance. 

```{r out.width = "40%"}
  plot(x, xlim = c(-3, 3), ylim= c(-3, 3), pch = 19, cex = 0.75)
  abline(h = 0, col = "red")
  abline(v = 0, col = "red")

  # This line is obtained from performing PCA
  pc1 = princomp(x)$loadings[,1]
  abline(a = 0, b = pc1[2]/pc1[1], col = "deepskyblue", lwd = 4)
  
  # The direction 
  pc1
```

Once we have the first direction, we can also remove the projection from the original covariates, and search for a direction $\bv_2$ that is orthogonal to $\bv_1$, with $\bv_1^\T \bv_2 = 0$, such that it contains large variation of $\bX$.  


```{r out.width = "40%"}
  par(mar=c(2, 2, 0.3, 0.3))
  plot(x, xlim = c(-3.5, 3.5), ylim= c(-3.5, 3.5), pch = 19, cex = 0.5)
  abline(h = 0, col = "red")
  abline(v = 0, col = "red")

  # largest PC 
  pc1 = princomp(x)$loadings[,1]
  abline(a = 0, b = pc1[2]/pc1[1], col = "deepskyblue", lwd = 4)

  # second largest PC
  pc2 = princomp(x)$loadings[,2]
  abline(a = 0, b = pc2[2]/pc2[1], col = "darkorange", lwd = 3)
  
  pc2
  t(pc1) %*% pc2
```

We can also see how much variation these two directions accounts for in the original data. The following shows the corresponding standard deviation. 

```{r}
  princomp(x)$sdev
```

Formally, we can generalized this to $\bX$ with any dimensions. And the key tool is to perform the singular value decomposition (SVD):

$$\bX = \bU \bD \bV^\T$$
The $\bV$ matrix here corresponds to the directions we found. Hence, $\bv_1$ is its first column, $\bv_1$ is its second column, etc.. $\bD$ is a diagonal matrix ordered from the largest to the smallest values, correspond to the standard deviation of the spreads. And $\bU$ represents the coordinates once we project $\bX$ onto those directions. An alternative way to understand this is by matrix approximation, if we want to find a rank-1 matrix that best approximate $\bX$ with the Frobenius norm, we optimize 

$$\text{minimize} \quad \lVert \bX - \bu_1 d_1 \bv_1^\T \rVert_2^2$$

This can be generalized into any dimensional problem. Another alternative formulation is to use eigen-decomposition of $\bX^\T \bX$, which can be written as 

$$\bX^\T \bX = \bV \bD \bU^\T \bU \bD \bV^\T = \bV \bD^2 \bV^\T$$

But one thing we usually need to take care of is the centering issue. This is why we used `scale()` function at the beginning. However, we only center, but not scale the data. If we do not center, then the first principle component (PC) could be a direction that points to the center of the data. Note that when $\bX$ is already centered, $\bX^\T \bX$ is the covariance matrix. Hence PCA is also performing eigen-decomposition to the covariance matrix. 

```{r include = FALSE}
    par(mfrow=c(1,2))
    par(mar=c(2, 2, 2, 0.3))
```    
    
```{r fig.dim = c(12, 6), out.width = "70%"}
    plot(x_org, main = "Before Centering", 
         xlim = c(-5, 5), ylim= c(-5, 5), pch = 19, cex = 0.5)
    abline(h = 0, col = "red")
    abline(v = 0, col = "red")
    
    par(mar=c(2, 2, 2, 0.3))
    plot(x, main = "After Centering", 
         xlim = c(-5, 5), ylim= c(-5, 5), pch = 19, cex = 0.5)
    abline(h = 0, col = "red")
    abline(v = 0, col = "red")    
```

Finally, for any dimensional data $\bX$, we usually visualize them in the first two directions, or three. Note that the coordinates on the PC's can be obtained using either the `scores` ($\bU$) in the fitted object of `princomp`, or simply multiply the original data matrix by the loading matrix $\bV$.

```{r include = FALSE}
    par(mfrow=c(1,1))
```    

```{r fig.dim = c(6, 6), out.width = "40%"}
    pcafit <- princomp(x)

    # the new coordinates on PC's
    head(pcafit$scores)
    
    # direct calculation based on projection 
    head(x %*% pcafit$loadings)

    # visualize the data on the PCs
    # Note that the both axies are scaled 
    par(mar=c(4, 4.2, 0.3, 0.3))
    plot(pcafit$scores[,1], pcafit$scores[,2], xlab = "First PC", ylab = "Second PC", pch = 19, cex.lab = 1.5)
    abline(h = 0, col = "deepskyblue", lwd = 4)
    abline(v = 0, col = "darkorange", lwd = 4)
```

There are many different functions in `R` that performs PCA. `princomp` and `prcomp` are the most popular ones.

### Note: Scaling

You should always center the variables when performing PCA, however, whether to use scaling (force each variable to have a standard deviation of 1) depends on the particular application. When you have variables that are extremely disproportionate, e.g., age vs. RNA expression, scaling should be used. This is to prevent some variables from dominating the PC loadings due to their large scales. When all the variables are of the similar type, e.g., color intensities of pixels in a figure, it is better to use the original scale. This is because the variables with larger variations may carry more signal. Scaling may lose that information.

## Example 1: `iris` Data

We use the `iris` data again. All four variables are considered in this analysis. We plot the first and second PC directions. 

```{r fig.width=6, fig.height=4.5}
    iris_pc <- prcomp(iris[, 1:4])
    library(ggplot2)
    ggplot(data = data.frame(iris_pc$x), aes(x=PC1, y=PC2)) + 
        geom_point(color=c("chartreuse4", "darkorange", "deepskyblue")[iris$Species], size = 3)
```

One may be interested in plotting all pair-wise direction to see if lower PC's provide useful information. 

```{r fig.width=7, fig.height=6.5}
    pairs(iris_pc$x, col=c("chartreuse4", "darkorange", "deepskyblue")[iris$Species], pch = 19)
```

However, usually, the lower PC's are less informative. This can also be speculated from the eigenvalue plot, which shows how influential each PC is. 

```{r fig.width=5, fig.height=4.5}
    plot(iris_pc, type = "l", pch = 19, main = "Iris PCA Variance")
```

Feature contributions to the PC can be accessed through the magnitude of the loadings. This table shows that `Petal.Length` is the most influential variable on the first PC, with loading $\approx 0.8567$. 

```{r}
    iris_pc$rotation
```

We can further visualize this on a plot. This can be helpful when the number of variables is large. 

```{r fig.width=6, fig.height=4}
    features = row.names(iris_pc$rotation)
    ggplot(data = data.frame(iris_pc$rotation), aes(x=PC1, y=PC2, label=features,color=features)) + 
        geom_point(size = 3) + geom_text(size=3)
```

## Example 2: Handwritten Digits

The handwritten zip code digits data contains 7291 training data and 2007 testing data. Each image is a $16 \times 16$-pixel gray-scale image. Hence they are converted to a vector of 256 variables. 

```{r}
    library(ElemStatLearn)
    # Handwritten Digit Recognition Data
    # the first column is the true digit
    dim(zip.train)
```

Here is a sample of some images:

```{r, fig.width=6, fig.height=4, echo = FALSE, results='hide', fig.keep='all'}
    par(mar=c(0.3, 0.3, 0.3, 0.3))
    # a plot of some samples 
    findRows <- function(zip, n) {
    # Find n (random) rows with zip representing 0,1,2,...,9
    res <- vector(length=10, mode="list")
    names(res) <- 0:9
    ind <- zip[,1]
    for (j in 0:9) {
    res[[j+1]] <- sample( which(ind==j), n ) }
    return(res) }

    # Making a plot like that on page 4 of HTF:
    digits <- vector(length=10, mode="list")
    names(digits) <- 0:9
    rows <- findRows(zip.train, 6)
    for (j in 0:9) {
    digits[[j+1]] <- do.call("cbind", lapply(as.list(rows[[j+1]]),
    function(x) zip2image(zip.train, x)) )
    }
    im <- do.call("rbind", digits)
    
    image(im, col=gray(256:0/256), zlim=c(0,1), 
          xaxt="n", yaxt="n", xlab="", ylab="")
```

Let's do a simpler task, using just three letters: 1, 4 and 8. 

```{r, fig.width=6, fig.height=4}
    zip.sub = zip.train[zip.train[,1] %in% c(1,4,8), -1]
    zip.sub.truth = as.factor(zip.train[zip.train[,1] %in% c(1,4,8), 1])
    dim(zip.sub)
    zip_pc = prcomp(zip.sub)
    plot(zip_pc, type = "l", pch = 19, main = "Digits 1, 4, and 8: PCA Variance")
```

The eigenvalue results suggest that the first two principal components are much more influential than the rest. A pair-wise PC plot of the first four PC's may further confirm that speculation. 

```{r, fig.width=8, fig.height=6}
    pairs(zip_pc$x[, 1:4], col=c("chartreuse4", "darkorange", "deepskyblue")[zip.sub.truth], pch = 19)
```

Let's look at the first two PCs more closely. Even without knowing the true class (no colors) we can still vaguely see 3 clusters. 

```{r fig.width=5, fig.height=4}
    library(ggplot2)
    ggplot(data = data.frame(zip_pc$x), aes(x=PC1, y=PC2)) + 
        geom_point(size = 2)
```

Finally, let's briefly look at the results of PCA for all 10 different digits. Of course, more PC's are needed for this task. You can also plot other PC's to get more information. 

```{r fig.width=5, fig.height=4}
    library(colorspace)

    zip_pc <- prcomp(zip.train)

    plot(zip_pc, type = "l", pch = 19, main = "All Digits: PCA Variance")

    ggplot(data = data.frame(prcomp(zip.train)$x), aes(x=PC1, y=PC2)) + 
    geom_point(color = rainbow_hcl(10)[zip.train[,1]+1], size = 1)
```

<!--chapter:end:06.3-pca.Rmd-->

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Self-Organizing Map

## Basic Concepts 

I found the best demonstration of the Self-Organizing Map algorithm is the following graph that displays it over iterations. It is available at [this website](https://annalyzin.wordpress.com/2017/11/02/self-organizing-map/):

<center>
![](images/SOM2.gif){width=400px}
</center>

Let's understand this by pairing it with the algorithm. There are several different algorithms available, but one of the most popular ones is proposed by @kohonen1990self. Here, we present a SOM with a 2-dimensional output. The following are the inputs:

* $\{x_i\}_{i=1}^n$ is a set of $n$ observations, with dimension $p$ (the yellow and green dots in the figure).
* $w_{ij}$, $i = 1, \ldots p$, $j = 1, \ldots q$ are a grid of centers (the connected black dots). They are similar to the centers in a k-mean algorithm. However, they also preserve some geometric relationships among $w_{ij}$'s, meaning that $w_{ij}$'s are closer if their indices ${i, j}$ are closer (connected in the figure).
* $\alpha$ this is a learning rate between $[0, 1]$. This controls how fast the $w_{ij}$'s are updated.
* $r$ is also a tuning parameter. This controls how many $w_{ij}$'s will be updated at each iteration

Now, we look at the algorithm. This is different from $k$-means because we do not use all the observations immediately. The algorithm works by stream-in the observations one-by-one. Whenever a new observation $x_k$, $k = 1, \ldots, n$ comes in, we will update the centers $w_{ij}$'s by the following:

* For all $w_{ij}$, calculate the distance between each $w_{ij}$ and $x_k$. Let $d_{ij} = \lVert x_k - w_{ij} \rVert$. By default, we use Euclidean distance. 
* Select the closest $w_{ij}$, denoted as $w_{\ast}$
* Update each $w_{ij}$ based on the fomular $w_{ij} = w_{ij} + \alpha \, h(w_\ast, w_{ij}, r) \, \lVert x_k - w_{ij} \rVert$

After each iteration (updating with one more observation), we will decrease the value of $\alpha$ and $r$. In the `kohonen` package, the $\alpha$ starts at 0.05, and gradually decreases to 0.01, while $r$ is chosen to be 2/3 of all cluster means at the first iteration.

Using the `kohonen` package, we perform a SOM on the Handwritten Digit Recognition Data. The heatmap shows how each $w_{ij}$ is away from it's neighboring $w_{ij}$'s. The extreme bright one means that the center is quite isolated by itself. 

```{r}
  library(kohonen)

  # Handwritten Digit Recognition Data
  library(ElemStatLearn)

  # the first column is the true digit
  dim(zip.train)
  
  # for speed concern, I only use a few variables (pixels)
  zip.SOM <- som(zip.train[, seq(2, 257, length.out = 10)], 
                 grid = somgrid(10, 10, "rectangular"))
  plot(zip.SOM, type = "dist.neighbours")
  plot(zip.SOM, type = "mapping", pchs = 20, 
       main = "Mapping Type SOM")
  # plot(zip.SOM, main = "Default SOM Plot")
  
  # you can try using all the pixels
  # zip.SOM <- som(zip.train[, 2:257], 
  #            grid = somgrid(10, 10, "rectangular"))
  # plot(zip.SOM, type = "dist.neighbours")
```
We could also look at the class labels (digits) coming out of the SOM. Particularly the plot on the right-hand side shows the proportion of subjects with each label for the subjects in each cluster (using a pie chart). 

```{r}
    set.seed(1)
    zip.SOM2 <- xyf(zip.train[, seq(2, 257, length.out = 10)], 
                    classvec2classmat(zip.train[, 1]),
                    grid = somgrid(10, 10, "hexagonal"), rlen = 300)
    par(mfrow = c(1, 2))
    plot(zip.SOM2, type = "codes", main = c("Codes X", "Codes Y"))
    zip.SOM2.hc <- cutree(hclust(dist(zip.SOM2$codes[[2]])), 10)
    add.cluster.boundaries(zip.SOM2, zip.SOM2.hc)
```

<!--chapter:end:06.4-som.Rmd-->

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Spectral Clustering

## Basic Concepts 

Spectral clustering essentially consists of two steps. First, we construct the graph Laplacian $\mathbf{L}$ (or normalized version), then we perform eigen-decomposition of the matrix. Lets show an example, replicated from @von2007tutorial.

```{r}
  set.seed(1)
  n = 50
  x = c(rnorm(n, 0, 0.2), rnorm(n, 2, 0.2), rnorm(n, 4, 0.2), rnorm(n, 6, 0.2))
  hist(x, breaks = 100)
```

We use the adjacency matrix defined as $$w_{ij} = \exp\bigg\{\frac{- \lVert x_i - x_j\rVert^2 }{2 \sigma^2 } \bigg\},$$ and calculate the Laplacian $$\mathbf{L} = \mathbf{D} - \mathbf{W}.$$ We can then use the eigen decomposition to recover the underlying features. 

```{r}
  # construct the adjacency matrix
  W = as.matrix(exp(-dist(as.matrix(x))^2) / 4)
  heatmap(W, Rowv = NA, Colv=NA, symm = TRUE, revC = TRUE)
  
  # compute the degree of each vertex
  d = colSums(W)
  
  # the laplacian matrix
  L = diag(d) - W
  
  # eigen-decomposition
  f = eigen(L, symmetric = TRUE)
  
  # plot the eigen-values 
  # we need the samll ones, but notice that the smallest one should be exactly zero
  plot(rev(f$values)[1:20], pch = 19, ylab = "eigen-values", 
       col = c(rep("red", 4), rep("blue", 196)))
  
```
  
```{r echo = FALSE}
  par(mfrow=c(1,4))
```
  
These are the feature embedding we obtained. But the eigen-value associated with the smallest one will not be used. 

```{r fig.dim = c(14, 4), out.width = '95%'}
  # plot the last four eigen-vectors
  plot(f$vectors[, 200], type = "l", ylab = "eigen-values", ylim = c(-0.15, 0.15))
  plot(f$vectors[, 199], type = "l", ylab = "eigen-values", ylim = c(-0.15, 0.15))
  plot(f$vectors[, 198], type = "l", ylab = "eigen-values", ylim = c(-0.15, 0.15))
  plot(f$vectors[, 197], type = "l", ylab = "eigen-values", ylim = c(-0.15, 0.15))
```

On the other hand, if we use an adjacency matrix that contains several non-connected blocks, then we would observe four zero eigen-values. For example, using the KNN adjacency index, we may obtain four separated blocks. 

```{r}
  library(FNN)
  nn = get.knn(x, k=10)
  W = matrix(0, 200, 200)
  for (i in 1:200)
    W[i, nn$nn.index[i, ]] = 1
  
  # W is not necessary symmetric
  W = pmax(W, t(W))
  
  heatmap(W, Rowv = NA, Colv=NA, symm = TRUE, revC = TRUE)
```

We use a normalized graph Laplacian, defined as 

$$\mathbf{L}_\text{sym} = \mathbf{I} - \mathbf{D^{-1/2} \mathbf{W} D^{-1/2}}$$
```{r echo = FALSE}
  par(mfrow=c(1,1))
```

```{r fig.dim = c(6, 6), out.width = '45%'}
  # compute the degree of each vertex
  d = colSums(W)
  
  # the laplacian matrix
  L = diag(200) - diag(1/sqrt(d)) %*% W %*% diag(1/sqrt(d))
  
  # eigen-decomposition
  f = eigen(L, symmetric = TRUE)
  
  # plot the eigen-values 
  # we need the smallest ones
  plot(rev(f$values)[1:20], pch = 19, ylab = "eigen-values", 
       col = c(rep("red", 4), rep("blue", 196)))
```

```{r echo = FALSE}
  par(mfrow=c(1,4))
```

```{r fig.dim = c(14, 4), out.width = '95%'}
  # plot the last four eigen-vectors
  plot(f$vectors[, 200], type = "l", ylab = "eigen-values", ylim = c(-0.15, 0.15))
  plot(f$vectors[, 199], type = "l", ylab = "eigen-values", ylim = c(-0.15, 0.15))
  plot(f$vectors[, 198], type = "l", ylab = "eigen-values", ylim = c(-0.15, 0.15))
  plot(f$vectors[, 197], type = "l", ylab = "eigen-values", ylim = c(-0.15, 0.15))
```

We can then perform $k$-means clustering using the top eigen-vectors (except the smallest one) as the features.

```{r echo = FALSE}
  par(mfrow=c(1,1))
```

```{r fig.dim = c(14, 4), out.width = '95%'}
  cl = kmeans(f$vectors[, 197:199], centers = 4, nstart = 100)
  plot(x, cl$cluster, ylab = "Cluster", col = cl$cluster, pch = 3)
```





<!--chapter:end:06.5-spectral.Rmd-->

# (PART) Reference {-}

# Reference

<!--chapter:end:99-reference.Rmd-->

