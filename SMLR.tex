% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{amsmath, amssymb, amscd, mathrsfs}
\usepackage{bm}
\usepackage{color}

\DeclareMathAlphabet{\mathbbm}{U}{bbm}{m}{n}% from bbm.sty
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}

\definecolor{deepskyblue}{HTML}{00BFFF}
\definecolor{darkorange}{HTML}{FF8C00}
\definecolor{newblue}{HTML}{005AB5}
\definecolor{newred}{HTML}{DC3220}

% UIUC colors

\definecolor{PrimeOrange}{HTML}{E84A27}
\definecolor{PrimeBlue}{HTML}{13294B}
\definecolor{AltOrange}{HTML}{E87722}
\definecolor{AltBlue}{HTML}{606EB2}

% math equations

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\def\d{{\mathrm{d}}}
\def\P{{\mathrm{P}}}

\def\balpha{{\boldsymbol{\alpha}}}
\def\bbeta{{\boldsymbol{\beta}}}
\def\btheta{{\boldsymbol{\theta}}}
\def\bgamma{{\boldsymbol{\gamma}}}

\def\hLambda{{\widehat{\Lambda}}}
\def\hlambda{{\widehat{\lambda}}}
\def\bLambda{{\overline{\Lambda}}}


\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\T{{\text T}}
% all math notations

\def\bA{{\bf A}}
\def\bB{{\bf B}}
\def\bC{{\bf C}}
\def\bD{{\bf D}}
\def\bE{{\bf E}}
\def\bF{{\bf F}}
\def\bG{{\bf G}}
\def\bH{{\bf H}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bK{{\bf K}}
\def\bL{{\bf L}}
\def\bM{{\bf M}}
\def\bN{{\bf N}}
\def\bO{{\bf O}}
\def\bP{{\bf P}}
\def\bQ{{\bf Q}}
\def\bR{{\bf R}}
\def\bS{{\bf S}}
\def\bT{{\bf T}}
\def\bU{{\bf U}}
\def\bV{{\bf V}}
\def\bW{{\bf W}}
\def\bX{{\mathbf{X}}
\def\bY{{\bf Y}}
\def\bZ{{\bf Z}}

\def\ba{{\bf a}}
\def\bb{{\bf b}}
\def\bc{{\bf c}}
\def\bd{{\bf d}}
\def\be{{\bf e}}
%\def\bf{{\bf f}}
\def\bg{{\bf g}}
\def\bh{{\bf h}}
\def\bi{{\bf i}}
\def\bj{{\bf j}}
\def\bk{{\bf k}}
\def\bl{{\bf l}}
\def\bm{{\bf m}}
\def\bn{{\bf n}}
\def\bo{{\bf o}}
\def\bp{{\bf p}}
\def\bq{{\bf q}}
\def\br{{\bf r}}
\def\bs{{\bf s}}
\def\bt{{\bf t}}
\def\bu{{\bf u}}
\def\bv{{\bf v}}
\def\bw{{\bf w}}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bz{{\bf z}}

\def\cA{{\cal A}}
\def\cB{{\cal B}}
\def\cC{{\cal C}}
\def\cD{{\cal D}}
\def\cE{{\cal E}}
\def\cF{{\cal F}}
\def\cG{{\cal G}}
\def\cH{{\cal H}}
\def\cI{{\cal I}}
\def\cJ{{\cal J}}
\def\cK{{\cal K}}
\def\cL{{\cal L}}
\def\cM{{\cal M}}
\def\cN{{\cal N}}
\def\cO{{\cal O}}
\def\cP{{\cal P}}
\def\cQ{{\cal Q}}
\def\cR{{\cal R}}
\def\cS{{\cal S}}
\def\cT{{\cal T}}
\def\cU{{\cal U}}
\def\cV{{\cal V}}
\def\cW{{\cal W}}
\def\cX{{\cal X}}
\def\cY{{\cal Y}}
\def\cZ{{\cal Z}}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Statistical Learning and Machine Learning with R},
  pdfauthor={Ruoqing Zhu, PhD},
  colorlinks=true,
  linkcolor={cyan},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={cyan},
  pdfcreator={LaTeX via pandoc}}

\title{Statistical Learning and Machine Learning with R}
\author{\href{https://sites.google.com/site/teazrq/}{Ruoqing Zhu, PhD}}
\date{2023-08-19}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

Welcome to \emph{Statistical Learning and Machine Learning with R}! I started this project during the summer of 2018 when I was preparing for the Stat 432 course. At that time, our faculty member \href{https://daviddalpiaz.com/teaching.html}{Dr.~David Dalpiaz}, had decided to move to The Ohio State University (although he moved back to UIUC later on). David introduced to me this awesome way of publishing website on GitHub, which is a very efficient approach for developing courses. Since I have also taught Stat 542 (Statistical Learning) for several years, I figured it could be beneficial to integrate what I have to this \href{https://daviddalpiaz.github.io/r4sl/}{existing book} by David and use it as the R material for both courses. For Stat 542, the main focus is to learn the numerical optimization behind these learning algorithms, and also be familiar with the theoretical background. As you can tell, I am not being very creative on the name, so \texttt{SMLR} it is. You can find the source file of this book on my \href{https://teazrq.github.io/SMLR/}{GitHub}.

\hypertarget{target-audience}{%
\section*{Target Audience}\label{target-audience}}
\addcontentsline{toc}{section}{Target Audience}

This book can be suitable for students ranging from advanced undergraduate to first/second year Ph.D students who have prior knowledge in statistics. Although a student at the masters level will likely benefit most from the material. Previous experience with both basic mathematics (mainly linear algebra), statistical modeling (such as linear regressions) and R are assumed.

\hypertarget{whats-covered}{%
\section*{What's Covered?}\label{whats-covered}}
\addcontentsline{toc}{section}{What's Covered?}

This book currently covers the following topics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Basic Knowledge

  \begin{itemize}
  \tightlist
  \item
    R, R Studio and R Markdown
  \item
    Linear regression and linear algebra
  \item
    Numerical optimization
  \end{itemize}
\item
  Penalized linear models and model selection
\item
  Nonlinear and Nonparametric Models

  \begin{itemize}
  \tightlist
  \item
    Spline
  \item
    K-nearest neighbor
  \item
    Kernel smoothing
  \end{itemize}
\item
  Classification models

  \begin{itemize}
  \tightlist
  \item
    Logistic regression
  \item
    Discriminant analysis
  \end{itemize}
\item
  Machine Learning Models

  \begin{itemize}
  \tightlist
  \item
    Support vector machine
  \item
    Kernel ridge regression
  \item
    Tree models
  \item
    Random forests
  \item
    Boosting
  \end{itemize}
\item
  Unsupervised Learning

  \begin{itemize}
  \tightlist
  \item
    K-means
  \item
    Hierarchical clustering
  \item
    PCA
  \item
    self-organizing map
  \item
    Spectral clustering
  \item
    UMAP
  \end{itemize}
\end{enumerate}

The goal of this book is to introduce not only how to run some of the popular statistical learning models in \texttt{R}, know the algorithms and programming techniques for solving these models and also understand some of the fundamental statistical theory behind them. For example, for graduate students, these topics will be discuss in more detail:

\begin{itemize}
\tightlist
\item
  Optimization

  \begin{itemize}
  \tightlist
  \item
    Lagrangian
  \item
    Primal vs.~dual
  \end{itemize}
\item
  EM and MM algorithm
\item
  Bias-variance trade-off in

  \begin{itemize}
  \tightlist
  \item
    Linear regression
  \item
    KNN
  \item
    Kernel density estimation
  \end{itemize}
\item
  Kernel Trick and RKHS
\item
  Representer Theorem

  \begin{itemize}
  \tightlist
  \item
    SVM
  \item
    Spline
  \end{itemize}
\end{itemize}

For each section, the difficulty will gradually increase from an undergraduate level to a graduate level.

It will be served as a supplement to \href{http://www-bcf.usc.edu/~gareth/ISL/}{An Introduction to Statistical Learning} (\protect\hyperlink{ref-james2013introduction}{James et al. 2013}) for \href{https://go.illinois.edu/stat432}{STAT 432 - Basics of Statistical Learning} and to \href{https://web.stanford.edu/~hastie/ElemStatLearn/}{The Elements of
Statistical Learning: Data Mining, Inference, and Prediction} (\protect\hyperlink{ref-hastie2001elements}{Hastie, Tibshirani, and Friedman 2001}) for \href{https://go.illinois.edu/stat542}{STAT 542 - Statistical Learning} at the \href{http://illinois.edu/}{University of Illinois at Urbana-Champaign}.

\textbf{This book is under active development}. Hence, you may encounter errors ranging from typos to broken code, to poorly explained topics. If you do, please let me know! Simply send an email and I will make the changes as soon as possible (\texttt{rqzhu\ AT\ illinois\ DOT\ edu}). Or, if you know \texttt{R\ Markdown} and are familiar with GitHub, \href{https://github.com/teazrq/SLWR}{make a pull request and fix an issue yourself}! These contributions will be acknowledged.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

The initial contents are derived from Dr.~David Dalpiaz's book. My STAT 542 course materials are also inspired by \href{https://stat.illinois.edu/directory/profile/liangf}{Dr.~Feng Liang} and \href{https://stat.illinois.edu/directory/profile/jimarden}{Dr.~John Marden} who developed earlier versions of this course. And I also incorporated many online resources, which I cannot put into a comprehensive list. If you think I missed some references, please let me know.

\hypertarget{license}{%
\section*{License}\label{license}}
\addcontentsline{toc}{section}{License}

\begin{figure}
\centering
\includegraphics[width=0.15\textwidth,height=\textheight]{images/cc.png}
\caption{This work is licensed under a \href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License}.}
\end{figure}

\hypertarget{part-basics-knowledge}{%
\part{Basics Knowledge}\label{part-basics-knowledge}}

\hypertarget{r-and-rstudio}{%
\chapter{R and RStudio}\label{r-and-rstudio}}

\hypertarget{installing-r-and-rstudio}{%
\section{Installing R and RStudio}\label{installing-r-and-rstudio}}

The first step is to download and install \href{https://www.r-project.org/}{R} and \href{https://www.rstudio.com/products/rstudio/download/\#download}{RStudio}. Most steps should be self-explanatory. You can also find many online guides for step-by-step instruction, such as \href{https://www.youtube.com/watch?v=cX532N_XLIs\&t=19s/}{this YouTube video}. However, be aware that some details may have been changed over the years.

After installing both, open your RStudio, you should see four panes, which can be seen below:

\begin{itemize}
\tightlist
\item
  Source pane on top-left where you write code in to files
\item
  Console on bottom-left where the code is inputted into R
\item
  Environment (and other tabs) on top-right where you can see current variables and objects you defined
\item
  File (and other tabs) on bottom-right which is essentially a file borrower
\end{itemize}

\includegraphics[width=1\textwidth,height=\textheight]{images/RStudio.png}

We will mainly use the left two panes. You can either directly input code into the console to run for results, or edit your code in a file and run them in chunks or as a whole.

\hypertarget{r-basic}{%
\section{Resources and Guides}\label{r-basic}}

There are many online resources for how to use R, RStudio. For example, David Dalpiaz's other online book \href{http://daviddalpiaz.github.io/appliedstats/}{Applied Statistics with R} contains an introduction to using them. There are also other online documentation such as

\begin{itemize}
\tightlist
\item
  \href{https://www.youtube.com/watch?v=cX532N_XLIs\&t=19s/}{Install R and RStudio}
\item
  \href{http://www.r-tutor.com/r-introduction}{R tutorial}
\item
  \href{https://www.youtube.com/playlist?list=PLBgxzZMu3GpPojVSoriMTWQCUno_3hjNi}{Data in R Play-list (video)}
\item
  \href{https://www.youtube.com/playlist?list=PLBgxzZMu3GpMjYhX7jLm5B9gEV7AOOJ5w}{R and RStudio Play-list (video)}
\end{itemize}

It is worth to mention that once you become an advanced user, and possibly a developer of R packages using \texttt{C/C++} (add-on of R for performing specific tasks), and you also happen to use Windows like I do, you will have to install \href{https://cran.r-project.org/bin/windows/Rtools/}{Rtools} that contains the gcc compilers. This is also needed if you want to install any R package from a ``source'' (\texttt{.tar.gz}) file instead of using the so-called ``binaries'' (\texttt{.zip} files).

\hypertarget{basic-mathematical-operations}{%
\section{Basic Mathematical Operations}\label{basic-mathematical-operations}}

Basic R calculations and operations should be self-explanatory. Try to type-in the following commands into your R console and start to explore yourself. Lines with a \texttt{\#} in the front are comments, which will not be executed. Lines with \texttt{\#\#} in the front are outputs you should expect.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# Basic mathematical operations}
    \DecValTok{1} \SpecialCharTok{+} \DecValTok{3}
\DocumentationTok{\#\# [1] 4}
    \DecValTok{1} \SpecialCharTok{{-}} \DecValTok{3}
\DocumentationTok{\#\# [1] {-}2}
    \DecValTok{1} \SpecialCharTok{*} \DecValTok{3}
\DocumentationTok{\#\# [1] 3}
    \DecValTok{1} \SpecialCharTok{/} \DecValTok{3}
\DocumentationTok{\#\# [1] 0.3333333}
    \DecValTok{3}\SpecialCharTok{\^{}}\DecValTok{5}
\DocumentationTok{\#\# [1] 243}
    \DecValTok{4}\SpecialCharTok{\^{}}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\# [1] 0.5}
\NormalTok{    pi}
\DocumentationTok{\#\# [1] 3.141593}
    
    \CommentTok{\# some math functions}
    \FunctionTok{sqrt}\NormalTok{(}\DecValTok{4}\NormalTok{)}
\DocumentationTok{\#\# [1] 2}
    \FunctionTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\DocumentationTok{\#\# [1] 2.718282}
    \FunctionTok{log}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\DocumentationTok{\#\# [1] 1.098612}
    \FunctionTok{log2}\NormalTok{(}\DecValTok{16}\NormalTok{)}
\DocumentationTok{\#\# [1] 4}
    \FunctionTok{log}\NormalTok{(}\DecValTok{15}\NormalTok{, }\AttributeTok{base =} \DecValTok{3}\NormalTok{)}
\DocumentationTok{\#\# [1] 2.464974}
    \FunctionTok{factorial}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\DocumentationTok{\#\# [1] 120}
    \FunctionTok{sin}\NormalTok{(pi)}
\DocumentationTok{\#\# [1] 1.224606e{-}16}
\end{Highlighting}
\end{Shaded}

If you want to see more information about a particular function or operator in R, the easiest way is to get the reference document. Put a question mark in front of a function name:

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# In a default R console window, this will open up a web browser.}
    \CommentTok{\# In RStudio, this will be displayed at the ‘Help’ window at the bottom{-}right penal (Help tab). }
\NormalTok{    ?log10}
\NormalTok{    ?cos}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-objects}{%
\section{Data Objects}\label{data-objects}}

Data objects can be a complicated topic for people who never used R before. The most common data objects are \texttt{vector}, \texttt{matrix}, \texttt{list}, and \texttt{data.frame}. They are defined using a specific syntax. To define a vector, we use \texttt{c} followed by \texttt{()}, where the elements within the parenthesis are separated using comma. You can save the vector and name as something else. For example

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# creating a vector}
    \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\DocumentationTok{\#\# [1] 1 2 3 4}
    \FunctionTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{)}
\DocumentationTok{\#\# [1] "a" "b" "c"}
    
    \CommentTok{\# define a new vector object, called \textasciigrave{}x\textasciigrave{}}
\NormalTok{    x }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

After defining this object \texttt{x}, it should also appear on your top-right environment pane. To access elements in an object, we use the \texttt{{[}{]}} operator, like a \texttt{C} programming reference style.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# getting the second element in x}
\NormalTok{    x[}\DecValTok{2}\NormalTok{]}
\DocumentationTok{\#\# [1] 1}
  
    \CommentTok{\# getting the second to the fourth element in x}
\NormalTok{    x[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\DocumentationTok{\#\# [1] 1 1 0}
\end{Highlighting}
\end{Shaded}

Similarly, we can create and access elements in a matrix:

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# create a matrix by providing all of its elements}
    \CommentTok{\# the elements are filled to the matrix by column}
    \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{), }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#      [,1] [,2]}
\DocumentationTok{\#\# [1,]    1    3}
\DocumentationTok{\#\# [2,]    2    4}
  
    \CommentTok{\# create a matrix by column{-}bind vectors}
\NormalTok{    y }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{)}
    \FunctionTok{cbind}\NormalTok{(x, y)}
\DocumentationTok{\#\#      x y}
\DocumentationTok{\#\# [1,] 1 1}
\DocumentationTok{\#\# [2,] 1 0}
\DocumentationTok{\#\# [3,] 1 1}
\DocumentationTok{\#\# [4,] 0 0}
\DocumentationTok{\#\# [5,] 0 1}
\DocumentationTok{\#\# [6,] 0 0}
  
    \CommentTok{\# access elements in a matrix}
    \CommentTok{\# Note that in R, upper and lower cases are treated as two different objects}
\NormalTok{    X }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{16}\NormalTok{), }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{    X}
\DocumentationTok{\#\#      [,1] [,2] [,3] [,4]}
\DocumentationTok{\#\# [1,]    1    5    9   13}
\DocumentationTok{\#\# [2,]    2    6   10   14}
\DocumentationTok{\#\# [3,]    3    7   11   15}
\DocumentationTok{\#\# [4,]    4    8   12   16}
\NormalTok{    X[}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{]}
\DocumentationTok{\#\# [1] 10}
\NormalTok{    X[}\DecValTok{1}\NormalTok{, ]}
\DocumentationTok{\#\# [1]  1  5  9 13}
    
    \CommentTok{\# getting a sub{-}matrix of X}
\NormalTok{    X[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{3}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\DocumentationTok{\#\#      [,1] [,2]}
\DocumentationTok{\#\# [1,]    9   13}
\DocumentationTok{\#\# [2,]   10   14}
\end{Highlighting}
\end{Shaded}

Mathematical operations on vectors and matrices are, by default, element-wise. For matrix multiplications, you should use \texttt{\%*\%}.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# adding two vectors}
\NormalTok{    (x }\SpecialCharTok{+}\NormalTok{ y)}\SpecialCharTok{\^{}}\DecValTok{2}
\DocumentationTok{\#\# [1] 4 1 4 0 1 0}
  
    \CommentTok{\# getting the length of a vector}
    \FunctionTok{length}\NormalTok{(x)}
\DocumentationTok{\#\# [1] 6}
    
    \CommentTok{\# matrix multiplication}
\NormalTok{    X }\SpecialCharTok{\%*\%}\NormalTok{ X}
\DocumentationTok{\#\#      [,1] [,2] [,3] [,4]}
\DocumentationTok{\#\# [1,]   90  202  314  426}
\DocumentationTok{\#\# [2,]  100  228  356  484}
\DocumentationTok{\#\# [3,]  110  254  398  542}
\DocumentationTok{\#\# [4,]  120  280  440  600}
    
    \CommentTok{\# getting the dimension of a matrix}
    \FunctionTok{dim}\NormalTok{(X)}
\DocumentationTok{\#\# [1] 4 4}
    
    \CommentTok{\# A warning will be issued when R detects something wrong}
    \CommentTok{\# Results may still be produced however}
\NormalTok{    y }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\DocumentationTok{\#\# Warning in y + c(1, 2, 3, 4): longer object length is not a multiple of shorter object length}
\DocumentationTok{\#\# [1] 2 2 4 4 2 2}
\end{Highlighting}
\end{Shaded}

\texttt{list()} creates a list of objects (of any type). However, some operators cannot be directly applied to a list in a similar way as to vectors or matrices. Model fitting results in R are usually stored as a list. For example, the \texttt{lm()} function, which will be introduced later.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# creating a list}
\NormalTok{    x }\OtherTok{=} \FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\StringTok{"hello"}\NormalTok{, }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{), }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
  
    \CommentTok{\# accessing its elements using double brackets \textasciigrave{}[[]]\textasciigrave{} }
\NormalTok{    x[[}\DecValTok{1}\NormalTok{]]}
\DocumentationTok{\#\# [1] 1 2}
\end{Highlighting}
\end{Shaded}

\texttt{data.frame()} creates a list of vectors of equal length, and display them as a matrix-like object, in which each vector is a column of the matrix. It is mainly used for storing data. This will be our most frequently used data object for analysis. For example, in the famous \texttt{iris} data, the first four columns are numerical variables, while the last column is a categorical variable with three levels: \texttt{setosa}, \texttt{versicolor}, and \texttt{virginica}:

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# The iris data is included with base R, so we can use them directly}
    \CommentTok{\# This will create a copy of the data into your environment}
    \FunctionTok{data}\NormalTok{(iris)}
  
    \CommentTok{\# the head function peeks the first several rows of the dataset }
    \FunctionTok{head}\NormalTok{(iris, }\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\DocumentationTok{\#\#   Sepal.Length Sepal.Width Petal.Length Petal.Width Species}
\DocumentationTok{\#\# 1          5.1         3.5          1.4         0.2  setosa}
\DocumentationTok{\#\# 2          4.9         3.0          1.4         0.2  setosa}
\DocumentationTok{\#\# 3          4.7         3.2          1.3         0.2  setosa}
    
    \CommentTok{\# each column usually contains a column (variable) name }
    \FunctionTok{colnames}\NormalTok{(iris)}
\DocumentationTok{\#\# [1] "Sepal.Length" "Sepal.Width"  "Petal.Length" "Petal.Width"  "Species"}
    
    \CommentTok{\# data frame can be called by each individual column, which will be a vector}
    \CommentTok{\# iris$Species}
\NormalTok{    iris}\SpecialCharTok{$}\NormalTok{Species[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\DocumentationTok{\#\# [1] setosa setosa setosa}
\DocumentationTok{\#\# Levels: setosa versicolor virginica}
    
    \CommentTok{\# the summary function can be used to view summary statistics of all variables}
    \FunctionTok{summary}\NormalTok{(iris)}
\DocumentationTok{\#\#   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width          Species  }
\DocumentationTok{\#\#  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100   setosa    :50  }
\DocumentationTok{\#\#  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300   versicolor:50  }
\DocumentationTok{\#\#  Median :5.800   Median :3.000   Median :4.350   Median :1.300   virginica :50  }
\DocumentationTok{\#\#  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199                  }
\DocumentationTok{\#\#  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800                  }
\DocumentationTok{\#\#  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500}
\end{Highlighting}
\end{Shaded}

\texttt{factor} is a special type of vector. It is frequently used to store a categorical variable with more than two categories. The last column of the iris data is a factor. You need to be a little bit careful when dealing with factor variables when during modeling since some functions do not take care of them automatically or they do it in a different way than you thought. For example, changing a factor variable into numerical ones will ignore any potential relationship among different categories.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{levels}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Species)}
\DocumentationTok{\#\# [1] "setosa"     "versicolor" "virginica"}
    \FunctionTok{as.numeric}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Species)}
\DocumentationTok{\#\#   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1}
\DocumentationTok{\#\#  [48] 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2}
\DocumentationTok{\#\#  [95] 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3}
\DocumentationTok{\#\# [142] 3 3 3 3 3 3 3 3 3}
\end{Highlighting}
\end{Shaded}

\hypertarget{readin-and-save-data}{%
\section{Readin and save data}\label{readin-and-save-data}}

Data can be imported from a variety of sources. More commonly, a dataset can be stored in \texttt{.txt} and \texttt{.csv} files. Such data reading methods require specific structures in the source file: the first row should contain column names, and there should be equal number of elements in each row. Hence you should always check your file before reading them in.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# read{-}in data}
\NormalTok{    birthrate }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/birthrate.csv"}\NormalTok{)}
    \FunctionTok{head}\NormalTok{(birthrate)}
\DocumentationTok{\#\#   Year Birthrate}
\DocumentationTok{\#\# 1 1917     183.1}
\DocumentationTok{\#\# 2 1918     183.9}
\DocumentationTok{\#\# 3 1919     163.1}
\DocumentationTok{\#\# 4 1920     179.5}
\DocumentationTok{\#\# 5 1921     181.4}
\DocumentationTok{\#\# 6 1922     173.4}
    
    \CommentTok{\# to see how many observations (rows) and variables (columns) in a dataset}
    \FunctionTok{dim}\NormalTok{(birthrate)}
\DocumentationTok{\#\# [1] 87  2}
\end{Highlighting}
\end{Shaded}

R data can also be saved into other formats. The more efficient way, assuming that you are going to load these file back to R in the future, is to save them as \texttt{.RData} file. Usually, for a larger dataset, this reduces the time spend on reading the data.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# saving a object to .RData file}
    \FunctionTok{save}\NormalTok{(birthrate, }\AttributeTok{file =} \StringTok{"mydata.RData"}\NormalTok{)}
  
    \CommentTok{\# you can specify multiple objects to be saved into the same file}
    \FunctionTok{save}\NormalTok{(birthrate, iris, }\AttributeTok{file =} \StringTok{"mydata.RData"}\NormalTok{)}
    
    \CommentTok{\# load the data again back to your environment}
    \FunctionTok{load}\NormalTok{(}\StringTok{"mydata.RData"}\NormalTok{)}
    
    \CommentTok{\# alternatively, you can also save data to a .csv file}
    \FunctionTok{write.csv}\NormalTok{(birthrate, }\AttributeTok{file =} \StringTok{"mydata.csv"}\NormalTok{)}
    
    \CommentTok{\# you can notice that this .csv file contains an extra column of "ID number", without a column name}
    \CommentTok{\# Hence, when you read this file back into R, you should specify \textasciigrave{}row.names = 1\textasciigrave{} to indicate that.}
    \CommentTok{\# Otherwise this will produce an error}
    \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"mydata.csv"}\NormalTok{, }\AttributeTok{row.names =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-and-defining-functions}{%
\section{Using and defining functions}\label{using-and-defining-functions}}

We have already used many functions. You can also define your own functions, and even build them into packages (more on this later) for other people to use. This is the main advantage of R. For example, let's consider writing a function that returns the minimum and maximum of a vector. Suppose we already know the \texttt{min()} and \texttt{max()} functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    myrange }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) }\CommentTok{\# x is the argument that your function takes in}
\NormalTok{    \{}
      \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(x), }\FunctionTok{max}\NormalTok{(x))) }\CommentTok{\# return a vector that contains two elements}
\NormalTok{    \}}
  
\NormalTok{    x }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
    \FunctionTok{myrange}\NormalTok{(x)}
\DocumentationTok{\#\# [1]  1 10}
    
    \CommentTok{\# R already has this function}
    \FunctionTok{range}\NormalTok{(x)}
\DocumentationTok{\#\# [1]  1 10}
\end{Highlighting}
\end{Shaded}

\hypertarget{distribution-and-random-numbers}{%
\section{Distribution and random numbers}\label{distribution-and-random-numbers}}

Three distributions that are most frequently used in this course are Bernoulli, Gaussian (normal), and \(t\) distributions. Bernoulli distributions can be used to describe binary variables, while Gaussian distribution is often used to describe continuous ones. The following code generates some random variables

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# read the documentation of rbinom() using ?rbinom}
\NormalTok{    x }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.4}\NormalTok{)}
    \FunctionTok{table}\NormalTok{(x)}
\DocumentationTok{\#\# x}
\DocumentationTok{\#\#  0  1 }
\DocumentationTok{\#\# 62 38}
\end{Highlighting}
\end{Shaded}

However, this result cannot be replicated by others, since the next time we run this code, the random numbers will be different. Hence it is important to set and keep the random seed when a random algorithm is involved. The following code will always generate the same result

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{    x }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.4}\NormalTok{)}
\NormalTok{    y }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\CommentTok{\# by default, this is mean 0 and variance 1}
    
    \FunctionTok{table}\NormalTok{(x)}
\DocumentationTok{\#\# x}
\DocumentationTok{\#\#  0  1 }
\DocumentationTok{\#\# 57 43}
    \FunctionTok{hist}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{boxplot}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-19-2} \end{center}

\hypertarget{using-packages-and-other-resources}{%
\section{Using packages and other resources}\label{using-packages-and-other-resources}}

Packages are written and contributed to R by individuals. They provide additional features (functions or data) that serve particular needs. For example, the \texttt{ggplot2} package is developed by the RStudio team that provides nice features to plot data. We will have more examples of this later on, but first, let's install and load the package so that we can use these features. More details will be provided in the data visualization section.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# to install a package}
    \FunctionTok{install.packages}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# to load the package}
    \FunctionTok{library}\NormalTok{(ggplot2)}
  
    \CommentTok{\# use the ggplot() function to produce a plot}
    \CommentTok{\# Sepal.Length is the horizontal axis}
    \CommentTok{\# Sepal.Width is the vertical axis}
    \CommentTok{\# Species labels are used as color}
    \FunctionTok{ggplot}\NormalTok{(iris, }\FunctionTok{aes}\NormalTok{(Sepal.Length, Sepal.Width, }\AttributeTok{colour =}\NormalTok{ Species)) }\SpecialCharTok{+} 
      \FunctionTok{geom\_point}\NormalTok{()  }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-21-1} \end{center}

You may also noticed that in our previous examples, all tables only displayed the first several rows. One may be interested in looking at the entire dataset, however, it would take too much space to display the whole table. Here is a package that would allow you to display it in a compact window. It also provides searching and sorting tools. You can integrate this into your R Markdown reports.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(DT)}
    \FunctionTok{datatable}\NormalTok{(iris, }\AttributeTok{filter =} \StringTok{"top"}\NormalTok{, }\AttributeTok{rownames =} \ConstantTok{FALSE}\NormalTok{,}
              \AttributeTok{options =} \FunctionTok{list}\NormalTok{(}\AttributeTok{pageLength =} \DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Often times, you may want to perform a new task and you don't know what function can be used to achieve that. Google Search or Stack Overflow are probably your best friends. I used to encounter this problem: I have a list of objects, and each of them is a vector. I then need to extract the first element of all these vectors. However, doing this using a for-loop can be slow, and I am also interested in a cleaner code. So I found \href{https://stackoverflow.com/questions/44176908/r-list-get-first-item-of-each-element}{this post}, which provided a simple answer:

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# create the list}
\NormalTok{    a }\OtherTok{=} \FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{))}
    
    \CommentTok{\# extract the first element in each vector of the list}
    \FunctionTok{sapply}\NormalTok{(a, }\StringTok{"[["}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\DocumentationTok{\#\# [1] 1 2 3}
\end{Highlighting}
\end{Shaded}

\hypertarget{practice-questions}{%
\section{Practice questions}\label{practice-questions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Attach a new numerical column to the \texttt{iris} data, as the product of \texttt{Petal.Length} and \texttt{Petal.Width} and name the column as \texttt{Petal.Prod}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  iris }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(iris, }\StringTok{"Petal.Prod"} \OtherTok{=}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Petal.Length}\SpecialCharTok{*}\NormalTok{iris}\SpecialCharTok{$}\NormalTok{Petal.Width)}
  \FunctionTok{head}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Attach a new numerical column to the \texttt{iris} data, with value 1 if the observation is \texttt{setosa}, 2 for \texttt{versicolor} and 3 for \texttt{virginica}, and name the column as \texttt{Species.Num}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  iris }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(iris, }\StringTok{"Species.Num"} \OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Species))}
  \FunctionTok{head}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Change \texttt{Species.Num} to a factor variable such that it takes value ``Type1'' if the observation is \texttt{setosa} and ``NA'' otherwise.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  iris}\SpecialCharTok{$}\NormalTok{Species.Num }\OtherTok{=} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Species }\SpecialCharTok{==} \StringTok{"setosa"}\NormalTok{, }\StringTok{"Type1"}\NormalTok{, }\StringTok{"NA"}\NormalTok{))}
  \FunctionTok{head}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Define a function that takes in a numerical vector, and output the mean of that vector. Do this without using the \texttt{mean()} and \texttt{sum()} function.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  mymean }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x)}
\NormalTok{  \{}
\NormalTok{    sum }\OtherTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(x))}
\NormalTok{      sum }\OtherTok{=}\NormalTok{ sum }\SpecialCharTok{+}\NormalTok{ x[i]}
    
    \FunctionTok{return}\NormalTok{(}\AttributeTok{sum =}\NormalTok{ sum }\SpecialCharTok{/} \FunctionTok{length}\NormalTok{(x))}
\NormalTok{  \}}
  
\NormalTok{  x }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
  \FunctionTok{mymean}\NormalTok{(x)}
  
  \FunctionTok{mean}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\hypertarget{rmarkdown}{%
\chapter{RMarkdown}\label{rmarkdown}}

\hypertarget{basics-and-resources}{%
\section{Basics and Resources}\label{basics-and-resources}}

R Markdown is a built-in feature of RStudio. It integrates plain text with chunks of R code in to a single file, which is extremely useful when constructing class notes or building a website. A \texttt{.rmd} file can be compiled into nice-looking \texttt{.html}, \texttt{.pdf}, and \texttt{.docx} file. For example, this entire guide is created using R Markdown. With RStudio, you can install R Markdown from R console using the following code. Note that this should be automatically done the first time you create and compile a \texttt{.rmd} file in RStudio.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# Install R Markdown from CRAN}
    \FunctionTok{install.packages}\NormalTok{(}\StringTok{"rmarkdown"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Again there are many online guides for R Markdown, and these may not be the best ones.

\begin{itemize}
\tightlist
\item
  \href{https://bookdown.org/yihui/rmarkdown/}{R Markdown: The Definitive Guide}
\item
  \href{https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf}{R Markdown Cheat Sheet}
\item
  \href{https://www.youtube.com/playlist?list=PLBgxzZMu3GpNgd07DwmS-2odHtMO6MWGH}{R Markdown Play-list (video)}
\end{itemize}

To get started, create an R Markdown template file by clicking \texttt{File} -\textgreater{} \texttt{New\ File} -\textgreater{} \texttt{R\ Markdown...}

\includegraphics[width=0.7\textwidth,height=\textheight]{images/Create.png}

You can then \texttt{Knit} the template file and start to explore its features.

\includegraphics[width=0.7\textwidth,height=\textheight]{images/Knit.png}

Please note that this guide is provided in the \texttt{.html} format. However, your homework report should be in \texttt{.pdf} format. This can be done by selecting the \texttt{Knit\ to\ PDF} option from the Knit button. Again there are many online guides, and these may not be the best ones.

\begin{itemize}
\tightlist
\item
  \href{https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf}{R Markdown Cheat Sheet}
\item
  \href{https://www.youtube.com/playlist?list=PLBgxzZMu3GpNgd07DwmS-2odHtMO6MWGH}{R Markdown Play-list (video)}
\end{itemize}

\hypertarget{formatting-text}{%
\section{Formatting Text}\label{formatting-text}}

Formatting text is easy. Bold can be done using \texttt{**} or \texttt{\_\_} before and after the text. Italics can be done using \texttt{*} or \texttt{\_} before and after the text. For example, \textbf{This is bold.} \emph{This is italics.} and \textbf{\emph{this is bold italics}}. \texttt{This\ text\ appears\ as\ monospaced.}

\begin{itemize}
\tightlist
\item
  Unordered list element 1.
\item
  Unordered list element 2.
\item
  Unordered list element 3.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ordered list element 1.
\item
  Ordered list element 2.
\item
  Ordered list element 3.
\end{enumerate}

We could mix lists and links. Note that a link can be constructed in the format \texttt{{[}display\ text{]}(http\ link)}. If colors are desired, we can customize it using, for example, \texttt{{[}\textbackslash{}textcolor\{blue\}\{display\ text\}{]}(http\ link)}. But this only works in \texttt{.pdf} format. For \texttt{.html}, use \texttt{\textless{}span\ style="color:\ red;"\textgreater{}text\textless{}/span\textgreater{}}.

\begin{itemize}
\tightlist
\item
  A default link: \href{http://rmarkdown.rstudio.com/}{RMarkdown Documentation}
\item
  colored link 1: \href{https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf}{\textcolor{blue}{RMarkdown Cheatsheet}} (Not shown because it only works in PDF)
\item
  colored link 2: \href{http://www.tablesgenerator.com/markdown_tables}{{Table Generator}} (only works in HTML)
\end{itemize}

Tables are sometimes tricky using Markdown. See the above link for a helpful Markdown table generator. Note that you can also adjust the alignment by using a \texttt{:} sign.

\begin{longtable}[]{@{}cll@{}}
\toprule\noalign{}
A & B & C \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 2 & 3 \\
Middle & Left & Right \\
\end{longtable}

\hypertarget{adding-r-code}{%
\section{\texorpdfstring{Adding \texttt{R} Code}{Adding R Code}}\label{adding-r-code}}

So far we have only used Markdown to create the text part. This is useful by itself, but the real power of RMarkdown comes when we add \texttt{R}. There are two ways we can do this. We can use \texttt{R} code chunks, or run \texttt{R} inline.

\hypertarget{r-chunks}{%
\subsection{\texorpdfstring{\texttt{R} Chunks}{R Chunks}}\label{r-chunks}}

The following is an example of an \texttt{R} code chunk. Start the chunk with \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}} and end with \texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}}:

\texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}

\(\quad\) \texttt{set.seed(123)}

\(\quad\) \texttt{rnorm(5)}

\texttt{\textasciigrave{}\textasciigrave{}\textasciigrave{}}

This generates five random observations from the standard normal distribution. We also set the seed so that the results can be later on replicated. The result looks like the following

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
    \FunctionTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\DocumentationTok{\#\# [1] {-}0.56047565 {-}0.23017749  1.55870831  0.07050839  0.12928774}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# define function}
\NormalTok{    get\_sd }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(x, }\AttributeTok{biased =} \ConstantTok{FALSE}\NormalTok{) \{}
\NormalTok{      n }\OtherTok{=} \FunctionTok{length}\NormalTok{(x) }\SpecialCharTok{{-}} \DecValTok{1} \SpecialCharTok{*} \SpecialCharTok{!}\NormalTok{biased}
      \FunctionTok{sqrt}\NormalTok{((}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ n) }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{((x }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(x)) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{))}
\NormalTok{    \}}
    
    \CommentTok{\# generate random sample data}
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{    (}\AttributeTok{test\_sample =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{mean =} \DecValTok{2}\NormalTok{, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{))}
\DocumentationTok{\#\#  [1]  8.8547922 {-}0.8234909  3.8156421  5.1643130  4.0213416  1.4693774  9.5576100  1.5267048}
\DocumentationTok{\#\#  [9] 12.0921186  1.6864295}
    
    \CommentTok{\# run function on generated data}
    \FunctionTok{get\_sd}\NormalTok{(test\_sample)}
\DocumentationTok{\#\# [1] 4.177244}
\end{Highlighting}
\end{Shaded}

There is a lot going on here. In the \texttt{.Rmd} file, notice the syntax that creates and ends the chunk. Also note that \texttt{example\_chunk} is the chunk name. Everything between the start and end syntax must be valid \texttt{R} code. Chunk names are not necessary, but can become useful as your documents grow in size.

In this example, we define a function, generate some random data in a reproducible manner, displayed the data, then ran our function.

\hypertarget{inline-r}{%
\subsection{\texorpdfstring{Inline \texttt{R}}{Inline R}}\label{inline-r}}

\texttt{R} can also be run in the middle of the exposition. For example, the mean of the data we generated is 4.7364838.

\hypertarget{importing-data}{%
\section{Importing Data}\label{importing-data}}

When using RMarkdown, any time you \emph{knit} your document to its final form, say \texttt{.html}, a number of programs run in the background. Your current \texttt{R} environment seen in RStudio will be reset. Any objects you created while working interactively inside RStudio will be ignored. Essentially a new \texttt{R} session will be spawned in the background and the code in your document is run there from start to finish. For this reason, things such as importing data must be explicitly coded into your document.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(readr)}
\NormalTok{    example\_data }\OtherTok{=} \FunctionTok{read\_table}\NormalTok{(}\StringTok{"data/skincancer.txt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The above loads the online file. In many cases, you will load a file that is locally stored in your own computer. In that case, you can either specify the full file path, or simply use, for example \texttt{read\_csv("filename.csv")} if that file is stored at your \textbf{working directory}. The \textbf{working directory} will usually be the directory that contains your \texttt{.Rmd} file. You are recommended to reference data in this manner. Note that we use the newer \texttt{read\_csv()} from the \texttt{readr} package instead of the default \texttt{read.csv()}.

\hypertarget{working-directory}{%
\section{Working Directory}\label{working-directory}}

Whenever \texttt{R} code is run, there is always a current working directory. This allows for relative references to external files, in addition to absolute references. Since the working directory when knitting a file is always the directory that contains the \texttt{.Rmd} file, it can be helpful to set the working directory inside RStudio to match while working interactively.

To do so, select \texttt{Session\ \textgreater{}\ Set\ Working\ Directory\ \textgreater{}\ To\ Source\ File\ Location} while editing a \texttt{.Rmd} file. This will set the working directory to the path that contains the \texttt{.Rmd}. You can also use \texttt{getwd()} and \texttt{setwd()} to manipulate your working directory programmatically. These should only be used interactively. Using them inside an RMarkdown document would likely result in lessened reproducibility.

\textbf{As of recent RStudio updates, this practice is not always necessary when working interactively.} If lines of code are being ``Output Inline,'' then the working directory is automatically the directory which contains the \texttt{.Rmd} file.

\hypertarget{plotting}{%
\section{Plotting}\label{plotting}}

The following generates a simple plot, which displays the skin cancer mortality. By default, the figure is aligned on the left, with size 3 by 5 inches.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{plot}\NormalTok{(Mort }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Lat, }\AttributeTok{data =}\NormalTok{ example\_data)}
\end{Highlighting}
\end{Shaded}

\begin{flushleft}\includegraphics[width=0.5\linewidth]{SMLR_files/figure-latex/unnamed-chunk-34-1} \end{flushleft}

In our R introduction, we used \texttt{ggplot2} to create a more interesting plot. You may also polish a plot with basic functions. Notice it is \emph{huge} in the resulting document, since we have modified some \emph{chunk options} (\texttt{fig.height\ =\ 6,\ fig.width\ =\ 8}) in the RMarkdown file to manipulate its size.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{plot}\NormalTok{(Mort }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Lat, }\AttributeTok{data =}\NormalTok{ example\_data,}
         \AttributeTok{xlab =} \StringTok{"Latitude"}\NormalTok{,}
         \AttributeTok{ylab =} \StringTok{"Skin Cancer Mortality Rate"}\NormalTok{,}
         \AttributeTok{main =} \StringTok{"Skin Cancer Mortality vs. State Latitude"}\NormalTok{,}
         \AttributeTok{pch  =} \DecValTok{19}\NormalTok{,}
         \AttributeTok{cex  =} \FloatTok{1.5}\NormalTok{,}
         \AttributeTok{col  =} \StringTok{"deepskyblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{flushleft}\includegraphics[width=0.5\linewidth]{SMLR_files/figure-latex/unnamed-chunk-35-1} \end{flushleft}

But you can also notice that the labels and the plots becomes disproportional when the figure size is set too small. This can be resolved using a scaling option such as \texttt{out.width\ =\ \textquotesingle{}60\%}, but enlarge the original figure size. We also align the figure at the center using \texttt{fig.align\ =\ \textquotesingle{}center\textquotesingle{}}

\begin{center}\includegraphics[width=0.6\linewidth]{SMLR_files/figure-latex/unnamed-chunk-36-1} \end{center}

\hypertarget{chunk-options}{%
\section{Chunk Options}\label{chunk-options}}

We have already seen chunk options \texttt{fig.height}, \texttt{fig.width}, and \texttt{out.width} which modified the size of plots from a particular chunk. There are many \href{http://yihui.name/knitr/options/}{chunk options}, but we will discuss some others which are frequently used including; \texttt{eval}, \texttt{echo}, \texttt{message}, and \texttt{warning}. If you noticed, the plot above was displayed without showing the code.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{install.packages}\NormalTok{(}\StringTok{"rmarkdown"}\NormalTok{)}
\NormalTok{    ?log}
    \FunctionTok{View}\NormalTok{(mpg)}
\end{Highlighting}
\end{Shaded}

Using \texttt{eval\ =\ FALSE} the above chunk displays the code, but it is not run. We've already discussed not wanting install code to run. The \texttt{?} code pulls up documentation of a function. This will spawn a browser window when knitting, or potentially crash during knitting. Similarly, using \texttt{View()} is an issue with RMarkdown. Inside RStudio, this would pull up a window which displays the data. However, when knitting, \texttt{R} runs in the background and RStudio is not modifying the \texttt{View()} function. This, on OSX especially, usually causes knitting to fail.

\begin{verbatim}
## [1] "Hello World!"
\end{verbatim}

Above, we see output, but no code! This is done using \texttt{echo\ =\ FALSE}, which is often useful.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    x }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
\NormalTok{    y }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}
    \FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x))}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Call:}
\DocumentationTok{\#\# lm(formula = y \textasciitilde{} x)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Residuals:}
\DocumentationTok{\#\#        Min         1Q     Median         3Q        Max }
\DocumentationTok{\#\# {-}5.661e{-}16 {-}1.157e{-}16  4.273e{-}17  2.153e{-}16  4.167e{-}16 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Coefficients:}
\DocumentationTok{\#\#              Estimate Std. Error   t value Pr(\textgreater{}|t|)    }
\DocumentationTok{\#\# (Intercept) 1.123e{-}15  2.458e{-}16 4.571e+00  0.00182 ** }
\DocumentationTok{\#\# x           1.000e+00  3.961e{-}17 2.525e+16  \textless{} 2e{-}16 ***}
\DocumentationTok{\#\# {-}{-}{-}}
\DocumentationTok{\#\# Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Residual standard error: 3.598e{-}16 on 8 degrees of freedom}
\DocumentationTok{\#\# Multiple R{-}squared:      1,  Adjusted R{-}squared:      1 }
\DocumentationTok{\#\# F{-}statistic: 6.374e+32 on 1 and 8 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

The above code produces a warning, for reasons we will discuss later. Sometimes, in final reports, it is nice to hide these, which we have done here. \texttt{message\ =\ FALSE} and \texttt{warning\ =\ FALSE} can be used to do so. Messages are often created when loading packages to give the user information about the effects of loading the package. These should be suppressed in final reports. Be careful about suppressing these messages and warnings too early in an analysis as you could potentially miss important information!

\hypertarget{adding-math-with-latex}{%
\section{Adding Math with LaTeX}\label{adding-math-with-latex}}

Another benefit of RMarkdown is the ability to add \href{https://www.latex-project.org/about/}{Latex for mathematics typesetting}. Like \texttt{R} code, there are two ways we can include Latex; displaystyle and inline.

Note that use of LaTeX is somewhat dependent on the resulting file format. For example, it cannot be used at all with \texttt{.docx}. To use it with \texttt{.pdf} you must have LaTeX installed on your machine.

With \texttt{.html} the LaTeX is not actually rendered during knitting, but actually rendered in your browser using MathJax.

\hypertarget{displaystyle-latex}{%
\subsection{Displaystyle LaTeX}\label{displaystyle-latex}}

Displaystyle is used for larger equations which appear centered on their own line. This is done by putting \texttt{\$\$} before and after the mathematical equation.

\[
\widehat \sigma = \sqrt{\frac{1}{n - 1}\sum_{i=1}^{n}(x_i - \bar{x})^2}
\]

\hypertarget{inline-latex}{%
\subsection{Inline LaTex}\label{inline-latex}}

We could mix LaTeX commands in the middle of exposition, for example: \(t = 2\). We could actually mix \texttt{R} with Latex as well! For example: \(\bar{x} = 4.7364838\).

\hypertarget{output-options}{%
\section{Output Options}\label{output-options}}

At the beginning of the document, there is a code which describes some metadata and settings of the document. The default code looks like

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    title}\SpecialCharTok{:} \StringTok{"R Notebook"}
\NormalTok{    output}\SpecialCharTok{:}\NormalTok{ html\_notebook}
\end{Highlighting}
\end{Shaded}

You can easily add your name and date to it, and add a Table of Contents, using \texttt{toc:\ yes}. Note that the following code would specify the theme of an \texttt{html} file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    title}\SpecialCharTok{:} \StringTok{"My RMarkdown Template"}
\NormalTok{    author}\SpecialCharTok{:} \StringTok{"Your Name"}
\NormalTok{    date}\SpecialCharTok{:} \StringTok{"Aug 26, 2021"}
\NormalTok{    output}\SpecialCharTok{:}
\NormalTok{      html\_document}\SpecialCharTok{:} 
\NormalTok{        toc}\SpecialCharTok{:}\NormalTok{ yes}
\end{Highlighting}
\end{Shaded}

You can edit this yourself, or click the settings button at the top of the document and select \texttt{Output\ Options...}. Here you can explore other themes and syntax highlighting options, as well as many additional options. Using this method will automatically modify this information in the document.

\hypertarget{try-it}{%
\section{Try It!}\label{try-it}}

Be sure to play with this document! Change it. Break it. Fix it. The best way to learn RMarkdown (or really almost anything) is to try, fail, then find out what you did wrong.

RStudio has provided a number of \href{http://rmarkdown.rstudio.com/lesson-1.html}{beginner tutorials} which have been greatly improved recently and detail many of the specifics potentially not covered in this document. RMarkdown is continually improving, and this document covers only the very basics.

\hypertarget{visual-studio-code}{%
\chapter{Visual Studio Code}\label{visual-studio-code}}

\hypertarget{basics-and-resources-1}{%
\section{Basics and Resources}\label{basics-and-resources-1}}

Visual Studio Code (VS Code) is another popular IDE for programming in pretty much all languages. With the recent development of GitHub Copilot (X), it makes VS Code a very attractive platform. Here is a place to get things started. Please note that in Windows, running the applications as Administrator is probably needed to install related components.

\begin{itemize}
\tightlist
\item
  \href{https://code.visualstudio.com/docs/languages/r}{Official Overview}
\end{itemize}

To get things working, you only need to

\begin{itemize}
\tightlist
\item
  Install R
\item
  Install VS Code
\item
  Install R Extension in VS Code
\item
  Install the \texttt{languageserver} package \texttt{install.packages("languageserver")}
\end{itemize}

To install \texttt{radian} (for interactive R terminal), you need to

\begin{itemize}
\tightlist
\item
  Install Python and \href{https://pip.pypa.io/en/stable/installation/}{pip} (if you have Anaconda, that should be included already)
\item
  Use \texttt{pip3\ install\ -U\ radian} to install \texttt{radian}
\item
  Go to settings in VS Code, and find \texttt{r.path}, add your \texttt{radian.exe} file path to it
\end{itemize}

\hypertarget{linear-algebra-basics}{%
\chapter{Linear Algebra Basics}\label{linear-algebra-basics}}

You should already be familiar with some basic linear algebra concepts such as matrix and vector multiplications. Here we review some basic concepts and properties that will be used in this course. For the most part, they are used in deriving linear regression results.

\hypertarget{definition}{%
\section{Definition}\label{definition}}

We usually use \(\bX\) to denote an \(n \times p\) dimensional design matrix, where \(n\) is the number of observations and \(p\) is the number of variables. The columns of \(\bx\) are denoted as \(\bx_1, \ldots, \bx_p\):

\[
\bX = \begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1p}\\
x_{21} & x_{22} & \cdots & x_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
x_{m1} & x_{n2} & \cdots & x_{np}\\
\end{pmatrix} = 
\begin{pmatrix}
\bx_1 & \bx_2 & \cdots & \bx_p\\
\end{pmatrix}
\]
The \textbf{column space}, \(\cal{C}(\bX)\) of \(\bX\) is the set of all linear combinations of \(\bx_1, \bx_2, \ldots, \bx_p\), i.e.,

\[c_1 \bx_1 + c_2 \bx_2 + \cdots c_p \bx_p.\]
This is also called the \textbf{span} of these vectors, \(\text{span}(\bx_1, \bx_2, \ldots, \bx_p)\). Its orthogonal space is

\[\{\bv: \bX^\T \bv = 0\}.\]
A projection matrix \(\bP\) is a matrix such that

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{optimization-basics}{%
\chapter{Optimization Basics}\label{optimization-basics}}

Optimization is heavily involved in statistics and machine learning. Almost all methods introduced in this book can be viewed as some form of optimization. It would be good to have some prior knowledge of it so that later chapters can use these concepts without difficulties. Especially, one should be familiar with concepts such as constrains, gradient methods, and be able to implement them using existing R functions. Since optimization is such a broad topic, we refer readers to Boyd and Vandenberghe (\protect\hyperlink{ref-boyd2004convex}{2004}) and Nocedal and Wright (\protect\hyperlink{ref-nocedal2006numerical}{2006}) for more further reading.

We will use a slightly different set of notations in this Chapter so that we are consistent with the literature. This means that for the most part, we will use \(x\) as our parameter of interest and optimize a function \(f(x)\). This is in contrast to optimizing \(\theta\) in a statistical model \(f_\theta(x)\) where \(x\) is the observed data. However, in the example of linear regression, we may again switch back to the regular notation of \(x^\text{T} \boldsymbol \beta\). These transitions will only happen under clear context and should not create ambiguity.

\hypertarget{basic-concept}{%
\section{Basic Concept}\label{basic-concept}}

We usually consider a convex optimization problem (non-convex problems are a bit too involving although we will also see some examples of that), meaning that we optimize (minimize) a convex function in a convex domain. A \textbf{\emph{convex function}} \(f(\mathbf{x})\) maps some subset \(C \in \mathbb{R}^p\) into \(\mathbb{R}^p\), but enjoys the property that

\[ f(t \mathbf{x}_1 + (1 - t) \mathbf{x}_2) \leq t f(\mathbf{x}_1) + ( 1- t) f(\mathbf{x}_2), \]
for all \(t \in [0, 1]\) and any two points \(\mathbf{x}_1\), \(\mathbf{x}_2\) in the domain of \(f\).

\begin{figure}
\centering
\includegraphics[width=0.55\textwidth,height=\textheight]{images/ConvexFunction.png}
\caption{An example of convex function, \href{https://en.wikipedia.org/wiki/Convex_function}{from wikipedia}}
\end{figure}

Note that if you have a concave function (the bowl faces downwards) then \(-f(\mathbf{x})\) would be convex. Examples of convex functions:

\begin{itemize}
\tightlist
\item
  Univariate functions: \(x^2\), \(\exp(x)\), \(-log(x)\)
\item
  Affine map: \(a^\text{T}\mathbf{x}+ b\) is both convex and concave
\item
  A quadratic function \(\frac{1}{2}\mathbf{x}^\text{T}\mathbf{A}\mathbf{x}+ b^\text{T}\mathbf{x}+ c\), if \(\mathbf{A}\) is positive semidefinite
\item
  All \(p\) norms are convex, following the Triangle inequality and properties of a norm.
\item
  A sin function is neither convex or concave
\end{itemize}

On the other hand, a \textbf{\emph{convex set}} \(C\) means that if we have two points \(x_1\) and \(x_2\) in \(C\), the line segment joining these two points has to lie within \(C\), i.e.,

\[\mathbf{x}_1, \mathbf{x}_2 \in C \quad \Longrightarrow \quad t \mathbf{x}_1 + (1 - t) \mathbf{x}_2 \in C,\]
for all \(t \in [0, 1]\).

\begin{figure}
\centering
\includegraphics[width=0.55\textwidth,height=\textheight]{images/ConvexSet.png}
\caption{An example of convex set}
\end{figure}

Examples of convex set include

\begin{itemize}
\tightlist
\item
  Real line: \(\mathbb{R}\)
\item
  Norm ball: \(\{ \mathbf{x}: \lVert \mathbf{x}\rVert \leq r \}\)
\item
  Hyperplane: \(\{ \mathbf{x}: a^\text{T}\mathbf{x}= b \}\)
\end{itemize}

Consider a simple optimization problem:

\[ \text{minimize} \quad f(x_1, x_2) = x_1^2 + x_2^2\]

Clearly, \(f(x_1, x_2)\) is a convex function, and we know that the solution of this problem is \(x_1 = x_2 = 0\). However, the problem might be a bit more complicated if we restrict that in a certain (convex) region, for example,

\begin{align}
&\underset{x_1, x_2}{\text{minimize}} & \quad f(x_1, x_2) &= x_1^2 + x_2^2 \\
&\text{subject to} & x_1 + x_2 &\leq -1 \\
& & x_1 + x_2 &> -2
\end{align}

Here the convex set \(C = \{x_1, x_2 \in \mathbb{R}: x_1 + x_2 \leq -1 \,\, \text{and} \,\, x_1 + x_2 > -2\}\). And our problem looks like the following, which attains it minimum at \((-0.5, -0.5)\).

In general, we will be dealing with a problem in the form of

\begin{align}
&\underset{\mathbf{x}}{\text{minimize}} & \quad f(\mathbf{x}) \\
&\text{subject to} & g_i(\mathbf{x}) & \leq 0, \, i = 1,\ldots, m \\
& & h_j(\mathbf{x}) &= 0, \, j = 1,\ldots, k
\end{align}

where \(g_i(\mathbf{x})\)s are a set of inequality constrains, and \(h_j(\mathbf{x})\)s are equality constrains. There are established result showing what type of constrains would lead to a convex set, but let's assuming for now that we will be dealing a well behaved problem. We shall see in later chapters that many models such as, Lasso, Ridge and support vector machines can all be formulated into this form.

\hypertarget{global_local}{%
\section{Global vs.~Local Optima}\label{global_local}}

Although we would like to deal with convex optimization problems, non-convex problems appears more and more frequently. For example, deep learning models are almost always non-convex except overly simplified ones. However, \textbf{for convex optimization problems, a local minimum is also a global minimum}, i.e., a \(x^\ast\) such that for any \(x\) in the feasible set, \(f(x^\ast) \leq f(x)\). This can be achieved by a variety of descent algorithms, to be introduced. However, for non-convex problems, we may still be interested in a local minimum, which satisfies that for any \(x\) in a \textbf{neighboring set of \(x^\ast\)}, \(f(x^\ast) \leq f(x)\). The comparison of these two cases can be demonstrated in the following plots. Again, a descent algorithm can help us find a local minimum, except for some very special cases, such as a saddle point. However, we will not discuss these issues in this book.

\begin{center}\includegraphics[width=0.75\linewidth]{SMLR_files/figure-latex/unnamed-chunk-49-1} \end{center}

\hypertarget{example-linear-regression-using-optim}{%
\section{\texorpdfstring{Example: Linear Regression using \texttt{optim()}}{Example: Linear Regression using optim()}}\label{example-linear-regression-using-optim}}

Although completely not necessary, we may also view linear regression as an optimization problem. This is of course an unconstrained problem, meaning that \(C \in \mathbb{R}^p\). Such problems can be solved using the \texttt{optim()} function. Also, let's temporarily switch back to the \(\boldsymbol \beta\) notation of parameters. Hence, if we observe a set of observations \(\{\mathbf{x}_i, y_i\}_{i = 1}^n\), our optimization problem is to minimize the objection function, i.e., residual sum of squares (RSS):

\begin{align}
\underset{\boldsymbol \beta}{\text{minimize}} \quad f(\boldsymbol \beta) = \frac{1}{n} \sum_i (y_i - \mathbf{x}_i^\text{T}\boldsymbol \beta)^2 \\
\end{align}

We generate 200 random observations, and also write a function to calculate the RSS for any given \(\boldsymbol \beta\) values. The objective function looks like the following:

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# generate data from a simple linear model }
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{20}\NormalTok{)}
\NormalTok{    n }\OtherTok{=} \DecValTok{200}
\NormalTok{    x }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{rnorm}\NormalTok{(n))}
\NormalTok{    y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{\%*\%} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
    
    \CommentTok{\# calculate the residual sum of squares for a grid of beta values}
\NormalTok{    rss }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(b, trainx, trainy) }\FunctionTok{sum}\NormalTok{((trainy }\SpecialCharTok{{-}}\NormalTok{ trainx }\SpecialCharTok{\%*\%}\NormalTok{ b)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now the question is how to solve this problem. The \texttt{optim()} function uses the following syntax:

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# The solution can be solved by any optimization algorithm }
\NormalTok{    lm.optim }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ rss, }\AttributeTok{trainx =}\NormalTok{ x, }\AttributeTok{trainy =}\NormalTok{ y)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  The \texttt{par} argument specifies an initial value, in this case, \(\beta_0 = \beta_1 = 2\)
\item
  The \texttt{fn} argument specifies the name of an \texttt{R} function that can calculate the objective function. Please note that the first argument in this function has be the parameter being optimized, i.e, \(\boldsymbol \beta\). Also, it must be a vector, not a matrix or other types.
\item
  The arguments \texttt{trainx\ =\ x}, \texttt{trainy\ =\ y} specifies any additional arguments that the objective function \texttt{fn}, i.e., \texttt{rss} needs. It behaves the same as if you are supplying this to \texttt{rss}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    lm.optim}
\DocumentationTok{\#\# $par}
\DocumentationTok{\#\# [1] 0.4532072 0.9236502}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $value}
\DocumentationTok{\#\# [1] 203.5623}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $counts}
\DocumentationTok{\#\# function gradient }
\DocumentationTok{\#\#       63       NA }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $convergence}
\DocumentationTok{\#\# [1] 0}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $message}
\DocumentationTok{\#\# NULL}
\end{Highlighting}
\end{Shaded}

The result shows that the estimated parameters (\texttt{\$par}) are 0.453 and 0.924, with a functional value 203.562. The convergence code is 0, meaning that the algorithm converged. The parameter estimates are almost the same as \texttt{lm()}, with small numerical errors.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# The solution form lm()}
    \FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))}\SpecialCharTok{$}\NormalTok{coefficients}
\DocumentationTok{\#\#     Estimate Std. Error   t value     Pr(\textgreater{}|t|)}
\DocumentationTok{\#\# x1 0.4530498 0.07177854  6.311772 1.773276e{-}09}
\DocumentationTok{\#\# x2 0.9236934 0.07226742 12.781602 1.136397e{-}27}
\end{Highlighting}
\end{Shaded}

What we will be introducing in the following are some basic approaches to solve such a numerical problem. We will start with unconstrained problems, then introduce constrained problems.

\hypertarget{first-and-second-order-properties}{%
\section{First and Second Order Properties}\label{first-and-second-order-properties}}

These properties are usually applied to unconstrained optimization problems. They are essentially just describing the landscape around a point \(\mathbf{x}^\ast\) such that it becomes the local optimizer. Since we generally concerns a convex problem, a local solution is also the global solution. However, these properties are still generally applied when solving a non-convex problem. Note that these statements are multi-dimensional.

\textbf{First-Order Necessary Conditions}: If \(f\) is continuously differentiable in an open neighborhood of local minimum \(\mathbf{x}^\ast\), then \(\nabla f(\mathbf{x}^\ast) = \mathbf{0}\).

When we have a point \(\mathbf{x}^\ast\) with \(\nabla f(\mathbf{x}^\ast) = \mathbf{0}\), we call \(\mathbf{x}^\ast\) a \textbf{stationary point}. This is only a necessary condition, but not sufficient. Since example, \(f(x) = x^3\) has zero derivative at \(x = 0\), but this is not an optimizer. The figure in @ref(global\_local) also contains such a point. TO further strengthen this, we have

\textbf{Second-order Necessary Conditions}: If \(f\) is twice continuously differentiable in an open neighborhood of local minimum \(\mathbf{x}^\ast\), then \(\nabla f(\mathbf{x}^\ast) = \mathbf{0}\) and \(\nabla^2 f(\mathbf{x}^\ast)\) is positive semi-definite.

This does rule out some cases, with a higher cost (\(f\) needs to be twice continuously differentiable). But requiring positive semi-definite would not ensure everything. The same example \(f(x) = x^3\) still satisfies this, but its not a local minimum. A positive definite \(\nabla^2 f(\mathbf{x}^\ast)\) would be sufficient:

\textbf{Second-order Sufficient Conditions}: \(f\) is twice continuously differentiable in an open neighborhood of \(\mathbf{x}^\ast\). If \(\nabla f(\mathbf{x}^\ast) = \mathbf{0}\) and \(\nabla^2 f(\mathbf{x}^\ast)\) is positive definite, i.e.,
\[
\nabla^2 f(\mathbf{x}) = \left(\frac{\partial^2 f(\mathbf{x})}{\partial x_i \partial x_j}\right) = \mathbf{H}(\mathbf{x}) \succeq 0
\]

then \(\mathbf{x}^\ast\) is a strict local minimizer of \(f\). Here \(\mathbf{H}(\mathbf{x})\) is called the \textbf{Hessian matrix}, which will be frequently used in second-order methods.

\hypertarget{algorithm}{%
\section{Algorithm}\label{algorithm}}

Most optimization algorithms follow the same idea: starting from a point \(\mathbf{x}^{(0)}\) (which is usually specified by the user) and move to a new point \(\mathbf{x}^{(1)}\) that improves the objective function value. Repeatedly performing this to get a sequence of points \(\mathbf{x}^{(0)}, \mathbf{x}^{(1)}, \ldots\) until the certain stopping criterion is reached.

A \textbf{stopping criterion} could be

\begin{itemize}
\tightlist
\item
  Using the gradient of the objective function: \(\lVert \nabla f(\mathbf{x}^{(k)}) \rVert < \epsilon\)
\item
  Using the (relative) change of distance: \(\lVert \mathbf{x}^{(k)} - \mathbf{x}^{(k-1)} \rVert / \lVert \mathbf{x}^{(k-1)}\rVert< \epsilon\) or \(\lVert \mathbf{x}^{(k)} - \mathbf{x}^{(k-1)} \rVert < \epsilon\)
\item
  Using the (relative) change of functional value: \(| f(\mathbf{x}^{(k)}) - f(\mathbf{x}^{(k-1)})| < \epsilon\) or \(| f(\mathbf{x}^{(k)}) - f(\mathbf{x}^{(k-1)})| / |f(\mathbf{x}^{(k)})| < \epsilon\)
\item
  Stop at a pre-specified number of iterations.
\end{itemize}

Most algorithms differ in terms of how to move from the current point \(\mathbf{x}^{(k)}\) to the next, better target point \(\mathbf{x}^{(k+1)}\). This may depend on the smoothness or structure of \(f\), constrains on the domain, computational complexity, memory limitation, and many others.

\hypertarget{second-order-methods}{%
\section{Second-order Methods}\label{second-order-methods}}

\hypertarget{newtons-method}{%
\subsection{Newton's Method}\label{newtons-method}}

Now, let's discuss several specific methods. One of the oldest one is \textbf{Newton's method}. This is motivated form a quadratic approximation (essentially Taylor expansion) at a current point \(\mathbf{x}\),

\[f(\mathbf{x}^\ast) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^\text{T}(\mathbf{x}^\ast - \mathbf{x}) + \frac{1}{2} (\mathbf{x}^\ast - \mathbf{x})^\text{T}\mathbf{H}(\mathbf{x}) (\mathbf{x}^\ast - \mathbf{x})\]
Our goal is to find a new stationary point \(\mathbf{x}^\ast\) such that \(\nabla f(\mathbf{x}^\ast) = 0\). By taking derivative of the above equation on both sides, with respect to \(\mathbf{x}^\ast\), we need

\[0 = \nabla f(\mathbf{x}^\ast) = 0 + \nabla f(\mathbf{x}) + (\mathbf{x}^\ast - \mathbf{x})^\text{T}\mathbf{H}(\mathbf{x})\]
which leads to

\[\mathbf{x}^\ast = \mathbf{x}-  \mathbf{H}(\mathbf{x})^{-1} \nabla f(\mathbf{x}).\]

Hence, if we are currently at a point \(\mathbf{x}^{(k)}\), we need to calculate the gradient \(\nabla f(\mathbf{x}^{(k)})\) and Hessian \(\mathbf{H}(\mathbf{x})\) at this point, then move to the new point using \(\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \mathbf{H}(\mathbf{x}^{(k)})^{-1} \nabla f(\mathbf{x}^{(k)})\). Some properties and things to concern regarding Newton's method:

\begin{itemize}
\tightlist
\item
  Newton's method is scale invariant, meaning that you do not need to worry about the step size. It is automatically taken care of by the Hessian matrix. However, in practice, the local approximation may not be accurate, which makes the new point \(\mathbf{x}^{(k+1)}\) behaves differently than what we expect. Hence, it might still be safe to introduce a smaller step size \(\delta \in (0, 1)\) and move with\\
  \[\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} -  \delta \, \mathbf{H}(\mathbf{x}^{(k)})^{-1} \nabla f(\mathbf{x}^{(k)})\]
\item
  There are also alternatives to take care of the step size. For example, \textbf{line search} is frequently used, which will try to find the optimal \(\delta\) that minimizes the function
  \[f(\mathbf{x}^{(k)} + \delta \mathbf{v})\]
  where the direction \(\mathbf{v}\) in this case is \(\mathbf{v}= \mathbf{H}(\mathbf{x}^{(k)})^{-1} \nabla f(\mathbf{x}^{(k)})\). It is also popular to use \textbf{backtracking line search}, which reduces the computational cost. The idea is to start with a large \(\delta\) and gradually reduces it by a certain proportion if the new point doesn't significantly improves, i.e.,
  \[f(\mathbf{x}^{(k)} + \delta \mathbf{v}) > f(\mathbf{x}^{(k)}) - \frac{1}{2} \delta \nabla f(\mathbf{x}^{(k)})^\text{T}\mathbf{v}\]
  Note that when the direction \(\mathbf{v}\) is \(\mathbf{H}(\mathbf{x}^{(k)})^{-1} \nabla f(\mathbf{x}^{(k)})\), \(\nabla f(\mathbf{x}^{(k)})^\text{T}\mathbf{v}\) is essentially the norm defined by the Hessian matrix.
\item
  When you do not have the explicit formula of Hessian and even the gradient, you may \textbf{numerically approximate the derivative} using the definition. For example, we could use
  \[ \frac{f(\mathbf{x}^{(k)} + \epsilon) - f(\mathbf{x}^{(k)})}{\epsilon} \]
  with \(\epsilon\) small enough, e.g., \(10^{-5}\). However, this is very costly for the Hessian matrix if the number of variables is large.
\end{itemize}

\hypertarget{quasi-newton-methods}{%
\subsection{Quasi-Newton Methods}\label{quasi-newton-methods}}

Since the idea of Newton's method is to solve a vector \(\mathbf{v}\) such that

\[\mathbf{H}(\mathbf{x}^{(k)}) \mathbf{v}= - \nabla f(\mathbf{x}^{(k)}), \]
If \(\mathbf{H}\) is difficult to compute, we may use some matrix to substitute it. For example, if we simplify use the identity matrix \(\mathbf{I}\), then this reduces to a first-order method to be introduced later. However, if we can get a good approximation, we can still solve this linear system and get to a better point. Then the question is how to obtain such matrix in a computationally inexpensive way. The Broyden--Fletcher--Goldfarb--Shanno (BFSG) algorithm is such an approach by iteratively updating its (inverse) estimation. The algorithm proceed as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Start with \(x^{(0)}\) and a positive definite matrix, e.g., \(\mathbf{B}^{(0)} = \mathbf{I}\)
\item
  For \(k = 0, 1, 2, \ldots\),

  \begin{itemize}
  \tightlist
  \item
    Search a updating direction by solving the linear system \(\mathbf{B}^{(k)} \mathbf{p}_k = - \nabla f(\mathbf{x}^{(k)})\)
  \item
    Perform line search in the direction of \(\mathbf{v}_k\) and obtain the next point \(\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \delta \mathbf{p}_k\)
  \item
    Update the approximation by
    \[ \mathbf{B}^{(k+1)} = \mathbf{B}^{(k)} + \frac{\mathbf{y}_k^\text{T}\mathbf{y}_{k}}{ \mathbf{y}_{k}^\text{T}\mathbf{s}_{k} } -  \frac{\mathbf{B}^{(k)}\mathbf{s}_{k}\mathbf{s}_{k}^\text{T}{\mathbf{B}^{(k)}}^\text{T}}{\mathbf{s}_{k}^\text{T}\mathbf{B}^{(k)} \mathbf{s}_{k} }, \]
    where \(\mathbf{y}_k = \nabla f(\mathbf{x}^{(k+1)}) - \nabla f(\mathbf{x}^{(k)})\) and \(\mathbf{s}_{k} = \mathbf{x}^{(k+1)} - \mathbf{x}^{(k)}\).
  \end{itemize}
\end{enumerate}

The BFGS is performing a rank-two update by assuming that
\[ \mathbf{B}^{(k+1)} = \mathbf{B}^{(k)} + a \mathbf{u}\mathbf{u}^\text{T}+ b \mathbf{v}\mathbf{v}^\text{T},\]
Alternatives of such type of methods include the symmetric rank-one and Davidon-Fletcher-Powell (DFP) updates.

\hypertarget{first-order-methods}{%
\section{First-order Methods}\label{first-order-methods}}

\hypertarget{gradient-descent}{%
\subsection{Gradient Descent}\label{gradient-descent}}

When simply using \(\mathbf{H}= \mathbf{I}\), we update
\[\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \delta \nabla f(\mathbf{x}^{(k)}).\]
However, it is then crucial to figure out the step size \(\delta\). A step size too large may not even converge at all, however, a step size too small will take too many iterations to converge. Alternatively, line search could be used.

\hypertarget{gradient-descent-example-linear-regression}{%
\subsection{Gradient Descent Example: Linear Regression}\label{gradient-descent-example-linear-regression}}

We use linear regression as an example. The objective function for linear regression is:

\[ \ell(\boldsymbol \beta) = \frac{1}{2n}||\mathbf{y} - \mathbf{X} \boldsymbol \beta ||^2 \]
with solution is

\[\widehat{\boldsymbol \beta} = \left(\mathbf{X}^\text{T}\mathbf{X}\right)^{-1} \mathbf{X}^\text{T} \mathbf{y} \]

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
  \FunctionTok{library}\NormalTok{(MASS)}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{  n }\OtherTok{=} \DecValTok{200}
  
  \CommentTok{\# create some data with linear model}
\NormalTok{  X }\OtherTok{=} \FunctionTok{mvrnorm}\NormalTok{(n, }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{  y }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{2}\SpecialCharTok{*}\NormalTok{X[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ X[,}\DecValTok{2}\NormalTok{])}
  
\NormalTok{  beta1 }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\FloatTok{0.005}\NormalTok{)}
\NormalTok{  beta2 }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\FloatTok{0.005}\NormalTok{)}
\NormalTok{  allbeta }\OtherTok{\textless{}{-}} \FunctionTok{data.matrix}\NormalTok{(}\FunctionTok{expand.grid}\NormalTok{(beta1, beta2))}
\NormalTok{  rss }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{apply}\NormalTok{(allbeta, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(b, X, y) }\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ b)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), X, y), }
                \FunctionTok{length}\NormalTok{(beta1), }\FunctionTok{length}\NormalTok{(beta2))}
  
  \CommentTok{\# quantile levels for drawing contour}
\NormalTok{  quanlvl }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.025}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{)}
  
  \CommentTok{\# plot the contour}
  \FunctionTok{contour}\NormalTok{(beta1, beta2, rss, }\AttributeTok{levels =} \FunctionTok{quantile}\NormalTok{(rss, quanlvl))}
  \FunctionTok{box}\NormalTok{()}
  
  \CommentTok{\# the truth}
\NormalTok{  b }\OtherTok{=} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ X) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ y}
  \FunctionTok{points}\NormalTok{(b[}\DecValTok{1}\NormalTok{], b[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-55-1} \end{center}

We use an optimization approach to solve this problem. By taking the derivative with respect to \(\boldsymbol \beta\), we have the gradient

\[
\begin{align}
\frac{\partial \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta} = -\frac{1}{n} \sum_{i=1}^n (y_i - x_i^\text{T} \boldsymbol \beta) x_i.
\end{align}
\]
To perform the optimization, we will first set an initial beta value, say \(\boldsymbol \beta = \mathbf{0}\) for all entries, then proceed with the update

\[ \boldsymbol \beta^\text{new} = \boldsymbol \beta^\text{old} - \frac{\partial \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta} \times \delta.\]

Let's set \(\delta = 0.2\) for now. The following function performs gradient descent.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# gradient descent function, which also record the path}
\NormalTok{  mylm\_g }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y, }
                     \AttributeTok{b0 =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(x)), }\CommentTok{\# initial value}
                     \AttributeTok{delta =} \FloatTok{0.2}\NormalTok{, }\CommentTok{\# step size}
                     \AttributeTok{epsilon =} \FloatTok{1e{-}6}\NormalTok{, }\CommentTok{\#stopping rule}
                     \AttributeTok{maxitr =} \DecValTok{5000}\NormalTok{) }\CommentTok{\# maximum iterations}
\NormalTok{  \{}
    \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.matrix}\NormalTok{(x)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"x must be a matrix"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.vector}\NormalTok{(y)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"y must be a vector"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{nrow}\NormalTok{(x) }\SpecialCharTok{!=} \FunctionTok{length}\NormalTok{(y)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"number of observations different"}\NormalTok{)}
    
    \CommentTok{\# initialize beta values}
\NormalTok{    allb }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(b0, }\DecValTok{1}\NormalTok{, }\FunctionTok{length}\NormalTok{(b0))}

    \CommentTok{\# iterative update}
    \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{maxitr)}
\NormalTok{    \{}
      \CommentTok{\# the new beta value}
\NormalTok{      b1 }\OtherTok{=}\NormalTok{ b0 }\SpecialCharTok{+} \FunctionTok{t}\NormalTok{(x) }\SpecialCharTok{\%*\%}\NormalTok{ (y }\SpecialCharTok{{-}}\NormalTok{ x }\SpecialCharTok{\%*\%}\NormalTok{ b0) }\SpecialCharTok{*}\NormalTok{ delta }\SpecialCharTok{/} \FunctionTok{length}\NormalTok{(y)      }

      \CommentTok{\# record the new beta}
\NormalTok{      allb }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(allb, }\FunctionTok{as.vector}\NormalTok{(b1))}
      
      \CommentTok{\# stopping rule}
      \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{max}\NormalTok{(}\FunctionTok{abs}\NormalTok{(b0 }\SpecialCharTok{{-}}\NormalTok{ b1)) }\SpecialCharTok{\textless{}}\NormalTok{ epsilon)}
        \ControlFlowTok{break}\NormalTok{;}
      
      \CommentTok{\# reset beta0}
\NormalTok{      b0 }\OtherTok{=}\NormalTok{ b1}
\NormalTok{    \}}

    \ControlFlowTok{if}\NormalTok{ (k }\SpecialCharTok{==}\NormalTok{ maxitr) }\FunctionTok{cat}\NormalTok{(}\StringTok{"maximum iteration reached}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{"allb"} \OtherTok{=}\NormalTok{ allb, }\StringTok{"beta"} \OtherTok{=}\NormalTok{ b1))}
\NormalTok{  \}}

  \CommentTok{\# fit the model }
\NormalTok{  mybeta }\OtherTok{=} \FunctionTok{mylm\_g}\NormalTok{(X, y, }\AttributeTok{b0 =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}
  
  \FunctionTok{par}\NormalTok{(}\AttributeTok{bg=}\StringTok{"transparent"}\NormalTok{)}
  \FunctionTok{contour}\NormalTok{(beta1, beta2, rss, }\AttributeTok{levels =} \FunctionTok{quantile}\NormalTok{(rss, quanlvl))}
  \FunctionTok{points}\NormalTok{(mybeta}\SpecialCharTok{$}\NormalTok{allb[,}\DecValTok{1}\NormalTok{], mybeta}\SpecialCharTok{$}\NormalTok{allb[,}\DecValTok{2}\NormalTok{], }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(b[}\DecValTok{1}\NormalTok{], b[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
  \FunctionTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-56-1} \end{center}

The descent path is very smooth because we choose a very small step size. However, if the step size is too large, we may observe unstable results or even unable to converge. For example, if We set \(\delta = 1\) or \(\delta = 1.5\).

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}

  \CommentTok{\# fit the model with a larger step size}
\NormalTok{  mybeta }\OtherTok{=} \FunctionTok{mylm\_g}\NormalTok{(X, y, }\AttributeTok{b0 =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{delta =} \DecValTok{1}\NormalTok{)}

  \FunctionTok{contour}\NormalTok{(beta1, beta2, rss, }\AttributeTok{levels =} \FunctionTok{quantile}\NormalTok{(rss, quanlvl))}
  \FunctionTok{points}\NormalTok{(mybeta}\SpecialCharTok{$}\NormalTok{allb[,}\DecValTok{1}\NormalTok{], mybeta}\SpecialCharTok{$}\NormalTok{allb[,}\DecValTok{2}\NormalTok{], }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(b[}\DecValTok{1}\NormalTok{], b[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
  \FunctionTok{box}\NormalTok{()}
  
  \CommentTok{\# and even larger}
\NormalTok{  mybeta }\OtherTok{=} \FunctionTok{mylm\_g}\NormalTok{(X, y, }\AttributeTok{b0 =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{delta =} \FloatTok{1.5}\NormalTok{, }\AttributeTok{maxitr =} \DecValTok{6}\NormalTok{)}
\DocumentationTok{\#\# maximum iteration reached}
  
  \FunctionTok{contour}\NormalTok{(beta1, beta2, rss, }\AttributeTok{levels =} \FunctionTok{quantile}\NormalTok{(rss, quanlvl))}
  \FunctionTok{points}\NormalTok{(mybeta}\SpecialCharTok{$}\NormalTok{allb[,}\DecValTok{1}\NormalTok{], mybeta}\SpecialCharTok{$}\NormalTok{allb[,}\DecValTok{2}\NormalTok{], }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(b[}\DecValTok{1}\NormalTok{], b[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
  \FunctionTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-57-1} \end{center}

\hypertarget{coordinate}{%
\section{Coordinate Descent}\label{coordinate}}

Instead of updating all parameters at a time, we can also update one parameter each time. The \textbf{Gauss-Seidel style} coordinate descent algorithm at the \(k\)th iteration will sequentially update all \(p\) parameters:

\begin{align}
    x_1^{(k+1)} &= \underset{\color{OrangeRed}{x_1}}{\arg\min} \quad f(\color{OrangeRed}{x_1}, x_2^{(k)}, \ldots, x_p^{(k)}) \nonumber \\
    x_2^{(k+1)} &= \underset{\color{OrangeRed}{x_2}}{\arg\min} \quad f(x_1^{\color{DodgerBlue}{(k+1)}}, \color{OrangeRed}{\mathbf{x}_2}, \ldots, x_p^{(k)}) \nonumber \\
    \cdots &\nonumber \\
    x_p^{(k+1)} &= \underset{\color{OrangeRed}{x_p}}{\arg\min} \quad f(x_1^{\color{DodgerBlue}{(k+1)}}, x_2^{\color{DodgerBlue}{(k+1)}}, \ldots, \color{OrangeRed}{x_p}) \nonumber \\
\end{align}

Note that after updating one coordinate, the new parameter value is used for updating the next coordinate. After we complete this loop, all \(j\) are updated to their new values, and we proceed to the next step.

Another type of update is the \textbf{Jacobi style}, which can be performed in parallel at the \(k\)th iteration:

\begin{align}
    x_1^{(k+1)} &= \underset{\color{OrangeRed}{x_1}}{\arg\min} \quad f(\color{OrangeRed}{x_1}, x_2^{(k)}, \ldots, x_p^{(k)}) \nonumber \\
    x_2^{(k+1)} &= \underset{\color{OrangeRed}{x_2}}{\arg\min} \quad f(x_1^{(k+1)}, \color{OrangeRed}{\mathbf{x}_2}, \ldots, x_p^{(k)}) \nonumber \\
    \cdots &\nonumber \\
    x_p^{(k+1)} &= \underset{\color{OrangeRed}{x_p}}{\arg\min} \quad f(x_1^{(k+1)}, x_2^{(k+1)}, \ldots, \color{OrangeRed}{x_p}) \nonumber \\
\end{align}

For differentiable convex functions \(f\), we can ensure that if all parameters are optimized then the entire problem is also optimized. If \(f\) is not differentiable, we may have trouble (see the example on \href{https://en.wikipedia.org/wiki/Coordinate_descent}{wiki}). However, there are also cases where coordinate descent would still guarantee a convergence, e.g., a sperable case:

\[f(\mathbf{x}) = g(\mathbf{x}) + \sum_{j=1}^p h_j(x_j)\]
This is the Lasso formulation which will be discussed in later section.

\hypertarget{coordinate-descent-example-linear-regression}{%
\subsection{Coordinate Descent Example: Linear Regression}\label{coordinate-descent-example-linear-regression}}

Coordinate descent for linear regression is not really necessary. However, we will still use this as an example. Note that the update for a single parameter is

\[
\underset{\boldsymbol \beta_j}{\text{argmin}} \frac{1}{2n} ||\mathbf{y}- X_j \beta_j - \mathbf{X}_{(-j)} \boldsymbol \beta_{(-j)} ||^2
\]

where \(\mathbf{X}_{(-j)}\) is the data matrix without the \(j\)th column. Note that when updating \(\beta_j\) coordinate-wise, we can first calculate the residual defined as \(\mathbf{r} = \mathbf{y} - \mathbf{X}_{(-j)} \boldsymbol \beta_{(-j)}\) which does not depend on \(\beta_j\), and optimize the rest of the formula for \(\beta_j\). This is essentially the same as performing a one-dimensional regression by regressing \(\mathbf{r}\) on \(X_j\) and obtain the update.
\[
\beta_j = \frac{X_j^T \mathbf{r}}{X_j^T X_j}
\]
The coordinate descent usually does not involve choosing a step size. Note that the following function is \textbf{NOT} efficient because there are a lot of wasted calculations. It is only for demonstration purpose. Here we use the Gauss-Seidel style update.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# gradient descent function, which also record the path}
\NormalTok{  mylm\_c }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y, }\AttributeTok{b0 =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(x)), }\AttributeTok{epsilon =} \FloatTok{1e{-}6}\NormalTok{, }\AttributeTok{maxitr =} \DecValTok{5000}\NormalTok{)}
\NormalTok{  \{}
    \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.matrix}\NormalTok{(x)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"x must be a matrix"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.vector}\NormalTok{(y)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"y must be a vector"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{nrow}\NormalTok{(x) }\SpecialCharTok{!=} \FunctionTok{length}\NormalTok{(y)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"number of observations different"}\NormalTok{)}
    
    \CommentTok{\# initialize beta values}
\NormalTok{    allb }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(b0, }\DecValTok{1}\NormalTok{, }\FunctionTok{length}\NormalTok{(b0))}
    
    \CommentTok{\# iterative update}
    \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{maxitr)}
\NormalTok{    \{}
      \CommentTok{\# initiate a vector for new beta}
\NormalTok{      b1 }\OtherTok{=}\NormalTok{ b0}
      
      \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(x))}
\NormalTok{      \{}
        \CommentTok{\# calculate the residual}
\NormalTok{        r }\OtherTok{=}\NormalTok{ y }\SpecialCharTok{{-}}\NormalTok{ x[, }\SpecialCharTok{{-}}\NormalTok{j, drop }\OtherTok{=} \ConstantTok{FALSE}\NormalTok{] }\SpecialCharTok{\%*\%}\NormalTok{ b1[}\SpecialCharTok{{-}}\NormalTok{j]}
        
        \CommentTok{\# update jth coordinate}
\NormalTok{        b1[j] }\OtherTok{=} \FunctionTok{t}\NormalTok{(r) }\SpecialCharTok{\%*\%}\NormalTok{ x[,j] }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{t}\NormalTok{(x[,j, }\AttributeTok{drop =} \ConstantTok{FALSE}\NormalTok{]) }\SpecialCharTok{\%*\%}\NormalTok{ x[,j])}
        
        \CommentTok{\# record the update}
\NormalTok{        allb }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(allb, }\FunctionTok{as.vector}\NormalTok{(b1))}
\NormalTok{      \}}

      \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{max}\NormalTok{(}\FunctionTok{abs}\NormalTok{(b0 }\SpecialCharTok{{-}}\NormalTok{ b1)) }\SpecialCharTok{\textless{}}\NormalTok{ epsilon)}
        \ControlFlowTok{break}\NormalTok{;}
      
      \CommentTok{\# reset beta0}
\NormalTok{      b0 }\OtherTok{=}\NormalTok{ b1}
\NormalTok{    \}}

    \ControlFlowTok{if}\NormalTok{ (k }\SpecialCharTok{==}\NormalTok{ maxitr) }\FunctionTok{cat}\NormalTok{(}\StringTok{"maximum iteration reached}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{"allb"} \OtherTok{=}\NormalTok{ allb, }\StringTok{"beta"} \OtherTok{=}\NormalTok{ b1))}
\NormalTok{  \}}

  \CommentTok{\# fit the model }
\NormalTok{  mybeta }\OtherTok{=} \FunctionTok{mylm\_c}\NormalTok{(X, y, }\AttributeTok{b0 =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}

  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
  \FunctionTok{contour}\NormalTok{(beta1, beta2, rss, }\AttributeTok{levels =} \FunctionTok{quantile}\NormalTok{(rss, quanlvl))}
  \FunctionTok{points}\NormalTok{(mybeta}\SpecialCharTok{$}\NormalTok{allb[,}\DecValTok{1}\NormalTok{], mybeta}\SpecialCharTok{$}\NormalTok{allb[,}\DecValTok{2}\NormalTok{], }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(b[}\DecValTok{1}\NormalTok{], b[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
  \FunctionTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-58-1} \end{center}

\hypertarget{stocastic-gradient-descent}{%
\section{Stocastic Gradient Descent}\label{stocastic-gradient-descent}}

The main advantage of using Stochastic Gradient Descent (SGD) is its computational speed. Calculating the gradient using all observations can be costly. Instead, we consider update the parameter \textbf{based on a single observation}. Hence, the gradient is defined as

\[
\frac{\partial \ell_i(\boldsymbol \beta)}{\partial \boldsymbol \beta} = - (y_i - x_i^\text{T} \boldsymbol \beta) x_i.
\]
Compared with using all observations, this is \(1/n\) of the cost. However, because this is rater not accurate for each iteration, but can still converge in the long run. There is a decay rate involved in SGD step size. If the step size does not decreases to 0, the algorithm cannot converge. However, it also has to sum up to infinite to allow us to go as far as we can. For example, a choice could be \(\delta_k = 1/k\), hence \(\sum \delta_k = \infty\) and \(\sum \delta_k^2 < \infty\).

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# gradient descent function, which also record the path}
\NormalTok{  mylm\_sgd }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y, }\AttributeTok{b0 =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(x)), }\AttributeTok{delta =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{maxitr =} \DecValTok{10}\NormalTok{)}
\NormalTok{  \{}
    \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.matrix}\NormalTok{(x)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"x must be a matrix"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.vector}\NormalTok{(y)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"y must be a vector"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{nrow}\NormalTok{(x) }\SpecialCharTok{!=} \FunctionTok{length}\NormalTok{(y)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"number of observations different"}\NormalTok{)}
    
    \CommentTok{\# initialize beta values}
\NormalTok{    allb }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(b0, }\DecValTok{1}\NormalTok{, }\FunctionTok{length}\NormalTok{(b0))}
    
    \CommentTok{\# iterative update}
    \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{maxitr)}
\NormalTok{    \{}
      \CommentTok{\# going through all samples}
      \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(x)))}
\NormalTok{      \{}
        \CommentTok{\# update based on the gradient of a single subject}
\NormalTok{        b0 }\OtherTok{=}\NormalTok{ b0 }\SpecialCharTok{+}\NormalTok{ (y[i] }\SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(x[i, ] }\SpecialCharTok{*}\NormalTok{ b0)) }\SpecialCharTok{*}\NormalTok{ x[i, ] }\SpecialCharTok{*}\NormalTok{ delta}

        \CommentTok{\# record the update}
\NormalTok{        allb }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(allb, }\FunctionTok{as.vector}\NormalTok{(b0))}
        
        \CommentTok{\# learning rate decay}
\NormalTok{        delta }\OtherTok{=}\NormalTok{ delta }\SpecialCharTok{*} \DecValTok{1}\SpecialCharTok{/}\NormalTok{k}
\NormalTok{      \}}
\NormalTok{    \}}
    
    \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{"allb"} \OtherTok{=}\NormalTok{ allb, }\StringTok{"beta"} \OtherTok{=}\NormalTok{ b0))}
\NormalTok{  \}}

  \CommentTok{\# fit the model }
\NormalTok{  mybeta }\OtherTok{=} \FunctionTok{mylm\_sgd}\NormalTok{(X, y, }\AttributeTok{b0 =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{maxitr =} \DecValTok{3}\NormalTok{)}
  
  \FunctionTok{contour}\NormalTok{(beta1, beta2, rss, }\AttributeTok{levels =} \FunctionTok{quantile}\NormalTok{(rss, quanlvl))}
  \FunctionTok{points}\NormalTok{(mybeta}\SpecialCharTok{$}\NormalTok{allb[,}\DecValTok{1}\NormalTok{], mybeta}\SpecialCharTok{$}\NormalTok{allb[,}\DecValTok{2}\NormalTok{], }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(b[}\DecValTok{1}\NormalTok{], b[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
  \FunctionTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-59-1} \end{center}

\hypertarget{mini-batch-stocastic-gradient-descent}{%
\subsection{Mini-batch Stocastic Gradient Descent}\label{mini-batch-stocastic-gradient-descent}}

Instead of using just one observation, we could also consider splitting the data into several small ``batches'' and use one batch of sample to calculate the gradient at each iteration.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# gradient descent function, which also record the path}
\NormalTok{  mylm\_sgd\_mb }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y, }\AttributeTok{b0 =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(x)), }\AttributeTok{delta =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{maxitr =} \DecValTok{20}\NormalTok{)}
\NormalTok{  \{}
    \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.matrix}\NormalTok{(x)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"x must be a matrix"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.vector}\NormalTok{(y)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"y must be a vector"}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{nrow}\NormalTok{(x) }\SpecialCharTok{!=} \FunctionTok{length}\NormalTok{(y)) }\FunctionTok{stop}\NormalTok{(}\StringTok{"number of observations different"}\NormalTok{)}
    
    \CommentTok{\# initiate batches with 10 observations each}
\NormalTok{    batch }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{floor}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(x)}\SpecialCharTok{/}\DecValTok{10}\NormalTok{), }\AttributeTok{length.out =} \FunctionTok{nrow}\NormalTok{(x)))}
  
    \CommentTok{\# initialize beta values}
\NormalTok{    allb }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(b0, }\DecValTok{1}\NormalTok{, }\FunctionTok{length}\NormalTok{(b0))}
    
    \CommentTok{\# iterative update}
    \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{maxitr)}
\NormalTok{    \{}
      \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{max}\NormalTok{(batch)) }\CommentTok{\# loop through batches}
\NormalTok{      \{}
        \CommentTok{\# update based on the gradient of a single subject}
\NormalTok{        b0 }\OtherTok{=}\NormalTok{ b0 }\SpecialCharTok{+} \FunctionTok{t}\NormalTok{(x[batch}\SpecialCharTok{==}\NormalTok{i, ]) }\SpecialCharTok{\%*\%}\NormalTok{ (y[batch}\SpecialCharTok{==}\NormalTok{i] }\SpecialCharTok{{-}}\NormalTok{ x[batch}\SpecialCharTok{==}\NormalTok{i, ] }\SpecialCharTok{\%*\%}\NormalTok{ b0) }\SpecialCharTok{*} 
\NormalTok{          delta }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(batch}\SpecialCharTok{==}\NormalTok{i)}
        
        \CommentTok{\# record the update}
\NormalTok{        allb }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(allb, }\FunctionTok{as.vector}\NormalTok{(b0))}
        
        \CommentTok{\# learning rate decay}
\NormalTok{        delta }\OtherTok{=}\NormalTok{ delta }\SpecialCharTok{*} \DecValTok{1}\SpecialCharTok{/}\NormalTok{k}
\NormalTok{      \}}
\NormalTok{    \}}
    
    \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{"allb"} \OtherTok{=}\NormalTok{ allb, }\StringTok{"beta"} \OtherTok{=}\NormalTok{ b0))}
\NormalTok{  \}}

  \CommentTok{\# fit the model }
\NormalTok{  mybeta }\OtherTok{=} \FunctionTok{mylm\_sgd\_mb}\NormalTok{(X, y, }\AttributeTok{b0 =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{maxitr =} \DecValTok{3}\NormalTok{)}

  \FunctionTok{contour}\NormalTok{(beta1, beta2, rss, }\AttributeTok{levels =} \FunctionTok{quantile}\NormalTok{(rss, quanlvl))}
  \FunctionTok{points}\NormalTok{(mybeta}\SpecialCharTok{$}\NormalTok{allb[,}\DecValTok{1}\NormalTok{], mybeta}\SpecialCharTok{$}\NormalTok{allb[,}\DecValTok{2}\NormalTok{], }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(b[}\DecValTok{1}\NormalTok{], b[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
  \FunctionTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-60-1} \end{center}

You may further play around with these tuning parameters to see how sensitive the optimization is to them. A stopping rule can be difficult to determine, hence in practice, early stop is also used.

\hypertarget{lagrangian-multiplier-for-constrained-problems}{%
\section{Lagrangian Multiplier for Constrained Problems}\label{lagrangian-multiplier-for-constrained-problems}}

Constrained optimization problems appear very frequently. Both Lasso and Ridge regressions can be viewed as constrained problems, while support vector machines (SVM) is another example, which will be introduced later on. Let's investigate this using a toy example. Suppose we have an optimization problem

\[\text{minimize} \quad f(x, y) = x^2 + y^2\]
\[\text{subj. to} \quad g(x, y) = xy - 4 = 0\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{  mygrid }\OtherTok{\textless{}{-}} \FunctionTok{data.matrix}\NormalTok{(}\FunctionTok{expand.grid}\NormalTok{(x, y))}
\NormalTok{  f }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(mygrid[,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ mygrid[,}\DecValTok{2}\NormalTok{]}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\FunctionTok{length}\NormalTok{(x), }\FunctionTok{length}\NormalTok{(y))}

\NormalTok{  f2 }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(mygrid[,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{*}\NormalTok{mygrid[,}\DecValTok{2}\NormalTok{], }\FunctionTok{length}\NormalTok{(x), }\FunctionTok{length}\NormalTok{(y))}
  
  \CommentTok{\# plot the contour}
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
  \FunctionTok{contour}\NormalTok{(x, y, f, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{16}\NormalTok{))}
  \FunctionTok{contour}\NormalTok{(x, y, f2, }\AttributeTok{levels =} \DecValTok{4}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
  \FunctionTok{box}\NormalTok{()}

  \FunctionTok{lines}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\DecValTok{4}\SpecialCharTok{{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)    }
  \FunctionTok{points}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-61-1} \end{center}

The problem itself is very simple. We know that the optimizer is the red dot. But an interesting point of view is to look at the level curves of the objective function. As it is growing (expanding), there is one point (the red dot) at which level curve barely touches the constrain curve (blue line). This should be the optimizer. But this also implies that the tangent line (orange line) of this leveling curve must coincide with the tangent line of the constraint. Noticing that the tangent line can be obtained by taking the derivative of the function, this observation implies that gradients of the two functions (the objective function and the constraint function) must be a multiple of the other. Hence,

\[ 
\begin{align}
& \bigtriangledown f = \lambda \bigtriangledown g \\
\\
\Longrightarrow \qquad &  \begin{cases}
    2x = \lambda y & \text{by taking derivative w.r.t.} \,\, x\\
    2y = \lambda x & \text{by taking derivative w.r.t.} \,\, y\\
    xy - 4 = 0 & \text{the constraint itself}
  \end{cases}
\end{align}
\]

The three equations put together is very easy to solve. We have \(x = y = 0\) or \(\lambda = \pm 2\) based on the first two equations. The first one is not feasible based on the constraint. The second solution leads to two feasible solutions: \(x = y = 2\) or \(x = y = -2\). Hence, we now know that there are two solutions.

Now, looking back at the equation \(\bigtriangledown f = \lambda \bigtriangledown g\), this is simply the derivative of the \textbf{Lagrangian function} defined as

\[{\cal L}(x, y, \lambda) = f(x, y) - \lambda g(x, y),\]
while solving for the solution of the constrained problem becomes finding the stationary point of the Lagrangian. Be aware that in some cases, the solution you found can be maximizers instead of minimizers. Hence, its necessary to compare all of them and see which one is smaller.

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{part-linear-and-penalized-linear-models}{%
\part{Linear and Penalized Linear Models}\label{part-linear-and-penalized-linear-models}}

\hypertarget{linear-regression-and-model-selection}{%
\chapter{Linear Regression and Model Selection}\label{linear-regression-and-model-selection}}

This chapter severs several purposes. First, we will review some basic knowledge of linear regression. This includes the concept of vector space, projection, which leads to estimating parameters of a linear regression. Most of these knowledge are covered in the prerequisite so you shouldn't find these concepts too difficult to understand. Secondly, we will mainly use the \texttt{lm()} function as an example to demonstrate some features of \texttt{R}. This includes extracting results, visualizations, handling categorical variables, prediction and model selection. These concepts will be useful for other models. Finally, we will introduce several model selection criteria and algorithms to perform model selection.

\hypertarget{example-real-estate-data}{%
\section{Example: real estate data}\label{example-real-estate-data}}

This \href{https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set}{Real Estate data} (\protect\hyperlink{ref-yeh2018building}{Yeh and Hsu 2018}) is provided on the \href{https://archive.ics.uci.edu/ml/index.php}{UCI machine learning repository}. The goal of this dataset is to predict the unit house price based on six different covariates:

\begin{itemize}
\tightlist
\item
  \texttt{date}: The transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)
\item
  \texttt{age}: The house age (unit: year)
\item
  \texttt{distance}: The distance to the nearest MRT station (unit: meter)
\item
  \texttt{stores}: The number of convenience stores in the living circle on foot (integer)
\item
  \texttt{latitude}: Latitude (unit: degree)
\item
  \texttt{longitude}: Longitude (unit: degree)
\item
  \texttt{price}: House price of unit area
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    realestate }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/realestate.csv"}\NormalTok{, }\AttributeTok{row.names =} \DecValTok{1}\NormalTok{)}

    \FunctionTok{library}\NormalTok{(DT)}
    \FunctionTok{datatable}\NormalTok{(realestate, }\AttributeTok{filter =} \StringTok{"top"}\NormalTok{, }\AttributeTok{rownames =} \ConstantTok{FALSE}\NormalTok{,}
              \AttributeTok{options =} \FunctionTok{list}\NormalTok{(}\AttributeTok{pageLength =} \DecValTok{8}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{notation-and-basic-properties}{%
\section{Notation and Basic Properties}\label{notation-and-basic-properties}}

We usually denote the observed covariates data as the design matrix \(\mathbf{X}\), with dimension \(n \times p\). Hence in this case, the dimension of \(\mathbf{X}\) is \(414 \times 7\). The \(j\)th variable is simply the \(j\)th column of this matrix, which is denoted as \(\mathbf{x}_j\). The outcome \(\mathbf{y}\) (\texttt{price}) is a vector of length \(414\). Please note that we usually use a ``bold'' symbol to represent a vector, while for a single element (scalar), such as the \(j\)th variable of subject \(i\), we use \(x_{ij}\).

A linear regression concerns modeling the relationship (in matrix form)

\[\mathbf{y}_{n \times 1} = \mathbf{X}_{n \times p} \boldsymbol \beta_{p \times 1} + \boldsymbol \epsilon_{n \times 1}\]
And we know that the solution is obtained by minimizing the residual sum of squares (RSS):

\[ 
\begin{align}
\widehat{\boldsymbol \beta} &= \underset{\boldsymbol \beta}{\arg\min} \sum_{i=1}^n \left(y_i - x_i^\text{T}\boldsymbol \beta\right)^2 \\
&= \underset{\boldsymbol \beta}{\arg\min} \big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)^\text{T}\big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)
\end{align}
\]
Classic solution can be obtained by taking the derivative of RSS w.r.t \(\boldsymbol \beta\) and set it to zero. This leads to the well known normal equation:

\[
\begin{align}
    \frac{\partial \text{RSS}}{\partial \boldsymbol \beta} &= -2 \mathbf{X}^\text{T}(\mathbf{y}- \mathbf{X}\boldsymbol \beta) \doteq 0 \\
    \Longrightarrow \quad \mathbf{X}^\text{T}\mathbf{y}&= \mathbf{X}^\text{T}\mathbf{X}\boldsymbol \beta
\end{align}
\]
Assuming that \(\mathbf{X}\) is full rank, then \(\mathbf{X}^\text{T}\mathbf{X}\) is invertible. Then, we have

\[
\widehat{\boldsymbol \beta} = (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}\mathbf{y}
\]
Some additional concepts are frequently used. The fitted values \(\widehat{\mathbf{y}}\) are essentially the prediction of the original \(n\) training data points:

\[ 
\begin{align}
\widehat{\mathbf{y}} =& \mathbf{X}\boldsymbol \beta\\
=& \underbrace{\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}}_{\mathbf{H}} \mathbf{y}\\
\doteq& \mathbf{H}_{n \times n} \mathbf{y}
\end{align}
\]
where \(\mathbf{H}\) is called the ``hat'' matrix. It is a projection matrix that projects any vector (\(\mathbf{y}\) in our case) onto the column space of \(\mathbf{X}\). A project matrix enjoys two properties

\begin{itemize}
\tightlist
\item
  Symmetric: \(\mathbf{H}^\text{T}= \mathbf{H}\)
\item
  Idempotent \(\mathbf{H}\mathbf{H}= \mathbf{H}\)
\end{itemize}

The residuals \(\mathbf{r}\) can also be obtained using the hat matrix:

\[ \mathbf{r}= \mathbf{y}- \widehat{\mathbf{y}} = (\mathbf{I}- \mathbf{H}) \mathbf{y}\]
From the properties of a projection matrix, we also know that \(\mathbf{r}\) should be orthogonal to any vector from the column space of \(\mathbf{X}\). Hence,

\[\mathbf{X}^\text{T}\mathbf{r}= \mathbf{0}_{p \times 1}\]

The residuals is also used to estimate the error variance:

\[\widehat\sigma^2 = \frac{1}{n-p} \sum_{i=1}^n r_i^2 = \frac{\text{RSS}}{n-p}\]
When the data are indeed generated from a linear model, and with suitable conditions on the design matrix and random errors \(\boldsymbol \epsilon\), we can conclude that \(\widehat{\boldsymbol \beta}\) is an \textbf{unbiased} estimator of \(\boldsymbol \beta\). Its variance-covariance matrix satisfies

\[
\begin{align}
    \text{Var}(\widehat{\boldsymbol \beta}) &= \text{Var}\big( (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}\mathbf{y}\big) \nonumber \\
    &= \text{Var}\big( (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}(\mathbf{X}\boldsymbol \beta+ \boldsymbol \epsilon) \big) \nonumber \\
    &= \text{Var}\big( (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}\boldsymbol \epsilon) \big) \nonumber \\
    &= (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{I}\sigma^2 \nonumber \\
    &= (\mathbf{X}^\text{T}\mathbf{X})^{-1}\sigma^2
\end{align}
\]
All of the above mentioned results are already implemented in R through the \texttt{lm()} function to fit a linear regression.

\hypertarget{using-the-lm-function}{%
\section{\texorpdfstring{Using the \texttt{lm()} Function}{Using the lm() Function}}\label{using-the-lm-function}}

Let's consider a simple regression that uses \texttt{age} and \texttt{distance} to model \texttt{price}. We will save the fitted object as \texttt{lm.fit}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    lm.fit }\OtherTok{=} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ distance, }\AttributeTok{data =}\NormalTok{ realestate)}
\end{Highlighting}
\end{Shaded}

This syntax contains three components:

\begin{itemize}
\tightlist
\item
  \texttt{data\ =} specifies the dataset
\item
  The outcome variable should be on the left hand side of \texttt{\textasciitilde{}}
\item
  The covariates should be on the right hand side of \texttt{\textasciitilde{}}
\end{itemize}

To look at the detailed model fitting results, use the \texttt{summary()} function

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    lm.summary }\OtherTok{=} \FunctionTok{summary}\NormalTok{(lm.fit)}
\NormalTok{    lm.summary}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Call:}
\DocumentationTok{\#\# lm(formula = price \textasciitilde{} age + distance, data = realestate)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Residuals:}
\DocumentationTok{\#\#     Min      1Q  Median      3Q     Max }
\DocumentationTok{\#\# {-}36.032  {-}4.742  {-}1.037   4.533  71.930 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Coefficients:}
\DocumentationTok{\#\#               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\DocumentationTok{\#\# (Intercept) 49.8855858  0.9677644  51.547  \textless{} 2e{-}16 ***}
\DocumentationTok{\#\# age         {-}0.2310266  0.0420383  {-}5.496 6.84e{-}08 ***}
\DocumentationTok{\#\# distance    {-}0.0072086  0.0003795 {-}18.997  \textless{} 2e{-}16 ***}
\DocumentationTok{\#\# {-}{-}{-}}
\DocumentationTok{\#\# Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Residual standard error: 9.73 on 411 degrees of freedom}
\DocumentationTok{\#\# Multiple R{-}squared:  0.4911, Adjusted R{-}squared:  0.4887 }
\DocumentationTok{\#\# F{-}statistic: 198.3 on 2 and 411 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

This shows that both \texttt{age} and \texttt{distance} are highly significant for predicting the price. In fact, this fitted object (\texttt{lm.fit}) and the summary object (\texttt{lm.summary}) are both saved as a list. This is pretty common to handle an output object with many things involved. We may peek into this object to see what are provided using a \texttt{\$} after the object.

\includegraphics[width=0.4\textwidth,height=\textheight]{images/reactive.png}

The \texttt{str()} function can also display all the items in a list.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{str}\NormalTok{(lm.summary)}
\end{Highlighting}
\end{Shaded}

Usually, printing out the summary is sufficient. However, further details can be useful for other purposes. For example, if we interested in the residual vs.~fits plot, we may use

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{plot}\NormalTok{(lm.fit}\SpecialCharTok{$}\NormalTok{fitted.values, lm.fit}\SpecialCharTok{$}\NormalTok{residuals, }
         \AttributeTok{xlab =} \StringTok{"Fitted Values"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Residuals"}\NormalTok{,}
         \AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-68-1} \end{center}

It seems that the error variance is not constant (as a function of the fitted values), hence additional techniques may be required to handle this issue. However, that is beyond the scope of this book.

\hypertarget{adding-covariates}{%
\subsection{Adding Covariates}\label{adding-covariates}}

It is pretty simple if we want to include additional variables. This is usually done by connecting them with the \texttt{+} sign on the right hand side of \texttt{\textasciitilde{}}. R also provide convenient ways to include interactions and higher order terms. The following code with the interaction term between \texttt{age} and \texttt{distance}, and a squared term of \texttt{distance} should be self-explanatory.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    lm.fit2 }\OtherTok{=} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ distance }\SpecialCharTok{+}\NormalTok{ age}\SpecialCharTok{*}\NormalTok{distance }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(distance}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), }\AttributeTok{data =}\NormalTok{ realestate)}
    \FunctionTok{summary}\NormalTok{(lm.fit2)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Call:}
\DocumentationTok{\#\# lm(formula = price \textasciitilde{} age + distance + age * distance + I(distance\^{}2), }
\DocumentationTok{\#\#     data = realestate)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Residuals:}
\DocumentationTok{\#\#     Min      1Q  Median      3Q     Max }
\DocumentationTok{\#\# {-}37.117  {-}4.160  {-}0.594   3.548  69.683 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Coefficients:}
\DocumentationTok{\#\#                 Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\DocumentationTok{\#\# (Intercept)    5.454e+01  1.099e+00  49.612  \textless{} 2e{-}16 ***}
\DocumentationTok{\#\# age           {-}2.615e{-}01  4.931e{-}02  {-}5.302 1.87e{-}07 ***}
\DocumentationTok{\#\# distance      {-}1.603e{-}02  1.133e{-}03 {-}14.152  \textless{} 2e{-}16 ***}
\DocumentationTok{\#\# I(distance\^{}2)  1.907e{-}06  2.416e{-}07   7.892 2.75e{-}14 ***}
\DocumentationTok{\#\# age:distance   8.727e{-}06  4.615e{-}05   0.189     0.85    }
\DocumentationTok{\#\# {-}{-}{-}}
\DocumentationTok{\#\# Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Residual standard error: 8.939 on 409 degrees of freedom}
\DocumentationTok{\#\# Multiple R{-}squared:  0.5726, Adjusted R{-}squared:  0.5684 }
\DocumentationTok{\#\# F{-}statistic:   137 on 4 and 409 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

If you choose to include all covariates presented in the data, then simply use \texttt{.} on the right hand side of \texttt{\textasciitilde{}}. However, you should always be careful when doing this because some dataset would contain meaningless variables such as subject ID.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    lm.fit3 }\OtherTok{=} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ realestate)}
\end{Highlighting}
\end{Shaded}

\hypertarget{categorical-variables}{%
\subsection{Categorical Variables}\label{categorical-variables}}

The \texttt{store} variable has several different values. We can see that it has 11 different values. One strategy is to model this as a continuous variable. However, we may also consider to discretize it. For example, we may create a new variable, say \texttt{store.cat}, defined as follows

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{table}\NormalTok{(realestate}\SpecialCharTok{$}\NormalTok{stores)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#  0  1  2  3  4  5  6  7  8  9 10 }
\DocumentationTok{\#\# 67 46 24 46 31 67 37 31 30 25 10}

  \CommentTok{\# define a new factor variable}
\NormalTok{  realestate}\SpecialCharTok{$}\NormalTok{store.cat }\OtherTok{=} \FunctionTok{as.factor}\NormalTok{((realestate}\SpecialCharTok{$}\NormalTok{stores }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ (realestate}\SpecialCharTok{$}\NormalTok{stores }\SpecialCharTok{\textgreater{}} \DecValTok{4}\NormalTok{))}
  \FunctionTok{table}\NormalTok{(realestate}\SpecialCharTok{$}\NormalTok{store.cat)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#   0   1   2 }
\DocumentationTok{\#\#  67 147 200}
  \FunctionTok{levels}\NormalTok{(realestate}\SpecialCharTok{$}\NormalTok{store.cat) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"None"}\NormalTok{, }\StringTok{"Several"}\NormalTok{, }\StringTok{"Many"}\NormalTok{)}
  \FunctionTok{head}\NormalTok{(realestate}\SpecialCharTok{$}\NormalTok{store.cat)}
\DocumentationTok{\#\# [1] Many    Many    Many    Many    Many    Several}
\DocumentationTok{\#\# Levels: None Several Many}
\end{Highlighting}
\end{Shaded}

This variable is defined as a factor, which is often used for categorical variables. Since this variable has three different categories, if we include it in the linear regression, it will introduce two additional variables (using the third as the reference):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    lm.fit3 }\OtherTok{=} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ distance }\SpecialCharTok{+}\NormalTok{ store.cat, }\AttributeTok{data =}\NormalTok{ realestate)}
    \FunctionTok{summary}\NormalTok{(lm.fit3)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Call:}
\DocumentationTok{\#\# lm(formula = price \textasciitilde{} age + distance + store.cat, data = realestate)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Residuals:}
\DocumentationTok{\#\#     Min      1Q  Median      3Q     Max }
\DocumentationTok{\#\# {-}38.656  {-}5.360  {-}0.868   3.913  76.797 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Coefficients:}
\DocumentationTok{\#\#                   Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\DocumentationTok{\#\# (Intercept)      43.712887   1.751608  24.956  \textless{} 2e{-}16 ***}
\DocumentationTok{\#\# age              {-}0.229882   0.040095  {-}5.733 1.91e{-}08 ***}
\DocumentationTok{\#\# distance         {-}0.005404   0.000470 {-}11.497  \textless{} 2e{-}16 ***}
\DocumentationTok{\#\# store.catSeveral  0.838228   1.435773   0.584     0.56    }
\DocumentationTok{\#\# store.catMany     8.070551   1.646731   4.901 1.38e{-}06 ***}
\DocumentationTok{\#\# {-}{-}{-}}
\DocumentationTok{\#\# Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Residual standard error: 9.279 on 409 degrees of freedom}
\DocumentationTok{\#\# Multiple R{-}squared:  0.5395, Adjusted R{-}squared:  0.535 }
\DocumentationTok{\#\# F{-}statistic: 119.8 on 4 and 409 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

There are usually two types of categorical variables:

\begin{itemize}
\tightlist
\item
  Ordinal: the numbers representing each category is ordered, e.g., how many stores in the neighborhood. Oftentimes nominal data can be treated as a continuous variable.
\item
  Nominal: they are not ordered and can be represented using either numbers or letters, e.g., ethnic group.
\end{itemize}

The above example is treating \texttt{store.cat} as a nominal variable, and the \texttt{lm()} function is using dummy variables for each category. This should be our default approach to handle nominal variables.

\hypertarget{model-selection-criteria}{%
\section{Model Selection Criteria}\label{model-selection-criteria}}

We will use the \texttt{diabetes} dataset from the \texttt{lars} package as a demonstration of model selection. Ten baseline variables include age, sex, body mass index, average blood pressure, and six blood serum measurements. These measurements were obtained for each of n = 442 diabetes patients, as well as the outcome of interest, a quantitative measure of disease progression one year after baseline. More details are available in Efron et al. (\protect\hyperlink{ref-efron2004least}{2004}). Our goal is to select a linear model, preferably with a small number of variables, that can predict the outcome. To select the best model, commonly used strategies include Marrow's \(C_p\), AIC (Akaike information criterion) and BIC (Bayesian information criterion). Further derivations will be provide at a later section.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# load the diabetes data}
    \FunctionTok{library}\NormalTok{(lars)}
\DocumentationTok{\#\# Loaded lars 1.3}
    \FunctionTok{data}\NormalTok{(diabetes)}
\NormalTok{    diab }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(diabetes}\SpecialCharTok{$}\NormalTok{x, }\StringTok{"Y"} \OtherTok{=}\NormalTok{ diabetes}\SpecialCharTok{$}\NormalTok{y))}

    \CommentTok{\# fit linear regression with all covariates}
\NormalTok{    lm.fit }\OtherTok{=} \FunctionTok{lm}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data=}\NormalTok{diab)}
\end{Highlighting}
\end{Shaded}

The idea of model selection is to apply some penalty on the number of parameters used in the model. In general, they are usually in the form of

\[\text{Goodness-of-Fit} + \text{Complexity Penality}\]

\hypertarget{using-marrows-c_p}{%
\subsection{\texorpdfstring{Using Marrows' \(C_p\)}{Using Marrows' C\_p}}\label{using-marrows-c_p}}

For example, the Marrows' \(C_p\) criterion minimize the following quantity (a derivation is provided at Section \ref{marrows-cp}):

\[\text{RSS} + 2 p \widehat\sigma_{\text{full}}^2\]
Note that the \(\sigma_{\text{full}}^2\) refers to the residual variance estimation based on the full model, i.e., will all variables. Hence, this formula cannot be used when \(p > n\) because you would not be able to obtain a valid estimation of \(\sigma_{\text{full}}^2\). Nonetheless, we can calculate this quantity with the diabetes dataset

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# number of variables (including intercept)}
\NormalTok{    p }\OtherTok{=} \DecValTok{11}
\NormalTok{    n }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(diab)}
      
    \CommentTok{\# obtain residual sum of squares}
\NormalTok{    RSS }\OtherTok{=} \FunctionTok{sum}\NormalTok{(}\FunctionTok{residuals}\NormalTok{(lm.fit)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
    
    \CommentTok{\# use the formula directly to calculate the Cp criterion }
\NormalTok{    Cp }\OtherTok{=}\NormalTok{ RSS }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{p}\SpecialCharTok{*}\FunctionTok{summary}\NormalTok{(lm.fit)}\SpecialCharTok{$}\NormalTok{sigma}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{    Cp}
\DocumentationTok{\#\# [1] 1328502}
\end{Highlighting}
\end{Shaded}

We can compare this with another sub-model, say, with just \texttt{age} and \texttt{glu}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    lm.fit\_sub }\OtherTok{=} \FunctionTok{lm}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ glu, }\AttributeTok{data=}\NormalTok{diab)}
  
    \CommentTok{\# obtain residual sum of squares}
\NormalTok{    RSS\_sub }\OtherTok{=} \FunctionTok{sum}\NormalTok{(}\FunctionTok{residuals}\NormalTok{(lm.fit\_sub)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
    
    \CommentTok{\# use the formula directly to calculate the Cp criterion }
\NormalTok{    Cp\_sub }\OtherTok{=}\NormalTok{ RSS\_sub }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\DecValTok{3}\SpecialCharTok{*}\FunctionTok{summary}\NormalTok{(lm.fit)}\SpecialCharTok{$}\NormalTok{sigma}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{    Cp\_sub}
\DocumentationTok{\#\# [1] 2240019}
\end{Highlighting}
\end{Shaded}

Comparing this with the previous one, the full model is better.

\hypertarget{using-aic-and-bic}{%
\subsection{Using AIC and BIC}\label{using-aic-and-bic}}

Calculating the AIC and BIC criteria in \texttt{R} is a lot simpler, with the existing functions. The AIC score is given by

\[-2 \text{Log-likelihood} + 2 p,\]
while the BIC score is given by

\[-2 \text{Log-likelihood} + \log(n) p,\]

Interestingly, when assuming that the error distribution is Gaussian, the log-likelihood part is just a function of the RSS. In general, AIC performs similarly to \(C_p\), while BIC tend to select a much smaller set due to the larger penalty. Theoretically, both AIC and \(C_p\) are interested in the prediction error, regardless of whether the model is specified correctly, while BIC is interested in selecting the true set of variables, while assuming that the true model is being considered.

The AIC score can be done using the \texttt{AIC()} function. We can match this result by writing out the normal density function and plug in the estimated parameters. Note that this requires one additional parameter, which is the variance. Hence the total number of parameters is 12. We can calculate this with our own code:

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# ?AIC}
    \CommentTok{\# a build{-}in function for calculating AIC using {-}2log likelihood}
    \FunctionTok{AIC}\NormalTok{(lm.fit) }
\DocumentationTok{\#\# [1] 4795.985}

    \CommentTok{\# Match the result}
\NormalTok{    n}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(RSS}\SpecialCharTok{/}\NormalTok{n) }\SpecialCharTok{+}\NormalTok{ n }\SpecialCharTok{+}\NormalTok{ n}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi) }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{p}
\DocumentationTok{\#\# [1] 4795.985}
\end{Highlighting}
\end{Shaded}

Alternatively, the \texttt{extractAIC()} function can calculate both AIC and BIC. However, note that the \texttt{n\ +\ n*log(2*pi)\ +\ 2} part in the above code does not change regardless of how many parameters we use. Hence, this quantify does not affect the comparison between different models. Then we can safely remove this part and only focus on the essential ones.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# ?extractAIC}
    \CommentTok{\# AIC for the full model}
    \FunctionTok{extractAIC}\NormalTok{(lm.fit)}
\DocumentationTok{\#\# [1]   11.000 3539.643}
\NormalTok{    n}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(RSS}\SpecialCharTok{/}\NormalTok{n) }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{p}
\DocumentationTok{\#\# [1] 3539.643}

    \CommentTok{\# BIC for the full model}
    \FunctionTok{extractAIC}\NormalTok{(lm.fit, }\AttributeTok{k =} \FunctionTok{log}\NormalTok{(n))}
\DocumentationTok{\#\# [1]   11.000 3584.648}
\NormalTok{    n}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(RSS}\SpecialCharTok{/}\NormalTok{n) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(n)}\SpecialCharTok{*}\NormalTok{p}
\DocumentationTok{\#\# [1] 3584.648}
\end{Highlighting}
\end{Shaded}

Now, we can compare AIC or BIC using of two different models and select whichever one that gives a smaller value. For example the AIC of the previous sub-model is

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# AIC for the sub{-}model}
    \FunctionTok{extractAIC}\NormalTok{(lm.fit\_sub)}
\DocumentationTok{\#\# [1]    3.000 3773.077}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-selection-algorithms}{%
\section{Model Selection Algorithms}\label{model-selection-algorithms}}

In previous examples, we have to manually fit two models and calculate their respective selection criteria and compare them. This is a rather tedious process if we have many variables and a huge number of combinations to consider. To automatically compare different models and select the best one, there are two common computational approaches: best subset regression and step-wise regression. As their name suggest, the best subset selection will exhaust all possible combination of variables, while the step-wise regression would adjust the model by adding or subtracting one variable at a time to reach the best model.

\hypertarget{best-subset-selection-with-leaps}{%
\subsection{\texorpdfstring{Best Subset Selection with \texttt{leaps}}{Best Subset Selection with leaps}}\label{best-subset-selection-with-leaps}}

Since the penalty is only affected by the number of variables, we may first choose the best model with the smallest RSS for each model size, and then compare across these models by attaching the penalty terms of their corresponding sizes. The \texttt{leaps} package can be used to calculate the best model of each model size. It essentially performs an exhaustive search, however, still utilizing some tricks to skip some really bad models. Note that the \texttt{leaps} package uses the data matrix directly, instead of specifying a formula.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(leaps)}
    
    \CommentTok{\# The package specifies the X matrix and outcome y vector}
\NormalTok{    RSSleaps }\OtherTok{=} \FunctionTok{regsubsets}\NormalTok{(}\AttributeTok{x =} \FunctionTok{as.matrix}\NormalTok{(diab[, }\SpecialCharTok{{-}}\DecValTok{11}\NormalTok{]), }\AttributeTok{y =}\NormalTok{ diab[, }\DecValTok{11}\NormalTok{])}
    \FunctionTok{summary}\NormalTok{(RSSleaps, }\AttributeTok{matrix=}\NormalTok{T)}
\DocumentationTok{\#\# Subset selection object}
\DocumentationTok{\#\# 10 Variables  (and intercept)}
\DocumentationTok{\#\#     Forced in Forced out}
\DocumentationTok{\#\# age     FALSE      FALSE}
\DocumentationTok{\#\# sex     FALSE      FALSE}
\DocumentationTok{\#\# bmi     FALSE      FALSE}
\DocumentationTok{\#\# map     FALSE      FALSE}
\DocumentationTok{\#\# tc      FALSE      FALSE}
\DocumentationTok{\#\# ldl     FALSE      FALSE}
\DocumentationTok{\#\# hdl     FALSE      FALSE}
\DocumentationTok{\#\# tch     FALSE      FALSE}
\DocumentationTok{\#\# ltg     FALSE      FALSE}
\DocumentationTok{\#\# glu     FALSE      FALSE}
\DocumentationTok{\#\# 1 subsets of each size up to 8}
\DocumentationTok{\#\# Selection Algorithm: exhaustive}
\DocumentationTok{\#\#          age sex bmi map tc  ldl hdl tch ltg glu}
\DocumentationTok{\#\# 1  ( 1 ) " " " " "*" " " " " " " " " " " " " " "}
\DocumentationTok{\#\# 2  ( 1 ) " " " " "*" " " " " " " " " " " "*" " "}
\DocumentationTok{\#\# 3  ( 1 ) " " " " "*" "*" " " " " " " " " "*" " "}
\DocumentationTok{\#\# 4  ( 1 ) " " " " "*" "*" "*" " " " " " " "*" " "}
\DocumentationTok{\#\# 5  ( 1 ) " " "*" "*" "*" " " " " "*" " " "*" " "}
\DocumentationTok{\#\# 6  ( 1 ) " " "*" "*" "*" "*" "*" " " " " "*" " "}
\DocumentationTok{\#\# 7  ( 1 ) " " "*" "*" "*" "*" "*" " " "*" "*" " "}
\DocumentationTok{\#\# 8  ( 1 ) " " "*" "*" "*" "*" "*" " " "*" "*" "*"}
\end{Highlighting}
\end{Shaded}

The results is summarized in a matrix, with each row representing a model size. The \texttt{"*"} sign indicates that the variable is include in the model for the corresponding size. Hence, there should be only one of such in the first row, two in the second row, etc.

By default, the algorithm would only consider models up to size 8. This is controlled by the argument \texttt{nvmax}. If we want to consider larger model sizes, then set this to a larger number. However, be careful that this many drastically increase the computational cost.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# Consider maximum of 10 variables}
\NormalTok{    RSSleaps }\OtherTok{=} \FunctionTok{regsubsets}\NormalTok{(}\AttributeTok{x =} \FunctionTok{as.matrix}\NormalTok{(diab[, }\SpecialCharTok{{-}}\DecValTok{11}\NormalTok{]), }\AttributeTok{y =}\NormalTok{ diab[, }\DecValTok{11}\NormalTok{], }\AttributeTok{nvmax =} \DecValTok{10}\NormalTok{)}
    \FunctionTok{summary}\NormalTok{(RSSleaps,}\AttributeTok{matrix=}\NormalTok{T)}
\DocumentationTok{\#\# Subset selection object}
\DocumentationTok{\#\# 10 Variables  (and intercept)}
\DocumentationTok{\#\#     Forced in Forced out}
\DocumentationTok{\#\# age     FALSE      FALSE}
\DocumentationTok{\#\# sex     FALSE      FALSE}
\DocumentationTok{\#\# bmi     FALSE      FALSE}
\DocumentationTok{\#\# map     FALSE      FALSE}
\DocumentationTok{\#\# tc      FALSE      FALSE}
\DocumentationTok{\#\# ldl     FALSE      FALSE}
\DocumentationTok{\#\# hdl     FALSE      FALSE}
\DocumentationTok{\#\# tch     FALSE      FALSE}
\DocumentationTok{\#\# ltg     FALSE      FALSE}
\DocumentationTok{\#\# glu     FALSE      FALSE}
\DocumentationTok{\#\# 1 subsets of each size up to 10}
\DocumentationTok{\#\# Selection Algorithm: exhaustive}
\DocumentationTok{\#\#           age sex bmi map tc  ldl hdl tch ltg glu}
\DocumentationTok{\#\# 1  ( 1 )  " " " " "*" " " " " " " " " " " " " " "}
\DocumentationTok{\#\# 2  ( 1 )  " " " " "*" " " " " " " " " " " "*" " "}
\DocumentationTok{\#\# 3  ( 1 )  " " " " "*" "*" " " " " " " " " "*" " "}
\DocumentationTok{\#\# 4  ( 1 )  " " " " "*" "*" "*" " " " " " " "*" " "}
\DocumentationTok{\#\# 5  ( 1 )  " " "*" "*" "*" " " " " "*" " " "*" " "}
\DocumentationTok{\#\# 6  ( 1 )  " " "*" "*" "*" "*" "*" " " " " "*" " "}
\DocumentationTok{\#\# 7  ( 1 )  " " "*" "*" "*" "*" "*" " " "*" "*" " "}
\DocumentationTok{\#\# 8  ( 1 )  " " "*" "*" "*" "*" "*" " " "*" "*" "*"}
\DocumentationTok{\#\# 9  ( 1 )  " " "*" "*" "*" "*" "*" "*" "*" "*" "*"}
\DocumentationTok{\#\# 10  ( 1 ) "*" "*" "*" "*" "*" "*" "*" "*" "*" "*"}
    
    \CommentTok{\# Obtain the matrix that indicates the variables}
\NormalTok{    sumleaps }\OtherTok{=} \FunctionTok{summary}\NormalTok{(RSSleaps, }\AttributeTok{matrix =}\NormalTok{ T)}
    
    \CommentTok{\# This object includes the RSS results, which is needed to calculate the scores}
\NormalTok{    sumleaps}\SpecialCharTok{$}\NormalTok{rss}
\DocumentationTok{\#\#  [1] 1719582 1416694 1362708 1331430 1287879 1271491 1267805 1264712 1264066 1263983}
    
    \CommentTok{\# This matrix indicates whether a variable is in the best model(s)}
\NormalTok{    sumleaps}\SpecialCharTok{$}\NormalTok{which}
\DocumentationTok{\#\#    (Intercept)   age   sex  bmi   map    tc   ldl   hdl   tch   ltg   glu}
\DocumentationTok{\#\# 1         TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE}
\DocumentationTok{\#\# 2         TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE}
\DocumentationTok{\#\# 3         TRUE FALSE FALSE TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE}
\DocumentationTok{\#\# 4         TRUE FALSE FALSE TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE}
\DocumentationTok{\#\# 5         TRUE FALSE  TRUE TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE}
\DocumentationTok{\#\# 6         TRUE FALSE  TRUE TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE}
\DocumentationTok{\#\# 7         TRUE FALSE  TRUE TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE}
\DocumentationTok{\#\# 8         TRUE FALSE  TRUE TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE}
\DocumentationTok{\#\# 9         TRUE FALSE  TRUE TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE}
\DocumentationTok{\#\# 10        TRUE  TRUE  TRUE TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE}
    
    \CommentTok{\# The package automatically produces the Cp statistic}
\NormalTok{    sumleaps}\SpecialCharTok{$}\NormalTok{cp}
\DocumentationTok{\#\#  [1] 148.352561  47.072229  30.663634  21.998461   9.148045   5.560162   6.303221   7.248522}
\DocumentationTok{\#\#  [9]   9.028080  11.000000}
\end{Highlighting}
\end{Shaded}

We can calculate different model selection criteria with the best models of each size. The model fitting result already produces the \(C_p\) and BIC results. However, please note that both quantities are modified slightly. For the \(C_p\) statistics, the quantity is divided by the estimated error variance, and also adjust for the sample size. For the BIC, the difference is a constant regardless of the model size. Hence these difference do will not affect the model selection result because the modification is the same regardless of the number of variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    modelsize}\OtherTok{=}\FunctionTok{apply}\NormalTok{(sumleaps}\SpecialCharTok{$}\NormalTok{which,}\DecValTok{1}\NormalTok{,sum)}
    
\NormalTok{    Cp }\OtherTok{=}\NormalTok{ sumleaps}\SpecialCharTok{$}\NormalTok{rss}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{summary}\NormalTok{(lm.fit)}\SpecialCharTok{$}\NormalTok{sigma}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{modelsize }\SpecialCharTok{{-}}\NormalTok{ n;}
\NormalTok{    AIC }\OtherTok{=}\NormalTok{ n}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(sumleaps}\SpecialCharTok{$}\NormalTok{rss}\SpecialCharTok{/}\NormalTok{n) }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{modelsize;}
\NormalTok{    BIC }\OtherTok{=}\NormalTok{ n}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(sumleaps}\SpecialCharTok{$}\NormalTok{rss}\SpecialCharTok{/}\NormalTok{n) }\SpecialCharTok{+}\NormalTok{ modelsize}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(n);}
    
    \CommentTok{\# Comparing the Cp scores }
    \FunctionTok{cbind}\NormalTok{(}\StringTok{"Our Cp"} \OtherTok{=}\NormalTok{ Cp, }\StringTok{"leaps Cp"} \OtherTok{=}\NormalTok{ sumleaps}\SpecialCharTok{$}\NormalTok{cp) }
\DocumentationTok{\#\#        Our Cp   leaps Cp}
\DocumentationTok{\#\# 1  148.352561 148.352561}
\DocumentationTok{\#\# 2   47.072229  47.072229}
\DocumentationTok{\#\# 3   30.663634  30.663634}
\DocumentationTok{\#\# 4   21.998461  21.998461}
\DocumentationTok{\#\# 5    9.148045   9.148045}
\DocumentationTok{\#\# 6    5.560162   5.560162}
\DocumentationTok{\#\# 7    6.303221   6.303221}
\DocumentationTok{\#\# 8    7.248522   7.248522}
\DocumentationTok{\#\# 9    9.028080   9.028080}
\DocumentationTok{\#\# 10  11.000000  11.000000}
    
    \CommentTok{\# Comparing the BIC results. The difference is a constant, }
    \CommentTok{\# which is the score of an intercept model}
    \FunctionTok{cbind}\NormalTok{(}\StringTok{"Our BIC"} \OtherTok{=}\NormalTok{ BIC, }\StringTok{"leaps BIC"} \OtherTok{=}\NormalTok{ sumleaps}\SpecialCharTok{$}\NormalTok{bic, }
          \StringTok{"Difference"} \OtherTok{=}\NormalTok{ BIC}\SpecialCharTok{{-}}\NormalTok{sumleaps}\SpecialCharTok{$}\NormalTok{bic, }
          \StringTok{"Intercept Score"} \OtherTok{=}\NormalTok{ n}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(}\FunctionTok{sum}\NormalTok{((diab[,}\DecValTok{11}\NormalTok{] }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(diab[,}\DecValTok{11}\NormalTok{]))}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{n)))}
\DocumentationTok{\#\#     Our BIC leaps BIC Difference Intercept Score}
\DocumentationTok{\#\# 1  3665.879 {-}174.1108    3839.99         3839.99}
\DocumentationTok{\#\# 2  3586.331 {-}253.6592    3839.99         3839.99}
\DocumentationTok{\#\# 3  3575.249 {-}264.7407    3839.99         3839.99}
\DocumentationTok{\#\# 4  3571.077 {-}268.9126    3839.99         3839.99}
\DocumentationTok{\#\# 5  3562.469 {-}277.5210    3839.99         3839.99}
\DocumentationTok{\#\# 6  3562.900 {-}277.0899    3839.99         3839.99}
\DocumentationTok{\#\# 7  3567.708 {-}272.2819    3839.99         3839.99}
\DocumentationTok{\#\# 8  3572.720 {-}267.2702    3839.99         3839.99}
\DocumentationTok{\#\# 9  3578.585 {-}261.4049    3839.99         3839.99}
\DocumentationTok{\#\# 10 3584.648 {-}255.3424    3839.99         3839.99}
\end{Highlighting}
\end{Shaded}

Finally, we may select the best model, using any of the criteria. The following code would produced a plot to visualize it. We can see that BIC selects 6 variables, while both AIC and \(C_p\) selects 7.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# Rescale Cp, AIC and BIC to (0,1).}
\NormalTok{    inrange }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{ (x }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(x)) }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{max}\NormalTok{(x) }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(x)) \}}
    
\NormalTok{    Cp }\OtherTok{=} \FunctionTok{inrange}\NormalTok{(Cp)}
\NormalTok{    BIC }\OtherTok{=} \FunctionTok{inrange}\NormalTok{(BIC)}
\NormalTok{    AIC }\OtherTok{=} \FunctionTok{inrange}\NormalTok{(AIC)}

    \FunctionTok{plot}\NormalTok{(}\FunctionTok{range}\NormalTok{(modelsize), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.4}\NormalTok{), }\AttributeTok{type=}\StringTok{"n"}\NormalTok{, }
         \AttributeTok{xlab=}\StringTok{"Model Size (with Intercept)"}\NormalTok{, }
         \AttributeTok{ylab=}\StringTok{"Model Selection Criteria"}\NormalTok{, }\AttributeTok{cex.lab =} \FloatTok{1.5}\NormalTok{)}

    \FunctionTok{points}\NormalTok{(modelsize, Cp, }\AttributeTok{col =} \StringTok{"green4"}\NormalTok{, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
    \FunctionTok{points}\NormalTok{(modelsize, AIC, }\AttributeTok{col =} \StringTok{"orange"}\NormalTok{, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
    \FunctionTok{points}\NormalTok{(modelsize, BIC, }\AttributeTok{col =} \StringTok{"purple"}\NormalTok{, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
    \FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\AttributeTok{legend=}\FunctionTok{c}\NormalTok{(}\StringTok{"Cp"}\NormalTok{, }\StringTok{"AIC"}\NormalTok{, }\StringTok{"BIC"}\NormalTok{),}
           \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"green4"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"purple"}\NormalTok{), }
           \AttributeTok{lty =} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-82-1} \end{center}

\hypertarget{step-wise-regression-using-step}{%
\subsection{\texorpdfstring{Step-wise regression using \texttt{step()}}{Step-wise regression using step()}}\label{step-wise-regression-using-step}}

The idea of step-wise regression is very simple: we start with a certain model (e.g.~the intercept or the full mode), and add or subtract one variable at a time by making the best decision to improve the model selection score. The \texttt{step()} function implements this procedure. The following example starts with the full model and uses AIC as the selection criteria (default of the function). After removing several variables, the model ends up with six predictors.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# k = 2 (AIC) is default; }
    \FunctionTok{step}\NormalTok{(lm.fit, }\AttributeTok{direction=}\StringTok{"both"}\NormalTok{, }\AttributeTok{k =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\# Start:  AIC=3539.64}
\DocumentationTok{\#\# Y \textasciitilde{} age + sex + bmi + map + tc + ldl + hdl + tch + ltg + glu}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#        Df Sum of Sq     RSS    AIC}
\DocumentationTok{\#\# {-} age   1        82 1264066 3537.7}
\DocumentationTok{\#\# {-} hdl   1       663 1264646 3537.9}
\DocumentationTok{\#\# {-} glu   1      3080 1267064 3538.7}
\DocumentationTok{\#\# {-} tch   1      3526 1267509 3538.9}
\DocumentationTok{\#\# \textless{}none\textgreater{}              1263983 3539.6}
\DocumentationTok{\#\# {-} ldl   1      5799 1269782 3539.7}
\DocumentationTok{\#\# {-} tc    1     10600 1274583 3541.3}
\DocumentationTok{\#\# {-} sex   1     45000 1308983 3553.1}
\DocumentationTok{\#\# {-} ltg   1     56015 1319998 3556.8}
\DocumentationTok{\#\# {-} map   1     72103 1336086 3562.2}
\DocumentationTok{\#\# {-} bmi   1    179028 1443011 3596.2}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Step:  AIC=3537.67}
\DocumentationTok{\#\# Y \textasciitilde{} sex + bmi + map + tc + ldl + hdl + tch + ltg + glu}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#        Df Sum of Sq     RSS    AIC}
\DocumentationTok{\#\# {-} hdl   1       646 1264712 3535.9}
\DocumentationTok{\#\# {-} glu   1      3001 1267067 3536.7}
\DocumentationTok{\#\# {-} tch   1      3543 1267608 3536.9}
\DocumentationTok{\#\# \textless{}none\textgreater{}              1264066 3537.7}
\DocumentationTok{\#\# {-} ldl   1      5751 1269817 3537.7}
\DocumentationTok{\#\# {-} tc    1     10569 1274635 3539.4}
\DocumentationTok{\#\# + age   1        82 1263983 3539.6}
\DocumentationTok{\#\# {-} sex   1     45831 1309896 3551.4}
\DocumentationTok{\#\# {-} ltg   1     55963 1320029 3554.8}
\DocumentationTok{\#\# {-} map   1     73850 1337915 3560.8}
\DocumentationTok{\#\# {-} bmi   1    179079 1443144 3594.2}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Step:  AIC=3535.9}
\DocumentationTok{\#\# Y \textasciitilde{} sex + bmi + map + tc + ldl + tch + ltg + glu}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#        Df Sum of Sq     RSS    AIC}
\DocumentationTok{\#\# {-} glu   1      3093 1267805 3535.0}
\DocumentationTok{\#\# {-} tch   1      3247 1267959 3535.0}
\DocumentationTok{\#\# \textless{}none\textgreater{}              1264712 3535.9}
\DocumentationTok{\#\# {-} ldl   1      7505 1272217 3536.5}
\DocumentationTok{\#\# + hdl   1       646 1264066 3537.7}
\DocumentationTok{\#\# + age   1        66 1264646 3537.9}
\DocumentationTok{\#\# {-} tc    1     26840 1291552 3543.2}
\DocumentationTok{\#\# {-} sex   1     46382 1311094 3549.8}
\DocumentationTok{\#\# {-} map   1     73536 1338248 3558.9}
\DocumentationTok{\#\# {-} ltg   1     97509 1362221 3566.7}
\DocumentationTok{\#\# {-} bmi   1    178537 1443249 3592.3}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Step:  AIC=3534.98}
\DocumentationTok{\#\# Y \textasciitilde{} sex + bmi + map + tc + ldl + tch + ltg}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#        Df Sum of Sq     RSS    AIC}
\DocumentationTok{\#\# {-} tch   1      3686 1271491 3534.3}
\DocumentationTok{\#\# \textless{}none\textgreater{}              1267805 3535.0}
\DocumentationTok{\#\# {-} ldl   1      7472 1275277 3535.6}
\DocumentationTok{\#\# + glu   1      3093 1264712 3535.9}
\DocumentationTok{\#\# + hdl   1       738 1267067 3536.7}
\DocumentationTok{\#\# + age   1         0 1267805 3537.0}
\DocumentationTok{\#\# {-} tc    1     26378 1294183 3542.1}
\DocumentationTok{\#\# {-} sex   1     44686 1312491 3548.3}
\DocumentationTok{\#\# {-} map   1     82154 1349959 3560.7}
\DocumentationTok{\#\# {-} ltg   1    102520 1370325 3567.3}
\DocumentationTok{\#\# {-} bmi   1    189970 1457775 3594.7}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Step:  AIC=3534.26}
\DocumentationTok{\#\# Y \textasciitilde{} sex + bmi + map + tc + ldl + ltg}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#        Df Sum of Sq     RSS    AIC}
\DocumentationTok{\#\# \textless{}none\textgreater{}              1271491 3534.3}
\DocumentationTok{\#\# + tch   1      3686 1267805 3535.0}
\DocumentationTok{\#\# + glu   1      3532 1267959 3535.0}
\DocumentationTok{\#\# + hdl   1       395 1271097 3536.1}
\DocumentationTok{\#\# + age   1        11 1271480 3536.3}
\DocumentationTok{\#\# {-} ldl   1     39378 1310869 3545.7}
\DocumentationTok{\#\# {-} sex   1     41858 1313349 3546.6}
\DocumentationTok{\#\# {-} tc    1     65237 1336728 3554.4}
\DocumentationTok{\#\# {-} map   1     79627 1351119 3559.1}
\DocumentationTok{\#\# {-} bmi   1    190586 1462077 3594.0}
\DocumentationTok{\#\# {-} ltg   1    294094 1565585 3624.2}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Call:}
\DocumentationTok{\#\# lm(formula = Y \textasciitilde{} sex + bmi + map + tc + ldl + ltg, data = diab)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Coefficients:}
\DocumentationTok{\#\# (Intercept)          sex          bmi          map           tc          ldl          ltg  }
\DocumentationTok{\#\#       152.1       {-}226.5        529.9        327.2       {-}757.9        538.6        804.2}
\end{Highlighting}
\end{Shaded}

We can also use different settings, such as which model to start with, which is the minimum/maximum model, and do we allow to adding/subtracting.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# use BIC (k = log(n))instead of AIC}
    \CommentTok{\# trace = 0 will suppress the output of intermediate steps }
    \FunctionTok{step}\NormalTok{(lm.fit, }\AttributeTok{direction=}\StringTok{"both"}\NormalTok{, }\AttributeTok{k =} \FunctionTok{log}\NormalTok{(n), }\AttributeTok{trace=}\DecValTok{0}\NormalTok{)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Call:}
\DocumentationTok{\#\# lm(formula = Y \textasciitilde{} sex + bmi + map + tc + ldl + ltg, data = diab)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Coefficients:}
\DocumentationTok{\#\# (Intercept)          sex          bmi          map           tc          ldl          ltg  }
\DocumentationTok{\#\#       152.1       {-}226.5        529.9        327.2       {-}757.9        538.6        804.2}

    \CommentTok{\# Start with an intercept model, and use forward selection (adding only)}
    \FunctionTok{step}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\DecValTok{1}\NormalTok{, }\AttributeTok{data=}\NormalTok{diab), }\AttributeTok{scope=}\FunctionTok{list}\NormalTok{(}\AttributeTok{upper=}\NormalTok{lm.fit, }\AttributeTok{lower=}\SpecialCharTok{\textasciitilde{}}\DecValTok{1}\NormalTok{), }
         \AttributeTok{direction=}\StringTok{"forward"}\NormalTok{, }\AttributeTok{trace=}\DecValTok{0}\NormalTok{)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Call:}
\DocumentationTok{\#\# lm(formula = Y \textasciitilde{} bmi + ltg + map + tc + sex + ldl, data = diab)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Coefficients:}
\DocumentationTok{\#\# (Intercept)          bmi          ltg          map           tc          sex          ldl  }
\DocumentationTok{\#\#       152.1        529.9        804.2        327.2       {-}757.9       {-}226.5        538.6}
\end{Highlighting}
\end{Shaded}

We can see that these results are slightly different from the best subset selection. So which is better? Of course the best subset selection is better because it considers all possible candidates, which step-wise regression may stuck at a sub-optimal model, while adding and subtracting any variable do not benefit further. Hence, the results of step-wise regression may be unstable. On the other hand, best subset selection not really feasible for high-dimensional problems because of the computational cost.

\hypertarget{marrows-cp}{%
\section{\texorpdfstring{Derivation of Marrows' \(C_p\)}{Derivation of Marrows' C\_p}}\label{marrows-cp}}

Suppose we have a set of training data \({\cal D}_n = \{x_i, \color{DodgerBlue}{y_i}\}_{i=1}^n\) and a set of testing data, with the same covariates \({\cal D}_n^\ast = \{x_i, \color{OrangeRed}{y_i^\ast}\}_{i=1}^n\). Hence, this is an \textbf{in-sample prediction} problem. However, the \(\color{OrangeRed}{y_i^\ast}\)s are newly observed. Assuming that the data are generated from a linear model, i.e., in vector form,

\def\rby{\color{OrangeRed}{\by^\ast}}
\def\rbe{\color{OrangeRed}{\be^\ast}}
\def\rbbeta{\color{OrangeRed}{\widehat{\bbeta}}}

\def\bby{\color{DodgerBlue}{\by}}
\def\bbe{\color{DodgerBlue}{\be}}
\def\bbbeta{\color{DodgerBlue}{\widehat{\bbeta}}}

\[\color{DodgerBlue}{\mathbf{y}}= \boldsymbol \mu+ \color{DodgerBlue}{\mathbf{e}}= \mathbf{X}\boldsymbol \beta+ \color{DodgerBlue}{\mathbf{e}},\]
and
\[\color{OrangeRed}{\mathbf{y}^\ast}= \boldsymbol \mu+ \color{OrangeRed}{\mathbf{e}^\ast}= \mathbf{X}\boldsymbol \beta+ \color{OrangeRed}{\mathbf{e}^\ast},\]
where the error terms are i.i.d with mean 0 and variance \(\sigma^2\). We want to know what is the best model that predicts \(\color{OrangeRed}{\mathbf{y}^\ast}\). Let's look at the testing error first:

\begin{align}
\text{E}[\color{OrangeRed}{\text{Testing Error}}] =& ~\text{E}\lVert \color{OrangeRed}{\mathbf{y}^\ast}- \mathbf{X}\color{DodgerBlue}{\widehat{\boldsymbol \beta}}\rVert^2 \\
=& ~\text{E}\lVert (\color{OrangeRed}{\mathbf{y}^\ast}- \mathbf{X}\boldsymbol \beta) + (\mathbf{X}\boldsymbol \beta- \mathbf{X}\color{DodgerBlue}{\widehat{\boldsymbol \beta}}) \rVert^2 \\
=& ~\text{E}\lVert \color{OrangeRed}{\mathbf{e}^\ast}\rVert^2 + \text{E}\lVert \mathbf{X}(\color{DodgerBlue}{\widehat{\boldsymbol \beta}}- \boldsymbol \beta) \rVert^2 \\
=& ~\color{OrangeRed}{n \sigma^2} + \text{E}\big[ \text{Trace}\big( (\color{DodgerBlue}{\widehat{\boldsymbol \beta}}- \boldsymbol \beta)^\text{T}\mathbf{X}^\text{T}\mathbf{X}(\color{DodgerBlue}{\widehat{\boldsymbol \beta}}- \boldsymbol \beta) \big) \big] \\
=& ~\color{OrangeRed}{n \sigma^2} + \text{Trace}\big(\mathbf{X}^\text{T}\mathbf{X}\text{Cov}(\color{DodgerBlue}{\widehat{\boldsymbol \beta}})\big) \\
=& ~\color{OrangeRed}{n \sigma^2} + \color{DodgerBlue}{p \sigma^2}.
\end{align}

In the above, we used properties

\begin{itemize}
\tightlist
\item
  \(\text{Trace}(ABC) = \text{Trace}(CBA)\)
\item
  \(\text{E}[\text{Trace}(A)] = \text{Trace}(\text{E}[A])\)
\end{itemize}

On the other hand, the training error is

\begin{align}
\text{E}[\color{DodgerBlue}{\text{Training Error}}] =& ~\text{E}\lVert \mathbf{y}- \color{DodgerBlue}{\mathbf{y}}\rVert^2 \\
=& ~\text{E}\lVert (\mathbf{I}- \mathbf{H})(\mathbf{X}\boldsymbol \beta+ \color{DodgerBlue}{\mathbf{e}}) \rVert^2 \\
=& ~\text{E}\lVert (\mathbf{I}- \mathbf{H})\color{DodgerBlue}{\mathbf{e}}\rVert^2 \\
=& ~\text{E}[\text{Trace}(\color{DodgerBlue}{\mathbf{e}}^\text{T}(\mathbf{I}- \mathbf{H})^\text{T}(\mathbf{I}- \mathbf{H}) \color{DodgerBlue}{\mathbf{e}})]\\
=& ~\text{Trace}((\mathbf{I}- \mathbf{H})^\text{T}(\mathbf{I}- \mathbf{H}) \text{Cov}(\color{DodgerBlue}{\mathbf{e}})]\\
=& ~\color{DodgerBlue}{(n - p) \sigma^2}.
\end{align}

In the above, we further used properties

\begin{itemize}
\tightlist
\item
  \(\mathbf{H}\) and \(\mathbf{I}- \mathbf{H}\) are projection matrices
\item
  \(\mathbf{H}\mathbf{X}= \mathbf{X}\)
\end{itemize}

If we contrast the two results above, the difference between the training and testing errors is \(2 p \sigma^2\). Hence, if we can obtain a valid estimation of \(\sigma^2\), then the training error plus \(2 p \widehat{\sigma}^2\) is a good approximation of the testing error, which we want to minimize. And that is exactly what Marrows' \(C_p\) does.

We can also generalize this result to the case when the underlying model is not a linear model. Assume that

\[\color{DodgerBlue}{\mathbf{y}}= f(\mathbf{X}) + \color{DodgerBlue}{\mathbf{e}}= \boldsymbol \mu+ \color{DodgerBlue}{\mathbf{e}},\]
and
\[\color{OrangeRed}{\mathbf{y}^\ast}= f(\mathbf{X}) + \color{OrangeRed}{\mathbf{e}^\ast}= \boldsymbol \mu+ \color{OrangeRed}{\mathbf{e}^\ast}.\]
In this case, a linear model would not estimate \(\boldsymbol \mu\). Instead, it is only capable to produce the best linear approximation of \(\boldsymbol \mu\) using the columns in \(\mathbf{X}\), which is \(\mathbf{H}\boldsymbol \mu\), the projection of \(\boldsymbol \mu\) on the column space of \(\mathbf{X}\). In general, \(\mathbf{H}\boldsymbol \mu\neq \boldsymbol \mu\), and the remaining part \(\boldsymbol \mu- \mathbf{H}\boldsymbol \mu\) is called \textbf{bias}. This is a new concept that will appear frequently in this book. Selection variables will essentially trade between bias and variance of a model. The following derivation shows this phenomenon:

\begin{align}
\text{E}[\color{OrangeRed}{\text{Testing Error}}] =& ~\text{E}\lVert \color{OrangeRed}{\mathbf{y}^\ast}- \mathbf{X}\color{DodgerBlue}{\widehat{\boldsymbol \beta}}\rVert^2 \\
=& ~\text{E}\lVert \color{OrangeRed}{\mathbf{y}^\ast}- \mathbf{H}\color{DodgerBlue}{\mathbf{y}}\rVert^2 \\
=& ~\text{E}\lVert (\color{OrangeRed}{\mathbf{y}^\ast}- \boldsymbol \mu) + (\boldsymbol \mu- \mathbf{H}\boldsymbol \mu) + (\mathbf{H}\boldsymbol \mu- \mathbf{H}\color{DodgerBlue}{\mathbf{y}}) \rVert^2 \\
=& ~\text{E}\lVert \color{OrangeRed}{\mathbf{y}^\ast}- \boldsymbol \mu\rVert^2 + \text{E}\lVert \boldsymbol \mu- \mathbf{H}\boldsymbol \mu\rVert^2 + \text{E}\lVert \mathbf{H}\boldsymbol \mu- \mathbf{H}\color{DodgerBlue}{\mathbf{y}}\rVert^2 \\
=& ~\text{E}\lVert \color{OrangeRed}{\mathbf{e}^\ast}\rVert^2 + \text{E}\lVert \boldsymbol \mu- \mathbf{H}\boldsymbol \mu\rVert^2 + \text{E}\lVert \mathbf{H}\color{DodgerBlue}{\mathbf{e}}\rVert^2 \\
=& ~\color{OrangeRed}{n \sigma^2} + \text{Bias}^2 + \color{DodgerBlue}{p \sigma^2},
\end{align}

while the training error is

\begin{align}
\text{E}[\color{DodgerBlue}{\text{Training Error}}] =& ~\text{E}\lVert \color{DodgerBlue}{\mathbf{y}}- \mathbf{X}\color{DodgerBlue}{\widehat{\boldsymbol \beta}}\rVert^2 \\
=& ~\text{E}\lVert \color{DodgerBlue}{\mathbf{y}}- \mathbf{H}\color{DodgerBlue}{\mathbf{y}}\rVert^2 \\
=& ~\text{E}\lVert (\mathbf{I}- \mathbf{H})(\boldsymbol \mu+ \color{DodgerBlue}{\mathbf{e}}) \rVert^2 \\
=& ~\text{E}\lVert (\mathbf{I}- \mathbf{H})\boldsymbol \mu\rVert^2 + \text{E}\lVert (\mathbf{I}- \mathbf{H})\color{DodgerBlue}{\mathbf{e}}\rVert^2\\
=& ~\text{Bias}^2 + \color{DodgerBlue}{(n - p) \sigma^2}.
\end{align}

We can notice again that the difference is \(2p\sigma^2\). Note that this is regardless of whether the linear model is correct or not.

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{ridge-regression}{%
\chapter{Ridge Regression}\label{ridge-regression}}

Ridge regression was proposed by Hoerl and Kennard (\protect\hyperlink{ref-hoerl1970ridge}{1970}), but is also a special case of Tikhonov regularization. The essential idea is very simple: Knowing that the ordinary least squares (OLS) solution is not unique in an ill-posed problem, i.e., \(\mathbf{X}^\text{T}\mathbf{X}\) is not invertible, a ridge regression adds a ridge (diagonal matrix) on \(\mathbf{X}^\text{T}\mathbf{X}\):

\[\widehat{\boldsymbol \beta}^\text{ridge} = (\mathbf{X}^\text{T}\mathbf{X}+ n \lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y},\]
It provides a solution of linear regression when multicollinearity happens, especially when the number of variables is larger than the sample size. Alternatively, this is also the solution of a regularized least square estimator. We add an \(\ell_2\) penalty to the residual sum of squares, i.e.,

\[
\begin{align}
\widehat{\boldsymbol \beta}^\text{ridge} =& \arg\min_{\boldsymbol \beta} (\mathbf{y}- \mathbf{X}\boldsymbol \beta)^\text{T}(\mathbf{y}- \mathbf{X}\boldsymbol \beta) + n \lambda \lVert\boldsymbol \beta\rVert_2^2\\
=& \arg\min_{\boldsymbol \beta} \frac{1}{n} \sum_{i=1}^n (y_i - x_i^\text{T}\boldsymbol \beta)^2 + \lambda \sum_{j=1}^p \beta_j^2,
\end{align}
\]

for some penalty \(\lambda > 0\). Another approach that leads to the ridge regression is a constraint on the \(\ell_2\) norm of the parameters, which will be introduced in the next Chapter. Ridge regression is used extensively in genetic analyses to address ``small-\(n\)-large-\(p\)'' problems. We will start with a motivation example and then discuss the bias-variance trade-off issue.

\hypertarget{motivation-correlated-variables-and-convexity}{%
\section{Motivation: Correlated Variables and Convexity}\label{motivation-correlated-variables-and-convexity}}

Ridge regression has many advantages. Most notably, it can address highly correlated variables. From an optimization point of view, having highly correlated variables means that the objective function (\(\ell_2\) loss) becomes ``flat'' along certain directions in the parameter domain. This can be seen from the following example, where the true parameters are both \(1\) while the estimated parameters concludes almost all effects to the first variable. You can change different seed to observe the variability of these parameter estimates and notice that they are quite large. Instead, if we fit a ridge regression, the parameter estimates are relatively stable.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(MASS)}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{  n }\OtherTok{=} \DecValTok{30}
  
  \CommentTok{\# create highly correlated variables and a linear model}
\NormalTok{  X }\OtherTok{=} \FunctionTok{mvrnorm}\NormalTok{(n, }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FloatTok{0.99}\NormalTok{, }\FloatTok{0.99}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{  y }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ X[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ X[,}\DecValTok{2}\NormalTok{])}
  
  \CommentTok{\# compare parameter estimates}
  \FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X}\DecValTok{{-}1}\NormalTok{))}\SpecialCharTok{$}\NormalTok{coef}
\DocumentationTok{\#\#     Estimate Std. Error    t value  Pr(\textgreater{}|t|)}
\DocumentationTok{\#\# X1 1.8461255   1.294541 1.42608527 0.1648987}
\DocumentationTok{\#\# X2 0.0990278   1.321283 0.07494822 0.9407888}
  
  \CommentTok{\# note that the true parameters are all 1\textquotesingle{}s}
  \CommentTok{\# Be careful that the \textasciigrave{}lambda\textasciigrave{} parameter in lm.ridge is our (n*lambda)}
  \FunctionTok{lm.ridge}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X}\DecValTok{{-}1}\NormalTok{, }\AttributeTok{lambda=}\DecValTok{5}\NormalTok{)}
\DocumentationTok{\#\#        X1        X2 }
\DocumentationTok{\#\# 0.9413221 0.8693253}
\end{Highlighting}
\end{Shaded}

The variance of both \(\beta_1\) and \(\beta_2\) are quite large. This is expected because we know from linear regression that the variance of \(\widehat{\boldsymbol \beta}\) is \(\sigma^2 (\mathbf{X}^\text{T}\mathbf{X})^{-1}\). However, since the columns of \(\mathbf{X}\) are highly correlated, the smallest eigenvalue of \(\mathbf{X}^\text{T}\mathbf{X}\) is close to 0, making the largest eigenvalue of \((\mathbf{X}^\text{T}\mathbf{X})^{-1}\) very large. This can also be interpreted through an optimization point of view. The objective function for an OLS estimator is demonstrated in the following.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  beta1 }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.005}\NormalTok{)}
\NormalTok{  beta2 }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.005}\NormalTok{)}
\NormalTok{  allbeta }\OtherTok{\textless{}{-}} \FunctionTok{data.matrix}\NormalTok{(}\FunctionTok{expand.grid}\NormalTok{(beta1, beta2))}
\NormalTok{  rss }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{apply}\NormalTok{(allbeta, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(b, X, y) }\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ b)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), X, y), }
                \FunctionTok{length}\NormalTok{(beta1), }\FunctionTok{length}\NormalTok{(beta2))}
  
  \CommentTok{\# quantile levels for drawing contour}
\NormalTok{  quanlvl }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.025}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{)}
  
  \CommentTok{\# plot the contour}
  \FunctionTok{contour}\NormalTok{(beta1, beta2, rss, }\AttributeTok{levels =} \FunctionTok{quantile}\NormalTok{(rss, quanlvl))}
  \FunctionTok{box}\NormalTok{()}
  
  \CommentTok{\# the truth}
  \FunctionTok{points}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
  
  \CommentTok{\# the data }
\NormalTok{  betahat }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X}\DecValTok{{-}1}\NormalTok{))}
  \FunctionTok{points}\NormalTok{(betahat[}\DecValTok{1}\NormalTok{], betahat[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-88-1} \end{center}

Over many simulation runs, the solution lies around the line of \(\beta_1 + \beta_2 = 2\).

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# the truth}
  \FunctionTok{plot}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{))}
  \FunctionTok{points}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
  
  \CommentTok{\# generate many datasets in a simulation }
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{200}\NormalTok{)}
\NormalTok{  \{}
\NormalTok{    X }\OtherTok{=} \FunctionTok{mvrnorm}\NormalTok{(n, }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FloatTok{0.99}\NormalTok{, }\FloatTok{0.99}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{    y }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ X[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ X[,}\DecValTok{2}\NormalTok{])}
    
\NormalTok{    betahat }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ X) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ y}
    \FunctionTok{points}\NormalTok{(betahat[}\DecValTok{1}\NormalTok{], betahat[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-89-1} \end{center}

\hypertarget{ridge-penalty-and-the-reduced-variation}{%
\section{Ridge Penalty and the Reduced Variation}\label{ridge-penalty-and-the-reduced-variation}}

If we add a ridge regression penalty, the contour is forced to be more convex due to the added eigenvalues on \(\mathbf{X}^\mathbf{X}\), making the eignvalues of \((\mathbf{X}^\mathbf{X})^{-1}\) smaller. Here is a plot of the Ridge \(\ell_2\) penalty.

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-90-1} \end{center}

Hence, by adding this to the OLS objective function, the solution is more stable. This may be interpreted in several different ways such as: 1) the objective function is more convex; 2) the variance of the estimator is smaller. However, this causes some bias too. Choosing the tuning parameter is a balance of the bias-variance trade-off, which will be discussed in the following.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

    \CommentTok{\# adding a L2 penalty to the objective function}
\NormalTok{    rss }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{apply}\NormalTok{(allbeta, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(b, X, y) }\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ b)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ b }\SpecialCharTok{\%*\%}\NormalTok{ b, X, y),}
                  \FunctionTok{length}\NormalTok{(beta1), }\FunctionTok{length}\NormalTok{(beta2))}
    
    \CommentTok{\# the ridge solution}
\NormalTok{    bh }\OtherTok{=} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ X }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ y}
    
    \FunctionTok{contour}\NormalTok{(beta1, beta2, rss, }\AttributeTok{levels =} \FunctionTok{quantile}\NormalTok{(rss, quanlvl))}
    \FunctionTok{points}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{points}\NormalTok{(bh[}\DecValTok{1}\NormalTok{], bh[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{box}\NormalTok{()}
    
    \CommentTok{\# adding a larger penalty}
\NormalTok{    rss }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{apply}\NormalTok{(allbeta, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(b, X, y) }\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ b)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \DecValTok{10}\SpecialCharTok{*}\NormalTok{b }\SpecialCharTok{\%*\%}\NormalTok{ b, X, y),}
                  \FunctionTok{length}\NormalTok{(beta1), }\FunctionTok{length}\NormalTok{(beta2))}
    
\NormalTok{    bh }\OtherTok{=} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ X }\SpecialCharTok{+} \DecValTok{10}\SpecialCharTok{*}\FunctionTok{diag}\NormalTok{(}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ y}
    
    \CommentTok{\# the ridge solution}
    \FunctionTok{contour}\NormalTok{(beta1, beta2, rss, }\AttributeTok{levels =} \FunctionTok{quantile}\NormalTok{(rss, quanlvl))}
    \FunctionTok{points}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{points}\NormalTok{(bh[}\DecValTok{1}\NormalTok{], bh[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-91-1} \end{center}

We can check the ridge solution over many simulation runs

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

  \CommentTok{\# the truth}
  \FunctionTok{plot}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{))}
  \FunctionTok{points}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
  
  \CommentTok{\# generate many datasets in a simulation }
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{200}\NormalTok{)}
\NormalTok{  \{}
\NormalTok{    X }\OtherTok{=} \FunctionTok{mvrnorm}\NormalTok{(n, }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FloatTok{0.99}\NormalTok{, }\FloatTok{0.99}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{    y }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ X[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ X[,}\DecValTok{2}\NormalTok{])}
    
    \CommentTok{\# betahat \textless{}{-} solve(t(X) \%*\% X + 2*diag(2)) \%*\% t(X) \%*\% y}
\NormalTok{    betahat }\OtherTok{\textless{}{-}} \FunctionTok{lm.ridge}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{2}\NormalTok{)}\SpecialCharTok{$}\NormalTok{coef}
    \FunctionTok{points}\NormalTok{(betahat[}\DecValTok{1}\NormalTok{], betahat[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-92-1} \end{center}

This effect is gradually changing as we increase the penalty level. The following simulation shows how the variation of \(\boldsymbol \beta\) changes. We show this with two penalty values, and see how the estimated parameters are away from the truth.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

  \CommentTok{\# small penalty}
  \FunctionTok{plot}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{))}
  \FunctionTok{points}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
  
  \CommentTok{\# generate many datasets in a simulation }
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{200}\NormalTok{)}
\NormalTok{  \{}
\NormalTok{    X }\OtherTok{=} \FunctionTok{mvrnorm}\NormalTok{(n, }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FloatTok{0.99}\NormalTok{, }\FloatTok{0.99}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{    y }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ X[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ X[,}\DecValTok{2}\NormalTok{])}
    
\NormalTok{    betahat }\OtherTok{\textless{}{-}} \FunctionTok{lm.ridge}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{2}\NormalTok{)}\SpecialCharTok{$}\NormalTok{coef}
    \FunctionTok{points}\NormalTok{(betahat[}\DecValTok{1}\NormalTok{], betahat[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  \}}
  
  \CommentTok{\# large penalty}
  \FunctionTok{plot}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{))}
  \FunctionTok{points}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{) }
  
  \CommentTok{\# generate many datasets in a simulation }
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{200}\NormalTok{)}
\NormalTok{  \{}
\NormalTok{    X }\OtherTok{=} \FunctionTok{mvrnorm}\NormalTok{(n, }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FloatTok{0.99}\NormalTok{, }\FloatTok{0.99}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{    y }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ X[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ X[,}\DecValTok{2}\NormalTok{])}
    
    \CommentTok{\# betahat \textless{}{-} solve(t(X) \%*\% X + 30*diag(2)) \%*\% t(X) \%*\% y}
\NormalTok{    betahat }\OtherTok{\textless{}{-}} \FunctionTok{lm.ridge}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{30}\NormalTok{)}\SpecialCharTok{$}\NormalTok{coef}
    \FunctionTok{points}\NormalTok{(betahat[}\DecValTok{1}\NormalTok{], betahat[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{SMLR_files/figure-latex/unnamed-chunk-93-1} \end{center}

\hypertarget{bias-and-variance-of-ridge-regression}{%
\section{Bias and Variance of Ridge Regression}\label{bias-and-variance-of-ridge-regression}}

We can set a relationship between Ridge and OLS, assuming that the OLS estimator exist.

\begin{align}
\widehat{\boldsymbol \beta}^\text{ridge} =& (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y}\\
=& (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} (\mathbf{X}^\text{T}\mathbf{X}) \color{OrangeRed}{(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}\mathbf{y}}\\
=& (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} (\mathbf{X}^\text{T}\mathbf{X}) \color{OrangeRed}{\widehat{\boldsymbol \beta}^\text{ols}}
\end{align}

This leads to a biased estimator (since the OLS estimator is unbiased) if we use any nonzero \(\lambda\).

\begin{itemize}
\tightlist
\item
  As \(\lambda \rightarrow 0\), the ridge solution is eventually the same as OLS
\item
  As \(\lambda \rightarrow \infty\), \(\widehat{\boldsymbol \beta}^\text{ridge} \rightarrow 0\)
\end{itemize}

It can be easier to analyze a case with \(\mathbf{X}^\text{T}\mathbf{X}= n \mathbf{I}\), i.e, with standardized and orthogonal columns in \(\mathbf{X}\). Note that in this case, each \(\beta_j^{\text{ols}}\) is just the projection of \(\mathbf{y}\) onto \(\mathbf{x}_j\), the \(j\)th column of the design matrix. We also have

\begin{align}
\widehat{\boldsymbol \beta}^\text{ridge} =& (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} (\mathbf{X}^\text{T}\mathbf{X}) \widehat{\boldsymbol \beta}^\text{ols}\\
=& (\mathbf{I}+ \lambda \mathbf{I})^{-1}\widehat{\boldsymbol \beta}^\text{ols}\\
=& (1 + \lambda)^{-1} \widehat{\boldsymbol \beta}^\text{ols}\\

\Longrightarrow \beta_j^{\text{ridge}} =& \frac{1}{1 + \lambda} \beta_j^\text{ols}
\end{align}

Then in this case, the bias and variance of the ridge estimator can be explicitly expressed:

\begin{itemize}
\tightlist
\item
  \(\text{Bias}(\beta_j^{\text{ridge}}) = \frac{-\lambda}{1 + \lambda} \beta_j^\text{ols}\) (not zero)
\item
  \(\text{Var}(\beta_j^{\text{ridge}}) = \frac{1}{(1 + \lambda)^2} \text{Var}(\beta_j^\text{ols})\) (reduced from OLS)
\end{itemize}

Of course, we can ask the question: is it worth it? We could proceed with a simple analysis of the MSE of \(\beta\) (dropping \(j\)):

\begin{align}
\text{MSE}(\beta) &= \text{E}(\widehat{\beta} - \beta)^2 \\
&= \text{E}[\widehat{\beta} - \text{E}(\widehat{\beta})]^2 + \text{E}[\widehat{\beta} - \beta]^2 \\
&= \text{E}[\widehat{\beta} - \text{E}(\widehat{\beta})]^2 + 0 + [\text{E}(\widehat{\beta}) - \beta]^2 \\
&= \text{Var}(\widehat{\beta}) + \text{Bias}^2.
\end{align}

This bias-variance breakdown formula will appear multiple times. Now, plug-in the results developed earlier based on the orthogonal design matrix, and investigate the derivative of the MSE of the Ridge estimator, we have

\begin{align}
\frac{\partial \text{MSE}(\widehat{\beta}^\text{ridge})}{ \partial \lambda} =& \frac{\partial}{\partial \lambda} \left[ \frac{1}{(1+\lambda)^2} \text{Var}(\widehat{\beta}^\text{ols}) + \frac{\lambda^2}{(1 + \lambda)^2} \beta^2 \right] \\
=& \frac{2}{(1+\lambda)^3} \left[ \lambda \beta^2 - \text{Var}(\widehat{\beta}^\text{ols}) \right]
\end{align}

Note that when the derivative is negative, increasing \(\lambda\) would decrease the MSE. This implies that we can reduce the MSE by choosing a small \(\lambda\). Of course the situation is much more involving when the columns in \(\mathbf{X}\) are not orthogonal. However, the following analysis helps to understand a non-orthogonal case. It is essentially re-organizing the columns of \(\mathbf{X}\) into its principle components so that they are still orthogonal.

Let's first take a singular value decomposition (SVD) of \(\mathbf{X}\), with \(\mathbf{X}= \mathbf{U}\mathbf{D}\mathbf{V}^\text{T}\), then the columns in \(\mathbf{U}\) form an orthonormal basis and columns in \(\mathbf{U}\mathbf{D}\) are the \textbf{principal components} and \(\mathbf{V}\) defines the principle directions. In addition, we have \(n \widehat{\boldsymbol \Sigma} = \mathbf{X}^\text{T}\mathbf{X}= \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}\). Assuming that \(p < n\), and \(\mathbf{X}\) has full column ranks, then the Ridge estimator fitted \(\mathbf{y}\) value can be decomposed as

\begin{align}
\widehat{\mathbf{y}}^\text{ridge} =& \mathbf{X}\widehat{\beta}^\text{ridge} \\ 
=& \mathbf{X}(\mathbf{X}^\text{T}\mathbf{X}+ n \lambda)^{-1} \mathbf{X}^\text{T}\mathbf{y}\\
=& \mathbf{U}\mathbf{D}\mathbf{V}^\text{T}( \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}+ n \lambda \mathbf{V}\mathbf{V}^\text{T})^{-1} \mathbf{V}\mathbf{D}\mathbf{U}^\text{T}\mathbf{y}\\
=& \mathbf{U}\mathbf{D}^2 (n \lambda + \mathbf{D}^2)^{-1} \mathbf{U}^\text{T}\mathbf{y}\\
=& \sum_{j = 1}^p \mathbf{u}_j \left( \frac{d_j^2}{n \lambda + d_j^2} \mathbf{u}_j^\text{T}\mathbf{y}\right),
\end{align}

where \(d_j\) is the \(j\)th eigenvalue of the PCA. Hence, the Ridge regression fitted value can be understood as

\begin{itemize}
\tightlist
\item
  Perform PCA of \(\mathbf{X}\)
\item
  Project \(\mathbf{y}\) onto the PCs
\item
  Shrink the projection \(\mathbf{u}_j^\text{T}\mathbf{y}\) by the factor \(d_j^2 / (n \lambda + d_j^2)\)
\item
  Reassemble the PCs using all the shrunken length
\end{itemize}

Hence, the bias-variance notion can be understood as the trade-off on these derived directions \(\mathbf{u}_j\) and their corresponding parameters \(\mathbf{u}_j^\text{T}\mathbf{y}\).

\hypertarget{degrees-of-freedom}{%
\section{Degrees of Freedom}\label{degrees-of-freedom}}

We know that for a linear model, the degrees of freedom (DF) is simply the number of parameters used. There is a formal definition, using

\begin{align}
\text{DF}(\widehat{f}) =& \frac{1}{\sigma^2} \text{Cov}(\widehat{y}_i, y_i)\\
=& \frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\widehat{\mathbf{y}}, \mathbf{y})]
\end{align}

We can check that for a linear regression (assuming the intercept is already included in \(\mathbf{X}\)), the DF is

\begin{align}
\frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\widehat{\mathbf{y}}^\text{ols}, \mathbf{y})] =& \frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}\mathbf{y}, \mathbf{y})] \\
=& \text{Trace}(\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}) \\
=& \text{Trace}(\mathbf{I}_{p\times p})\\
=& p
\end{align}

For the Ridge regression, we can perform the same analysis on ridge regression.

\begin{align}
\frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\widehat{\mathbf{y}}^\text{ridge}, \mathbf{y})] =& \frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X}+ n \lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y}, \mathbf{y})] \\
=& \text{Trace}(\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}) \\
=& \text{Trace}(\mathbf{U}\mathbf{D}\mathbf{V}^\text{T}( \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}+ n \lambda \mathbf{V}\mathbf{V}^\text{T})^{-1} \mathbf{V}\mathbf{D}\mathbf{U}^\text{T})\\
=& \sum_{j = 1}^p \frac{d_j^2}{d_j^2 + n\lambda}
\end{align}

Note that this is smaller than \(p\) as long as \(\lambda \neq 0\). This implies that the Ridge regression does not use the full potential of all \(p\) variables, since there is a risk of over-fitting.

\hypertarget{using-the-lm.ridge-function}{%
\section{\texorpdfstring{Using the \texttt{lm.ridge()} function}{Using the lm.ridge() function}}\label{using-the-lm.ridge-function}}

We have seen how the \texttt{lm.ridge()} can be used to fit a Ridge regression. However, keep in mind that the \texttt{lambda} parameter used in the function actually specifies the \(n \lambda\) entirely we used in our notation. However, regardless, our goal is mainly to tune this parameter to achieve a good balance of bias-variance trade off. However, the difficulty here is to evaluate the performance without knowing the truth. Let's first use a simulated example, in which we do know the truth and then introduce the cross-validation approach for real data where we do not know the truth.

We use the prostate cancer data \texttt{prostate} from the \texttt{ElemStatLearn} package. The dataset contains 8 explanatory variables and one outcome \texttt{lpsa}, the log prostate-specific antigen value.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# ElemStatLearn is currently archived, install a previous version}
  \CommentTok{\# library(devtools)}
  \CommentTok{\# install\_version("ElemStatLearn", version = "2015.6.26", repos = "http://cran.r{-}project.org")}
  \FunctionTok{library}\NormalTok{(ElemStatLearn)}
  \FunctionTok{head}\NormalTok{(prostate)}
\DocumentationTok{\#\#       lcavol  lweight age      lbph svi       lcp gleason pgg45       lpsa train}
\DocumentationTok{\#\# 1 {-}0.5798185 2.769459  50 {-}1.386294   0 {-}1.386294       6     0 {-}0.4307829  TRUE}
\DocumentationTok{\#\# 2 {-}0.9942523 3.319626  58 {-}1.386294   0 {-}1.386294       6     0 {-}0.1625189  TRUE}
\DocumentationTok{\#\# 3 {-}0.5108256 2.691243  74 {-}1.386294   0 {-}1.386294       7    20 {-}0.1625189  TRUE}
\DocumentationTok{\#\# 4 {-}1.2039728 3.282789  58 {-}1.386294   0 {-}1.386294       6     0 {-}0.1625189  TRUE}
\DocumentationTok{\#\# 5  0.7514161 3.432373  62 {-}1.386294   0 {-}1.386294       6     0  0.3715636  TRUE}
\DocumentationTok{\#\# 6 {-}1.0498221 3.228826  50 {-}1.386294   0 {-}1.386294       6     0  0.7654678  TRUE}
\end{Highlighting}
\end{Shaded}

\hypertarget{scaling-issue}{%
\subsection{Scaling Issue}\label{scaling-issue}}

We can use \texttt{lm.ridge()} with a fixed \(\lambda\) value, as we have shown in the previous example. Its syntax is again similar to the \texttt{lm()} function, with an additional argument \texttt{lambda}. We can also compare that with our own code.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# lm.ridge function from the MASS package}
  \FunctionTok{lm.ridge}\NormalTok{(lpsa }\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ prostate[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{], }\AttributeTok{lambda =} \DecValTok{1}\NormalTok{)}
\DocumentationTok{\#\#                  lcavol     lweight         age        lbph         svi         lcp     gleason }
\DocumentationTok{\#\#  0.14716982  0.55209405  0.61998311 {-}0.02049376  0.09488234  0.74846397 {-}0.09399009  0.05227074 }
\DocumentationTok{\#\#       pgg45 }
\DocumentationTok{\#\#  0.00424397}

  \CommentTok{\# using our own code}
\NormalTok{  X }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{data.matrix}\NormalTok{(prostate[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{]))}
\NormalTok{  y }\OtherTok{=}\NormalTok{ prostate[, }\DecValTok{9}\NormalTok{]}
  \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ X }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(}\DecValTok{9}\NormalTok{)) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ y}
\DocumentationTok{\#\#                [,1]}
\DocumentationTok{\#\#          0.07941225}
\DocumentationTok{\#\# lcavol   0.55985143}
\DocumentationTok{\#\# lweight  0.60398302}
\DocumentationTok{\#\# age     {-}0.01957258}
\DocumentationTok{\#\# lbph     0.09395770}
\DocumentationTok{\#\# svi      0.68809341}
\DocumentationTok{\#\# lcp     {-}0.08863685}
\DocumentationTok{\#\# gleason  0.06288206}
\DocumentationTok{\#\# pgg45    0.00416878}
\end{Highlighting}
\end{Shaded}

However, they look different. This is because ridge regression has a scaling issue: it would shrink parameters differently if the corresponding covariates have different scales. This can be seen from our previous development of the SVD analysis. Since the shrinkage is the same for all \(d_j\)s, it would apply a larger shrinkage for small \(d_j\). A commonly used approach to deal with the scaling issue is to \textbf{standardize all covariates} such that they are treated the same way. In addition, we will also \textbf{center both \(\mathbf{X}\) and \(\mathbf{y}\)} before performing the ridge regression. An interesting consequence of centering is that we do not need the intercept anymore, since \(\mathbf{X}\boldsymbol \beta= \mathbf{0}\) for all \(\boldsymbol \beta\). One last point is that when performing scaling, \texttt{lm.ridge()} use the \(n\) factor instead of \(n-1\) when calculating the standard deviation. Hence, incorporating all these, we have

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# perform centering and scaling}
\NormalTok{  X }\OtherTok{=} \FunctionTok{scale}\NormalTok{(}\FunctionTok{data.matrix}\NormalTok{(prostate[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{]), }\AttributeTok{center =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{)}

  \CommentTok{\# use n instead of (n{-}1) for standardization}
\NormalTok{  n }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(X)}
\NormalTok{  X }\OtherTok{=}\NormalTok{ X }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(n }\SpecialCharTok{/}\NormalTok{ (n}\DecValTok{{-}1}\NormalTok{))}
  
  \CommentTok{\# center y but not scaling}
\NormalTok{  y }\OtherTok{=} \FunctionTok{scale}\NormalTok{(prostate[, }\DecValTok{9}\NormalTok{], }\AttributeTok{center =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale =} \ConstantTok{FALSE}\NormalTok{)}
  
  \CommentTok{\# getting the estimated parameter}
\NormalTok{  mybeta }\OtherTok{=} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ X }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(}\DecValTok{8}\NormalTok{)) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ y}
\NormalTok{  ridge.fit }\OtherTok{=} \FunctionTok{lm.ridge}\NormalTok{(lpsa }\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ prostate[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{], }\AttributeTok{lambda =} \DecValTok{1}\NormalTok{)}
  
  \CommentTok{\# note that $coef obtains the coefficients internally from lm.ridge}
  \CommentTok{\# however coef() would transform these back to the original scale version}
  \FunctionTok{cbind}\NormalTok{(mybeta, ridge.fit}\SpecialCharTok{$}\NormalTok{coef)}
\DocumentationTok{\#\#                [,1]        [,2]}
\DocumentationTok{\#\# lcavol   0.64734891  0.64734891}
\DocumentationTok{\#\# lweight  0.26423507  0.26423507}
\DocumentationTok{\#\# age     {-}0.15178989 {-}0.15178989}
\DocumentationTok{\#\# lbph     0.13694453  0.13694453}
\DocumentationTok{\#\# svi      0.30825889  0.30825889}
\DocumentationTok{\#\# lcp     {-}0.13074243 {-}0.13074243}
\DocumentationTok{\#\# gleason  0.03755141  0.03755141}
\DocumentationTok{\#\# pgg45    0.11907848  0.11907848}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiple-lambda-values}{%
\subsection{\texorpdfstring{Multiple \(\lambda\) values}{Multiple \textbackslash lambda values}}\label{multiple-lambda-values}}

Since we now face the problem of bias-variance trade-off, we can fit the model with multiple \(\lambda\) values and select the best. This can be done using the following code.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{  fit }\OtherTok{=} \FunctionTok{lm.ridge}\NormalTok{(lpsa}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,  }\AttributeTok{data =}\NormalTok{ prostate[, }\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{], }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{, }\AttributeTok{by=}\FloatTok{0.2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

For each \(\lambda\), the coefficients of all variables are recorded. The plot shows how these coefficients change as a function of \(\lambda\). We can easily see that as \(\lambda\) becomes larger, the coefficients are shrunken towards 0. This is consistent with our understanding of the bias. On the very left hand size of the plot, the value of each parameter corresponds to the OLS result since no penalty is applied. Be careful that the coefficients of the fitted objects \texttt{fit\$coef} are scaled by the standard deviation of the covariates. If you need the original scale, make sure to use \texttt{coef(fit)}.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{matplot}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{], }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Lambda"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Coefficients"}\NormalTok{)}
    \FunctionTok{text}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{8}\NormalTok{), }\FunctionTok{coef}\NormalTok{(fit)[}\DecValTok{1}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{], }\FunctionTok{colnames}\NormalTok{(prostate)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{])}
    \FunctionTok{title}\NormalTok{(}\StringTok{"Prostate Cancer Data: Ridge Coefficients"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-99-1} \end{center}

To select the best \(\lambda\) value, there can be several different methods. We will discuss two approaches among them: \(k\)-fold cross-validation and generalized cross-validation (GCV).

\hypertarget{cross-validation}{%
\section{Cross-validation}\label{cross-validation}}

Cross-validation (CV) is a technique to evaluate the performance of a model on an independent set of data. The essential idea is to separate out a subset of the data and do not use that part during the training, while using it for testing. We can then rotate to or sample a different subset as the testing data. Different cross-validation methods differs on the mechanisms of generating such testing data. \textbf{\(K\)-fold cross-validation} is probably the the most popular among them. The method works in the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Randomly split the data into \(K\) equal portions
\item
  For each \(k\) in \(1, \ldots, K\): use the \(k\)th portion as the testing data and the rest as training data, obtain the testing error
\item
  Average all \(K\) testing errors
\end{enumerate}

Here is a graphical demonstration of a \(10\)-fold CV:

\includegraphics[width=0.8\textwidth,height=\textheight]{images/kfoldcv.png}

There are also many other cross-validation procedures, for example, the \textbf{Monte Carlo cross-validation} randomly splits the data into training and testing (instead of fix \(K\) portions) each time and repeat the process as many times as we like. The benefit of such procedure is that if this is repeated enough times, the estimated testing error becomes fairly stable, and not affected much by the random mechanism. On the other hand, we can also repeat the entire \(K\)-fold CV process many times, then average the errors. This is also trying to reduced the influence of randomness.

\hypertarget{leave-one-out-cross-validation}{%
\section{Leave-one-out cross-validation}\label{leave-one-out-cross-validation}}

Regarding the randomness, the leave-one-out cross-validation is completely nonrandom. It is essentially the \(k\)-fold CV approach, but with \(k\) equal to \(n\), the sample size. A standard approach would require to re-fit the model \(n\) times, however, some linear algebra can show that there is an equivalent form using the ``Hat'' matrix when fitting a linear regression:

\begin{align}
\text{CV}(n) =& \frac{1}{n}\sum_{i=1}^n (y_i - \widehat{y}_{[i]})^2\\
=& \frac{1}{n} \sum_{i=1}^n \left( \frac{y_i - \widehat{y}_i}{1 - \mathbf{H}_{ii}} \right)^2,
\end{align}
where \(\widehat{y}_{i}\) is the fitted value using the whole dataset, but \(\widehat{y}_{[i]}\) is the prediction of \(i\)th observation using the data without it when fitting the model. And \(\mathbf{H}_{ii}\) is the \(i\)th diagonal element of the hat matrix \(\mathbf{H}= \mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}\). The proof is essentially an application of the \href{https://en.wikipedia.org/wiki/Sherman\%E2\%80\%93Morrison_formula}{Sherman--Morrison--Woodbury (SMW)} formula, which is also used when deriving the rank-one update of a quasi-Newton optimization approach.

\begin{proof}
Denote \(\mathbf{X}_{[i]}\) and \(\mathbf{y}_{[i]}\) the data derived from \(\mathbf{X}\) and \(\mathbf{y}\), but with the \(i\) observation (\(x_i\), \(y_i\)) removed. We then have the properties that

\[\mathbf{X}_{[i]}^\text{T}\mathbf{X}_{[i]} = \mathbf{X}^\text{T}\mathbf{X}- x_i x_i^\text{T}, \]

and

\[\mathbf{H}_{ii} = x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i.\]

By the SMW formula, we have

\[(\mathbf{X}_{[i]}^\text{T}\mathbf{X}_{[i]})^{-1} = (\mathbf{X}^\text{T}\mathbf{X})^{-1} + \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1}x_i x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1}}{ 1 - \mathbf{H}_{ii}}, \]

Further notice that

\[\mathbf{X}_{[i]}^\text{T}\mathbf{y}_{[i]} = \mathbf{X}^\text{T}\mathbf{y}- x_i y_i, \]

we can then reconstruct the fitted parameter when observation \(i\) is removed:

\begin{align}
\widehat{\boldsymbol \beta}_{[i]} =& (\mathbf{X}_{[i]}^\text{T}\mathbf{X}_{[i]})^{-1} \mathbf{X}_{[i]}^\text{T}\mathbf{y}_{[i]} \\
=& \left[ (\mathbf{X}^\text{T}\mathbf{X})^{-1} + \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1}x_i x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1}}{ 1 - \mathbf{H}_{ii}} \right] (\mathbf{X}^\text{T}\mathbf{y}- x_i y_i)\\
=& (\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}\mathbf{y}+ \left[ - (\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i y_i +  \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1}x_i x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1}}{ 1 - \mathbf{H}_{ii}} (\mathbf{X}^\text{T}\mathbf{y}- x_i y_i) \right] \\
=& \widehat{\boldsymbol \beta} - \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i}{1 - \mathbf{H}_{ii}} \left[ y_i (1 - \mathbf{H}_{ii}) - x_i^\text{T}\widehat{\boldsymbol \beta} + \mathbf{H}_{ii} y_i \right]\\
=& \widehat{\boldsymbol \beta} - \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i}{1 - \mathbf{H}_{ii}} \left( y_i - x_i^\text{T}\widehat{\boldsymbol \beta} \right)
\end{align}

Then the error of the \(i\)th obervation from the leave-one-out model is

\begin{align}
y _i - \widehat{y}_{[i]} =& y _i - x_i^\text{T}\widehat{\boldsymbol \beta}_{[i]} \\
=& y _i - x_i^\text{T}\left[ \widehat{\boldsymbol \beta} - \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i}{1 - \mathbf{H}_{ii}} \left( y_i - x_i^\text{T}\widehat{\boldsymbol \beta} \right)  \right]\\
=& y _i - x_i^\text{T}\widehat{\boldsymbol \beta} + \frac{x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i}{1 - \mathbf{H}_{ii}} \left( y_i - x_i^\text{T}\widehat{\boldsymbol \beta} \right)\\
=& y _i - x_i^\text{T}\widehat{\boldsymbol \beta} + \frac{\mathbf{H}_{ii}}{1 - \mathbf{H}_{ii}} \left( y_i - x_i^\text{T}\widehat{\boldsymbol \beta} \right)\\
=& \frac{y_i - x_i^\text{T}\widehat{\boldsymbol \beta}}{1 - \mathbf{H}_{ii}}
\end{align}

This completes the proof.
\end{proof}

\hypertarget{generalized-cross-validation}{%
\subsection{Generalized cross-validation}\label{generalized-cross-validation}}

The generalized cross-validation (GCV, Golub, Heath, and Wahba (\protect\hyperlink{ref-golub1979generalized}{1979})) is a modified version of the leave-one-out CV:

\[\text{GCV}(\lambda) = \frac{\sum_{i=1}^n (y_i - x_i^\text{T}\widehat{\boldsymbol \beta}^\text{ridge}_\lambda)}{(n - \text{Trace}(\mathbf{S}_\lambda))}\]
where \(\mathbf{S}_\lambda\) is the hat matrix corresponding to the ridge regression:

\[\mathbf{S}_\lambda = \mathbf{X}(\mathbf{X}^\text{T}\mathbf{X}+ \lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\]

The following plot shows how GCV value changes as a function of \(\lambda\).

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# use GCV to select the best lambda}
    \FunctionTok{plot}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{lambda[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{500}\NormalTok{], fit}\SpecialCharTok{$}\NormalTok{GCV[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{500}\NormalTok{], }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }
         \AttributeTok{ylab =} \StringTok{"GCV"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Lambda"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"Prostate Cancer Data: GCV"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-101-1} \end{center}

We can select the best \(\lambda\) that produces the smallest GCV.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    fit}\SpecialCharTok{$}\NormalTok{lambda[}\FunctionTok{which.min}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{GCV)]}
\DocumentationTok{\#\# [1] 6.8}
    \FunctionTok{round}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit)[}\FunctionTok{which.min}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{GCV), ], }\DecValTok{4}\NormalTok{)}
\DocumentationTok{\#\#          lcavol lweight     age    lbph     svi     lcp gleason   pgg45 }
\DocumentationTok{\#\#  0.0170  0.4949  0.6050 {-}0.0169  0.0863  0.6885 {-}0.0420  0.0634  0.0034}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-glmnet-package}{%
\section{\texorpdfstring{The \texttt{glmnet} package}{The glmnet package}}\label{the-glmnet-package}}

The \texttt{glmnet} package implements the \(k\)-fold cross-validation. To perform a ridge regression with cross-validation, we need to use the \texttt{cv.glmnet()} function with \(alpha = 0\). Here, the \(\alpha\) is a parameter that controls the \(\ell_2\) and \(\ell_1\) (Lasso) penalties. In addition, the lambda values are also automatically selected, on the log-scale.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(glmnet)}
\DocumentationTok{\#\# Loading required package: Matrix}
\DocumentationTok{\#\# Loaded glmnet 4.1{-}7}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{  fit2 }\OtherTok{=} \FunctionTok{cv.glmnet}\NormalTok{(}\FunctionTok{data.matrix}\NormalTok{(prostate[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{]), prostate}\SpecialCharTok{$}\NormalTok{lpsa, }\AttributeTok{nfolds =} \DecValTok{10}\NormalTok{, }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{)}
  \FunctionTok{plot}\NormalTok{(fit2}\SpecialCharTok{$}\NormalTok{glmnet.fit, }\StringTok{"lambda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-103-1} \end{center}

It is useful to plot the cross-validation error against the \(\lambda\) values , then select the corresponding \(\lambda\) with the smallest error. The corresponding coefficient values can be obtained using the \texttt{s\ =\ "lambda.min"} option in the \texttt{coef()} function. However, this can still be subject to over-fitting, and sometimes practitioners use \texttt{s\ =\ "lambda.1se"} to select a slightly heavier penalized version based on the variations observed from different folds.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{plot}\NormalTok{(fit2)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-104-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{coef}\NormalTok{(fit2, }\AttributeTok{s =} \StringTok{"lambda.min"}\NormalTok{)}
\DocumentationTok{\#\# 9 x 1 sparse Matrix of class "dgCMatrix"}
\DocumentationTok{\#\#                       s1}
\DocumentationTok{\#\# (Intercept)  0.011566731}
\DocumentationTok{\#\# lcavol       0.492211875}
\DocumentationTok{\#\# lweight      0.604155671}
\DocumentationTok{\#\# age         {-}0.016727236}
\DocumentationTok{\#\# lbph         0.085820464}
\DocumentationTok{\#\# svi          0.685477646}
\DocumentationTok{\#\# lcp         {-}0.039717080}
\DocumentationTok{\#\# gleason      0.063806235}
\DocumentationTok{\#\# pgg45        0.003411982}
  \FunctionTok{coef}\NormalTok{(fit2, }\AttributeTok{s =} \StringTok{"lambda.1se"}\NormalTok{)}
\DocumentationTok{\#\# 9 x 1 sparse Matrix of class "dgCMatrix"}
\DocumentationTok{\#\#                       s1}
\DocumentationTok{\#\# (Intercept)  0.035381749}
\DocumentationTok{\#\# lcavol       0.264613825}
\DocumentationTok{\#\# lweight      0.421408730}
\DocumentationTok{\#\# age         {-}0.002555681}
\DocumentationTok{\#\# lbph         0.049916919}
\DocumentationTok{\#\# svi          0.452500472}
\DocumentationTok{\#\# lcp          0.075346975}
\DocumentationTok{\#\# gleason      0.083894617}
\DocumentationTok{\#\# pgg45        0.002615235}
\end{Highlighting}
\end{Shaded}

\hypertarget{scaling-issue-1}{%
\subsection{Scaling Issue}\label{scaling-issue-1}}

The \texttt{glmnet} package would using the same strategies for scaling: center and standardize \(\mathbf{X}\) and center \(\mathbf{y}\). A slight difference is that it considers using \(1/(2n)\) as the normalizing factor of the residual sum of squares, but also uses \(\lambda/2 \lVert \boldsymbol \beta\rVert_2^2\) as the penalty. This does not affect our formulation since the \(1/2\) cancels out. However, it would slightly affect the Lasso formulation introduced in the next Chapter since the \(\ell_1\) penalty does not apply this \(1/2\) factor. Nonetheless, we can check the (nearly) equivalence between \texttt{lm.ridge} and \texttt{glmnet()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  n }\OtherTok{=} \DecValTok{100}
\NormalTok{  p }\OtherTok{=} \DecValTok{5}

\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{scale}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{p), n, p)))}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{scale}\NormalTok{(X[, }\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ X[,}\DecValTok{2}\NormalTok{]}\SpecialCharTok{*}\FloatTok{0.5} \SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)))}
 
\NormalTok{  lam }\OtherTok{=} \DecValTok{10}\SpecialCharTok{\^{}}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
 
\NormalTok{  fit1 }\OtherTok{\textless{}{-}} \FunctionTok{lm.ridge}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{lambda =}\NormalTok{ lam)}
\NormalTok{  fit2 }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(X, y, }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{, }\AttributeTok{lambda =}\NormalTok{ lam }\SpecialCharTok{/} \FunctionTok{nrow}\NormalTok{(X))}
 
  \CommentTok{\# the estimated parameters}
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
  \FunctionTok{matplot}\NormalTok{(}\FunctionTok{apply}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit1), }\DecValTok{2}\NormalTok{, rev), }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{main =} \StringTok{"lm.ridge"}\NormalTok{)}
  \FunctionTok{matplot}\NormalTok{(}\FunctionTok{t}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit2))), }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{main =} \StringTok{"glmnet"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{SMLR_files/figure-latex/unnamed-chunk-105-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
  
  \CommentTok{\# Check differences}
  \FunctionTok{max}\NormalTok{(}\FunctionTok{abs}\NormalTok{(}\FunctionTok{apply}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit1), }\DecValTok{2}\NormalTok{, rev) }\SpecialCharTok{{-}} \FunctionTok{t}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit2)))))}
\DocumentationTok{\#\# [1] 0.0009968625}
\end{Highlighting}
\end{Shaded}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{lasso}{%
\chapter{Lasso}\label{lasso}}

Lasso (\protect\hyperlink{ref-tibshirani1996regression}{Tibshirani 1996}) is among the most popular machine learning models. Different from the Ridge regression, its adds \(\ell_1\) penalty on the fitted parameters:

\begin{align}
\widehat{\boldsymbol \beta}^\text{lasso} =& \arg\min_{\boldsymbol \beta} (\mathbf{y}- \mathbf{X}\boldsymbol \beta)^\text{T}(\mathbf{y}- \mathbf{X}\boldsymbol \beta) + n \lambda \lVert\boldsymbol \beta\rVert_1\\
=& \arg\min_{\boldsymbol \beta} \frac{1}{n} \sum_{i=1}^n (y_i - x_i^\text{T}\boldsymbol \beta)^2 + \lambda \sum_{i=1}^p |\beta_j|,
\end{align}

The main advantage of adding such a penalty is that small \(\widehat{\beta}_j\) values can be \textbf{shrunk to zero}. This may prevents over-fitting and also improve the interpretability especially when the number of variables is large. We will analyze the Lasso starting with a single variable case, and then discuss the application of coordinate descent algorithm to obtain the solution.

\hypertarget{one-variable-lasso-and-shrinkage}{%
\section{One-Variable Lasso and Shrinkage}\label{one-variable-lasso-and-shrinkage}}

To illustrate how Lasso shrink a parameter estimate to zero, let's consider an orthogonal design matrix case, i.e., \(\mathbf{X}^\text{T}\mathbf{X}= n \mathbf{I}\), which will eventually reduce to a one-variable problem. Note that the intercept term is not essential because we can always pre-center the observed data \(x_i\) and \(y_i\)s so that they can be recovered after this one variable problem. Our objective function is

\[\frac{1}{n}\lVert \mathbf{y}- \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert\boldsymbol \beta\rVert_1\]
We are going to relate the solution the OLS solution, which exists in this case because \(\mathbf{X}^\text{T}\mathbf{X}\) is invertible. Hence, we have

\begin{align}
&\frac{1}{n}\lVert \mathbf{y}- \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert\boldsymbol \beta\rVert_1\\
=&\frac{1}{n}\lVert \mathbf{y}- \color{OrangeRed}{\mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} + \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols}} - \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert\boldsymbol \beta\rVert_1\\
=&\frac{1}{n}\lVert \mathbf{y}- \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} \rVert^2 + \frac{1}{n} \lVert \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} - \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert\boldsymbol \beta\rVert_1
\end{align}

The cross-term is zero because the OLS residual term is orthogonal to the columns of \(\mathbf{X}\):

\begin{align}
&2(\mathbf{y}- \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols})^\text{T}(\mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} - \mathbf{X}\boldsymbol \beta)\\
=& 2\mathbf{r}^\text{T}\mathbf{X}(\widehat{\boldsymbol \beta}^\text{ols} - \boldsymbol \beta)\\
=& 0
\end{align}

Then we just need to optimize the part that involves \(\boldsymbol \beta\):

\begin{align}
&\underset{\boldsymbol \beta}{\arg\min} \frac{1}{n}\lVert \mathbf{y}- \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} \rVert^2 + \frac{1}{n} \lVert \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} - \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert\boldsymbol \beta\rVert_1\\
=&\underset{\boldsymbol \beta}{\arg\min} \frac{1}{n} \lVert \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} - \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert\boldsymbol \beta\rVert_1\\
=&\underset{\boldsymbol \beta}{\arg\min} \frac{1}{n} (\widehat{\boldsymbol \beta}^\text{ols} - \boldsymbol \beta)^\text{T}\mathbf{X}^\text{T}\mathbf{X}(\widehat{\boldsymbol \beta}^\text{ols} - \boldsymbol \beta)  + \lambda \lVert\boldsymbol \beta\rVert_1\\
=&\underset{\boldsymbol \beta}{\arg\min} \frac{1}{n} (\widehat{\boldsymbol \beta}^\text{ols} - \boldsymbol \beta)^\text{T}n \mathbf{I}(\widehat{\boldsymbol \beta}^\text{ols} - \boldsymbol \beta)  + \lambda \lVert\boldsymbol \beta\rVert_1\\
=&\underset{\boldsymbol \beta}{\arg\min} \sum_{j = 1}^p (\widehat{\boldsymbol \beta}^\text{ols}_j - \boldsymbol \beta_j )^2 + \lambda \sum_j |\boldsymbol \beta_j|\\
\end{align}

This is a separable problem meaning that we can solve each \(\beta_j\) independently since they do not interfere each other. Then the univariate problem is

\[\underset{\beta}{\arg\min} \,\, (\beta - a)^2 + \lambda |\beta|\]
We learned that to solve for an optimizer, we can set the gradient to be zero. However, the function is not everywhere differentiable. Still, we can separate this into two cases: \(\beta > 0\) and \(\beta < 0\). For the positive side, we have

\begin{align}
0 =& \frac{\partial}{\partial \beta} \,\, (\beta - a)^2 + \lambda |\beta| = 2 (\beta - a) + \lambda \\
\Longrightarrow \quad \beta =&\, a - \lambda/2
\end{align}

However, this will maintain positive only when \(\beta\) is greater than \(a - \lambda/2\). The negative size is similar. And whenever \(\beta\) falls in between, it will be shrunk to zero. Overall, for our previous univariate optimization problem, the solution is

\begin{align}
\hat\beta_j^\text{lasso} &=
        \begin{cases}
        \hat\beta_j^\text{ols} - \lambda/2 & \text{if} \quad \hat\beta_j^\text{ols} > \lambda/2 \\
        0 & \text{if} \quad |\hat\beta_j^\text{ols}| < \lambda/2 \\
        \hat\beta_j^\text{ols} + \lambda/2 & \text{if} \quad \hat\beta_j^\text{ols} < -\lambda/2 \\
        \end{cases}\\
        &= \text{sign}(\hat\beta_j^\text{ols}) \left(|\hat\beta_j^\text{ols}| - \lambda/2 \right)_+ \\
        &\doteq \text{SoftTH}(\hat\beta_j^\text{ols}, \lambda)
\end{align}

This is called a \textbf{soft-thresholding function}. This implies that when \(\lambda\) is large enough, the estimated \(\beta\) parameter of Lasso will be shrunk towards zero. The following animated figure demonstrates how adding an \(\ell_1\) penalty can change the optimizer. The objective function is \(0.5 + (\beta - 1)^2\) and based on our previous analysis, once the penalty is larger than 2, the optimizer would stay at 0.

\hypertarget{constrained-optimization-view}{%
\section{Constrained Optimization View}\label{constrained-optimization-view}}

Of course in a multivariate case, this is much more complicated since one variable may affect the optimizer of another. A commonly used alternative interpretation of the Lasso problem is the constrained optimization formulation:

\begin{align}
\min_{\boldsymbol \beta} \,\,& \lVert \mathbf{y}- \mathbf{X}\boldsymbol \beta\rVert^2\\
\text{subject to} \,\, & \lVert\boldsymbol \beta\rVert_1 \leq t
\end{align}

We can see from the left penal of the following figure that, the Lasso penalty imposes a constraint with the rhombus, i.e., the solution has to stay within the shaded area. The objective function is shown with the contour, and once the contained area is sufficiently small, some \(\beta\) parameter will be shrunk to exactly zero. On the other hand, the Ridge regression also has a similar interpretation. However, since the constrained areas is a circle, it will never for the estimated parameters to be zero.

\includegraphics[width=1\textwidth,height=\textheight]{images/lassoridge2.png} Figure from online sources.

\hypertarget{the-solution-path}{%
\section{The Solution Path}\label{the-solution-path}}

We are interested in getting the fitted model with a given \(\lambda\) value, however, for selecting the tuning parameter, it would be much more stable to obtain the solution on a sequence of \(\lambda\) values. The corresponding \(\boldsymbol \beta\) parameter estimates are called the solution path, i.e., the path how parameter changes as \(\lambda\) changes. We have seen an example of this with the Ridge regression. For Lasso, the the solution path has an interpretation as the \textbf{forward-stagewise} regression. This is different than the forward stepwise model we introduced before. A forward stagewise regression works in the following way:

\begin{itemize}
\tightlist
\item
  Start with the Null model (intercept) and choose the best variable out of all \(p\), such that when its parameter grows by a small magnitude \(\epsilon\) (either positive or negative), the RSS reduces the most. Grow the parameter estimate of this variable by \(\epsilon\) and repeat.
\end{itemize}

The stage-wise regression solution has been shown to give the same solution path as the Lasso, if we start with a sufficiently large \(\lambda\), and gradually reduces it towards zero. This can be done with the least angle regression (\texttt{lars}) package. Note that the \texttt{lars} package introduces another computationally more efficient approach to obtain the same solution, but we will not discuss it in details. We comparing the two approaches (stagewise and stepwise) using the \texttt{prostate} data from the \texttt{ElemStatLearn} package.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(lars)}
  \FunctionTok{library}\NormalTok{(ElemStatLearn)}
  \FunctionTok{data}\NormalTok{(prostate)}
  
\NormalTok{  lars.fit }\OtherTok{=} \FunctionTok{lars}\NormalTok{(}\AttributeTok{x =} \FunctionTok{data.matrix}\NormalTok{(prostate[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{]), }\AttributeTok{y =}\NormalTok{ prostate}\SpecialCharTok{$}\NormalTok{lpsa, }
                  \AttributeTok{type =} \StringTok{"forward.stagewise"}\NormalTok{)}
  \FunctionTok{plot}\NormalTok{(lars.fit)}
  
\NormalTok{  lars.fit }\OtherTok{=} \FunctionTok{lars}\NormalTok{(}\AttributeTok{x =} \FunctionTok{data.matrix}\NormalTok{(prostate[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{]), }\AttributeTok{y =}\NormalTok{ prostate}\SpecialCharTok{$}\NormalTok{lpsa, }
                  \AttributeTok{type =} \StringTok{"stepwise"}\NormalTok{)}
  \FunctionTok{plot}\NormalTok{(lars.fit)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-110-1} \end{center}

At each vertical line, a new variable enters the model by growing its parameter out of zero. You can relate this to our previous animated graph where as \(\lambda\) decreases, the parameter estimate eventually comes out of zero. However, they may change their grow rate as a new variable comes. This is due to the covariance structure.

\hypertarget{path-wise-coordinate-descent}{%
\section{Path-wise Coordinate Descent}\label{path-wise-coordinate-descent}}

The coordinate descent algorithm (\protect\hyperlink{ref-friedman2010regularization}{J. Friedman, Hastie, and Tibshirani 2010}) is probably the most efficient way to solve the Lasso solution up to now. The idea shares similarities with the stage-wise regression. However, with some careful analysis, we can obtain coordinate updates exactly, instead of moving a small step size. And this is done on a decreasing grid of \(\lambda\) values. A pseudo algorithm proceed in the following way:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Start with a \(\lambda\) value sufficiently large such that all parameter estimates are zero.
\item
  Reduce \(\lambda\) by a fraction, e.g., 0.05, and perform coordinate descent updates:

  \begin{enumerate}
  \def\labelenumii{\roman{enumii})}
  \tightlist
  \item
    For \(j = 1, \ldots p\), update \(\beta_j\) using a one-variable penalized formulation.
  \item
    Repeat i) until convergence.
  \end{enumerate}
\item
  Record the corresponding \(\widehat{\boldsymbol \beta}^\text{lasso}_\lambda\).
\item
  Repeat steps 2) and 3) until \(\lambda\) is sufficiently small or there are already \(n\) nonzero parameters entered into the model. Output \(\widehat{\boldsymbol \beta}^\text{lasso}_\lambda\) for all \(\lambda\) values.
\end{enumerate}

The crucial step is then figuring out the explicit formula of the coordinate update. Recall that in a coordinate descent algorithm of OLS at Section \ref{coordinate}, we update \(\beta_j\) using

\[
\underset{\boldsymbol \beta_j}{\text{argmin}} \,\, \frac{1}{n} ||\mathbf{y}- X_j \beta_j - \mathbf{X}_{(-j)} \boldsymbol \beta_{(-j)} ||^2
\]
Since this is a one-variable OLS problem, the solution is

\[
\beta_j = \frac{X_j^T \mathbf{r}}{X_j^T X_j}
\]

with \(\mathbf{r}= \mathbf{y}- \mathbf{X}_{(-j)} \boldsymbol \beta_{(-j)}\). Now, adding the penalty \(|\beta_j|\), we essentially reduces back to the previous example of the single variable lasso problem, where we have the OLS solution. Hence, all we need to do is to apply the soft-thresholding function. The the Lasso coordinate update becomes

\[\beta_j^\text{new} = \text{SoftTH}\left(\frac{X_j^T \mathbf{r}}{X_j^T X_j}, \lambda\right) \]
Incorporate this into the previous algorithm, we can obtain the entire solution path of a Lasso problem. This algorithm is implemented in the \texttt{glmnet} package. We will show an example of it.

\hypertarget{using-the-glmnet-package}{%
\section{\texorpdfstring{Using the \texttt{glmnet} package}{Using the glmnet package}}\label{using-the-glmnet-package}}

We still use the prostate cancer data \texttt{prostate} data. The dataset contains 8 explanatory variables and one outcome \texttt{lpsa}, the log prostate-specific antigen value. We fit the model using the \texttt{glmnet} package. The tuning parameter can be selected using cross-validation with the \texttt{cv.glmnet} function. You can specify \texttt{nfolds} for the number of folds in the cross-validation. The default is 10. For Lasso, we should use \texttt{alpha\ =\ 1}, while \texttt{alpha\ =\ 0} is for Ridge. However, it is the default value that you do not need to specify.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(glmnet)}
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{    fit2 }\OtherTok{=} \FunctionTok{cv.glmnet}\NormalTok{(}\FunctionTok{data.matrix}\NormalTok{(prostate[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{]), prostate}\SpecialCharTok{$}\NormalTok{lpsa, }\AttributeTok{nfolds =} \DecValTok{10}\NormalTok{, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The left plot demonstrates how \(\lambda\) changes the cross-validation error. There are two vertical lines, which represents \texttt{lambda.min} and \texttt{lambda.1se} respectively. The right plot shows how \(\lambda\) changes the parameter values, with each line representing a variable. The x-axis in the figure is in terms of \(\log(\lambda)\), hence their is a larger penalty to the right. Note that the \texttt{glmnet} package uses \(1/(2n)\) in the loss function instead of \(1/n\), hence the corresponding soft-thresholding function would reduce the magnitude of \(\lambda\) by \(\lambda\) instead of half of it. Moreover, the package will perform scaling before the model fitting, which essentially changes the corresponding one-variable OLS solution. The solution on the original scale will be retrieved once the entire solution path is finished. However, we usually do not need to worry about these computationally issues in practice. The main advantage of Lasso is shown here that the model can be sparse, with some parameter estimates shrunk to exactly 0.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
    \FunctionTok{plot}\NormalTok{(fit2)}
    \FunctionTok{plot}\NormalTok{(fit2}\SpecialCharTok{$}\NormalTok{glmnet.fit, }\StringTok{"lambda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-112-1} \end{center}

We can obtain the estimated coefficients from the best \(\lambda\) value. Similar to the ridge regression example, there are two popular options, \texttt{lambda.min} and \texttt{lambda.1se}. The first one is the value that minimizes the cross-validation error, the second one is slightly more conservative, which gives larger penalty value with more shrinkage. You can notice that \texttt{lambda.min} contains more nonzero parameters.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{coef}\NormalTok{(fit2, }\AttributeTok{s =} \StringTok{"lambda.min"}\NormalTok{)}
\DocumentationTok{\#\# 9 x 1 sparse Matrix of class "dgCMatrix"}
\DocumentationTok{\#\#                        s1}
\DocumentationTok{\#\# (Intercept)  0.1537694867}
\DocumentationTok{\#\# lcavol       0.5071477800}
\DocumentationTok{\#\# lweight      0.5455934491}
\DocumentationTok{\#\# age         {-}0.0084065349}
\DocumentationTok{\#\# lbph         0.0618168146}
\DocumentationTok{\#\# svi          0.5899942923}
\DocumentationTok{\#\# lcp          .           }
\DocumentationTok{\#\# gleason      0.0009732887}
\DocumentationTok{\#\# pgg45        0.0023140828}
    \FunctionTok{coef}\NormalTok{(fit2, }\AttributeTok{s =} \StringTok{"lambda.1se"}\NormalTok{)}
\DocumentationTok{\#\# 9 x 1 sparse Matrix of class "dgCMatrix"}
\DocumentationTok{\#\#                    s1}
\DocumentationTok{\#\# (Intercept) 0.6435469}
\DocumentationTok{\#\# lcavol      0.4553889}
\DocumentationTok{\#\# lweight     0.3142829}
\DocumentationTok{\#\# age         .        }
\DocumentationTok{\#\# lbph        .        }
\DocumentationTok{\#\# svi         0.3674270}
\DocumentationTok{\#\# lcp         .        }
\DocumentationTok{\#\# gleason     .        }
\DocumentationTok{\#\# pgg45       .}
\end{Highlighting}
\end{Shaded}

Prediction can be done using the \texttt{predict()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(fit2, }\FunctionTok{data.matrix}\NormalTok{(prostate[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{]), }\AttributeTok{s =} \StringTok{"lambda.min"}\NormalTok{)}
    \CommentTok{\# training error}
    \FunctionTok{mean}\NormalTok{((pred }\SpecialCharTok{{-}}\NormalTok{ prostate}\SpecialCharTok{$}\NormalTok{lpsa)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\# [1] 0.4594359}
\end{Highlighting}
\end{Shaded}

\hypertarget{elastic-net}{%
\section{Elastic-Net}\label{elastic-net}}

Lasso may suffer in the case where two variables are strongly correlated. The situation is similar to OLS, however, in Lasso, it would only select one out of the two, instead of letting both parameter estimates to be large. This is not preferred in some practical situations such as genetic studies because expressions of genes from the same pathway may have large correlation, but biologist want to identify all of them instead of just one. The Ridge penalty may help in this case because it naturally considers the correlation structure. Hence the \textbf{Elastic-Net} (\protect\hyperlink{ref-zou2005regularization}{Zou and Hastie 2005}) penalty has been proposed to address this issue: the data contains many correlated variables and we want to select them together if they are important for prediction. The \texttt{glmnet} package uses the following definition of an Elastic-Net penalty, which is a mixture of \(\ell_1\) and \(\ell_2\) penalties:

\[\lambda \left[ (1 - \alpha)/2 \lVert \boldsymbol \beta\rVert_2^2 + \alpha |\boldsymbol \beta|_1 \right],\]
which involves two tuning parameters. However, in practice, it is very common to simply use \(\alpha = 0.5\).

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}
\usepackage{amsmath}

\hypertarget{part-nonparametric-models}{%
\part{Nonparametric Models}\label{part-nonparametric-models}}

\hypertarget{spline}{%
\chapter{Spline}\label{spline}}

\hypertarget{from-linear-to-nonlinear}{%
\section{From Linear to Nonlinear}\label{from-linear-to-nonlinear}}

In previous chapters, we mainly focused linear models. Modeling nonlinear trends can still be done with linear model by introducing higher-order terms, or nonlinear transformations. For example, \(x^2\), \(\log(x)\) are all very commonly used approaches to model nonlinear effects. There is another class of approaches that is more flexible with nice theoretical properties, the splines. In this chapter, we mainly focus on univariate regression problems.

\hypertarget{a-motivating-example-and-polynomials}{%
\section{A Motivating Example and Polynomials}\label{a-motivating-example-and-polynomials}}

We use the U.S. birth rate data as an example. The data records birth rates from 1917 to 2003. The birth rate trend is obviously very nonlinear.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    birthrates}\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/birthrate2.csv"}\NormalTok{, }\AttributeTok{row.names =} \DecValTok{1}\NormalTok{)}
    \FunctionTok{head}\NormalTok{(birthrates)}
\DocumentationTok{\#\#   Year Birthrate}
\DocumentationTok{\#\# 1 1917     183.1}
\DocumentationTok{\#\# 2 1918     183.9}
\DocumentationTok{\#\# 3 1919     163.1}
\DocumentationTok{\#\# 4 1920     179.5}
\DocumentationTok{\#\# 5 1921     181.4}
\DocumentationTok{\#\# 6 1922     173.4}
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
    \FunctionTok{plot}\NormalTok{(birthrates, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-117-1} \end{center}

It might be interesting to fit a linear regression with high order polynomials to approximate this curve. This can be carried out using the \texttt{poly()} function, which calculates all polynomials up to a certain power. Please note that this is a more stable method compared with writing out the powers such as \texttt{I(Year\^{}2)}, \texttt{I(Year\^{}3)} etc because the \texttt{Year} variable is very large, and is numerically unstable.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}

    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\NormalTok{    lmfit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Birthrate }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(Year, }\DecValTok{3}\NormalTok{), }\AttributeTok{data =}\NormalTok{ birthrates)}
    \FunctionTok{plot}\NormalTok{(birthrates, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
    \FunctionTok{lines}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year, lmfit}\SpecialCharTok{$}\NormalTok{fitted.values, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"degree = 3"}\NormalTok{)}
    
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\NormalTok{    lmfit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Birthrate }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(Year, }\DecValTok{5}\NormalTok{), }\AttributeTok{data =}\NormalTok{ birthrates)}
    \FunctionTok{plot}\NormalTok{(birthrates, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
    \FunctionTok{lines}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year, lmfit}\SpecialCharTok{$}\NormalTok{fitted.values, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"degree = 5"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{SMLR_files/figure-latex/unnamed-chunk-118-1} \end{center}

These fittings do not seem to perform very well. How about we take a different approach to model the curve locally. Well, we know there is an approach that works in a similar way -- \(k\)NN. But we will try something new. Let's first divide the year range into several non-overlapping intervals, say, every 10 years. Then we will estimate the regression coefficients within each interval by averaging the observations, just like \(k\)NN. The only difference is that for prediction, we do not recalculate the neighbors anymore, just check the intervals.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{))}

\NormalTok{    mybasis }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(birthrates), }\DecValTok{8}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ (l }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{)}
\NormalTok{        mybasis[, l] }\OtherTok{=}\NormalTok{ birthrates}\SpecialCharTok{$}\NormalTok{Year}\SpecialCharTok{*}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textgreater{}=} \DecValTok{1917} \SpecialCharTok{+}\NormalTok{ l}\SpecialCharTok{*}\DecValTok{10}\NormalTok{)}
        
\NormalTok{    lmfit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Birthrate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(mybasis))}
    \FunctionTok{plot}\NormalTok{(birthrates, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
    \FunctionTok{lines}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year, lmfit}\SpecialCharTok{$}\NormalTok{fitted.values, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{type =} \StringTok{"s"}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"Histgram Regression"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-119-1} \end{center}

The method is called a histogram regression. Suppose the interval that contains a given testing point \(x\) is \(\phi(x)\), then, we are fitting a model with

\[\widehat{f}(x) = \frac{\sum_{i=1}^n Y_i \,\, I\{X_i \in \phi(x)\} }{ \sum_{i=1}^n I\{X_i \in \phi(x)\}}\]

You may know the word histogram from the plotting the density of a set of observations. Yes, these two are actually motivated by the same philosophy. We will discuss the connection later on. For the purpose of fitting a regression function, the histogram regression does not seem to perform ideally since there will be jumps at the edge of an interval. Hence we need a more flexible framework.

\hypertarget{piecewise-polynomials}{%
\section{Piecewise Polynomials}\label{piecewise-polynomials}}

Instead of fitting constant functions within each interval (between two knots), we may consider fitting a line. Consider a simpler case, where we just use 3 knots at 1938, 1960, 1978, which gives 4 intervals.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\NormalTok{    myknots }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1936}\NormalTok{, }\DecValTok{1960}\NormalTok{, }\DecValTok{1978}\NormalTok{)}
\NormalTok{    bounds }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1917}\NormalTok{, myknots, }\DecValTok{2003}\NormalTok{)  }
    
    \CommentTok{\# piecewise constant}
\NormalTok{    mybasis }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\StringTok{"x\_1"} \OtherTok{=}\NormalTok{ (birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textless{}}\NormalTok{ myknots[}\DecValTok{1}\NormalTok{]), }
                    \StringTok{"x\_2"} \OtherTok{=}\NormalTok{ (birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textgreater{}=}\NormalTok{ myknots[}\DecValTok{1}\NormalTok{])}\SpecialCharTok{*}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textless{}}\NormalTok{ myknots[}\DecValTok{2}\NormalTok{]), }
                    \StringTok{"x\_3"} \OtherTok{=}\NormalTok{ (birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textgreater{}=}\NormalTok{ myknots[}\DecValTok{2}\NormalTok{])}\SpecialCharTok{*}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textless{}}\NormalTok{ myknots[}\DecValTok{3}\NormalTok{]),}
                    \StringTok{"x\_4"} \OtherTok{=}\NormalTok{ (birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textgreater{}=}\NormalTok{ myknots[}\DecValTok{3}\NormalTok{]))}
        
\NormalTok{    lmfit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Birthrate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(mybasis))}
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{))    }
    \FunctionTok{plot}\NormalTok{(birthrates, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =}\NormalTok{ myknots, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"Piecewise constant"}\NormalTok{)}
    
    \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)}
        \FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(bounds[k], bounds[k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{]), }\FunctionTok{rep}\NormalTok{(lmfit}\SpecialCharTok{$}\NormalTok{coefficients[k], }\DecValTok{2}\NormalTok{), }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }
               \AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
    
    \CommentTok{\# piecewise linear}
\NormalTok{    mybasis }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\StringTok{"x\_1"} \OtherTok{=}\NormalTok{ (birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textless{}}\NormalTok{ myknots[}\DecValTok{1}\NormalTok{]), }
                    \StringTok{"x\_2"} \OtherTok{=}\NormalTok{ (birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textgreater{}=}\NormalTok{ myknots[}\DecValTok{1}\NormalTok{])}\SpecialCharTok{*}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textless{}}\NormalTok{ myknots[}\DecValTok{2}\NormalTok{]), }
                    \StringTok{"x\_3"} \OtherTok{=}\NormalTok{ (birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textgreater{}=}\NormalTok{ myknots[}\DecValTok{2}\NormalTok{])}\SpecialCharTok{*}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textless{}}\NormalTok{ myknots[}\DecValTok{3}\NormalTok{]),}
                    \StringTok{"x\_4"} \OtherTok{=}\NormalTok{ (birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textgreater{}=}\NormalTok{ myknots[}\DecValTok{3}\NormalTok{]),}
                    \StringTok{"x\_11"} \OtherTok{=}\NormalTok{ birthrates}\SpecialCharTok{$}\NormalTok{Year}\SpecialCharTok{*}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textless{}}\NormalTok{ myknots[}\DecValTok{1}\NormalTok{]), }
                    \StringTok{"x\_21"} \OtherTok{=}\NormalTok{ birthrates}\SpecialCharTok{$}\NormalTok{Year}\SpecialCharTok{*}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textgreater{}=}\NormalTok{ myknots[}\DecValTok{1}\NormalTok{])}\SpecialCharTok{*}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textless{}}\NormalTok{ myknots[}\DecValTok{2}\NormalTok{]), }
                    \StringTok{"x\_31"} \OtherTok{=}\NormalTok{ birthrates}\SpecialCharTok{$}\NormalTok{Year}\SpecialCharTok{*}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textgreater{}=}\NormalTok{ myknots[}\DecValTok{2}\NormalTok{])}\SpecialCharTok{*}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textless{}}\NormalTok{ myknots[}\DecValTok{3}\NormalTok{]),}
                    \StringTok{"x\_41"} \OtherTok{=}\NormalTok{ birthrates}\SpecialCharTok{$}\NormalTok{Year}\SpecialCharTok{*}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{\textgreater{}=}\NormalTok{ myknots[}\DecValTok{3}\NormalTok{]))}
        
\NormalTok{    lmfit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Birthrate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(mybasis))}
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{))  }
    \FunctionTok{plot}\NormalTok{(birthrates, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =}\NormalTok{ myknots, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"Piecewise linear"}\NormalTok{)}
    
    \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)}
        \FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(bounds[k], bounds[k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{]), }
\NormalTok{               lmfit}\SpecialCharTok{$}\NormalTok{coefficients[k] }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(bounds[k], bounds[k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{])}\SpecialCharTok{*}\NormalTok{lmfit}\SpecialCharTok{$}\NormalTok{coefficients[k}\SpecialCharTok{+}\DecValTok{4}\NormalTok{], }
               \AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{SMLR_files/figure-latex/unnamed-chunk-120-1} \end{center}

\hypertarget{splines}{%
\section{Splines}\label{splines}}

However, these functions are not continuous. Hence we use a trick to construct continuous basis:

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{    pos }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) x}\SpecialCharTok{*}\NormalTok{(x}\SpecialCharTok{\textgreater{}}\DecValTok{0}\NormalTok{)}
\NormalTok{    mybasis }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\StringTok{"int"} \OtherTok{=} \DecValTok{1}\NormalTok{, }\StringTok{"x\_1"} \OtherTok{=}\NormalTok{ birthrates}\SpecialCharTok{$}\NormalTok{Year, }
                    \StringTok{"x\_2"} \OtherTok{=} \FunctionTok{pos}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{{-}}\NormalTok{ myknots[}\DecValTok{1}\NormalTok{]), }
                    \StringTok{"x\_3"} \OtherTok{=} \FunctionTok{pos}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{{-}}\NormalTok{ myknots[}\DecValTok{2}\NormalTok{]),}
                    \StringTok{"x\_4"} \OtherTok{=} \FunctionTok{pos}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year }\SpecialCharTok{{-}}\NormalTok{ myknots[}\DecValTok{3}\NormalTok{]))}
    
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{))}
    \FunctionTok{matplot}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year, mybasis[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{], }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }
            \AttributeTok{yaxt =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"Spline Basis Functions"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-121-1} \end{center}

With this definition, any fitted model will be

\begin{itemize}
\tightlist
\item
  Continuous everywhere
\item
  Linear everywhere except the knots
\item
  Has a different slot for each region
\end{itemize}

The resulted model is called a spline.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    lmfit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Birthrate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(mybasis))}
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{))  }
    \FunctionTok{plot}\NormalTok{(birthrates, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
    \FunctionTok{lines}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year, lmfit}\SpecialCharTok{$}\NormalTok{fitted.values, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =}\NormalTok{ myknots, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"Linear Spline"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-122-1} \end{center}

Of course, writing this out explicitly is very tedious, hence we have the \texttt{bs} function in the \texttt{splines} package to help us.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\NormalTok{    lmfit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Birthrate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ splines}\SpecialCharTok{::}\FunctionTok{bs}\NormalTok{(Year, }\AttributeTok{degree =} \DecValTok{1}\NormalTok{, }\AttributeTok{knots =}\NormalTok{ myknots), }\AttributeTok{data =}\NormalTok{ birthrates)}
    \FunctionTok{plot}\NormalTok{(birthrates, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
    \FunctionTok{lines}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year, lmfit}\SpecialCharTok{$}\NormalTok{fitted.values, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"Linear spline with the bs() function"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-123-1} \end{center}

The next step is to increase the degree to account for more complicated functions. A few things we need to consider here:

\begin{itemize}
\tightlist
\item
  How many knots should be used
\item
  Where to place the knots
\item
  What is the degree of functions in each region
\end{itemize}

For example, we consider this setting

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\NormalTok{    lmfit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Birthrate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ splines}\SpecialCharTok{::}\FunctionTok{bs}\NormalTok{(Year, }\AttributeTok{degree =} \DecValTok{3}\NormalTok{, }\AttributeTok{knots =}\NormalTok{ myknots), }\AttributeTok{data =}\NormalTok{ birthrates)}
    \FunctionTok{plot}\NormalTok{(birthrates, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
    \FunctionTok{lines}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year, lmfit}\SpecialCharTok{$}\NormalTok{fitted.values, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"Cubic spline with 3 knots"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-124-1} \end{center}

All of them affects the performance. In particular, the number of knots and the number of degrees in each region will determine the total number of degrees of freedom. For simplicity, we can control that using the \texttt{df} parameter. We use a total of 6 parameters, chosen by the function automatically. However, this does not seems to perform better than the knots we implemented. The choice of knots can be crucial.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\NormalTok{    lmfit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Birthrate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ splines}\SpecialCharTok{::}\FunctionTok{bs}\NormalTok{(Year, }\AttributeTok{df =} \DecValTok{5}\NormalTok{), }\AttributeTok{data =}\NormalTok{ birthrates)}
    \FunctionTok{plot}\NormalTok{(birthrates, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
    \FunctionTok{lines}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year, lmfit}\SpecialCharTok{$}\NormalTok{fitted.values, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"Linear spline with 6 degrees of parameters"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-125-1} \end{center}

\hypertarget{spline-basis}{%
\section{Spline Basis}\label{spline-basis}}

There are different ways to construct spline basis. We used two techniques previously, the regression spline and basis spline (B-spline). The B-spline has slight more advantages computationally. Here is a comparision of B-spline with different degrees.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{))}

    \ControlFlowTok{for}\NormalTok{ (d }\ControlFlowTok{in} \DecValTok{0}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)}
\NormalTok{    \{}
\NormalTok{        bs\_d }\OtherTok{=}\NormalTok{ splines2}\SpecialCharTok{::}\FunctionTok{bSpline}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\AttributeTok{degree =}\NormalTok{ d, }\AttributeTok{knots =} \FunctionTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{90}\NormalTok{, }\DecValTok{10}\NormalTok{), }\AttributeTok{intercept =} \ConstantTok{TRUE}\NormalTok{)}
        \FunctionTok{matplot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, bs\_d, }\AttributeTok{type =} \FunctionTok{ifelse}\NormalTok{(d }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, }\StringTok{"s"}\NormalTok{, }\StringTok{"l"}\NormalTok{), }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"spline"}\NormalTok{, }
                \AttributeTok{xaxt =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{yaxt =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.05}\NormalTok{, }\FloatTok{1.05}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
        \FunctionTok{title}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"degree ="}\NormalTok{, d))}
\NormalTok{    \}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-126-1} \end{center}

\hypertarget{natural-cubic-spline}{%
\section{Natural Cubic Spline}\label{natural-cubic-spline}}

Extrapolations are generally dangerous because the functions could be extream outside the range of the observed data. In linear models fit by \texttt{bs()}, extrapolations outside the boundaries will trigger a warning.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
    \FunctionTok{library}\NormalTok{(splines)}
\NormalTok{    fit.bs }\OtherTok{=} \FunctionTok{lm}\NormalTok{(Birthrate }\SpecialCharTok{\textasciitilde{}} \FunctionTok{bs}\NormalTok{(Year, }\AttributeTok{df=}\DecValTok{6}\NormalTok{), }\AttributeTok{data=}\NormalTok{birthrates)}
    \FunctionTok{plot}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year, birthrates}\SpecialCharTok{$}\NormalTok{Birthrate, }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{280}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }
         \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1900}\NormalTok{, }\DecValTok{2020}\NormalTok{), }\AttributeTok{xlab =} \StringTok{"year"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"rate"}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)   }
    \FunctionTok{lines}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1900}\NormalTok{, }\DecValTok{2020}\NormalTok{), }\FunctionTok{predict}\NormalTok{(fit.bs, }\FunctionTok{data.frame}\NormalTok{(}\StringTok{"Year"}\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{1900}\NormalTok{, }\DecValTok{2020}\NormalTok{))),}
          \AttributeTok{col=}\StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)    }
\DocumentationTok{\#\# Warning in bs(Year, degree = 3L, knots = c(\textasciigrave{}25\%\textasciigrave{} = 1938.5, \textasciigrave{}50\%\textasciigrave{} = 1960, : some \textquotesingle{}x\textquotesingle{} values beyond}
\DocumentationTok{\#\# boundary knots may cause ill{-}conditioned bases}
    
\NormalTok{    fit.ns }\OtherTok{=} \FunctionTok{lm}\NormalTok{(Birthrate }\SpecialCharTok{\textasciitilde{}} \FunctionTok{ns}\NormalTok{(Year, }\AttributeTok{df=}\DecValTok{6}\NormalTok{), }\AttributeTok{data=}\NormalTok{birthrates)    }
    \FunctionTok{lines}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1900}\NormalTok{, }\DecValTok{2020}\NormalTok{), }\FunctionTok{predict}\NormalTok{(fit.ns, }\FunctionTok{data.frame}\NormalTok{(}\StringTok{"Year"}\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{1900}\NormalTok{, }\DecValTok{2020}\NormalTok{))), }
          \AttributeTok{col=}\StringTok{"darkgreen"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
    \FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Cubic B{-}Spline"}\NormalTok{, }\StringTok{"Natural Cubic Spline"}\NormalTok{), }
           \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkgreen"}\NormalTok{), }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.2}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"Birth rate extrapolation"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-127-1} \end{center}

Hence this motivates us to consider setting additional constrains that forces the extrapolations to be come more regular. This is done by forcing the second and third derivatives to be 0 if beyond the two extreme knots.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\NormalTok{    ncs }\OtherTok{=} \FunctionTok{ns}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\AttributeTok{df =} \DecValTok{6}\NormalTok{, }\AttributeTok{intercept =} \ConstantTok{TRUE}\NormalTok{)}
    \FunctionTok{matplot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, ncs, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"spline"}\NormalTok{,}
            \AttributeTok{xaxt =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{yaxt =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\StringTok{"Natural Cubic Spline"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-128-1} \end{center}

\hypertarget{smoothing-spline}{%
\section{Smoothing Spline}\label{smoothing-spline}}

The motivation is to trying to solve the knots selection issue. Instead, let's start with a ``horrible'' idea, by putting knots at all each observed data point \((x_1, x_2, \ldots, x_n)\). Then, we can create \(n\) natural cubic spline basis. However, we also know that this leads to over-fitting since there are too many parameters. Let's utilize the ridge regression idea by adding some penalties. This leads to the objective function

\begin{equation}
\underset{\boldsymbol \beta}{\text{min}} \,\, \lVert \mathbf{y}- \bF \boldsymbol \beta\rVert^2 + \lambda \boldsymbol \beta^\text{T}\Omega \boldsymbol \beta, \label{eq:penalized}
\end{equation}
where \(\bF\) is the matrix of all \(n\) natural cubic spline basis, and \(\Omega\) is some covariance matrix that takes care of some form of relationship among different basis, which we will define later, and \(\lambda\) is similar to the ridge regression. The question is, will this type of regression problem provide a good solution with nice properties?

Let's consider fitting a regression model by solving a regression function \(g(x)\) with the following penalized criteria:

\begin{equation}
\frac{1}{n} \sum_{i=1}^n \big(y_i - g(x_i)\big)^2 + \lambda \int_a^b \big[g''(x) \big]^2 dx. \label{eq:roughness}
\end{equation}

This is the sum of \(\ell_2\) loss and a roughness penalty that enforce certain smoothness on \(g(x)\). And we shall show that the optimal \(g(x)\) that minimize this objective function will take the ridge penalty form in \eqref{eq:penalized} mentioned previously.

We will consider all absolutely continuous functions on \([a, b] = [\min(x_i), \max(x_i)]\), with finite roughness, i.e., \(\int_a^b \big[g''(x) \big]^2 dx < \infty\). This is known as the second order Sobolev space. Let's first define \(g(\cdot)\) as the optimal solution to Equation \eqref{eq:roughness}. Since the loss part in \eqref{eq:roughness} only involves \(n\) data points, we can find define a natural cubic spline (NCS) fit \(\tilde{g}(\cdot)\) such that it matches with \(g(\cdot)\) on all the observed data, i.e.,

\[g(x_i) = \widetilde{g}(x_i), \quad i = 1, \ldots, n.\]

Note that the roughness of NCS fit \(\widetilde{g}\) is also finite, hence \(\widetilde{g}\) is within the space of functions we are considering. Also, such matching on all observed data points is doable when we have \(n\) basis in the natural cubic spline. In this case, the loss corresponds to \(\tilde{g}\) and \(g\) are identical. Hence, it only matters if the penalty part of \(\tilde{g}(\cdot)\) is the same. To analyze this, we define the difference between these two functions as

\[h(x) = g(x) - \tilde{g}(x).\]
It is then obvious that \(h(x_i) = 0\) for all observed \(i\). Then we have

\[\int g''^2 dx = \int \widetilde{g}''^2 dx + \int h''^2 dx + 2 \int \widetilde{g}'' h'' dx\]
The first and second term on the right hand side are both non-negative. Hence, only the third term matters. WLOG, we assume that \(x_i\)'s are ordered from the smallest to the largest. Then

\begin{align}
\int \tilde{g}'' h'' dx =& ~\tilde{g}'' h' \Big|_a^b - \int_a^b h' \tilde{g}^{(3)} dx \nonumber \\
=&~ 0 - \int_a^b h' \tilde{g}^{(3)} dx \nonumber \\
=&~ - \sum_{i=1}^{n-1} \tilde{g}^{(3)}(x_j^+) \int_{x_j}^{x_{j+1}} h' dx \quad \nonumber \\
=&~ - \sum_{i=1}^{n-1} \tilde{g}^{(3)}(x_j^+) \big(h(x_{j+1}) - h(x_j)\big) \nonumber \\
=&~ 0 
\end{align}

The second equation is because \(\tilde{g}\) is a NCS and suppose to have 0 second derivative on the two boundaries \(a\) and \(b\). The third equation is because \(\tilde{g}\) is at most \(x^3\) on any regions and have constant third derivatives, which we can pull out of the integration. And the last equation is because \(h(x) = 0\) on all \(x_i\)'s.

Hence, this shows that the roughness penalty \[\int \widetilde{g}''^2 dx\] of our NCS solution is no larger than the best solution \(g\). Noticing that \(\widetilde{g}\) is also with the space of functions we are considering, then \(g\) must be our NCS solution.

Hence, \(g\) has a finite sample representation

\[\widehat g(x) = \sum_{j=1}^n \beta_j N_j(x)\]

where \(N_j\)'s are a set of natural cubic spline basis functions with knots at each of the unique \(x\) values. Then Equation \eqref{eq:roughness} becomes

\begin{align}
& \lVert \mathbf{y}- \sum_{j=1}^n \beta_j N_j(x) \rVert^2 + \int \Big( \sum_{j=1}^n \beta_j N_j''(x)\Big)^2 dx \\
=& \lVert \mathbf{y}- \bF\boldsymbol \beta\rVert^2 + \boldsymbol \beta^\text{T}\Omega \boldsymbol \beta,
\end{align}

where \(\bF\) is the design matrix corresponds to the \(n\) NCS basis \(N_j\)'s, and \(\Omega\) is an \(n \times n\) matrix with \(\Omega_{ij} = \int N_i''(x) N_j(x) dx.\) The solution is essentially a ridge solution:

\[\widehat{\boldsymbol \beta} = (\bF^\text{T}\bF + \lambda \Omega)^{-1} \bF^\text{T}\mathbf{y}.\]
and the penalty \(\lambda\) can be tuned using GCV.

\hypertarget{fitting-smoothing-splines}{%
\section{Fitting Smoothing Splines}\label{fitting-smoothing-splines}}

Fitting a smoothing spline can be done by using the \texttt{smooth.spline} package. However, since the birthrate data has little variation in adjacent years, over-fitting is quite severe. The function will automatically use GCV to tune the parameter.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# smoothing spline }
\NormalTok{    fit }\OtherTok{=} \FunctionTok{smooth.spline}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year, birthrates}\SpecialCharTok{$}\NormalTok{Birthrate)}
    \FunctionTok{plot}\NormalTok{(birthrates}\SpecialCharTok{$}\NormalTok{Year, birthrates}\SpecialCharTok{$}\NormalTok{Birthrate, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }
         \AttributeTok{xlab =} \StringTok{"Year"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"BirthRates"}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
    \FunctionTok{lines}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1917}\NormalTok{, }\DecValTok{2003}\NormalTok{), }\FunctionTok{predict}\NormalTok{(fit, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1917}\NormalTok{, }\DecValTok{2003}\NormalTok{))}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{col=}\StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-129-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
    
    \CommentTok{\# the degrees of freedom is very large}
\NormalTok{    fit}\SpecialCharTok{$}\NormalTok{df}
\DocumentationTok{\#\# [1] 60.7691}
\end{Highlighting}
\end{Shaded}

Let's look at another simulation example, where this method performs resonabaly well.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{    n }\OtherTok{=} \DecValTok{100}
\NormalTok{    x }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n)}
\NormalTok{    y }\OtherTok{=} \FunctionTok{sin}\NormalTok{(}\DecValTok{12}\SpecialCharTok{*}\NormalTok{(x}\FloatTok{+0.2}\NormalTok{))}\SpecialCharTok{/}\NormalTok{(x}\FloatTok{+0.2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
    
    \CommentTok{\# fit smoothing spline}
\NormalTok{    fit }\OtherTok{=} \FunctionTok{smooth.spline}\NormalTok{(x, y)}
    
    \CommentTok{\# the degrees of freedom}
\NormalTok{    fit}\SpecialCharTok{$}\NormalTok{df    }
\DocumentationTok{\#\# [1] 9.96443}
    
    \CommentTok{\# fitted model}
    \FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{xlab =} \StringTok{"x"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"y"}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
    \FunctionTok{lines}\NormalTok{(x, }\FunctionTok{sin}\NormalTok{(}\DecValTok{12}\SpecialCharTok{*}\NormalTok{(x}\FloatTok{+0.2}\NormalTok{))}\SpecialCharTok{/}\NormalTok{(x}\FloatTok{+0.2}\NormalTok{), }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)    }
    \FunctionTok{lines}\NormalTok{(x, }\FunctionTok{predict}\NormalTok{(fit, x)}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{col=}\StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
    \FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Truth"}\NormalTok{, }\StringTok{"Smoothing Splines"}\NormalTok{), }
           \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{), }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-130-1} \end{center}

\hypertarget{extending-splines-to-multiple-varibles}{%
\section{Extending Splines to Multiple Varibles}\label{extending-splines-to-multiple-varibles}}

Since all spline approaches can be transformed into some kind of linear model, if we postulate an additive structure, we can fit a multivariate model with

\[f(x) = \sum_j h_j(x_j) = \sum_j \sum_k N_{jk}(x_j) \beta_{jk}\]
where \(h_j(x_j)\) is a univariate function for \(x_j\) that can be approximated by splines basis \(N_{jk}(\cdot), k = 1, \ldots, K\). This works for both linear regression and generalized linear regressions. For the South Africa Heart Disease data, we use the \texttt{gam()} function in the \texttt{gam} (generalized additive models) package. We compute a logistic regression model using natural splines (note \texttt{famhist} is included as a factor).

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(ElemStatLearn)}
    \FunctionTok{library}\NormalTok{(gam)}
\DocumentationTok{\#\# Loading required package: foreach}
\DocumentationTok{\#\# Loaded gam 1.22{-}2}

\NormalTok{    form }\OtherTok{=} \FunctionTok{formula}\NormalTok{(}\StringTok{"chd \textasciitilde{} ns(sbp,df=4) + ns(tobacco,df=4) + }
\StringTok{                          ns(ldl,df=4) + famhist + ns(obesity,df=4) + }
\StringTok{                          ns(alcohol,df=4) + ns(age,df=4)"}\NormalTok{)}
    
    \CommentTok{\# note that we can also do }
    \CommentTok{\# m = glm(form, data=SAheart, family=binomial)}
    \CommentTok{\# print(summary(m), digits=3)}
    \CommentTok{\# however, the gam function provides more information }
    
\NormalTok{    m }\OtherTok{=} \FunctionTok{gam}\NormalTok{(form, }\AttributeTok{data=}\NormalTok{SAheart, }\AttributeTok{family=}\NormalTok{binomial)}
    \FunctionTok{summary}\NormalTok{(m)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Call: gam(formula = form, family = binomial, data = SAheart)}
\DocumentationTok{\#\# Deviance Residuals:}
\DocumentationTok{\#\#     Min      1Q  Median      3Q     Max }
\DocumentationTok{\#\# {-}1.7245 {-}0.8265 {-}0.3884  0.8870  2.9589 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# (Dispersion Parameter for binomial family taken to be 1)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#     Null Deviance: 596.1084 on 461 degrees of freedom}
\DocumentationTok{\#\# Residual Deviance: 457.6318 on 436 degrees of freedom}
\DocumentationTok{\#\# AIC: 509.6318 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Number of Local Scoring Iterations: 6 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Anova for Parametric Effects}
\DocumentationTok{\#\#                      Df Sum Sq Mean Sq F value    Pr(\textgreater{}F)    }
\DocumentationTok{\#\# ns(sbp, df = 4)       4   6.31  1.5783  1.4242  0.224956    }
\DocumentationTok{\#\# ns(tobacco, df = 4)   4  18.09  4.5218  4.0802  0.002941 ** }
\DocumentationTok{\#\# ns(ldl, df = 4)       4  12.05  3.0137  2.7194  0.029290 *  }
\DocumentationTok{\#\# famhist               1  19.70 19.7029 17.7788 3.019e{-}05 ***}
\DocumentationTok{\#\# ns(obesity, df = 4)   4   3.66  0.9161  0.8266  0.508701    }
\DocumentationTok{\#\# ns(alcohol, df = 4)   4   1.28  0.3200  0.2887  0.885278    }
\DocumentationTok{\#\# ns(age, df = 4)       4  17.64  4.4100  3.9794  0.003496 ** }
\DocumentationTok{\#\# Residuals           436 483.19  1.1082                      }
\DocumentationTok{\#\# {-}{-}{-}}
\DocumentationTok{\#\# Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
    
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{))}
    \FunctionTok{plot}\NormalTok{(m, }\AttributeTok{se =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{residuals =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-131-1} \end{center}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{k-neariest-neighber}{%
\chapter{K-Neariest Neighber}\label{k-neariest-neighber}}

\(K\) nearest neighbor (KNN) is a simple nonparametric method. It can be used for both regression and classification problems. However, the idea is quite different from models we introduced before. In a linear model, we have a set of parameters \(\boldsymbol \beta\) and our estimated function value, for any target point \(x_0\) is \(x_0^\text{T}\boldsymbol \beta\). In KNN, we don't really specify these parameters. Instead, we directed estimate \(f(x_0)\). This is traditionally called \textbf{nonparametric models} in statistics. Usually these models perform a local averaging technique to estimate \(f(x_0)\) using observations close to \(x_0\).

\hypertarget{definition-1}{%
\section{Definition}\label{definition-1}}

Suppose we collect a set of observations \(\{x_i, y_i\}_{i=1}^n\), the prediction at a new target point \(x_0\) is

\[\widehat y = \frac{1}{k} \sum_{x_i \in N_k(x_0)} y_i,\]
where \(N_k(x_0)\) defines the \(k\) samples from the training data that are closest to \(x_0\). As default, closeness is defined using a distance measure, such as the Euclidean distance. Here is a demonstration of the fitted regression function.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# generate training data with 2*sin(x) and random Gaussian errors}
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)    }
\NormalTok{    x }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi)}
\NormalTok{    y }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x))}
    
    \CommentTok{\# generate testing data points where we evaluate the prediction function}
\NormalTok{    test.x }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.001}\NormalTok{)}\SpecialCharTok{*}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}

    \CommentTok{\# "1{-}nearest neighbor" regression using kknn package}
    \FunctionTok{library}\NormalTok{(kknn)}
\NormalTok{    knn.fit }\OtherTok{=} \FunctionTok{kknn}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{train =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y), }
                   \AttributeTok{test =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ test.x),}
                   \AttributeTok{k =} \DecValTok{1}\NormalTok{, }\AttributeTok{kernel =} \StringTok{"rectangular"}\NormalTok{)}
\NormalTok{    test.pred }\OtherTok{=}\NormalTok{ knn.fit}\SpecialCharTok{$}\NormalTok{fitted.values}
    
    \CommentTok{\# plot the data}
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{))}
    \FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi), }\AttributeTok{pch =} \StringTok{"o"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{, }
         \AttributeTok{xlab =} \StringTok{""}\NormalTok{, }\AttributeTok{ylab =} \StringTok{""}\NormalTok{, }\AttributeTok{cex.lab =} \FloatTok{1.5}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\AttributeTok{main=}\StringTok{"1{-}Nearest Neighbor Regression"}\NormalTok{, }\AttributeTok{cex.main =} \FloatTok{1.5}\NormalTok{)}
    
    \CommentTok{\# plot the true regression line}
    \FunctionTok{lines}\NormalTok{(test.x, }\DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(test.x), }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
    
    \CommentTok{\# plot the fitted line}
    \FunctionTok{lines}\NormalTok{(test.x, test.pred, }\AttributeTok{type =} \StringTok{"s"}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
    \FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Fitted line"}\NormalTok{, }\StringTok{"True function"}\NormalTok{), }
           \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{), }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-134-1} \end{center}

\hypertarget{tuning-k}{%
\section{\texorpdfstring{Tuning \(k\)}{Tuning k}}\label{tuning-k}}

Tuning \(k\) is crucial for \(k\)NN. Let's observe its effect. This time, I generate 200 observations. For 1NN, the fitted regression line is very jumpy.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# generate more data}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  n }\OtherTok{=} \DecValTok{200}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi)}
\NormalTok{  y }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x))}

\NormalTok{  test.y }\OtherTok{=} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(test.x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(test.x))}
  
  \CommentTok{\# 1{-}nearest neighbor}
\NormalTok{  knn.fit }\OtherTok{=} \FunctionTok{kknn}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{train =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y), }
                 \AttributeTok{test =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ test.x),}
                 \AttributeTok{k =} \DecValTok{1}\NormalTok{, }\AttributeTok{kernel =} \StringTok{"rectangular"}\NormalTok{)}
\NormalTok{  test.pred }\OtherTok{=}\NormalTok{ knn.fit}\SpecialCharTok{$}\NormalTok{fitted.values}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-136-1} \end{center}

We can evaluate the prediction error of this model:

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# prediction error}
  \FunctionTok{mean}\NormalTok{((test.pred }\SpecialCharTok{{-}}\NormalTok{ test.y)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\# [1] 2.097488}
\end{Highlighting}
\end{Shaded}

If we consider different values of \(k\), we can observe the trade-off between bias and variance.

\begin{verbatim}
## Prediction Error for K = 1 : 2.09748780881932
## Prediction Error for K = 5 : 1.39071867992277
## Prediction Error for K = 10 : 1.24696415340282
## Prediction Error for K = 33 : 1.21589627474692
## Prediction Error for K = 66 : 1.37604707375972
## Prediction Error for K = 100 : 1.42868908518756
\end{verbatim}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-138-1} \end{center}

As \(k\) increases, we have a more stable model, i.e., smaller variance. However, the bias is also increased. As \(k\) decreases, the bias decreases, but the model is less stable.

\hypertarget{the-bias-variance-trade-off}{%
\section{The Bias-variance Trade-off}\label{the-bias-variance-trade-off}}

Formally, the prediction error (at a given target point \(x_0\)) can be broke down into three parts: the \textbf{irreducible error}, the \textbf{bias squared}, and the \textbf{variance}.

\begin{aligned}
\text{E}\Big[ \big( Y - \widehat f(x_0) \big)^2 \Big] &= \text{E}\Big[ \big( Y - f(x_0) + f(x_0) -  \text{E}[\widehat f(x_0)] + \text{E}[\widehat f(x_0)] - \widehat f(x_0) \big)^2 \Big] \\
&= \text{E}\Big[ \big( Y - f(x_0) \big)^2 \Big] + \text{E}\Big[ \big(f(x_0) - \text{E}[\widehat f(x_0)] \big)^2 \Big] + \text{E}\Big[ \big(E[\widehat f(x_0)] - \widehat f(x_0) \big)^2 \Big] + \text{Cross Terms}\\
&= \underbrace{\text{E}\Big[ ( Y - f(x_0))^2 \big]}_{\text{Irreducible Error}} +
\underbrace{\Big(f(x_0) - \text{E}[\widehat f(x_0)]\Big)^2}_{\text{Bias}^2} + 
\underbrace{\text{E}\Big[ \big(\widehat f(x_0) - \text{E}[\widehat f(x_0)] \big)^2 \Big]}_{\text{Variance}}
\end{aligned}

As we can see from the previous example, when \(k=1\), the prediction error is about 2. This is because for all the testing points, the theoretical irreducible error is 1 (variance of the error term), the bias is almost 0 since the function is smooth, and the variance is the variance of 1 nearest neighbor, which is again 1. On the other extreme side, when \(k = n\), the variance should be in the level of \(1/n\), the bias is the difference between the sin function and the overall average. Overall, we can expect the trend:

\begin{itemize}
\tightlist
\item
  As \(k\) increases, bias increases and variance decreases
\item
  As \(k\) decreases, bias decreases and variance increases
\end{itemize}

\hypertarget{knn-for-classification}{%
\section{KNN for Classification}\label{knn-for-classification}}

For classification, kNN is different from the regression model in term of finding neighbers. The only difference is to majority voting instead of averaging. Majority voting means that we look for the most popular class label among its neighbors. For 1NN, it is simply the class of the closest neighbor. The visualization of 1NN is a Voronoi tessellation. The plot on the left is some randomly observed data in \([0, 1]^2\), and the plot on the right is the corresponding 1NN classification model.

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-139-1} \end{center}

\hypertarget{example-1-an-artificial-data}{%
\section{Example 1: An artificial data}\label{example-1-an-artificial-data}}

We use artificial data from the \texttt{ElemStatLearn} package.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(ElemStatLearn)}
  
\NormalTok{  x }\OtherTok{\textless{}{-}}\NormalTok{ mixture.example}\SpecialCharTok{$}\NormalTok{x}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ mixture.example}\SpecialCharTok{$}\NormalTok{y}
\NormalTok{  xnew }\OtherTok{\textless{}{-}}\NormalTok{ mixture.example}\SpecialCharTok{$}\NormalTok{xnew}
  
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{))}
  \FunctionTok{plot}\NormalTok{(x, }\AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(y}\SpecialCharTok{==}\DecValTok{1}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{), }
       \AttributeTok{axes =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
  \FunctionTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-141-1} \end{center}

The decision boundary is highly nonlinear. We can utilize the \texttt{contour()} function to demonstrate the result.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# knn classification }
\NormalTok{  k }\OtherTok{=} \DecValTok{15}
\NormalTok{  knn.fit }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(x, xnew, y, }\AttributeTok{k=}\NormalTok{k)}
  
\NormalTok{  px1 }\OtherTok{\textless{}{-}}\NormalTok{ mixture.example}\SpecialCharTok{$}\NormalTok{px1}
\NormalTok{  px2 }\OtherTok{\textless{}{-}}\NormalTok{ mixture.example}\SpecialCharTok{$}\NormalTok{px2}
\NormalTok{  pred }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(knn.fit }\SpecialCharTok{==} \StringTok{"1"}\NormalTok{, }\FunctionTok{length}\NormalTok{(px1), }\FunctionTok{length}\NormalTok{(px2))}
  
  \FunctionTok{contour}\NormalTok{(px1, px2, pred, }\AttributeTok{levels=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{labels=}\StringTok{""}\NormalTok{,}\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{)}
  \FunctionTok{box}\NormalTok{()}
  \FunctionTok{title}\NormalTok{(}\FunctionTok{paste}\NormalTok{(k, }\StringTok{"{-}Nearest Neighbor"}\NormalTok{, }\AttributeTok{sep=} \StringTok{""}\NormalTok{))}
  \FunctionTok{points}\NormalTok{(x, }\AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(y}\SpecialCharTok{==}\DecValTok{1}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
\NormalTok{  mesh }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(px1, px2)}
  \FunctionTok{points}\NormalTok{(mesh, }\AttributeTok{pch=}\StringTok{"."}\NormalTok{, }\AttributeTok{cex=}\FloatTok{1.2}\NormalTok{, }\AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(pred, }\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-142-1} \end{center}

We can evaluate the in-sample prediction result of this model using a confusion matrix:

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# the confusion matrix}
\NormalTok{  knn.fit }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(x, x, y, }\AttributeTok{k =} \DecValTok{15}\NormalTok{)}
\NormalTok{  xtab }\OtherTok{=} \FunctionTok{table}\NormalTok{(knn.fit, y)}
  
  \FunctionTok{library}\NormalTok{(caret)}
  \FunctionTok{confusionMatrix}\NormalTok{(xtab)}
\DocumentationTok{\#\# Confusion Matrix and Statistics}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#        y}
\DocumentationTok{\#\# knn.fit  0  1}
\DocumentationTok{\#\#       0 82 13}
\DocumentationTok{\#\#       1 18 87}
\DocumentationTok{\#\#                                           }
\DocumentationTok{\#\#                Accuracy : 0.845           }
\DocumentationTok{\#\#                  95\% CI : (0.7873, 0.8922)}
\DocumentationTok{\#\#     No Information Rate : 0.5             }
\DocumentationTok{\#\#     P{-}Value [Acc \textgreater{} NIR] : \textless{}2e{-}16          }
\DocumentationTok{\#\#                                           }
\DocumentationTok{\#\#                   Kappa : 0.69            }
\DocumentationTok{\#\#                                           }
\DocumentationTok{\#\#  Mcnemar\textquotesingle{}s Test P{-}Value : 0.4725          }
\DocumentationTok{\#\#                                           }
\DocumentationTok{\#\#             Sensitivity : 0.8200          }
\DocumentationTok{\#\#             Specificity : 0.8700          }
\DocumentationTok{\#\#          Pos Pred Value : 0.8632          }
\DocumentationTok{\#\#          Neg Pred Value : 0.8286          }
\DocumentationTok{\#\#              Prevalence : 0.5000          }
\DocumentationTok{\#\#          Detection Rate : 0.4100          }
\DocumentationTok{\#\#    Detection Prevalence : 0.4750          }
\DocumentationTok{\#\#       Balanced Accuracy : 0.8450          }
\DocumentationTok{\#\#                                           }
\DocumentationTok{\#\#        \textquotesingle{}Positive\textquotesingle{} Class : 0               }
\DocumentationTok{\#\# }
\end{Highlighting}
\end{Shaded}

\hypertarget{tuning-with-the-caret-package}{%
\section{\texorpdfstring{Tuning with the \texttt{caret} Package}{Tuning with the caret Package}}\label{tuning-with-the-caret-package}}

The \texttt{caret} package has some built-in feature that can tune some popular machine learning models using cross-validation. The cross-validation settings need to be specified using the \(trainControl()\) function.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(caret)}
\NormalTok{  control }\OtherTok{\textless{}{-}} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There are other cross-validation methods, such as \texttt{repeatedcv} the repeats the CV several times, and leave-one-out CV \texttt{LOOCV}. For more details, you can read the \href{https://www.rdocumentation.org/packages/caret/versions/6.0-88/topics/trainControl}{documentation}. We can then setup the training by specifying a grid of \(k\) values, and also the CV setup. Make sure that you specify \texttt{method\ =\ "knn"} and also construct the outcome as a factor in a data frame.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  knn.cvfit }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{method =} \StringTok{"knn"}\NormalTok{, }
                     \AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(}\StringTok{"x"} \OtherTok{=}\NormalTok{ x, }\StringTok{"y"} \OtherTok{=} \FunctionTok{as.factor}\NormalTok{(y)),}
                     \AttributeTok{tuneGrid =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{k =} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
                     \AttributeTok{trControl =}\NormalTok{ control)}
  
  \FunctionTok{plot}\NormalTok{(knn.cvfit}\SpecialCharTok{$}\NormalTok{results}\SpecialCharTok{$}\NormalTok{k, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{knn.cvfit}\SpecialCharTok{$}\NormalTok{results}\SpecialCharTok{$}\NormalTok{Accuracy,}
       \AttributeTok{xlab =} \StringTok{"K"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Classification Error"}\NormalTok{, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{,}
       \AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-146-1} \end{center}

Print out the fitted object, we can see that the best \(k\) is 6. And there is a clear ``U'' shaped pattern that shows the potential bias-variance trade-off.

\hypertarget{distance-measures}{%
\section{Distance Measures}\label{distance-measures}}

Closeness between two points needs to be defined based on some distance measures. By default, we use the squared Euclidean distance (\(\ell_2\) norm) for continuous variables:

\[d^2(\mathbf{u}, \mathbf{v}) = \lVert \mathbf{u}- \mathbf{v}\rVert_2^2 = \sum_{j=1}^p (u_j, v_j)^2.\]
However, this measure is not scale invariant. A variable with large scale can dominate this measure. Hence, we often consider a normalized version:

\[d^2(\mathbf{u}, \mathbf{v}) = \sum_{j=1}^p \frac{(u_j, v_j)^2}{\sigma_j^2},\]
where \(\sigma_j^2\) can be estimated using the sample variance of variable \(j\). Another choice that further taking the covariance structure into consideration is the \textbf{Mahalanobis distance}:

\[d^2(\mathbf{u}, \mathbf{v}) = (\mathbf{u}- \mathbf{v})^\text{T}\Sigma^{-1} (\mathbf{u}- \mathbf{v}),\]
where \(\Sigma\) is the covariance matrix, and can be estimated using the sample covariance. In the following plot, the red cross and orange cross have the same Euclidean distance to the center. However, the red cross is more of a ``outlier'' based on the joint distribution. The Mahalanobis distance would reflect this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  x}\OtherTok{=}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{  y}\OtherTok{=}\DecValTok{1} \SpecialCharTok{+} \FloatTok{0.3}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+} \FloatTok{0.3}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}

  \FunctionTok{library}\NormalTok{(car)}
  \FunctionTok{dataEllipse}\NormalTok{(x, y, }\AttributeTok{levels=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.9}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{4}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{4}\NormalTok{, }\AttributeTok{cex =} \DecValTok{3}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-147-1} \end{center}

For categorical variables, the Hamming distance is commonly used:

\[d(\mathbf{u}, \mathbf{v}) = \sum_{j=1}^p I(u_j \neq v_j).\]
It simply counts how many entries are not the same.

\hypertarget{nn-error-bound}{%
\section{1NN Error Bound}\label{nn-error-bound}}

We can show that 1NN can achieve reasonable performance for fixed \(p\), as \(n \rightarrow \infty\) by showing that the 1NN error is no more than twice of the Bayes error, which is the smaller one of \(P(Y = 1 | X = x_0)\) and \(1 - P(Y = 1 | X = x_0)\).

Let's denote \(x_{1nn}\) the closest neighbor of \(x_0\), we have \(d(x_0, x_{1nn}) \rightarrow 0\) by the law of large numbers. Assuming smoothness, we have \(P(Y = 1 | x_{1nn}) \rightarrow P(Y = 1 | x_0)\). Hence, the 1NN error is the chance we make a wrong prediction, which can happen in two situations. WLOG, let's assume that \(P(Y = 1 | X = x_0)\) is larger than \(1-P(Y = 1 | X = x_0)\), then

\begin{align}
& \,P(Y=1|x_0)[ 1 - P(Y=1|x_{1nn})] + P(Y=1|x_0)[ 1 - P(Y=1|x_{1nn})]\\
\leq & \,[ 1 - P(Y=1|x_{1nn})] + [ 1 - P(Y=1|x_{1nn})]\\
\rightarrow & \, 2[ 1 - P(Y=1|x_0)]\\
= & \,2 \times \text{Bayes Error}\\
\end{align}

This is a crude bound, but shows that 1NN can still be a reasonable estimator when the noise is small (Bayes error small).

\hypertarget{example-2-handwritten-digit-data}{%
\section{Example 2: Handwritten Digit Data}\label{example-2-handwritten-digit-data}}

Let's consider another example using the handwritten digit data. Each observation in this data is a \(16 \times 16\) pixel image. Hence, the total number of variables is \(256\). At each pixel, we have the gray scale as the numerical value.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# Handwritten Digit Recognition Data}
  \FunctionTok{library}\NormalTok{(ElemStatLearn)}
  \CommentTok{\# the first column is the true digit}
  \FunctionTok{dim}\NormalTok{(zip.train)}
\DocumentationTok{\#\# [1] 7291  257}
  \FunctionTok{dim}\NormalTok{(zip.test)}
\DocumentationTok{\#\# [1] 2007  257}
  
  \CommentTok{\# look at one sample}
  \FunctionTok{image}\NormalTok{(}\FunctionTok{zip2image}\NormalTok{(zip.train, }\DecValTok{1}\NormalTok{), }\AttributeTok{col=}\FunctionTok{gray}\NormalTok{(}\DecValTok{256}\SpecialCharTok{:}\DecValTok{0}\SpecialCharTok{/}\DecValTok{256}\NormalTok{), }\AttributeTok{zlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }
        \AttributeTok{xlab=}\StringTok{""}\NormalTok{, }\AttributeTok{ylab=}\StringTok{""}\NormalTok{, }\AttributeTok{axes =} \ConstantTok{FALSE}\NormalTok{)}
\DocumentationTok{\#\# [1] "digit  6  taken"}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.2\linewidth]{SMLR_files/figure-latex/unnamed-chunk-148-1} \end{center}

We use 3NN to predict all samples in the testing data. The model is fairly accurate.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# fit 3nn model and calculate the error}
\NormalTok{  knn.fit }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(zip.train[, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{257}\NormalTok{], zip.test[, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{257}\NormalTok{], zip.train[, }\DecValTok{1}\NormalTok{], }\AttributeTok{k=}\DecValTok{3}\NormalTok{)}
  
  \CommentTok{\# overall prediction error}
  \FunctionTok{mean}\NormalTok{(knn.fit }\SpecialCharTok{!=}\NormalTok{ zip.test[,}\DecValTok{1}\NormalTok{])}
\DocumentationTok{\#\# [1] 0.05430992}
  
  \CommentTok{\# the confusion matrix}
  \FunctionTok{table}\NormalTok{(knn.fit, zip.test[,}\DecValTok{1}\NormalTok{])}
\DocumentationTok{\#\#        }
\DocumentationTok{\#\# knn.fit   0   1   2   3   4   5   6   7   8   9}
\DocumentationTok{\#\#       0 355   0   6   1   0   3   3   0   4   2}
\DocumentationTok{\#\#       1   0 258   0   0   2   0   0   1   0   0}
\DocumentationTok{\#\#       2   1   0 182   2   0   2   1   1   2   0}
\DocumentationTok{\#\#       3   0   0   1 153   0   4   0   1   5   0}
\DocumentationTok{\#\#       4   0   3   1   0 183   0   2   4   0   3}
\DocumentationTok{\#\#       5   0   0   0   7   2 146   0   0   1   0}
\DocumentationTok{\#\#       6   0   2   2   0   2   0 164   0   0   0}
\DocumentationTok{\#\#       7   2   1   2   1   2   0   0 138   1   4}
\DocumentationTok{\#\#       8   0   0   4   0   1   1   0   1 151   0}
\DocumentationTok{\#\#       9   1   0   0   2   8   4   0   1   2 168}
\end{Highlighting}
\end{Shaded}

\hypertarget{curse-of-dimensionality}{%
\section{Curse of Dimensionality}\label{curse-of-dimensionality}}

Many of the practical problems we encounter today are high-dimensional. The resolution of the handwritten digit example is \(16 \times 16 = 256\). Genetic studies often involves more than 25K gene expressions, etc. For a given sample size \(n\), as the number of variables \(p\) increases, the data becomes very sparse. Nearest neighbor methods usually do not perform very well on high-dimensional data. This is because for any given target point, there will not be enough training data that lies close enough. To see this, let's consider a \(p\)-dimensional hyper-cube. Suppose we have \(n=1000\) observations uniformly spread out in this cube, and we are interested in predicting a target point with \(k=10\) neighbors. If these neighbors are really close to the target point, then this would be a good estimation with small bias. Suppose \(p=2\), then we know that if we take a cube (square) with height and width both \(l = 0.1\), then there will be \(1000 \times 0.1^2 = 10\) observations within the square. In general, we have the relationship

\[l^p = \frac{k}{n}\]
Try different \(p\), we have

\begin{itemize}
\tightlist
\item
  If \(p = 2\), \(l = 0.1\)
\item
  If \(p = 10\), \(l = 0.63\)
\item
  If \(p = 100\), \(l = 0.955\)
\end{itemize}

This implies that if we have 100 dimensions, then the nearest 10 observations would be 0.955 away from the target point at each dimension, this is almost at the other corner of the cube. Hence there will be a very large bias. Decreasing \(k\) does not help much in this situation since even the closest point can be really far away, and the model would have large variance.

\includegraphics[width=0.3\textwidth,height=\textheight]{images/highd.png}

However, why our model performs well in the handwritten digit example? There is possibly (approximately) a lower dimensional representation of the data so that when you evaluate the distance on the high-dimensional space, it is just as effective as working on the low dimensional space. Dimension reduction is an important topic in statistical learning and machine learning. Many methods, such as sliced inverse regression (\protect\hyperlink{ref-li1991sliced}{Li 1991}) and UMAP (\protect\hyperlink{ref-mcinnes2018umap}{McInnes, Healy, and Melville 2018}) have been developed based on this concept.

\includegraphics[width=0.7\textwidth,height=\textheight]{images/manifold.png}

Image from Cayton (\protect\hyperlink{ref-cayton2005algorithms}{2005}).

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{kernel-smoothing}{%
\chapter{Kernel Smoothing}\label{kernel-smoothing}}

Fundamental ideas of local regression approaches are similar to \(k\)NN. But most approaches would address a fundamental drawback of \(k\)NN that the estimated function is not smooth. Having a smoothed estimation would also allow us to estimate the derivative, which is essentially used when estimating the density function. We will start with the intuition of the kernel estimator and then discuss the bias-variance trade-off using kernel density estimation as an example.

\hypertarget{knn-vs.-kernel}{%
\section{KNN vs.~Kernel}\label{knn-vs.-kernel}}

We first compare the \(K\)NN method with a Gaussian kernel regression. \(K\)NN has jumps while Gaussian kernel regression is smooth.

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-153-1} \end{center}

\hypertarget{kernel-density-estimations}{%
\section{Kernel Density Estimations}\label{kernel-density-estimations}}

A natural estimator, by using the counts, is

\[\widehat f(x) = \frac{\#\big\{x_i: x_i \in [x - \frac{h}{2}, x + \frac{h}{2}]\big\}}{h n}\]
This maybe compared with the histogram estimator

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(ggplot2)}
    \FunctionTok{data}\NormalTok{(mpg)}
    
    \CommentTok{\# histogram }
    \FunctionTok{hist}\NormalTok{(mpg}\SpecialCharTok{$}\NormalTok{hwy, }\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{2}\NormalTok{))}
    
    \CommentTok{\# uniform kernel}
\NormalTok{    xgrid }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{50}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{    histden }\OtherTok{=} \FunctionTok{sapply}\NormalTok{(xgrid, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(x, obs, h) }\FunctionTok{sum}\NormalTok{( ((x}\SpecialCharTok{{-}}\NormalTok{h}\SpecialCharTok{/}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\textless{}=}\NormalTok{ obs) }\SpecialCharTok{*}\NormalTok{ ((x}\SpecialCharTok{+}\NormalTok{h}\SpecialCharTok{/}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\textgreater{}}\NormalTok{ obs))}\SpecialCharTok{/}\NormalTok{h}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(obs), }
                     \AttributeTok{obs =}\NormalTok{ mpg}\SpecialCharTok{$}\NormalTok{hwy, }\AttributeTok{h =} \DecValTok{2}\NormalTok{)}
    
    \FunctionTok{plot}\NormalTok{(xgrid, histden, }\AttributeTok{type =} \StringTok{"s"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-154-1} \end{center}

This can be view in two ways. The easier interpretation is that, for each target point, we count how many observations are close-by. We can also interpret it as evenly distributing the point-mass of each observation to a close-by region with width \(h\), and then stack them all.

\[\widehat f(x) = \frac{1}{h n} \sum_i \,\underbrace{ \mathbf{1} \Big(x \in [x_i - \frac{h}{2}, x_i + \frac{h}{2}]}_{\text{uniform density centered at } x_i} \Big)\]
Here is a close-up demonstration of how those uniform density functions are stacked for all observations.

\begin{center}\includegraphics[width=0.55\linewidth]{SMLR_files/figure-latex/unnamed-chunk-156-1} \end{center}

However, this is will lead to jumps at the end of each small uniform density. Let's consider using a smooth function instead. Naturally, we can use the Gaussian kernel function to calculate the numerator in the above equation.

\begin{center}\includegraphics[width=0.55\linewidth]{SMLR_files/figure-latex/unnamed-chunk-157-1} \end{center}

We apply this to the \texttt{mpg} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  xgrid }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{50}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{  kernelfun }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, obs, h) }\FunctionTok{sum}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{((x}\SpecialCharTok{{-}}\NormalTok{obs)}\SpecialCharTok{/}\NormalTok{h)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi)}\SpecialCharTok{/}\NormalTok{h)}
  \FunctionTok{plot}\NormalTok{(xgrid, }\FunctionTok{sapply}\NormalTok{(xgrid, }\AttributeTok{FUN =}\NormalTok{ kernelfun, }\AttributeTok{obs =}\NormalTok{ mpg}\SpecialCharTok{$}\NormalTok{hwy, }\AttributeTok{h =} \FloatTok{1.5}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(mpg}\SpecialCharTok{$}\NormalTok{hwy), }\AttributeTok{type =} \StringTok{"l"}\NormalTok{,}
       \AttributeTok{xlab =} \StringTok{"MPG"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Estimated Density"}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.55\linewidth]{SMLR_files/figure-latex/unnamed-chunk-158-1} \end{center}

The \texttt{ggplot2} packages provides some convenient features to plot the density and histogram.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{ggplot}\NormalTok{(mpg, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{hwy)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y=}\NormalTok{..density..), }\AttributeTok{colour=}\StringTok{"black"}\NormalTok{, }\AttributeTok{fill=}\StringTok{"white"}\NormalTok{)}\SpecialCharTok{+}
    \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha=}\NormalTok{.}\DecValTok{2}\NormalTok{, }\AttributeTok{fill=}\StringTok{"\#ff8c00"}\NormalTok{)}
\DocumentationTok{\#\# Warning: The dot{-}dot notation (\textasciigrave{}..density..\textasciigrave{}) was deprecated in ggplot2 3.4.0.}
\DocumentationTok{\#\# i Please use \textasciigrave{}after\_stat(density)\textasciigrave{} instead.}
\DocumentationTok{\#\# This warning is displayed once every 8 hours.}
\DocumentationTok{\#\# Call \textasciigrave{}lifecycle::last\_lifecycle\_warnings()\textasciigrave{} to see where this warning was generated.}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-159-1} \end{center}

\hypertarget{bias-variance-trade-off}{%
\section{Bias-variance trade-off}\label{bias-variance-trade-off}}

Let's consider estimating a density, using the Parzen estimator

\[\widehat f(x) = \frac{1}{n} \sum_{i=1}^n K_{h} (x, x_i)\]
here, \(K_h(\mathbf{u}, \mathbf{v}) = K(|\mathbf{u}- \mathbf{v}|/h)/h\) is a kernel function that satisfies

\begin{itemize}
\tightlist
\item
  \(\int K(u)du = 1\) (a proper density)
\item
  \(K(-u) = K(u)\) (symmetric)
\item
  \(\int K(u) u^2 du \leq \infty\) (finite second moment)
\end{itemize}

Note that \(h\) simply scales the covariate and adjust the density accordingly. Our goal is to estimate a target point \(x\) using a set of iid data. First, we can analyze the bias:

\begin{align}
    \text{E}\big[ \widehat f(x) \big] &= \text{E}\left[ K\left( \frac{x - x_1}{h} \right) \Big/ h \right] \\
    &= \int_{-\infty}^\infty \frac{1}{h} K\left(\frac{x-x_1}{h}\right) f(x_1) d x_1 \\
    &= \int_{\infty}^{-\infty} \frac{1}{h} K(t) f(x - th) d (x-th) \\
    (\text{Taylor expansion}) \quad &= f(x) + \frac{h^2}{2} f''(x) \int_{-\infty}^\infty K(t) t^2 dt + o(h^2) \\
    (\text{as } ha \rightarrow 0) \quad &\rightarrow f(x)
\end{align}

Since the density is over the entire domain, we can define the integrated Bias\(^2\):

\begin{align}
\text{Bias}^2 &= \int \left( E[\widehat f(x)] - f(x)\right)^2 dx \\
    &\approx \frac{h^4 \sigma_K^4}{4} \int \big[ f''(x)\big]^2 dx
\end{align}
where \(\sigma_K^2 = \int_{-\infty}^\infty K(t) t^2 dt\).

On the other hand, the variance term is

\begin{align}
  \text{Var}\big[ \widehat f(x) \big] &= \frac{1}{n} \text{Var}\Big[\frac{1}{h}K\big( \frac{x - x_1}{h} \big) \Big] \\
  &= \frac{1}{n} \text{E}\bigg[ \frac{1}{h^2} K^2\big( \frac{x - x_1}{h}\big) - \text{E}\Big[ \frac{1}{h} K\big( \frac{x - x_1}{h} \big)\Big]^2 \bigg]\\
  &= \frac{1}{n} \Big[ \int \frac{1}{h} K^2( \frac{x - x_1}{h} ) f(x_1) dx_1 + O(1) \Big] \\
  &= \frac{1}{n} \Big[ \frac{1}{h} \int K^2( u ) f(x) du + O(1) \Big] \\
  &= \frac{f(x)}{nh} \int K^2( u ) du 
\end{align}

with the integrated variance being

\[\frac{1}{nh} \int K^2( u ) dt \]

By minimizing the asymptotic mean integrated squared error (AMISE), defined as the sum of integrated Bias\(^2\) and variance, we have the optimal \(h\) being

\[h^\text{opt} = \bigg[\frac{1}{n} \frac{\int K^2(u)du}{ \sigma^2_K \int f''(u)du} \bigg]^{1/5},\]

and the optimal \(h\) is in the order of \(\cal O(n^{-4/5})\).

\hypertarget{gaussian-kernel-regression}{%
\section{Gaussian Kernel Regression}\label{gaussian-kernel-regression}}

A Nadaraya-Watson kernel regression model has the following formula. Note that we use \(h\) as the bandwidth instead of \(h\).

\[\widehat f(x) = \frac{\sum_i K_h(x, x_i) y_i}{\sum_i K_h(x, x_i)},\]
where \(h\) is the bandwidth. At each target point \(x\), training data \(x_i\)s that are closer to \(x\) receives higher weights \(K_h(x, x_i)\), hence their \(y_i\) values are more influential in terms of estimating \(f(x)\). For the Gaussian kernel, we use

\[K_h(x, x_i) = \frac{1}{h\sqrt{2\pi}} \exp\left\{ -\frac{(x - x_i)^2}{2 h^2}\right\}\]

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-160-1} \end{center}

\hypertarget{bias-variance-trade-off-1}{%
\subsection{Bias-variance Trade-off}\label{bias-variance-trade-off-1}}

The bandwidth \(h\) is an important tuning parameter that controls the bias-variance trade-off. It behaves the same as the density estimation. By setting a large \(h\), the estimator is more stable but has more bias.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# a small bandwidth}
\NormalTok{  ksmooth.fit1 }\OtherTok{=} \FunctionTok{ksmooth}\NormalTok{(x, y, }\AttributeTok{bandwidth =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{kernel =} \StringTok{"normal"}\NormalTok{, }\AttributeTok{x.points =}\NormalTok{ testx)}

  \CommentTok{\# a large bandwidth}
\NormalTok{  ksmooth.fit2 }\OtherTok{=} \FunctionTok{ksmooth}\NormalTok{(x, y, }\AttributeTok{bandwidth =} \DecValTok{2}\NormalTok{, }\AttributeTok{kernel =} \StringTok{"normal"}\NormalTok{, }\AttributeTok{x.points =}\NormalTok{ testx)}
  
  \CommentTok{\# plot both}
  \FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{xaxt=}\StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{yaxt=}\StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{)}
  \FunctionTok{lines}\NormalTok{(testx, }\DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(testx), }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
  \FunctionTok{lines}\NormalTok{(testx, ksmooth.fit1}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{type =} \StringTok{"s"}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
  \FunctionTok{lines}\NormalTok{(testx, ksmooth.fit2}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{type =} \StringTok{"s"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
  \FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"h = 0.5"}\NormalTok{, }\StringTok{"h = 2"}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"darkorange"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }
         \AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-162-1} \end{center}

\hypertarget{choice-of-kernel-functions}{%
\section{Choice of Kernel Functions}\label{choice-of-kernel-functions}}

Other kernel functions can also be used. The most efficient kernel is the Epanechnikov kernel, which will minimize the mean integrated squared error (MISE). The efficiency is defined as

\[ \Big(\int u^2K(u) du\Big)^\frac{1}{2}  \int K^2(u) du, \]
Different kernel functions can be visualized in the following. Most kernels are bounded within \([-h/2, h/2]\), except the Gaussian kernel.

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-163-1} \end{center}

\hypertarget{local-linear-regression}{%
\section{Local Linear Regression}\label{local-linear-regression}}

Local averaging will suffer severe bias at the boundaries. One solution is to use the local polynomial regression. The following examples are local linear regressions, evaluated as different target points. We are solving for a linear model weighted by the kernel weights

\[\sum_{i = 1}^n K_h(x, x_i) \big( y_i - \beta_0 - \beta_1 x_i \big)^2\]

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-164-1} \end{center}

\hypertarget{local-polynomial-regression}{%
\section{Local Polynomial Regression}\label{local-polynomial-regression}}

The following examples are local polynomial regressions, evaluated as different target points. We can easily extend the local linear model to inccorperate higher orders terms:

\[\sum_{i=1}^n K_h(x, x_i) \Big[ y_i - \beta_0(x) - \sum_{r=1}^d \beta_j(x) x_i^r \Big]^2\]

The followings are local quadratic fittings, which will further correct the bias.

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-166-1} \end{center}

\hypertarget{r-implementations}{%
\section{R Implementations}\label{r-implementations}}

Some popular \texttt{R} functions implements the local polynomial regressions: \texttt{loess}, \texttt{locfit}, \texttt{locploy}, etc. These functions automatically calculate the fitted value for each target point (essentially all the observed points). This can be used in combination with \texttt{ggplot2}. The point-wise confidence intervals are also calculated.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{ggplot}\NormalTok{(mpg, }\FunctionTok{aes}\NormalTok{(displ, hwy)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
      \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{method =} \StringTok{"loess"}\NormalTok{, }\AttributeTok{span =} \FloatTok{0.5}\NormalTok{)}
\DocumentationTok{\#\# \textasciigrave{}geom\_smooth()\textasciigrave{} using formula = \textquotesingle{}y \textasciitilde{} x\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-167-1} \end{center}

A toy example that compares different bandwidth. Be careful that different methods may formulat the bandwidth parameter in different ways.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# local polynomial fitting using locfit and locpoly}
    
    \FunctionTok{library}\NormalTok{(KernSmooth)}
    \FunctionTok{library}\NormalTok{(locfit)}
    
\NormalTok{    n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{    x }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{    y }\OtherTok{\textless{}{-}} \FunctionTok{sin}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{x)}\SpecialCharTok{+}\FunctionTok{rnorm}\NormalTok{(n,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{    y }\OtherTok{=}\NormalTok{ y[}\FunctionTok{order}\NormalTok{(x)]}
\NormalTok{    x }\OtherTok{=} \FunctionTok{sort}\NormalTok{(x)}
    
    \FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
    \FunctionTok{points}\NormalTok{(x, }\FunctionTok{sin}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{x), }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{col =} \DecValTok{1}\NormalTok{)}
    \FunctionTok{lines}\NormalTok{(}\FunctionTok{locpoly}\NormalTok{(x, y, }\AttributeTok{bandwidth=}\FloatTok{0.15}\NormalTok{, }\AttributeTok{degree=}\DecValTok{2}\NormalTok{), }\AttributeTok{col=}\DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
    \FunctionTok{lines}\NormalTok{(}\FunctionTok{locfit}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\FunctionTok{lp}\NormalTok{(x, }\AttributeTok{nn =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{h=}\FloatTok{0.05}\NormalTok{, }\AttributeTok{deg=}\DecValTok{2}\NormalTok{)), }\AttributeTok{col=}\DecValTok{4}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
    
    \FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"locpoly"}\NormalTok{, }\StringTok{"locfit"}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{), }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{, }\AttributeTok{lwd =}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-169-1} \end{center}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{part-classification-models}{%
\part{Classification Models}\label{part-classification-models}}

\hypertarget{logistic-regression}{%
\chapter{Logistic Regression}\label{logistic-regression}}

\hypertarget{modeling-binary-outcomes}{%
\section{Modeling Binary Outcomes}\label{modeling-binary-outcomes}}

To model binary outcomes using a logistic regression, we will use the 0/1 coding of \(Y\). We need to set its connection with covariates. Recall in a linear regression, the outcome is continuous, and we set

\[Y = \beta_0 + \beta_1 X + \epsilon\]
However, this does not work for classification since \(Y\) can only be 0 or 1. Hence we turn to consider modeling the probability \(P(Y = 1 | X = \mathbf{x})\). Hence, \(Y\) is a Bernoulli random variable given \(X\), and this is modeled by a function of \(X\):

\[ P(Y = 1 | X = \mathbf{x}) = \frac{\exp(\mathbf{x}^\text{T}\boldsymbol \beta)}{1 + \exp(\mathbf{x}^\text{T}\boldsymbol \beta)}\]
Note that although \(\mathbf{x}^\text{T}\boldsymbol \beta\) may ranges from 0 to infinity as \(X\) changes, the probability will still be bounded between 0 and 1. This is an example of \textbf{Generalized Linear Models}. The relationship is still represented using a linear function of \(\mathbf{x}\), \(\mathbf{x}^\text{T}\boldsymbol \beta\). This is called a \textbf{logit link} function (a function to connect the conditional expectation of \(Y\) with \(\boldsymbol \beta^\text{T}\mathbf{x}\)):

\[\eta(a) = \frac{\exp(a)}{1 + \exp(a)}\]
Hence, \(P(Y = 1 | X = \mathbf{x}) = \eta(\mathbf{x}^\text{T}\boldsymbol \beta)\). We can reversely solve this and get

\begin{aligned}
P(Y = 1 | X = \mathbf{x}) = \eta(\mathbf{x}^\text{T}\boldsymbol \beta) &= \frac{\exp(\mathbf{x}^\text{T}\boldsymbol \beta)}{1 + \exp(\mathbf{x}^\text{T}\boldsymbol \beta)}\\
1 - \eta(\mathbf{x}^\text{T}\boldsymbol \beta) &= \frac{1}{1 + \exp(\mathbf{x}^\text{T}\boldsymbol \beta)} \\
\text{Odds} = \frac{\eta(\mathbf{x}^\text{T}\boldsymbol \beta)}{1-\eta(\mathbf{x}^\text{T}\boldsymbol \beta)} &= \exp(\mathbf{x}^\text{T}\boldsymbol \beta)\\
\log(\text{Odds}) = \mathbf{x}^\text{T}\boldsymbol \beta
\end{aligned}

Hence, the parameters in a logistic regression is explained as \textbf{log odds}. Let's look at a concrete example.

\hypertarget{example-cleveland-clinic-heart-disease-data}{%
\section{Example: Cleveland Clinic Heart Disease Data}\label{example-cleveland-clinic-heart-disease-data}}

We use use the \href{https://www.kaggle.com/aavigan/cleveland-clinic-heart-disease-dataset}{Cleveland clinic heart disease dataset}. The goal is to model and predict a class label of whether the patient has a hearth disease or not. This is indicated by whether the \texttt{num} variable is \(0\) (no presence) or \(>0\) (presence).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  heart }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/processed\_cleveland.csv"}\NormalTok{)}
\NormalTok{  heart}\SpecialCharTok{$}\NormalTok{Y }\OtherTok{=} \FunctionTok{as.factor}\NormalTok{(heart}\SpecialCharTok{$}\NormalTok{num }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}
  \FunctionTok{table}\NormalTok{(heart}\SpecialCharTok{$}\NormalTok{Y)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# FALSE  TRUE }
\DocumentationTok{\#\#   164   139}
\end{Highlighting}
\end{Shaded}

Let's model the probability of heart disease using the \texttt{Age} variable. This can be done using the \texttt{glm()} function, which stands for the Generalized Linear Model. The syntax of \texttt{glm()} is almost the same as a linear model. Note that it is important to use \texttt{family\ =\ binomial} to specify the logistic regression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  logistic.fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{age, }\AttributeTok{data =}\NormalTok{ heart, }\AttributeTok{family =}\NormalTok{ binomial)}
  \FunctionTok{summary}\NormalTok{(logistic.fit)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Call:}
\DocumentationTok{\#\# glm(formula = Y \textasciitilde{} age, family = binomial, data = heart)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Deviance Residuals: }
\DocumentationTok{\#\#    Min      1Q  Median      3Q     Max  }
\DocumentationTok{\#\# {-}1.596  {-}1.073  {-}0.835   1.173   1.705  }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Coefficients:}
\DocumentationTok{\#\#             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\DocumentationTok{\#\# (Intercept) {-}3.00591    0.75913  {-}3.960  7.5e{-}05 ***}
\DocumentationTok{\#\# age          0.05199    0.01367   3.803 0.000143 ***}
\DocumentationTok{\#\# {-}{-}{-}}
\DocumentationTok{\#\# Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# (Dispersion parameter for binomial family taken to be 1)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#     Null deviance: 417.98  on 302  degrees of freedom}
\DocumentationTok{\#\# Residual deviance: 402.54  on 301  degrees of freedom}
\DocumentationTok{\#\# AIC: 406.54}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Number of Fisher Scoring iterations: 4}
\end{Highlighting}
\end{Shaded}

The result is similar to a linear regression, with some differences. The parameter estimate of age is 0.05199. It is positive, meaning that increasing age would increase the change of having heart disease. However, this does not mean that 1 year older would increase the change by 0.05. Since, by our previous formula, the probably is not directly expressed as \(\mathbf{x}^\text{T}\boldsymbol \beta\).

This calculation can be realized when predicting a new target point. Let's consider a new subject with \texttt{Age\ =\ 55}. What is the predicted probability of heart disease? Based on our formula, we have

\[\beta_0 + \beta_1 X = -3.00591 + 0.05199 \times 55 = -0.14646\]
And the estimated probability is

\[ P(Y = 1 | X) = \frac{\exp(\beta_0 + \beta_1 X)}{1 + \exp(\beta_0 + \beta_1 X)} = \frac{\exp(-0.14646)}{1 + \exp(-0.14646)} = 0.4634503\]
Hence, the estimated probability for this subject is 46.3\%. This can be done using R code. Please note that if you want to predict the probability, you need to specify \texttt{type\ =\ "response"}. Otherwise, only \(\beta_0 + \beta_1 X\) is provided.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  testdata }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\StringTok{"age"} \OtherTok{=} \DecValTok{55}\NormalTok{)}
  \FunctionTok{predict}\NormalTok{(logistic.fit, }\AttributeTok{newdata =}\NormalTok{ testdata)}
\DocumentationTok{\#\#          1 }
\DocumentationTok{\#\# {-}0.1466722}
  \FunctionTok{predict}\NormalTok{(logistic.fit, }\AttributeTok{newdata =}\NormalTok{ testdata, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\DocumentationTok{\#\#         1 }
\DocumentationTok{\#\# 0.4633975}
\end{Highlighting}
\end{Shaded}

If we need to make a 0/1 decision about this subject, a natural idea is to see if the predicted probability is greater than 0.5. In this case, we would predict this subject as 0.

\hypertarget{interpretation-of-the-parameters}{%
\section{Interpretation of the Parameters}\label{interpretation-of-the-parameters}}

Recall that \(\mathbf{x}^\text{T}\boldsymbol \beta\) is the log odds, we can further interpret the effect of a single variable. Let's define the following two, with an arbitrary age value \(a\):

\begin{itemize}
\tightlist
\item
  A subject with \texttt{age} \(= a\)
\item
  A subject with \texttt{age} \(= a + 1\)
\end{itemize}

Then, if we look at the \textbf{odds ratio} corresponding to these two target points, we have

\begin{aligned}
\text{Odds Ratio} &= \frac{\text{Odds in Group 2}}{\text{Odds in Group 1}}\\
&= \frac{\exp(\beta_0 + \beta_1 (a+1))}{\exp(\beta_0 + \beta_1 a)}\\
&= \frac{\exp(\beta_0 + \beta_1 a) \times \exp(\beta_1)}{\exp(\beta_0 + \beta_1 a)}\\
&= \exp(\beta_1)
\end{aligned}

Taking \(\log\) on both sides, we have

\[\log(\text{Odds Ratio}) = \beta_1\]

Hence, the odds ratio between these two subjects (\textbf{they differ only with one unit of \texttt{age}}) can be directly interpreted as the exponential of the parameter of \texttt{age}. After taking the log, we can also say that

\begin{quote}
The parameter \(\beta\) of a varaible in a logistic regression represents the \textbf{log of odds ratio} associated with one-unit increase of this variable.
\end{quote}

Please note that we usually do not be explicit about what this odds ratio is about (what two subject we are comparing). Because the interpretation of the parameter does not change regardless of the value \(a\), as long as the two subjects differ in one unit.

And also note that this conclusion is regardless of the values of other covaraites. When we have a multivariate model, as long as all other covariates are held the same, the previous derivation will remain the same.

\hypertarget{solving-a-logistic-regression}{%
\section{Solving a Logistic Regression}\label{solving-a-logistic-regression}}

The logistic regression is solved by maximizing the log-likelihood function. Note that the log-likelihood is given by

\[\ell(\boldsymbol \beta) = \sum_{i=1}^n \log \, p(y_i | x_i, \boldsymbol \beta).\]
Using the probabilities of Bernoulli distribution, we have

\begin{align}
\ell(\boldsymbol \beta) =& \sum_{i=1}^n \log \left\{ \eta(\mathbf{x}_i)^{y_i} [1-\eta(\mathbf{x}_i)]^{1-y_i} \right\}\\
    =& \sum_{i=1}^n y_i \log \frac{\eta(\mathbf{x}_i)}{1-\eta(\mathbf{x}_i)} + \log [1-\eta(\mathbf{x}_i)] \\
    =& \sum_{i=1}^n y_i \mathbf{x}_i^\text{T}\boldsymbol \beta- \log [ 1 + \exp(\mathbf{x}_i^\text{T}\boldsymbol \beta)]
\end{align}

Since this objective function is relatively simple, we can use Newton's method to update. The gradient is given by

\[\frac{\partial \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta} =~ \sum_{i=1}^n y_i \mathbf{x}_i^\text{T}- \sum_{i=1}^n \frac{\exp(\mathbf{x}_i^\text{T}\boldsymbol \beta) \mathbf{x}_i^\text{T}}{1 + \exp(\mathbf{x}_i^\text{T}\boldsymbol \beta)},\]

and the Hessian matrix is given by

\[\frac{\partial^2 \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta\partial \boldsymbol \beta^\text{T}} =~ - \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^\text{T}\eta(\mathbf{x}_i) [1- \eta(\mathbf{x}_i)].\]
This leads to the update

\[\boldsymbol \beta^{\,\text{new}} = \boldsymbol \beta^{\,\text{old}} - \left[\frac{\partial^2 \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta\partial \boldsymbol \beta^\text{T}}\right]^{-1} \frac{\partial \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta}\]

\hypertarget{example-south-africa-heart-data}{%
\section{Example: South Africa Heart Data}\label{example-south-africa-heart-data}}

We use the South Africa heart data as a demonstration. The goal is to estimate the probability of \texttt{chd}, the indicator of coronary heart disease.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(ElemStatLearn)}
    \FunctionTok{data}\NormalTok{(SAheart)}
    
\NormalTok{    heart }\OtherTok{=}\NormalTok{ SAheart}
\NormalTok{    heart}\SpecialCharTok{$}\NormalTok{famhist }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(heart}\SpecialCharTok{$}\NormalTok{famhist)}\SpecialCharTok{{-}}\DecValTok{1}
\NormalTok{    n }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(heart)}
\NormalTok{    p }\OtherTok{=} \FunctionTok{ncol}\NormalTok{(heart)}
    
\NormalTok{    heart.full }\OtherTok{=} \FunctionTok{glm}\NormalTok{(chd}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data=}\NormalTok{heart, }\AttributeTok{family=}\NormalTok{binomial)}
    \FunctionTok{round}\NormalTok{(}\FunctionTok{summary}\NormalTok{(heart.full)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{dig=}\DecValTok{3}\NormalTok{)}
\DocumentationTok{\#\#             Estimate Std. Error z value Pr(\textgreater{}|z|)}
\DocumentationTok{\#\# (Intercept)   {-}6.151      1.308  {-}4.701    0.000}
\DocumentationTok{\#\# sbp            0.007      0.006   1.135    0.256}
\DocumentationTok{\#\# tobacco        0.079      0.027   2.984    0.003}
\DocumentationTok{\#\# ldl            0.174      0.060   2.915    0.004}
\DocumentationTok{\#\# adiposity      0.019      0.029   0.635    0.526}
\DocumentationTok{\#\# famhist        0.925      0.228   4.061    0.000}
\DocumentationTok{\#\# typea          0.040      0.012   3.214    0.001}
\DocumentationTok{\#\# obesity       {-}0.063      0.044  {-}1.422    0.155}
\DocumentationTok{\#\# alcohol        0.000      0.004   0.027    0.978}
\DocumentationTok{\#\# age            0.045      0.012   3.728    0.000}
    
    \CommentTok{\# fitted value }
\NormalTok{    yhat }\OtherTok{=}\NormalTok{ (heart.full}\SpecialCharTok{$}\NormalTok{fitted.values}\SpecialCharTok{\textgreater{}}\FloatTok{0.5}\NormalTok{)}
    \FunctionTok{table}\NormalTok{(yhat, SAheart}\SpecialCharTok{$}\NormalTok{chd)}
\DocumentationTok{\#\#        }
\DocumentationTok{\#\# yhat      0   1}
\DocumentationTok{\#\#   FALSE 256  77}
\DocumentationTok{\#\#   TRUE   46  83}
\end{Highlighting}
\end{Shaded}

Based on what we learned in class, we can solve this problem ourselves using numerical optimization. Here we will demonstrate an approach that uses general solver \texttt{optim()}. First, write the objective function of the logistic regression, for any value of \(\boldsymbol \beta\), \(\mathbf{X}\) and \(\mathbf{y}\).

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# the negative log{-}likelihood function of logistic regression }
\NormalTok{    my.loglik }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(b, x, y)}
\NormalTok{    \{}
\NormalTok{        bm }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(b)}
\NormalTok{        xb }\OtherTok{=}\NormalTok{  x }\SpecialCharTok{\%*\%}\NormalTok{ bm}
        \CommentTok{\# this returns the negative loglikelihood}
        \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{(y}\SpecialCharTok{*}\NormalTok{xb) }\SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(xb))))}
\NormalTok{    \}}

    \CommentTok{\# Gradient}
\NormalTok{    my.gradient }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(b, x, y)}
\NormalTok{    \{}
\NormalTok{        bm }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(b) }
\NormalTok{        expxb }\OtherTok{=}  \FunctionTok{exp}\NormalTok{(x }\SpecialCharTok{\%*\%}\NormalTok{ bm)}
        \FunctionTok{return}\NormalTok{(}\FunctionTok{t}\NormalTok{(x) }\SpecialCharTok{\%*\%}\NormalTok{ (y }\SpecialCharTok{{-}}\NormalTok{ expxb}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{expxb)))}
\NormalTok{    \}}
\end{Highlighting}
\end{Shaded}

Let's check the result of this function for some arbitrary \(\boldsymbol \beta\) value.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# prepare the data matrix, I am adding a column of 1 for intercept}
    
\NormalTok{    x }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\StringTok{"intercept"} \OtherTok{=} \DecValTok{1}\NormalTok{, heart[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{]))}
\NormalTok{    y }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(heart[,}\DecValTok{10}\NormalTok{])}
    
    \CommentTok{\# check my function}
\NormalTok{    b }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(x))}
    \FunctionTok{my.loglik}\NormalTok{(b, x, y) }\CommentTok{\# scalar}
\DocumentationTok{\#\# [1] {-}320.234}
    
    \CommentTok{\# check the optimal value and the likelihood}
    \FunctionTok{my.loglik}\NormalTok{(heart.full}\SpecialCharTok{$}\NormalTok{coefficients, x, y)}
\DocumentationTok{\#\# [1] {-}236.07}
\end{Highlighting}
\end{Shaded}

Then we optimize this objective function

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# Use a general solver to get the optimal value}
    \CommentTok{\# Note that we are doing maximization instead of minimization, }
    \CommentTok{\# we need to specify "fnscale" = {-}1}
    \FunctionTok{optim}\NormalTok{(b, }\AttributeTok{fn =}\NormalTok{ my.loglik, }\AttributeTok{gr =}\NormalTok{ my.gradient, }
          \AttributeTok{method =} \StringTok{"BFGS"}\NormalTok{, }\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\StringTok{"fnscale"} \OtherTok{=} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\DocumentationTok{\#\# $par}
\DocumentationTok{\#\#  [1] {-}6.150733305  0.006504017  0.079376464  0.173923988  0.018586578  0.925372019  0.039595096}
\DocumentationTok{\#\#  [8] {-}0.062909867  0.000121675  0.045225500}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $value}
\DocumentationTok{\#\# [1] {-}236.07}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $counts}
\DocumentationTok{\#\# function gradient }
\DocumentationTok{\#\#       74       16 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $convergence}
\DocumentationTok{\#\# [1] 0}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $message}
\DocumentationTok{\#\# NULL}
\end{Highlighting}
\end{Shaded}

This matches our \texttt{glm()} solution. Now, if we do not have a general solver, we should consider using the Newton-Raphson. You need to write a function to calculate the Hessian matrix and proceed with an optimization update.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# my Newton{-}Raphson method}
    \CommentTok{\# set up an initial value}
    \CommentTok{\# this is sometimes crucial...}
    
\NormalTok{    b }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(x))}
    
\NormalTok{    mybeta }\OtherTok{=} \FunctionTok{my.logistic}\NormalTok{(b, x, y, }\AttributeTok{tol =} \FloatTok{1e{-}10}\NormalTok{, }\AttributeTok{maxitr =} \DecValTok{20}\NormalTok{, }
                         \AttributeTok{gr =}\NormalTok{ my.gradient, }\AttributeTok{hess =}\NormalTok{ my.hessian, }\AttributeTok{verbose =} \ConstantTok{TRUE}\NormalTok{)}
\DocumentationTok{\#\# at iteration 1, current beta is }
\DocumentationTok{\#\# {-}4.032 0.005 0.066 0.133 0.009 0.694 0.024 {-}0.045 {-}0.001 0.027}
\DocumentationTok{\#\# at iteration 2, current beta is }
\DocumentationTok{\#\# {-}5.684 0.006 0.077 0.167 0.017 0.884 0.037 {-}0.061 0 0.041}
\DocumentationTok{\#\# at iteration 3, current beta is }
\DocumentationTok{\#\# {-}6.127 0.007 0.079 0.174 0.019 0.924 0.039 {-}0.063 0 0.045}
\DocumentationTok{\#\# at iteration 4, current beta is }
\DocumentationTok{\#\# {-}6.151 0.007 0.079 0.174 0.019 0.925 0.04 {-}0.063 0 0.045}
\DocumentationTok{\#\# at iteration 5, current beta is }
\DocumentationTok{\#\# {-}6.151 0.007 0.079 0.174 0.019 0.925 0.04 {-}0.063 0 0.045}
\DocumentationTok{\#\# at iteration 6, current beta is }
\DocumentationTok{\#\# {-}6.151 0.007 0.079 0.174 0.019 0.925 0.04 {-}0.063 0 0.045}
\DocumentationTok{\#\# at iteration 7, current beta is }
\DocumentationTok{\#\# {-}6.151 0.007 0.079 0.174 0.019 0.925 0.04 {-}0.063 0 0.045}
    
    \CommentTok{\# the parameter value}
\NormalTok{    mybeta}
\DocumentationTok{\#\#                    [,1]}
\DocumentationTok{\#\# intercept {-}6.1507208650}
\DocumentationTok{\#\# sbp        0.0065040171}
\DocumentationTok{\#\# tobacco    0.0793764457}
\DocumentationTok{\#\# ldl        0.1739238981}
\DocumentationTok{\#\# adiposity  0.0185865682}
\DocumentationTok{\#\# famhist    0.9253704194}
\DocumentationTok{\#\# typea      0.0395950250}
\DocumentationTok{\#\# obesity   {-}0.0629098693}
\DocumentationTok{\#\# alcohol    0.0001216624}
\DocumentationTok{\#\# age        0.0452253496}
    \CommentTok{\# get the standard error estimation }
\NormalTok{    mysd }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{solve}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{my.hessian}\NormalTok{(mybeta, x, y))))    }
\end{Highlighting}
\end{Shaded}

With this solution, I can then get the standard errors and the p-value. You can check them with the \texttt{glm()} function solution.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# my summary matrix}
    \FunctionTok{round}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\StringTok{"beta"} \OtherTok{=}\NormalTok{ mybeta, }\StringTok{"sd"} \OtherTok{=}\NormalTok{ mysd, }\StringTok{"z"} \OtherTok{=}\NormalTok{ mybeta}\SpecialCharTok{/}\NormalTok{mysd, }
          \StringTok{"pvalue"} \OtherTok{=} \DecValTok{2}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{pnorm}\NormalTok{(}\FunctionTok{abs}\NormalTok{(mybeta}\SpecialCharTok{/}\NormalTok{mysd)))), }\AttributeTok{dig=}\DecValTok{5}\NormalTok{)}
\DocumentationTok{\#\#               beta      sd        z  pvalue}
\DocumentationTok{\#\# intercept {-}6.15072 1.30826 {-}4.70145 0.00000}
\DocumentationTok{\#\# sbp        0.00650 0.00573  1.13500 0.25637}
\DocumentationTok{\#\# tobacco    0.07938 0.02660  2.98376 0.00285}
\DocumentationTok{\#\# ldl        0.17392 0.05966  2.91517 0.00355}
\DocumentationTok{\#\# adiposity  0.01859 0.02929  0.63458 0.52570}
\DocumentationTok{\#\# famhist    0.92537 0.22789  4.06053 0.00005}
\DocumentationTok{\#\# typea      0.03960 0.01232  3.21382 0.00131}
\DocumentationTok{\#\# obesity   {-}0.06291 0.04425 {-}1.42176 0.15509}
\DocumentationTok{\#\# alcohol    0.00012 0.00448  0.02714 0.97835}
\DocumentationTok{\#\# age        0.04523 0.01213  3.72846 0.00019}
      
    \CommentTok{\# check that with the glm fitting }
    \FunctionTok{round}\NormalTok{(}\FunctionTok{summary}\NormalTok{(heart.full)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{dig=}\DecValTok{5}\NormalTok{)}
\DocumentationTok{\#\#             Estimate Std. Error  z value Pr(\textgreater{}|z|)}
\DocumentationTok{\#\# (Intercept) {-}6.15072    1.30826 {-}4.70145  0.00000}
\DocumentationTok{\#\# sbp          0.00650    0.00573  1.13500  0.25637}
\DocumentationTok{\#\# tobacco      0.07938    0.02660  2.98376  0.00285}
\DocumentationTok{\#\# ldl          0.17392    0.05966  2.91517  0.00355}
\DocumentationTok{\#\# adiposity    0.01859    0.02929  0.63458  0.52570}
\DocumentationTok{\#\# famhist      0.92537    0.22789  4.06053  0.00005}
\DocumentationTok{\#\# typea        0.03960    0.01232  3.21382  0.00131}
\DocumentationTok{\#\# obesity     {-}0.06291    0.04425 {-}1.42176  0.15509}
\DocumentationTok{\#\# alcohol      0.00012    0.00448  0.02714  0.97835}
\DocumentationTok{\#\# age          0.04523    0.01213  3.72846  0.00019}
\end{Highlighting}
\end{Shaded}

\hypertarget{penalized-logistic-regression}{%
\section{Penalized Logistic Regression}\label{penalized-logistic-regression}}

Similar to a linear regression, we can also apply penalties to a logistic regression to address collinearity problems or select variables in a high-dimensional setting. For example, if we use the Lasso penalty, the objective function is

\[\sum_{i=1}^n \log \, p(y_i | x_i, \boldsymbol \beta) + \lambda |\boldsymbol \beta|_1\]
This can be done using the \texttt{glmnet} package. Specifying \texttt{family\ =\ "binomial"} will ensure that a logistic regression is used, even your \texttt{y} is not a factor (but as numerical 0/1).

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(glmnet)}
\NormalTok{  lasso.fit }\OtherTok{=} \FunctionTok{cv.glmnet}\NormalTok{(}\AttributeTok{x =} \FunctionTok{data.matrix}\NormalTok{(SAheart[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{]), }\AttributeTok{y =}\NormalTok{ SAheart[,}\DecValTok{10}\NormalTok{], }
                        \AttributeTok{nfold =} \DecValTok{10}\NormalTok{, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
  
  \FunctionTok{plot}\NormalTok{(lasso.fit)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-182-1} \end{center}

The procedure is essentially the same as in a linear regression. And we could obtain the estimated parameters by selecting the best \(\lambda\) value.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{coef}\NormalTok{(lasso.fit, }\AttributeTok{s =} \StringTok{"lambda.min"}\NormalTok{)}
\DocumentationTok{\#\# 10 x 1 sparse Matrix of class "dgCMatrix"}
\DocumentationTok{\#\#                       s1}
\DocumentationTok{\#\# (Intercept) {-}6.319899679}
\DocumentationTok{\#\# sbp          0.003381401}
\DocumentationTok{\#\# tobacco      0.067870752}
\DocumentationTok{\#\# ldl          0.137863725}
\DocumentationTok{\#\# adiposity    .          }
\DocumentationTok{\#\# famhist      0.777513058}
\DocumentationTok{\#\# typea        0.026945871}
\DocumentationTok{\#\# obesity     {-}0.008357962}
\DocumentationTok{\#\# alcohol      .          }
\DocumentationTok{\#\# age          0.042535589}
\end{Highlighting}
\end{Shaded}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{discriminant-analysis}{%
\chapter{Discriminant Analysis}\label{discriminant-analysis}}

When we model the probability of \(Y\) given \(X\), such as using a logistic regression, the approach is often called a soft classification, meaning that we do not directly produce the class label for prediction. However, we can also view the task as finding a function, with 0/1 as the output. In this case, the function is called a \textbf{classifier}:

\[f : \mathbb{R}^p \longrightarrow \{0, 1\}\]
In this case, we can directly evaluate the prediction error, which is calculated from the \textbf{0-1 loss}:

\[L\big(f(\mathbf{x}), y \big) = \begin{cases}
0 \quad \text{if} \quad y = f(\mathbf{x})\\
1 \quad \text{o.w.}
\end{cases}\]

The goal is to minimize the overall \textbf{risk}, the integrated loss:

\[\text{R}(f) = \text{E}_{X, Y} \left[ L\big(f(X), Y\big) \right]\]
Continuing the notation from the logistic regression, with \(\eta(\mathbf{x}) = \text{P}(Y = 1 | X = \mathbf{x})\), we can easily see the decision rule to minimize the risk is to take the dominate label for any given \(\mathbf{x}\), this leads to the \textbf{Bayes rule}:

\begin{align}
f_B(\mathbf{x}) = \underset{f}{\arg\min} \,\, \text{R}(f) =
    \begin{cases}
    1 & \text{if} \quad \eta(\mathbf{x}) \geq 1/2 \\
    0 & \text{if} \quad \eta(\mathbf{x}) < 1/2. \\
    \end{cases}
\end{align}

Note that it doesn't matter when \(\eta(\mathbf{x}) = 1/2\) since we will make 50\% mistake anyway. The risk associated with this rule is called the \textbf{Bayes risk}, which is the best risk we could achieve with a classification model with 0/1 loss.

\hypertarget{bayes-rule}{%
\section{Bayes Rule}\label{bayes-rule}}

The essential idea of Discriminant Analysis is to estimate the densities functions of each class, and compare the densities at any given target point to perform classification. Let's construct the Bayes rule from the Bayes prospective:

\begin{align}
  \text{P}(Y = 1 | X = \mathbf{x}) &= \frac{\text{P}(X = \mathbf{x}| Y = 1)\text{P}(Y = 1)}{\text{P}(X = \mathbf{x})} \\
  \text{P}(Y = 0 | X = \mathbf{x}) &= \frac{\text{P}(X = \mathbf{x}| Y = 0)\text{P}(Y = 0)}{\text{P}(X = \mathbf{x})}
\end{align}

Lets further define marginal probabilities (\textbf{prior}) \(\pi_1 = P(Y = 1)\) and \(\pi_0 = 1 - \pi_1 = P(Y = 0)\), then, denote the conditional densities of \(X\) as

\begin{align}
  f_1 = \text{P}(X = \mathbf{x}| Y = 1)\\
  f_0 = \text{P}(X = \mathbf{x}| Y = 0)\\
\end{align}

Note that the Bayes rule suggests to make the decision 1 when \(\eta(\mathbf{x}) \geq 1/2\), this is equivalent to \(\pi_1 > \pi_0\). Utilizing the \textbf{Bayes Theorem}, we have

\begin{align}
f_B(\mathbf{x}) = \underset{f}{\arg\min} \,\, \text{R}(f) =
    \begin{cases}
    1 & \text{if} \quad \pi_1 f_1(\mathbf{x}) \geq \pi_0 f_0(\mathbf{x}) \\
    0 & \text{if} \quad \pi_1 f_1(\mathbf{x}) < \pi_0 f_0(\mathbf{x}). \\
    \end{cases}
\end{align}

This suggests that we can model the conditional density of \(X\) given \(Y\) instead of modeling \(P(Y | X)\) to make the decision.

\hypertarget{example-linear-discriminant-analysis-lda}{%
\section{Example: Linear Discriminant Analysis (LDA)}\label{example-linear-discriminant-analysis-lda}}

We create two density functions that use the \textbf{same covariance matrix}: \(X_1 \sim \cal{N}(\mu_1, \Sigma)\) and \(X_2 \sim \cal{N}(\mu_2, \Sigma)\), with \(\mu_1 = (0.5, -1)^\text{T}\), \(\mu_2 = (-0.5, 0.5)^\text{T}\), and \(\Sigma_{2\times2}\) has diagonal elements 1 and off diagonal elements 0.5. Let's first generate some observations.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(mvtnorm)}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
  
  \CommentTok{\# generate two sets of samples}
\NormalTok{  Sigma }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{  mu1 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\NormalTok{  mu2 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
  
  \CommentTok{\# define prior}
\NormalTok{  p1 }\OtherTok{=} \FloatTok{0.4} 
\NormalTok{  p2 }\OtherTok{=} \FloatTok{0.6}
    
\NormalTok{  n }\OtherTok{=} \DecValTok{1000}
  
\NormalTok{  Class1 }\OtherTok{=} \FunctionTok{rmvnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{p1, }\AttributeTok{mean =}\NormalTok{ mu1, }\AttributeTok{sigma =}\NormalTok{ Sigma)}
\NormalTok{  Class2 }\OtherTok{=} \FunctionTok{rmvnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{p2, }\AttributeTok{mean =}\NormalTok{ mu2, }\AttributeTok{sigma =}\NormalTok{ Sigma)}

  \FunctionTok{plot}\NormalTok{(Class1, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{))}
  \FunctionTok{points}\NormalTok{(Class2, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-186-1} \end{center}

If we know their true density functions, then the decision line is linear.

\hypertarget{linear-discriminant-analysis}{%
\section{Linear Discriminant Analysis}\label{linear-discriminant-analysis}}

As we demonstrated earlier using the Bayes rule, the conditional probability can be formulated using Bayes Theorem. For this time, we will assume in generate that there are \(K\) classes instead of just two. However, the notation are similar to the previous case:

\begin{align}
\text{P}(Y = k | X = \mathbf{x}) =&~ \frac{\text{P}(X = \mathbf{x}| Y = k)\text{P}(Y = k)}{\text{P}(X = \mathbf{x})}\\
                     =&~ \frac{\text{P}(X = \mathbf{x}| Y = k)\text{P}(Y = k)}{\sum_{l=1}^K \text{P}(X = \mathbf{x}| Y = l) \text{P}(Y = l)}\\
                     =&~ \frac{\pi_k f_k(\mathbf{x})}{\sum_{l=1}^K \pi_l f_l(\mathbf{x})}
\end{align}

Given any target point \(\mathbf{x}\), the best prediction is simply picking the one that maximizes the posterior

\[\underset{k}{\arg\max} \,\, \pi_k f_k(x)\]
Both LDA and QDA model \(f_k\) as a normal density function. Suppose we model each class density as multivariate Gaussian \({\cal N}(\boldsymbol \mu_k, \Sigma_k)\), and \alert{assume that the covariance matrices are the same across all $k$, i.e., $\Sigma_k = \Sigma$}. Then

\[f_k(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp\left[ -\frac{1}{2} (\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma^{-1} (\mathbf{x}- \boldsymbol \mu_k) \right].\]
The log-likelihood function for the conditional distribution is

\begin{align}
\log f_k(\mathbf{x}) =&~ -\log \big((2\pi)^{p/2} |\Sigma|^{1/2} \big) - \frac{1}{2} (\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma^{-1} (\mathbf{x}- \boldsymbol \mu_k) \\
    =&~ - \frac{1}{2} (\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma^{-1} (\mathbf{x}- \boldsymbol \mu_k) + \text{Constant}
\end{align}

The \textbf{maximum a posteriori} probability (MAP) estimate is simply

\begin{align}
\widehat f(\mathbf{x}) =& ~\underset{k}{\arg\max} \,\, \log \big( \pi_k f_k(\mathbf{x}) \big) \\
    =& ~\underset{k}{\arg\max} \,\, - \frac{1}{2} (\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma^{-1} (\mathbf{x}- \boldsymbol \mu_k) + \log(\pi_k)
\end{align}

The term \((\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma^{-1} (\mathbf{x}- \boldsymbol \mu_k)\) is simply the \alert{Mahalanobis distance} between \(x\) and the centroid \(\boldsymbol \mu_k\) for class \(k\). Hence, this is essentially classifying \(x\) to the class label with the closest centroid (after adjusting the for prior). This sets a connection with the \(k\)NN algorithm. \textbf{A special case} is that when \(\Sigma = \mathbf{I}\), i.e., only Euclidean distance is needed, and we have

\[\underset{k}{\arg\max} \,\, - \frac{1}{2} \lVert x - \boldsymbol \mu_k \rVert^2 + \log(\pi_k)\]

The decision boundary of LDA, as its name suggests, is a linear function of \(\mathbf{x}\). To see this, let's look at the terms in the MAP. Note that anything that does not depends on the class index \(k\) is irrelevant to the decision.

\begin{align}
 & - \frac{1}{2} (\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma^{-1} (\mathbf{x}- \boldsymbol \mu_k) + \log(\pi_k)\\
=&~ \mathbf{x}^\text{T}\Sigma^{-1} \boldsymbol \mu_k - \frac{1}{2}\boldsymbol \mu_k^\text{T}\Sigma^{-1} \boldsymbol \mu_k + \log(\pi_k) \text{irrelevant terms} \\
=&~ \mathbf{x}^\text{T}\mathbf{w}_k + b_k + \text{irrelevant terms}
\end{align}

Then, the decision boundary between two classes, \(k\) and \(l\) is

\begin{align}
\mathbf{x}^\text{T}\mathbf{w}_k + b_k &= \mathbf{x}^\text{T}\mathbf{w}_l + b_l \\
\Leftrightarrow \quad \mathbf{x}^\text{T}(\mathbf{w}_k - \mathbf{w}_l) + (b_k - b_l) &= 0, \\
\end{align}

which is a linear function of \(\mathbf{x}\). The previous density plot already showed this effect. Estimating the parameters in LDA is very simple:

\begin{itemize}
\tightlist
\item
  Prior probabilities: \(\widehat{\pi}_k = n_k / n = n^{-1} \sum_k \mathbf{1}\{y_i = k\}\), where \(n_k\) is the number of observations in class \(k\).
\item
  Centroids: \(\widehat{\boldsymbol \mu}_k = n_k^{-1} \sum_{i: \,y_i = k} x_i\)
\item
  Pooled covariance matrix:
  \[\widehat \Sigma = \frac{1}{n-K} \sum_{k=1}^K \sum_{i : \, y_i = k} (\mathbf{x}_i - \widehat{\boldsymbol \mu}_k) (\mathbf{x}_i - \widehat{\boldsymbol \mu}_k)^\text{T}\]
\end{itemize}

\hypertarget{example-quadratic-discriminant-analysis-qda}{%
\section{Example: Quadratic Discriminant Analysis (QDA)}\label{example-quadratic-discriminant-analysis-qda}}

When we assume that each class has its own covariance structure, the decision boundary will not be linear anymore. Let's visualize this by creating two density functions that use different covariance matrices.

\hypertarget{quadratic-discriminant-analysis}{%
\section{Quadratic Discriminant Analysis}\label{quadratic-discriminant-analysis}}

QDA simply abandons the assumption of the common covariance matrix. Hence, \(\Sigma_k\)'s are not equal. In this case, the determinant \(|\Sigma_k|\) of each covariance matrix will be different. In addition, the MAP decision becomes a quadratic function of the target point \(\mathbf{x}\)

\begin{align}
 & \underset{k}{\max} \,\, \log \big( \pi_k f_k(x) \big) \\
=& ~\underset{k}{\max} \,\, -\frac{1}{2} \log |\Sigma_k| - \frac{1}{2} (\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma_k^{-1} (\mathbf{x}- \boldsymbol \mu_k) + \log(\pi_k) \\
=& \mathbf{x}^\text{T}\mathbf{W}_k \mathbf{x}+ \mathbf{w}_k^\text{T}\mathbf{x}+ b_k
\end{align}

This leads to quadratic decision boundary between class \(k\) and \(l\)

\[\big\{\mathbf{x}: \mathbf{x}^\text{T}(\mathbf{W}_k - \mathbf{W}_l) \mathbf{x}+ \mathbf{x}^\text{T}(\mathbf{w}_k - \mathbf{w}_l) + (b_k - b_l) = 0\big\}.\]

The estimation procedure is also similar:

\begin{itemize}
\tightlist
\item
  Prior probabilities: \(\widehat{\pi}_k = n_k / n = n^{-1} \sum_k \mathbf{1}\{y_i = k\}\), where \(n_k\) is the number of observations in class \(k\).
\item
  Centroid: \(\widehat{\boldsymbol \mu}_k = n_k^{-1} \sum_{i: \,y_i = k} \mathbf{x}_i\)
\item
  Sample covariance matrix for each class:
  \[\widehat \Sigma_k = \frac{1}{n_k-1} \sum_{i : \, y_i = k} (\mathbf{x}_i - \widehat{\boldsymbol \mu}_k)(\mathbf{x}_i - \widehat{\boldsymbol \mu}_k)^\text{T}\]
\end{itemize}

\hypertarget{example-the-hand-written-digit-data}{%
\section{Example: the Hand Written Digit Data}\label{example-the-hand-written-digit-data}}

We first sample 100 data from both the training and testing sets.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(ElemStatLearn)}
    \CommentTok{\# a plot of some samples }
\NormalTok{    findRows }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(zip, n) \{}
        \CommentTok{\# Find n (random) rows with zip representing 0,1,2,...,9}
\NormalTok{        res }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\AttributeTok{length=}\DecValTok{10}\NormalTok{, }\AttributeTok{mode=}\StringTok{"list"}\NormalTok{)}
        \FunctionTok{names}\NormalTok{(res) }\OtherTok{\textless{}{-}} \DecValTok{0}\SpecialCharTok{:}\DecValTok{9}
\NormalTok{        ind }\OtherTok{\textless{}{-}}\NormalTok{ zip[,}\DecValTok{1}\NormalTok{]}
        \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{0}\SpecialCharTok{:}\DecValTok{9}\NormalTok{) \{}
\NormalTok{        res[[j}\SpecialCharTok{+}\DecValTok{1}\NormalTok{]] }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{( }\FunctionTok{which}\NormalTok{(ind}\SpecialCharTok{==}\NormalTok{j), n ) \}}
        \FunctionTok{return}\NormalTok{(res) }
\NormalTok{    \}}
    
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
    
    \CommentTok{\# find 100 samples for each digit for both the training and testing data}
\NormalTok{    train.id }\OtherTok{\textless{}{-}} \FunctionTok{findRows}\NormalTok{(zip.train, }\DecValTok{100}\NormalTok{)}
\NormalTok{    train.id }\OtherTok{=} \FunctionTok{unlist}\NormalTok{(train.id)}
    
\NormalTok{    test.id }\OtherTok{\textless{}{-}} \FunctionTok{findRows}\NormalTok{(zip.test, }\DecValTok{100}\NormalTok{)}
\NormalTok{    test.id }\OtherTok{=} \FunctionTok{unlist}\NormalTok{(test.id)}
    
\NormalTok{    X }\OtherTok{=}\NormalTok{ zip.train[train.id, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{    Y }\OtherTok{=}\NormalTok{ zip.train[train.id, }\DecValTok{1}\NormalTok{]}
    \FunctionTok{dim}\NormalTok{(X)}
\DocumentationTok{\#\# [1] 1000  256}
    
\NormalTok{    Xtest }\OtherTok{=}\NormalTok{ zip.test[test.id, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{    Ytest }\OtherTok{=}\NormalTok{ zip.test[test.id, }\DecValTok{1}\NormalTok{]}
    \FunctionTok{dim}\NormalTok{(Xtest)}
\DocumentationTok{\#\# [1] 1000  256}
\end{Highlighting}
\end{Shaded}

We can then fit LDA and QDA and predict.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# fit LDA}
    \FunctionTok{library}\NormalTok{(MASS)}
    
\NormalTok{    dig.lda}\OtherTok{=}\FunctionTok{lda}\NormalTok{(X,Y)}
\NormalTok{    Ytest.pred}\OtherTok{=}\FunctionTok{predict}\NormalTok{(dig.lda, Xtest)}\SpecialCharTok{$}\NormalTok{class}
    \FunctionTok{table}\NormalTok{(Ytest, Ytest.pred)}
\DocumentationTok{\#\#      Ytest.pred}
\DocumentationTok{\#\# Ytest  0  1  2  3  4  5  6  7  8  9}
\DocumentationTok{\#\#     0 92  0  2  2  0  0  1  0  3  0}
\DocumentationTok{\#\#     1  0 94  0  0  4  0  2  0  0  0}
\DocumentationTok{\#\#     2  2  2 66  7  5  2  4  2 10  0}
\DocumentationTok{\#\#     3  2  0  3 75  2  8  0  3  6  1}
\DocumentationTok{\#\#     4  0  4  2  1 76  1  3  2  2  9}
\DocumentationTok{\#\#     5  2  0  3 10  0 79  0  0  3  3}
\DocumentationTok{\#\#     6  0  0  4  1  3  4 86  0  1  1}
\DocumentationTok{\#\#     7  0  0  0  2  5  0  0 87  0  6}
\DocumentationTok{\#\#     8  2  0  4  5  6  7  1  0 72  3}
\DocumentationTok{\#\#     9  0  0  0  1  4  0  0  5  0 90}
    \FunctionTok{mean}\NormalTok{(Ytest }\SpecialCharTok{!=}\NormalTok{ Ytest.pred)}
\DocumentationTok{\#\# [1] 0.183}
\end{Highlighting}
\end{Shaded}

However, QDA does not work in this case because there are too many parameters

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    dig.qda }\OtherTok{=} \FunctionTok{qda}\NormalTok{(X, Y) }\CommentTok{\# error message}
\end{Highlighting}
\end{Shaded}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{part-machine-learning-algorithms}{%
\part{Machine Learning Algorithms}\label{part-machine-learning-algorithms}}

\hypertarget{support-vector-machines}{%
\chapter{Support Vector Machines}\label{support-vector-machines}}

Support Vector Machine (SVM) is one of the most popular classification models. The original SVM was proposed by Vladimir Vapnik and Alexey Chervonenkis in 1963. Then two important improvements was developed in the 90's: the soft margin version (\protect\hyperlink{ref-cortes1995support}{Cortes and Vapnik 1995}) and the nonlinear SVM using the kernel trick (\protect\hyperlink{ref-boser1992training}{Boser, Guyon, and Vapnik 1992}). We will start with the hard margin version, and then introduce all other techniques.

\hypertarget{maximum-margin-classifier}{%
\section{Maximum-margin Classifier}\label{maximum-margin-classifier}}

This is the original SVM proposed in 1963. It shares similarities with the perception algorithm, but in certain sense is a stable version. We observe the training data \({\cal D}_n = \{\mathbf{x}_i, y_i\}_{i=1}^n\), where we code \(y_i\) as a binary outcome from \(\{-1, 1\}\). The advantages of using this coding instead of \(0/1\) will be seen later. The goal is to find a linear classification rule \(f(\mathbf{x}) = \beta_0 + \mathbf{x}^\text{T}\boldsymbol \beta\) such that the classification rule is the sign of \(f(\mathbf{x})\):

\[
\hat{y} = 
\begin{cases}
        +1, \quad \text{if} \quad f(\mathbf{x}) > 0\\ 
        -1, \quad \text{if} \quad f(\mathbf{x}) < 0
\end{cases}
\]
Hence, a correct classification would satisfy \(y_i f(\mathbf{x}_i) > 0\). Let's look at the following example of data from two classes.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{    n }\OtherTok{\textless{}{-}} \DecValTok{6}
\NormalTok{    p }\OtherTok{\textless{}{-}} \DecValTok{2}
    
    \CommentTok{\# Generate positive and negative examples}
\NormalTok{    xneg }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{p,}\AttributeTok{mean=}\DecValTok{0}\NormalTok{,}\AttributeTok{sd=}\DecValTok{1}\NormalTok{),n,p)}
\NormalTok{    xpos }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{p,}\AttributeTok{mean=}\DecValTok{3}\NormalTok{,}\AttributeTok{sd=}\DecValTok{1}\NormalTok{),n,p)}
\NormalTok{    x }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(xpos,xneg)}
\NormalTok{    y }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,n),}\FunctionTok{rep}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,n))))}
    
    \CommentTok{\# plot }
    \FunctionTok{plot}\NormalTok{(x,}\AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(y}\SpecialCharTok{\textgreater{}}\DecValTok{0}\NormalTok{,}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }
         \AttributeTok{xlab =} \StringTok{"X1"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"X2"}\NormalTok{, }\AttributeTok{cex.lab =} \FloatTok{1.5}\NormalTok{)}
    \FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Positive"}\NormalTok{, }\StringTok{"Negative"}\NormalTok{),}\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{),}
           \AttributeTok{pch=}\FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\DecValTok{19}\NormalTok{), }\AttributeTok{text.col=}\FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-195-1} \end{center}

There are many linear lines that can perfectly separate the two classes. But which is better? The SVM defines this as the line that maximizes the margin, which can be seen in the following.

We use the \texttt{e1071} package to fit the SVM. There is a cost parameter \(C\), with default value 1. This parameter has a significant impact on non-separable problems. However, for this \textbf{separable case}, we should set this to be a very large value, meaning that the cost for having a wrong classification is very large. We also need to specify the \texttt{linear} kernel.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(e1071)}
\NormalTok{    svm.fit }\OtherTok{\textless{}{-}} \FunctionTok{svm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(x, y), }\AttributeTok{type=}\StringTok{\textquotesingle{}C{-}classification\textquotesingle{}}\NormalTok{, }
                   \AttributeTok{kernel=}\StringTok{\textquotesingle{}linear\textquotesingle{}}\NormalTok{, }\AttributeTok{scale=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{cost =} \DecValTok{10000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The following code can recover the fitted linear separation margin. Note here that the points on the margins are the ones with \(\alpha_i > 0\) (will be introduced later):

\begin{itemize}
\tightlist
\item
  \texttt{coefs} provides the \(y_i \alpha_i\) for the support vectors
\item
  \texttt{SV} are the \(x_i\) values correspond to the support vectors
\item
  \texttt{rho} is negative \(\beta_0\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    b }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(svm.fit}\SpecialCharTok{$}\NormalTok{coefs) }\SpecialCharTok{\%*\%}\NormalTok{ svm.fit}\SpecialCharTok{$}\NormalTok{SV}
\NormalTok{    b0 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\NormalTok{svm.fit}\SpecialCharTok{$}\NormalTok{rho}
    
    \CommentTok{\# an alternative of b0 as the lecture note}
\NormalTok{    b0 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\NormalTok{(}\FunctionTok{max}\NormalTok{(x[y }\SpecialCharTok{==} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, ] }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(b)) }\SpecialCharTok{+} \FunctionTok{min}\NormalTok{(x[y }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, ] }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(b)))}\SpecialCharTok{/}\DecValTok{2}
    
    \CommentTok{\# plot on the data }
    \FunctionTok{plot}\NormalTok{(x,}\AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(y}\SpecialCharTok{\textgreater{}}\DecValTok{0}\NormalTok{,}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }
         \AttributeTok{xlab =} \StringTok{"X1"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"X2"}\NormalTok{, }\AttributeTok{cex.lab =} \FloatTok{1.5}\NormalTok{)}
    \FunctionTok{legend}\NormalTok{(}\StringTok{"bottomleft"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Positive"}\NormalTok{,}\StringTok{"Negative"}\NormalTok{),}\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{),}
           \AttributeTok{pch=}\FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\DecValTok{19}\NormalTok{),}\AttributeTok{text.col=}\FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{), }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{a=} \SpecialCharTok{{-}}\NormalTok{b0}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{b=}\SpecialCharTok{{-}}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    
    \CommentTok{\# mark the support vectors}
    \FunctionTok{points}\NormalTok{(x[svm.fit}\SpecialCharTok{$}\NormalTok{index, ], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{3}\NormalTok{)}
    
    \CommentTok{\# the two margin lines }
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{a=}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{b0}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{b=}\SpecialCharTok{{-}}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{a=}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{b0}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{b=}\SpecialCharTok{{-}}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-197-1} \end{center}

As we can see, the separation line is trying to have the maximum distance from both classes. This is why it is called the \textbf{Maximum-margin Classifier}.

\hypertarget{linearly-separable-svm}{%
\section{Linearly Separable SVM}\label{linearly-separable-svm}}

In linearly SVM, \(f(\mathbf{x}) = \beta_0 + \mathbf{x}^\text{T}\boldsymbol \beta\). When \(f(\mathbf{x}) = 0\), it corresponds to a hyperplane that separates the two classes:

\[\{ \mathbf{x}: \beta_0 + \mathbf{x}^\text{T} \boldsymbol \beta = 0 \}\]

Hence, for this separable case, all observations with \(y_i = 1\) are on one side \(f(\mathbf{x}) > 0\), and observations with \(y_i = -1\) are on the other side.

\includegraphics[width=0.4\textwidth,height=\textheight]{images/SVMdist.png}

First, let's calculate the \textbf{distance from any point \(\mathbf{x}\) to this hyperplane}. We can first find a point \(\mathbf{x}_0\) on the hyperplane, such that \(\mathbf{x}_0^\text{T}\boldsymbol \beta= - \beta_0\). By taking the difference between \(\mathbf{x}\) and \(\mathbf{x}_0\), and project this vector to the direction of \(\boldsymbol \beta\), we have that the distance from \(\mathbf{x}\) to the hyperplane is the projection of \(\mathbf{x}- \mathbf{x}_0\) onto the normed vector \(\frac{\boldsymbol \beta}{\lVert \boldsymbol \beta\lVert}\):

\begin{align}
& \left \langle  \frac{\boldsymbol \beta}{\lVert \boldsymbol \beta\lVert}, \mathbf{x}- \mathbf{x}_0 \right \rangle \\
=& \frac{1}{\lVert \boldsymbol \beta\lVert} (\mathbf{x}- \mathbf{x}_0)^\text{T}\boldsymbol \beta\\
=& \frac{1}{\lVert \boldsymbol \beta\lVert} (\mathbf{x}^\text{T}\boldsymbol \beta+ \beta_0) \\
=& \frac{1}{\lVert \boldsymbol \beta\lVert} f(\mathbf{x}) \\
\end{align}

Since the goal of SVM is to create the maximum margin, let's denote this as \(M\). Then we want all observations to be lied on the correct side, with at least an margin \(M\). This means \(y_i (\mathbf{x}_i^\text{T}\boldsymbol \beta+ \beta_0) \geq M\). But the scale of \(\boldsymbol \beta\) is also playing a role in calculating the margin. Hence, we will use the normed version. Then, the linearly separable SVM is to solve this constrained optimization problem:

\begin{align}
\underset{\boldsymbol \beta, \beta_0}{\text{max}} \quad & M \\
\text{subject to} \quad & \frac{1}{\lVert \boldsymbol \beta\lVert} y_i(\mathbf{x}^\text{T}\boldsymbol \beta+ \beta_0) \geq M, \,\, i = 1, \ldots, n.
\end{align}

Note that the scale of \(\boldsymbol \beta\) can be arbitrary, let's set it as \(\lVert \boldsymbol \beta\rVert = 1/M\). The maximization becomes minimization, and its equivalent to minimizing \(\frac{1}{2} \lVert \boldsymbol \beta\rVert^2\). Then we have the \textbf{primal form} of the SVM optimization problem.

\begin{align}
\text{min} \quad & \frac{1}{2} \lVert \boldsymbol \beta\rVert^2 \\
\text{subject to} \quad & y_i(\mathbf{x}^\text{T}\boldsymbol \beta+ \beta_0) \geq 1, \,\, i = 1, \ldots, n.
\end{align}

\hypertarget{from-primal-to-dual}{%
\subsection{From Primal to Dual}\label{from-primal-to-dual}}

This is a general inequality constrained optimization problem.

\begin{align}
\text{min} \quad & g(\boldsymbol \theta) \\
\text{subject to} \quad & h(\boldsymbol \theta) \leq 0, \,\, i = 1, \ldots, n.
\end{align}

We can consider the corresponding Lagrangian (with all \(\alpha_i\)'s positive):

\[{\cal L}(\boldsymbol \theta, \boldsymbol \alpha) = g(\boldsymbol \theta) + \sum_{i = 1}^n \alpha_i h_i(\boldsymbol \theta)\]
Then there can be two ways to optimize this. If we maximize \(\alpha_i\)'s first, for any fixed \(\boldsymbol \theta\), then for any \(\boldsymbol \theta\) that violates the constraint, i.e., \(h_i(\boldsymbol \theta) > 0\) for some \(i\), we can always choose an extremely large \(\alpha_i\) so that \(\cal{L}(\boldsymbol \theta, \boldsymbol \alpha)\) is infinity. Hence the solution of this \textbf{primal form} must satisfy the constraint.

\[\underset{\boldsymbol \theta}{\min} \underset{\boldsymbol \alpha\succeq 0}{\max} {\cal L}(\boldsymbol \theta, \boldsymbol \alpha)\]
On the other hand, if we minimize \(\boldsymbol \theta\) first, then maximize for \(\boldsymbol \alpha\), we have the \textbf{dual form}:

\[\underset{\boldsymbol \alpha\succeq 0}{\max} \underset{\boldsymbol \theta}{\min} {\cal L}(\boldsymbol \theta, \boldsymbol \alpha)\]
In general, the two are not the same:

\[\underbrace{\underset{\boldsymbol \alpha\succeq 0}{\max} \underset{\boldsymbol \theta}{\min} {\cal L}(\boldsymbol \theta, \boldsymbol \alpha)}_{\text{duel}} \leq \underbrace{\underset{\boldsymbol \theta}{\min} \underset{\boldsymbol \alpha\succeq 0}{\max} {\cal L}(\boldsymbol \theta, \boldsymbol \alpha)}_{\text{primal}}\]
But a sufficient condition is that if both \(g\) and \(h_i\)'s are convex and also the constraints \(h_i\)'s are feasible. We will use this technique to solve the SVM problem.

First, rewrite the problem as

\begin{align}
\text{min} \quad & \frac{1}{2} \lVert \boldsymbol \beta\rVert^2 \\
\text{subject to} \quad & - \{ y_i(\mathbf{x}^\text{T}\boldsymbol \beta+ \beta_0) - 1\} \leq 0, i = 1, \ldots, n.
\end{align}

Then the Lagrangian is

\[{\cal L}(\boldsymbol \beta, \beta_0, \boldsymbol \alpha) = \frac{1}{2} \lVert \boldsymbol \beta\rVert^2 - \sum_{i = 1}^n \alpha_i \big\{ y_i(\mathbf{x}_i^\text{T}\boldsymbol \beta+ \beta_0) - 1 \big\}\]
To solve this using the dual form, we first find the optimizer of \(\boldsymbol \beta\) and \(\beta_0\). We take derivatives with respect to them:

\begin{align}
    \boldsymbol \beta- \sum_{i = 1}^n \alpha_i y_i \mathbf{x}_i  =&~ 0 \quad (\nabla_\boldsymbol \beta{\cal L}= 0 ) \\
    \sum_{i = 1}^n \alpha_i y_i =&~ 0 \quad (\nabla_{\beta_0} {\cal L}= 0 )
\end{align}

Take these solution and plug them back into the Lagrangian, we have

\[{\cal L}(\boldsymbol \beta, \beta_0, \boldsymbol \alpha) = \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \mathbf{x}_i^\text{T}\mathbf{x}_j\]
Hence, the dual optimization problem is

\begin{align}
\underset{\boldsymbol \alpha}{\max} \quad & \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \mathbf{x}_i^\text{T}\mathbf{x}_j \nonumber \\
\text{subject to} \quad & \alpha_i \geq 0, \,\, i = 1, \ldots, n. \nonumber \\
& \sum_{i = 1}^n \alpha_i y_i = 0
\end{align}

Compared with the original primal form, this version has a trivial feasible solution with all \(\alpha_i\)'s being 0. One can start from this solution to search for the optimizer while maintaining within the contained region. However, the primal form is difficult since there is no apparent way to satisfy the constraint.

After solving the dual form, we have all the \(\alpha_i\) values. The ones with \(\alpha_i > 0\) are called the support vectors. Based on our previous analysis, \(\widehat{\boldsymbol \beta} = \sum_{i = 1}^n \alpha_i y_i x_i\), and we can also obtain \(\beta_0\) by calculating the midpoint of two ``closest'' support vectors to the separating hyperplane:

\[\widehat{\beta}_0 = - \,\, \frac{\max_{i: y_i = -1} \mathbf{x}_i^\text{T}\widehat{\boldsymbol \beta} + \min_{i: y_i = 1} \mathbf{x}_i^\text{T}\widehat{\boldsymbol \beta} }{2}\]
And the decision is \(\text{sign}(\mathbf{x}^\text{T}\widehat{\boldsymbol \beta} + \widehat{\beta}_0)\). An example has been demonstrated previously with the \texttt{e1071} package.

\hypertarget{linearly-non-separable-svm-with-slack-variables}{%
\section{Linearly Non-separable SVM with Slack Variables}\label{linearly-non-separable-svm-with-slack-variables}}

When we cannot have a perfect separation of the two classes, the original SVM cannot find a solution. Hence, a slack was introduce to incorporate such observations:

\[y_i (\mathbf{x}_i^\text{T}\boldsymbol \beta+ \beta_0) \geq (1 - \xi_i)\]
for a positive \(\xi\). Note that when \(\xi = 0\), the observation is lying at the correct side, with enough margin. When \(1 > \xi > 0\), the observation is lying at the correct side, but the margin is not sufficiently large. When \(\xi > 1\), the observation is lying on the wrong side of the separation hyperplane.

\includegraphics[width=0.4\textwidth,height=\textheight]{images/SVMslack.png}

This new optimization problem can be formulated as

\begin{align}
\text{min} \quad & \frac{1}{2}\lVert \boldsymbol \beta\rVert^2 + C \sum_{i=1}^n \xi_i \\
\text{subject to} \quad & y_i (\mathbf{x}_i^\text{T}\boldsymbol \beta+ \beta_0) \geq (1 - \xi_i), \,\, i = 1, \ldots, n, \\
\text{and} \quad & \xi_i \geq 0, \,\, i = 1, \ldots, n,
\end{align}

where \(C\) is a tuning parameter that controls the emphasis on the slack variable. Large \(C\) will be less tolerable on having positive slacks. We can again write the Lagrangian primal \({\cal L}(\boldsymbol \beta, \beta_0, \boldsymbol \alpha, \boldsymbol \xi)\) as

\[\frac{1}{2} \lVert \boldsymbol \beta\rVert^2 + C \sum_{i=1}^n \xi_i - \sum_{i = 1}^n \alpha_i \big\{ y_i(x_i^\text{T}\boldsymbol \beta+ \beta_0) - (1 - \xi_i) \big\} - \sum_{i = 1}^n \gamma_i \xi_i,\]
where \(\alpha_i\)'s and \(\gamma_i\)'s are all positive. We can similarly obtain the solution corresponding to \(\boldsymbol \beta\), \(\beta_0\) and \(\boldsymbol \xi\):

\begin{align}
\boldsymbol \beta- \sum_{i = 1}^n \alpha_i y_i x_i  =&~ 0 \quad (\nabla_\boldsymbol \beta{\cal L}= 0 ) \\
\sum_{i = 1}^n \alpha_i y_i =&~ 0 \quad (\nabla_{\beta_0} {\cal L}= 0 ) \\
C - \alpha_i - \gamma_i =&~ 0 \quad (\nabla_{\xi_i} {\cal L}= 0 )
\end{align}

Substituting them back into the Lagrangian, we have the dual form:

\begin{align}
\underset{\boldsymbol \alpha}{\max} \quad & \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \color{OrangeRed}{\langle \mathbf{x}_i, \mathbf{x}_j \rangle} \\
\text{subject to} \quad & 0 \leq \alpha_i \leq C, \,\, i = 1, \ldots, n, \\
\text{and} \quad & \sum_{i = 1}^n \alpha_i y_i = 0.
\end{align}

Here, the inner product \(\langle \mathbf{x}_i, \mathbf{x}_j \rangle\) is nothing but \(\mathbf{x}_i^\text{T}\mathbf{x}_j\). The observations with \(0 < \alpha_i < C\) are the \alert{support vectors} that lie on the margin. Hence, we can obtain these observations and perform the same calculations as before to obtain \(\widehat{\beta}_0\). The following code generates some data for this situation and fit SVM. We use the default \(C = 1\).

\begin{Shaded}
\begin{Highlighting}[]

    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{70}\NormalTok{)}
\NormalTok{    n }\OtherTok{\textless{}{-}} \DecValTok{10} \CommentTok{\# number of data points for each class}
\NormalTok{    p }\OtherTok{\textless{}{-}} \DecValTok{2} \CommentTok{\# dimension}

    \CommentTok{\# Generate the positive and negative examples}
\NormalTok{    xneg }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{p,}\AttributeTok{mean=}\DecValTok{0}\NormalTok{,}\AttributeTok{sd=}\DecValTok{1}\NormalTok{),n,p)}
\NormalTok{    xpos }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{p,}\AttributeTok{mean=}\FloatTok{1.5}\NormalTok{,}\AttributeTok{sd=}\DecValTok{1}\NormalTok{),n,p)}
\NormalTok{    x }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(xpos,xneg)}
\NormalTok{    y }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,n),}\FunctionTok{rep}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,n))))}

    \CommentTok{\# Visualize the data}
    
    \FunctionTok{plot}\NormalTok{(x,}\AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(y}\SpecialCharTok{\textgreater{}}\DecValTok{0}\NormalTok{,}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }
         \AttributeTok{xlab =} \StringTok{"X1"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"X2"}\NormalTok{, }\AttributeTok{cex.lab =} \FloatTok{1.5}\NormalTok{)}
    \FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Positive"}\NormalTok{,}\StringTok{"Negative"}\NormalTok{),}\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{),}
           \AttributeTok{pch=}\FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\DecValTok{19}\NormalTok{),}\AttributeTok{text.col=}\FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{), }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}

\NormalTok{    svm.fit }\OtherTok{\textless{}{-}} \FunctionTok{svm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(x, y), }\AttributeTok{type=}\StringTok{\textquotesingle{}C{-}classification\textquotesingle{}}\NormalTok{, }
                   \AttributeTok{kernel=}\StringTok{\textquotesingle{}linear\textquotesingle{}}\NormalTok{,}\AttributeTok{scale=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{cost =} \DecValTok{1}\NormalTok{)}

\NormalTok{    b }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(svm.fit}\SpecialCharTok{$}\NormalTok{coefs) }\SpecialCharTok{\%*\%}\NormalTok{ svm.fit}\SpecialCharTok{$}\NormalTok{SV}
\NormalTok{    b0 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\NormalTok{svm.fit}\SpecialCharTok{$}\NormalTok{rho}
    
    \FunctionTok{points}\NormalTok{(x[svm.fit}\SpecialCharTok{$}\NormalTok{index, ], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{3}\NormalTok{)     }
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{a=} \SpecialCharTok{{-}}\NormalTok{b0}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{b=}\SpecialCharTok{{-}}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{a=}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{b0}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{b=}\SpecialCharTok{{-}}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{a=}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{b0}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{b=}\SpecialCharTok{{-}}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{SMLR_files/figure-latex/unnamed-chunk-199-1} \end{center}

If we instead use a smaller \(C\):

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# Visualize the data}
    \FunctionTok{plot}\NormalTok{(x,}\AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(y}\SpecialCharTok{\textgreater{}}\DecValTok{0}\NormalTok{,}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }
         \AttributeTok{xlab =} \StringTok{"X1"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"X2"}\NormalTok{, }\AttributeTok{cex.lab =} \FloatTok{1.5}\NormalTok{)}
    \FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Positive"}\NormalTok{,}\StringTok{"Negative"}\NormalTok{),}\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{),}
           \AttributeTok{pch=}\FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\DecValTok{19}\NormalTok{),}\AttributeTok{text.col=}\FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{), }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}

    \CommentTok{\# fit SVM with C = 10}
\NormalTok{    svm.fit }\OtherTok{\textless{}{-}} \FunctionTok{svm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(x, y), }\AttributeTok{type=}\StringTok{\textquotesingle{}C{-}classification\textquotesingle{}}\NormalTok{, }
                   \AttributeTok{kernel=}\StringTok{\textquotesingle{}linear\textquotesingle{}}\NormalTok{,}\AttributeTok{scale=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{cost =} \FloatTok{0.1}\NormalTok{)}

\NormalTok{    b }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(svm.fit}\SpecialCharTok{$}\NormalTok{coefs) }\SpecialCharTok{\%*\%}\NormalTok{ svm.fit}\SpecialCharTok{$}\NormalTok{SV}
\NormalTok{    b0 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\NormalTok{svm.fit}\SpecialCharTok{$}\NormalTok{rho}
    
    \FunctionTok{points}\NormalTok{(x[svm.fit}\SpecialCharTok{$}\NormalTok{index, ], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{3}\NormalTok{)     }
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{a=} \SpecialCharTok{{-}}\NormalTok{b0}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{b=}\SpecialCharTok{{-}}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{a=}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{b0}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{b=}\SpecialCharTok{{-}}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{a=}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{b0}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{b=}\SpecialCharTok{{-}}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{b[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-200-1} \end{center}

\hypertarget{example-saheart-data}{%
\section{\texorpdfstring{Example: \texttt{SAheart} Data}{Example: SAheart Data}}\label{example-saheart-data}}

If you want to use the \texttt{1071e} package and perform cross-validation, you could consider using the \texttt{caret} package. Make sure that you specify \texttt{method\ =\ "svmLinear2"}. The following code is using the \texttt{SAheart} as an example.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(ElemStatLearn)}
  \FunctionTok{data}\NormalTok{(SAheart)}
  \FunctionTok{library}\NormalTok{(caret)}

\NormalTok{  cost.grid }\OtherTok{=} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{cost =} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\DecValTok{2}\NormalTok{, }\AttributeTok{length =} \DecValTok{20}\NormalTok{))}
\NormalTok{  train\_control }\OtherTok{=} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\AttributeTok{number=}\DecValTok{10}\NormalTok{, }\AttributeTok{repeats=}\DecValTok{3}\NormalTok{)}
  
\NormalTok{  svm2 }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(chd) }\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ SAheart, }\AttributeTok{method =} \StringTok{"svmLinear2"}\NormalTok{, }
                \AttributeTok{trControl =}\NormalTok{ train\_control,  }
                \AttributeTok{tuneGrid =}\NormalTok{ cost.grid)}
  
  \CommentTok{\# see the fitted model}
\NormalTok{  svm2}
\DocumentationTok{\#\# Support Vector Machines with Linear Kernel }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# 462 samples}
\DocumentationTok{\#\#   9 predictor}
\DocumentationTok{\#\#   2 classes: \textquotesingle{}0\textquotesingle{}, \textquotesingle{}1\textquotesingle{} }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# No pre{-}processing}
\DocumentationTok{\#\# Resampling: Cross{-}Validated (10 fold, repeated 3 times) }
\DocumentationTok{\#\# Summary of sample sizes: 416, 416, 416, 415, 416, 416, ... }
\DocumentationTok{\#\# Resampling results across tuning parameters:}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#   cost       Accuracy   Kappa    }
\DocumentationTok{\#\#   0.0100000  0.7142923  0.2844994}
\DocumentationTok{\#\#   0.1147368  0.7200123  0.3520308}
\DocumentationTok{\#\#   0.2194737  0.7164354  0.3454492}
\DocumentationTok{\#\#   0.3242105  0.7171600  0.3467866}
\DocumentationTok{\#\#   0.4289474  0.7164354  0.3453015}
\DocumentationTok{\#\#   0.5336842  0.7164354  0.3450704}
\DocumentationTok{\#\#   0.6384211  0.7157108  0.3438517}
\DocumentationTok{\#\#   0.7431579  0.7171600  0.3472755}
\DocumentationTok{\#\#   0.8478947  0.7157108  0.3437850}
\DocumentationTok{\#\#   0.9526316  0.7157108  0.3437850}
\DocumentationTok{\#\#   1.0573684  0.7171600  0.3479914}
\DocumentationTok{\#\#   1.1621053  0.7164354  0.3459484}
\DocumentationTok{\#\#   1.2668421  0.7164354  0.3459484}
\DocumentationTok{\#\#   1.3715789  0.7178847  0.3500130}
\DocumentationTok{\#\#   1.4763158  0.7171600  0.3479914}
\DocumentationTok{\#\#   1.5810526  0.7178847  0.3500130}
\DocumentationTok{\#\#   1.6857895  0.7171600  0.3479914}
\DocumentationTok{\#\#   1.7905263  0.7171600  0.3479914}
\DocumentationTok{\#\#   1.8952632  0.7171600  0.3479914}
\DocumentationTok{\#\#   2.0000000  0.7164354  0.3459484}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Accuracy was used to select the optimal model using the largest value.}
\DocumentationTok{\#\# The final value used for the model was cost = 0.1147368.}
\end{Highlighting}
\end{Shaded}

Note that when you fit the model, there are a few things you could consider:

\begin{itemize}
\tightlist
\item
  You can consider centering and scaling the covariates. This can be done during pre-processing. Or you may specify \texttt{preProcess\ =\ c("center",\ "scale")} in the \texttt{train()} function.
\item
  You may want to start with a wider range of cost values, then narrow down to a smaller range, since SVM can be quite sensitive to tuning in some cases.
\item
  There are many other SVM libraries, such as \texttt{kernlab}. This can be specified by using \texttt{method\ =\ "svmLinear"}. However, \texttt{kernlab} uses \texttt{C} as the parameter name for cost. We will show an example later.
\end{itemize}

\hypertarget{nonlinear-svm-via-kernel-trick}{%
\section{Nonlinear SVM via Kernel Trick}\label{nonlinear-svm-via-kernel-trick}}

The essential idea of kernel trick can be summarized as using the kernel function of two observations \(\mathbf{x}\) and \(\mathbf{z}\) to replace the inner product between some feature mapping of the two covariate vectors. In other words, if we want to create some nonlinear features of \(\mathbf{x}\), such as \(x_1^2\), \(\exp(x_2)\), \(\sqrt{x_3}\), etc., we may in general write them as

\[\Phi : {\cal X}\rightarrow {\cal F}, \,\,\, \Phi(\mathbf{x}) = (\phi_1(\mathbf{x}), \phi_2(\mathbf{x}), \ldots ),\]
where \({\cal F}\) has either finite or infinite dimensions. Then, we can still treat this as a linear SVM by constructing the decision rule as

\[f(x) = \langle \Phi(\mathbf{x}), \boldsymbol \beta\rangle = \Phi(\mathbf{x})^\text{T}\boldsymbol \beta.\]
This is why we used the \(\langle \cdot, \cdot\rangle\) operator in the previous example. Now, the kernel trick is essentially skipping the explicit calculation of \(\Phi(\mathbf{x})\) by utilizing the property that

\[K(\mathbf{x}, \mathbf{z}) = \langle \Phi(\mathbf{x}), \Phi(\mathbf{z}) \rangle\]
for some kernel function \(K(\mathbf{x}, \mathbf{z})\). Since \(\langle \Phi(\mathbf{x}), \Phi(\mathbf{z}) \rangle\) is all we need in the dual form, we can simply replace it by \(K(\mathbf{x}, \mathbf{z})\), which gives the kernel form:

\begin{align}
\underset{\boldsymbol \alpha}{\max} \quad & \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \color{OrangeRed}{K(\mathbf{x}_i, \mathbf{x}_j)} \\
\text{subject to} \quad & 0 \leq \alpha_i \leq C, \,\, i = 1, \ldots, n, \\
\text{and} \quad & \sum_{i = 1}^n \alpha_i y_i = 0.
\end{align}

One most apparent advantage of doing this is to save computational cost. This maybe understood using the following example:

\begin{itemize}
\tightlist
\item
  Consider kernel function \(K(\mathbf{x}, \mathbf{z}) = (\mathbf{x}^\text{T}\mathbf{z})^2\)
\item
  Consider \(\Phi(\mathbf{x})\) being the basis expansion that contains all second order interactions: \(x_k x_l\) for \(1 \leq k, l \leq p\)
\end{itemize}

We can show that the two gives equivalent results, however, the kernel version is much faster. \(K(\mathbf{x}, \mathbf{z})\) takes \(p+1\) operations, while \(\langle \Phi(\mathbf{x}_i), \Phi(\mathbf{x}_j) \rangle\) requires \(3p^2\).

\begin{align}
K(\mathbf{x}, \mathbf{z}) &=~ \left(\sum_{k=1}^p x_k z_k\right) \left(\sum_{l=1}^p x_l z_l\right) \\
&=~ \sum_{k=1}^p \sum_{l=1}^p x_k z_k x_l z_l \\
&=~ \sum_{k, l=1}^p (x_k x_l) (z_k z_l) \\
&=~ \langle \Phi(\mathbf{x}),  \Phi(\mathbf{z}) \rangle
\end{align}

Formally, this property is guaranteed by the \textbf{Mercer's theorem} that states: The kernel matrix \(K\) is positive semi-definite if and only if the function \(K(x_i ,x_j)\) is equivalent to some inner product \(\langle \Phi(\mathbf{x}), \Phi(\mathbf{z}) \rangle\).

Besides making the calculation of nonlinear functions easier, using the kernel trick also implies that if we use a proper kernel function, then it defines a space of functions \({\cal H}\) (reproducing kernel Hilbert space, RKHS) that can be represented in the form of \(f(x) = \sum_i \alpha_i K(x, x_i)\) for some \(x_i\) in \({\cal X}\) (see the Moore--Aronszajn theorem) with a proper definition of inner product. However, this space is of infinite dimension, noticing that \(i\) goes from 1 to infinity. However, as long as we search for the solution within \({\cal H}\), and also apply a proper penalty of the estimated function \(\widehat{f}(\mathbf{x})\), then our computational job will reduce to solving the \(\alpha_i\)'s that corresponds to the observed \(n\) data points, meaning that we only need to solve the solution within a finite space. This is guaranteed by the \textbf{Representer theorem}. There are numerous articles on the RKHS. Hence, we will not focus on introducing this technique. However, we will later on use this property in the penalized formulation of SVM.

\hypertarget{example-mixture.example-data}{%
\section{\texorpdfstring{Example: \texttt{mixture.example} Data}{Example: mixture.example Data}}\label{example-mixture.example-data}}

We use the \texttt{mixture.example} data in the \texttt{ElemStatLearn} package. In addition, we use a different package \texttt{kernlab}. The red dotted line indicates the true decision boundary.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(ElemStatLearn)}
    \FunctionTok{data}\NormalTok{(mixture.example)}

    \CommentTok{\# redefine data}
\NormalTok{    px1 }\OtherTok{=}\NormalTok{ mixture.example}\SpecialCharTok{$}\NormalTok{px1}
\NormalTok{    px2 }\OtherTok{=}\NormalTok{ mixture.example}\SpecialCharTok{$}\NormalTok{px2}
\NormalTok{    x }\OtherTok{=}\NormalTok{ mixture.example}\SpecialCharTok{$}\NormalTok{x}
\NormalTok{    y }\OtherTok{=}\NormalTok{ mixture.example}\SpecialCharTok{$}\NormalTok{y}
    
    \CommentTok{\# plot the data and true decision boundary}
\NormalTok{    prob }\OtherTok{\textless{}{-}}\NormalTok{ mixture.example}\SpecialCharTok{$}\NormalTok{prob}
\NormalTok{    prob.bayes }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(prob, }
                         \FunctionTok{length}\NormalTok{(px1), }
                         \FunctionTok{length}\NormalTok{(px2))}
    \FunctionTok{contour}\NormalTok{(px1, px2, prob.bayes, }\AttributeTok{levels=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }
            \AttributeTok{labels=}\StringTok{""}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"x1"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"x2"}\NormalTok{,}
            \AttributeTok{main=}\StringTok{"SVM with linear kernal"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{points}\NormalTok{(x, }\AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(y}\SpecialCharTok{==}\DecValTok{1}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}

    \CommentTok{\# train linear SVM using the kernlab package}
    \FunctionTok{library}\NormalTok{(kernlab)}
\NormalTok{    cost }\OtherTok{=} \DecValTok{10}
\NormalTok{    svm.fit }\OtherTok{\textless{}{-}} \FunctionTok{ksvm}\NormalTok{(x, y, }\AttributeTok{type=}\StringTok{"C{-}svc"}\NormalTok{, }\AttributeTok{kernel=}\StringTok{\textquotesingle{}vanilladot\textquotesingle{}}\NormalTok{, }\AttributeTok{C=}\NormalTok{cost)}
\DocumentationTok{\#\#  Setting default kernel parameters}

    \CommentTok{\# plot the SVM decision boundary}
    \CommentTok{\# Extract the indices of the support vectors on the margin:}
\NormalTok{    sv.alpha}\OtherTok{\textless{}{-}}\FunctionTok{alpha}\NormalTok{(svm.fit)[[}\DecValTok{1}\NormalTok{]][}\FunctionTok{which}\NormalTok{(}\FunctionTok{alpha}\NormalTok{(svm.fit)[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{\textless{}}\NormalTok{cost)]}
\NormalTok{    sv.index}\OtherTok{\textless{}{-}}\FunctionTok{alphaindex}\NormalTok{(svm.fit)[[}\DecValTok{1}\NormalTok{]][}\FunctionTok{which}\NormalTok{(}\FunctionTok{alpha}\NormalTok{(svm.fit)[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{\textless{}}\NormalTok{cost)]}
\NormalTok{    sv.matrix}\OtherTok{\textless{}{-}}\NormalTok{x[sv.index,]}
    \FunctionTok{points}\NormalTok{(sv.matrix, }\AttributeTok{pch=}\DecValTok{16}\NormalTok{, }\AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(y[sv.index] }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{), }\AttributeTok{cex=}\FloatTok{1.5}\NormalTok{)}

    \CommentTok{\# Plot the hyperplane and the margins:}
\NormalTok{    w }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\FunctionTok{coef}\NormalTok{(svm.fit)[[}\DecValTok{1}\NormalTok{]])) }\SpecialCharTok{\%*\%} \FunctionTok{xmatrix}\NormalTok{(svm.fit)[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{    b }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}} \FunctionTok{b}\NormalTok{(svm.fit)}

    \FunctionTok{abline}\NormalTok{(}\AttributeTok{a=} \SpecialCharTok{{-}}\NormalTok{b}\SpecialCharTok{/}\NormalTok{w[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{b=}\SpecialCharTok{{-}}\NormalTok{w[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{w[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{1}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{a=}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{b}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{w[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{b=}\SpecialCharTok{{-}}\NormalTok{w[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{w[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{a=}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{b}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{w[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{b=}\SpecialCharTok{{-}}\NormalTok{w[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\NormalTok{w[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-202-1} \end{center}

Let's also try a nonlinear SVM, using the radial kernel.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# fit SVM with radial kernel, with cost = 5}
\NormalTok{    dat }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{y =} \FunctionTok{factor}\NormalTok{(y), x)}
\NormalTok{    fit }\OtherTok{=} \FunctionTok{svm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ dat, }\AttributeTok{scale =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{kernel =} \StringTok{"radial"}\NormalTok{, }\AttributeTok{cost =} \DecValTok{5}\NormalTok{)}
    
    \CommentTok{\# extract the prediction}
\NormalTok{    xgrid }\OtherTok{=} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{X1 =}\NormalTok{ px1, }\AttributeTok{X2 =}\NormalTok{ px2)}
\NormalTok{    func }\OtherTok{=} \FunctionTok{predict}\NormalTok{(fit, xgrid, }\AttributeTok{decision.values =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    func }\OtherTok{=} \FunctionTok{attributes}\NormalTok{(func)}\SpecialCharTok{$}\NormalTok{decision}
    
    \CommentTok{\# visualize the decision rule}
\NormalTok{    ygrid }\OtherTok{=} \FunctionTok{predict}\NormalTok{(fit, xgrid)}
    \FunctionTok{plot}\NormalTok{(xgrid, }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(ygrid }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\StringTok{"bisque"}\NormalTok{, }\StringTok{"cadetblue1"}\NormalTok{), }
         \AttributeTok{pch =} \DecValTok{20}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{main=}\StringTok{"SVM with radial kernal"}\NormalTok{)}
    \FunctionTok{points}\NormalTok{(x, }\AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(y}\SpecialCharTok{==}\DecValTok{1}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
    
    \CommentTok{\# our estimated function value, cut at 0}
    \FunctionTok{contour}\NormalTok{(px1, px2, }\FunctionTok{matrix}\NormalTok{(func, }\DecValTok{69}\NormalTok{, }\DecValTok{99}\NormalTok{), }\AttributeTok{level =} \DecValTok{0}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    
    \CommentTok{\# the true probability, cut at 0.5}
    \FunctionTok{contour}\NormalTok{(px1, px2, }\FunctionTok{matrix}\NormalTok{(prob, }\DecValTok{69}\NormalTok{, }\DecValTok{99}\NormalTok{), }\AttributeTok{level =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{, }
            \AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-203-1} \end{center}

You may also consider some other popular kernels. The following ones are implemented in the \texttt{e1071} package, with additional tuning parameters \(\text{coef}_0\) and \(\gamma\).

\begin{itemize}
\tightlist
\item
  Linear: \(K(\mathbf{x}, \mathbf{z}) = \mathbf{x}^\text{T}\mathbf{z}\)
\item
  \(d\)th degree polynomial: \(K(\mathbf{x}, \mathbf{z}) = (\text{coef}_0 + \gamma \mathbf{x}^\text{T}\mathbf{z})^d\)
\item
  Radial basis: \(K(\mathbf{x}, \mathbf{z}) = \exp(- \gamma \lVert \mathbf{x}- \mathbf{z}\lVert^2)\)
\item
  Sigmoid: \(\tanh(\gamma \mathbf{x}^\text{T}\mathbf{z}+ \text{coef}_0)\)
\end{itemize}

Cross-validation can also be doing using the \texttt{caret} package. To specify the kernel, one must correctly specify the \texttt{method} parameter in the \texttt{train()} function. For this example, we use the \texttt{method\ =\ "svmRadial"} that uses the \texttt{kernlab} package to fit the model. For this choice, you need to tune just \texttt{sigma} and \texttt{C} (cost). More details are refereed to the \href{https://topepo.github.io/caret/train-models-by-tag.html\#support-vector-machines}{\texttt{caret} documentation}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  svm.radial }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ dat, }\AttributeTok{method =} \StringTok{"svmRadial"}\NormalTok{,}
                \AttributeTok{preProcess =} \FunctionTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
                \AttributeTok{tuneGrid =} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{C =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{sigma =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)),}
                \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{5}\NormalTok{))}
\NormalTok{  svm.radial}
\DocumentationTok{\#\# Support Vector Machines with Radial Basis Function Kernel }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# 200 samples}
\DocumentationTok{\#\#   2 predictor}
\DocumentationTok{\#\#   2 classes: \textquotesingle{}0\textquotesingle{}, \textquotesingle{}1\textquotesingle{} }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Pre{-}processing: centered (2), scaled (2) }
\DocumentationTok{\#\# Resampling: Cross{-}Validated (5 fold) }
\DocumentationTok{\#\# Summary of sample sizes: 160, 160, 160, 160, 160 }
\DocumentationTok{\#\# Resampling results across tuning parameters:}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#   C     sigma  Accuracy  Kappa}
\DocumentationTok{\#\#   0.01  1      0.715     0.43 }
\DocumentationTok{\#\#   0.01  2      0.760     0.52 }
\DocumentationTok{\#\#   0.01  3      0.770     0.54 }
\DocumentationTok{\#\#   0.10  1      0.720     0.44 }
\DocumentationTok{\#\#   0.10  2      0.790     0.58 }
\DocumentationTok{\#\#   0.10  3      0.800     0.60 }
\DocumentationTok{\#\#   0.50  1      0.795     0.59 }
\DocumentationTok{\#\#   0.50  2      0.815     0.63 }
\DocumentationTok{\#\#   0.50  3      0.830     0.66 }
\DocumentationTok{\#\#   1.00  1      0.795     0.59 }
\DocumentationTok{\#\#   1.00  2      0.825     0.65 }
\DocumentationTok{\#\#   1.00  3      0.835     0.67 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Accuracy was used to select the optimal model using the largest value.}
\DocumentationTok{\#\# The final values used for the model were sigma = 3 and C = 1.}
\end{Highlighting}
\end{Shaded}

\hypertarget{svm-as-a-penalized-model}{%
\section{SVM as a Penalized Model}\label{svm-as-a-penalized-model}}

Recall that in SVM, we need \(y_i f(\mathbf{x}_i)\) to be at least \(1 - \xi_i\), this implies that we would prefer \(1 - y_i f(\mathbf{x}_i)\) to be negative or 0. And observation with \(1 - y_i f(\mathbf{x}_i)\) should be penalized. Hence, recall that the objective function of dual form in SVM is \(\frac{1}{2}\lVert \boldsymbol \beta\rVert^2 + C \sum_{i=1}^n \xi_i\), we may rewrite this as a new version:

\[\min \,\, \sum_{i=1}^n \big[ 1 - y_i f(\mathbf{x}_i) \big]_{+} \, +\, \lambda \lVert \boldsymbol \beta\rVert^2.\]
Here, we converted \(1/(2C)\) to \(\lambda\). And this resembles a familiar form of ``Loss \(+\) Penalty'', where the slack variables becomes the loss and the norm of \(\boldsymbol \beta\) is the penalty. This particular loss function is called the \textbf{Hinge loss}, with

\[L(y, f(\mathbf{x})) = [1 - yf(\mathbf{x})]_+ = \max(0, 1 - yf(\mathbf{x}))\]
However, the Hinge loss is not differentiable. There are some other loss functions that can be used as substitute:

\begin{itemize}
\tightlist
\item
  Logistic loss:
  \[L(y, f(\mathbf{x})) = \log_2( 1 + e^{-y f(\mathbf{x})})\]
\item
  Modified Huber Loss:
  \[L(y, f(\mathbf{x})) = \begin{cases}
  \max(0, 1 - yf(\mathbf{x}))^2 & \text{for} \quad yf(\mathbf{x}) \geq -1 \\
  -4 yf(\mathbf{x})  & \text{otherwise}  \\
  \end{cases}\]
\end{itemize}

Here is a visualization of several different loss functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  t }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}

  \CommentTok{\# different loss functions}
\NormalTok{  hinge }\OtherTok{=} \FunctionTok{pmax}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ t) }
\NormalTok{  zeroone }\OtherTok{=}\NormalTok{ (t }\SpecialCharTok{\textless{}=} \DecValTok{0}\NormalTok{)}
\NormalTok{  logistic }\OtherTok{=} \FunctionTok{log2}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{t))}
\NormalTok{  modifiedhuber }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(t }\SpecialCharTok{\textgreater{}=} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, (}\FunctionTok{pmax}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ t))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{t)}
  
  \CommentTok{\# plot}
  \FunctionTok{plot}\NormalTok{(t, zeroone, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{),}
       \AttributeTok{main =} \StringTok{"Loss Functions"}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(t, hinge, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, )}
  \FunctionTok{points}\NormalTok{(t, logistic, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(t, modifiedhuber, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
  \FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Zero{-}one"}\NormalTok{, }\StringTok{"Hinge"}\NormalTok{, }\StringTok{"Logistic"}\NormalTok{, }\StringTok{"Modified Huber"}\NormalTok{),}
         \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{), }\AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }
         \AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{cex =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-205-1} \end{center}

For linear decision rules, with \(f(\mathbf{x}) = \beta_0 + \mathbf{x}^\text{T}\boldsymbol \beta\), this should be trivial to solve. However, we also want to consider nonlinear decision functions. But the above form does not contain a kernel function to use the kernel trick. The \textbf{Representer Theorem} (\protect\hyperlink{ref-kimeldorf1970correspondence}{Kimeldorf and Wahba 1970}) can help us in this case. This theorem was originally developed for in the setting of Chebyshev splines, but later on generalized. The theorem ensures that if we solve the function \(f\) with regularization with respect to the norm in the RKHS induced from a kernel function \(K\), then the solution must admits a finite representation of the form (although the space \({\cal H}\) we search for the solution is infinite):

\[\widehat{f}(\mathbf{x}) = \sum_{i = 1}^n \beta_i K(\mathbf{x}, \mathbf{x}_i).\]
This suggests that the optimization problem becomes

\[\sum_{i=1}^n L(y_i, \mathbf{K}_i^\text{T}\boldsymbol \beta) + \lambda \boldsymbol \beta^\text{T}\mathbf{K}\boldsymbol \beta,\]
where \(\mathbf{K}_{n \times n}\) is the kernel matrix with \(\mathbf{K}_{ij} = K(x_i, x_j)\), and \(\mathbf{K}_i\) is the \(i\) the column of \(\mathbf{K}\). This is an unconstrained optimization problem that can be solved using gradient decent if \(L\) is differentiable. More details will be presented in the next Chapter.

\hypertarget{kernel-and-feature-maps-another-example}{%
\section{Kernel and Feature Maps: Another Example}\label{kernel-and-feature-maps-another-example}}

We give another example about the equivalence of kernel and the inner product of feature maps, which is ensured by the Mercer's Theorem (\protect\hyperlink{ref-mercer1909xvi}{Mercer 1909}). Consider the Gaussian kernel \(e^{-\gamma \lVert \mathbf{x}- \mathbf{z}\rVert}\). We can write, using Tayler expansion,

\begin{align}
&e^{\gamma \lVert \mathbf{x}- \mathbf{z}\rVert} \nonumber \\
=& e^{-\gamma \lVert \mathbf{x}\rVert + 2 \gamma \mathbf{x}^\text{T}\mathbf{z}- \gamma \lVert \mathbf{z}\rVert} \nonumber \\
=& e^{-\gamma \lVert \mathbf{x}\rVert - \gamma \lVert \mathbf{z}\rVert} \bigg[ 1 + \frac{2 \gamma \mathbf{x}^\text{T}\mathbf{z}}{1!} + \frac{(2 \gamma \mathbf{x}^\text{T}\mathbf{z})^2}{2!} + \frac{(2 \gamma \mathbf{x}^\text{T}\mathbf{z})^3}{3!} + \cdots \bigg]
\end{align}

Note that \(\mathbf{x}^\text{T}\mathbf{z}\) is the inner product of all first order feature maps. We also showed previously \((\mathbf{x}^\text{T}\mathbf{z})^2\) is equivalent to the inner product of all second order feature maps (\(\Phi_2(\mathbf{x})\)), and \((\mathbf{x}^\text{T}\mathbf{z})^3\) would be equivalent to the third order version (\(\Phi_3(\mathbf{x})\)), etc.. Hence, the previous equation can be written as the inner product of feature maps in the form of

\[e^{-\gamma \lVert \mathbf{x}\rVert} \bigg[ 1, \sqrt{\frac{2\gamma}{1!}} \mathbf{x}^\text{T}, \sqrt{\frac{(2\gamma)^2}{2!}} \Phi_2^\text{T}(\mathbf{x}), \sqrt{\frac{(2\gamma)^3}{3!}} \Phi_3^\text{T}(\mathbf{x}), \cdots \bigg]\]

This shows the Gaussian kernel is corresponding to all polynomials with a scaling factor of \(e^{-\gamma \lVert \mathbf{x}\rVert}\)

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{reproducing-kernel-hilbert-space}{%
\chapter{Reproducing Kernel Hilbert Space}\label{reproducing-kernel-hilbert-space}}

In the previous chapter of SVM, we gave an example to show that instead of using the inner product \(\langle \Phi(\mathbf{x}), \Phi(\mathbf{z}) \rangle\) between the feature maps of \(\mathbf{x}\) and \(\mathbf{z}\), we can instead use the kernel trick \(K(\mathbf{x}, \mathbf{z})\) to perform the exact same calculation. And also, in the penalized kernel version, we mentioned that the decision rule can be expressed in the finite sample form of \(\sum_{i = 1}^n \beta_i K(\cdot, \mathbf{x}_i)\) by the Representer Theorem. All of these are based on a fundamental tool of the Reproducing Kernel Hilbert Space and we will provide some basic knowledge of it. We will also prove the Representer Theorem (\protect\hyperlink{ref-kimeldorf1970correspondence}{Kimeldorf and Wahba 1970}), which is very similar to the proof of smoothing spline.

\hypertarget{constructing-the-rkhs}{%
\section{Constructing the RKHS}\label{constructing-the-rkhs}}

Recall that in the smoothing spline example, we wanted to fit a regression model by solving for a function \(f\) in a quite complicated space, the second order Sobolev space. We could not exhaust all the candidates in this space because that would be computationally untraceable. However, the results there shows that the solution has a finite representation. In general, when we solve a regression problem using a \textbf{Loss \(+\) Penalty} form, we will also enjoy that property if we search the (regression or decision) function \(f\) within a RKHS. So let's first define what this space look like.

We start with the feature space \(\cal X\) of \(X\), where \(X\) is just the \(p\) dimensional feature we often deal with. Let's say we have a sample \(x_1\), then if we have a kernel function \(k(\cdot, \cdot)\), we can construct a new function called \(K(x_1, \cdot)\). Keep in mind that \(K(x_1, \cdot)\) is a function with argument \(\cdot\) and parameter \(x_1\) in this case. Similarly, we can do another sample, say \(x_2\) and generate a function based on that sample, called \(K(x_1, \cdot)\). The following plot shows three of such functions, using red, orange and blue lines, receptively.

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-208-1} \end{center}

Since we can have many samples from \(\cal X\), we will also have infinite such functions like \(K(x, \cdot)\), and also the linear combinations of them would also be interesting to us. Let's consider a space \({\cal G}\) of all such functions

\[{\cal G} = \left\{\sum_{i}^n \alpha_i K(x_i, \cdot) \mid \alpha \in \mathbb{R}, n \in \mathbb{N}, x_i \in {\cal X} \right\} \]
The black curve in the previous plot is an example of such linear combinations. We can see that the functions within \({\cal G}\) start to become more and more flexible as we consider all the linear combinations. And as one final step, we will consider the completion of this space, which leads to the RKHS.

\[\cal H = \bar{\cal G}.\]
Completion here means that \(\cal H\) will contain the limits of all Cauchy sequences of such functions in \(\cal G\).

\hypertarget{properties-of-rkhs}{%
\section{Properties of RKHS}\label{properties-of-rkhs}}

This space \(\cal H\) enjoys several important and useful properties. First, by the \href{https://en.wikipedia.org/wiki/Riesz_representation_theorem}{Riesz representation theorem}, we know that \(\cal H\) is a \textbf{Hilbert space with the reproducing property}. For a (real valued) Hilbert space, it must satisfy

\begin{itemize}
\tightlist
\item
  symmetric: \(\langle K_x, K_z \rangle = \langle K_z, K_x \rangle\)
\item
  linear: \(\langle a K_{x_1} + b K_{x_2}, K_z \rangle = a \langle K_{x_1}, K_z \rangle + b \langle K_{x_2}, K_z \rangle\)
\item
  positive definite: \(\langle K_x, K_x \rangle \geq 0\) and \(\langle K_x, K_x \rangle = 0\) iff \(K_x = 0\)
\end{itemize}

Also, the reproducing property means that when we evaluate a function \(f \in \cal H\) at a point \(x\), it is the same as calculating the inner product between \(f\) and \(K_x\). Formally,

\[f(x) = \langle f, K_x \rangle_{\cal H}\]

Now, we could simply take \(f = K_z\), that means, evaluating \(K_z(x)\) is

\[K_z(x) = \langle K_z, K_x \rangle_{\cal H}\]
Note that \(K_z(x) = K(z, x)\), this implies that the inner product in \(\cal H\) is done by the kernel:

\[\langle K_z, K_x \rangle_{\cal H} = K(z, x)\]
For example, if we have \(f(\cdot) = \sum_i \alpha_i K(x_i, \cdot)\), then evaluating \(f\) at \(x\) is

\begin{align}
f(x) =& \, \langle f, K(x, \cdot) \rangle_{\cal H} \nonumber \\
=& \, \left\langle \sum_i \alpha_i K(x_i, \cdot), K(x, \cdot) \right\rangle_{\cal H} \nonumber \\
=& \, \sum_i \alpha_i \left\langle K(x_i, \cdot), K(x, \cdot) \right\rangle_{\cal H} \nonumber \\
=& \, \sum_i \alpha_i K(x_i, x)
\end{align}

The Moore--Aronszajn theorem (\protect\hyperlink{ref-aronszajn1950theory}{Aronszajn 1950}) ensures that a positive definite kernel \(K(\cdot, \cdot)\) on \(\cal X\) would uniquely define such a RKHS, where \(K(\cdot, \cdot)\) itself is the reproducing kernel. Hence, all we need is the original \(\cal X\) and a kernel function. Then the RKHS can be defined as we stated previously, with all the nice properties. Besides these, another results by Mercer interprets kernels as feature maps, which we have already see in the SVM chapter that \(K(x, z)= \langle \Phi(x), \Phi(z) \rangle\). Overall, we set some relationships among these three quantities in their respective spaces:

\begin{itemize}
\tightlist
\item
  original features \(x\)
\item
  feature maps \(\Phi(x)\)
\item
  functions \(K(x, \cdot)\)
\end{itemize}

\hypertarget{the-representer-theorem}{%
\section{The Representer Theorem}\label{the-representer-theorem}}

\(\cal H\) is still a very large space of functions. And it is not clear if we want to find \(f\) in \(\cal H\) for our optimization problem, how do we computationally complete that task. It is unlikely that we can exhaust all such functions. Well, luckily, we don't need to. This is ensured by the \textbf{Representer Theorem}, which states that only a finite sample presentation is needed.

\begin{theorem}[Representer Theorem]
\protect\hypertarget{thm:unnamed-chunk-209}{}\label{thm:unnamed-chunk-209}If we are given a set of data \(\{x_i, y_i\}_{i=1}^n\), and we search for the best solution in \({\cal H}\) of the optimization problem
\[\widehat f = \underset{f \in \cal H}{\arg\min} \,\, {\cal L}(\{y_i, f(x_i)\}_{i=1}^n) + p(\| f \|_{{\cal H}}^2 ),\]
where \({\cal L}\) is the loss function, \(p\) is a monotone penalty function, and \({\cal H}\) is the RKHS with kernel \(K\). Then the solution must take the form
\[\widehat f = \sum_{i=1}^n w_i K(\cdot, x_i)\]
\end{theorem}

The proof is quite simple. The logic is the same as the smoothing spline proof.

\begin{proof}
We can first use the kernel \(K\) associated with \(\cal H\) to define a set of functions
\[K(\cdot, x_i), \, K(\cdot, x_2), \, \cdots, \, K(\cdot, x_n)\]

Then, suppose the solution is some function \(f \in {\cal H}\), we could find its projection on the space spaned by these functions. This means that we could write \(f\) as

\[f(\cdot) = \sum_{i=1}^n \alpha_i K(\cdot, x_i) + h(\cdot)\]

for some \(\alpha_i\) and \(h(\cdot)\). Also, since \(h(\cdot)\) is in the orthogonal space of all such \(K(\cdot, x_i)\), we have, by the reproducing property,

\[ h(x_i) = \langle K(x_i, \cdot), h(\cdot) \rangle = 0\]

for all \(i\). You may recall our proof in the smoothing spline for the same construction of \(h\) that has \(h(x_i) = 0\) for all \(i\). By the reproducing property, we have, for any observations in the training data,

\begin{align}
f(x_j) =& \langle f(\cdot), K(\cdot, x_j) \rangle \nonumber \\
=& \left\langle \sum_{i=1}^n \alpha_i K(x_i, \cdot) + h(\cdot), K(\cdot, x_j) \right\rangle \nonumber \\
=& \sum_{i=1}^n \alpha_i K(x_i, x_j) + \sum_{i=1}^n \alpha_i h(x_j) \nonumber \\
=& \sum_{i=1}^n \alpha_i K(x_i, x_j)
\end{align}

Which means that, the evaluation of \(f(x_j)\) would be the same as just evaluating it on this finite represtantation. Hence the loss function would be the same regardless of whether we have \(h\) or not. And also the penalty term of this finite represtantation would be better since

\begin{align}
\lVert f \rVert^2 =& \lVert \sum_{i=1}^n \alpha_i K(\cdot, x_i) + h(\cdot) \rVert^2 \nonumber \\
=& \lVert \sum_{i=1}^n \alpha_i K(\cdot, x_i) \rVert^2 + \lVert h(\cdot) \rVert^2 \nonumber \\
\geq& \lVert \sum_{i=1}^n \alpha_i K(\cdot, x_i) \rVert^2
\end{align}

This completes the proof since this finite represetnation would be the one being prefered than \(f\).
\end{proof}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{kernel-ridge-regression}{%
\chapter{Kernel Ridge Regression}\label{kernel-ridge-regression}}

With our understandings of the RKHS and the representer theorem, we can now say that for any regression function models, if we want the solution to be more flexible, we may solve it within a RKHS. For example, consider the following regression problem:

\[\widehat f = \underset{f \in {\cal H}}{\arg\min} \,\, \frac{1}{n} \sum_{i=1}^n \Big(y_i - \widehat f(x_i) \Big)^2 + \lambda \lVert f \rVert_{\cal H}^2\]
Since we know that the solution has to take the form

\[\widehat f = \sum_{i=1}^n \alpha_i K(x_i, \cdot),\]
we can instead solve the problem as a ridge regression type of problem:

\[\widehat f = \underset{f \in {\cal H}}{\arg\min} \,\, \frac{1}{n} \big\lVert \mathbf{y}- \mathbf{K}\boldsymbol \alpha\big\rVert^2 + \lambda \lVert f \rVert_{\cal H}^2,\]
where \(\mathbf{K}\) is an \(n \times n\) matrix with \(K(x_i, x_j)\) at its \((i,j)\)th element. With some simple calculation, we also have

\begin{align}
\lVert f \rVert_{\cal H}^2 =& \langle f, f \rangle \nonumber \\
=& \langle \sum_{i=1}^n \alpha_i K(x_i, \cdot), \sum_{j=1}^n \alpha_j K(x_j, \cdot) \rangle \nonumber \\
=& \sum_{i, j} \alpha_i \alpha_j \big\langle K(x_i, \cdot), K(x_j, \cdot) \big\rangle \nonumber \\
=& \sum_{i, j} \alpha_i \alpha_j K(x_i, x_j) \nonumber \\
=& \boldsymbol \alpha^\text{T}\mathbf{K}\boldsymbol \alpha
\end{align}

Hence, the problem becomes

\[\widehat f = \underset{f \in {\cal H}}{\arg\min} \,\, \frac{1}{n} \big\lVert \mathbf{y}- \mathbf{K}\boldsymbol \alpha\big\rVert^2 + \lambda \boldsymbol \alpha^\text{T}\mathbf{K}\boldsymbol \alpha.\]
By taking the derivative with respect to \(\boldsymbol \alpha\), we have (note that \(\mathbf{K}\) is symmetric),

\begin{align}
-\frac{1}{n} \mathbf{K}^\text{T}(\mathbf{y}- \mathbf{K}\boldsymbol \alpha) + \lambda \mathbf{K}\boldsymbol \alpha\overset{\text{set}}{=} \mathbf{0} \nonumber \\
\mathbf{K}(- \mathbf{y}+ \mathbf{K}\boldsymbol \alpha+ n\lambda \boldsymbol \alpha) = \mathbf{0}.
\end{align}
This implies

\[ \boldsymbol \alpha= (\mathbf{K}+ n\lambda \mathbf{I})^{-1} \mathbf{y}.\]
and we obtained the solution.

\hypertarget{example-linear-kernel-and-ridge-regression}{%
\section{Example: Linear Kernel and Ridge Regression}\label{example-linear-kernel-and-ridge-regression}}

When \(K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^\text{T}\mathbf{x}_j\), we also have \(\mathbf{K}= \mathbf{X}\mathbf{X}^\text{T}\). We should expect this to match the original ridge regression since this is essentially a linear regression. First, plug this into our previous result, we have

\[ \boldsymbol \alpha= (\mathbf{X}\mathbf{X}^\text{T}+ n\lambda \mathbf{I})^{-1} \mathbf{y}.\]
and the fitted value is

\[ \widehat{\mathbf{y}} = \mathbf{K}\boldsymbol \alpha= \mathbf{X}\mathbf{X}^\text{T}(\mathbf{X}\mathbf{X}^\text{T}+ n\lambda \mathbf{I})^{-1} \mathbf{y}\]
Using a matrix identity \((\bP \bQ + \mathbf{I})^{-1}\bP = \bP (\bQ \bP + \mathbf{I})^{-1}\), and let \(\bQ = \mathbf{X}= \bP^\text{T}\), we have

\[ \widehat{\mathbf{y}} = \mathbf{K}\boldsymbol \alpha= \mathbf{X}(\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y}\]
and

\[ \widehat{\mathbf{y}} = \mathbf{X}\underbrace{\big[ \mathbf{X}^\text{T}\boldsymbol \alpha\big]}_{\boldsymbol \beta} = \mathbf{X}\underbrace{\big[ (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y}\big]}_{\boldsymbol \beta}\]

which is simply the Ridge regression solution, and also the corresponding linear regression solution \(\widehat{\boldsymbol \beta} = \mathbf{X}^\text{T}\widehat{\boldsymbol \alpha}\). This makes the penalty term \(\boldsymbol \alpha^\text{T}\mathbf{K}\boldsymbol \alpha= \boldsymbol \alpha^\text{T}\mathbf{X}\mathbf{X}^\text{T}\boldsymbol \alpha= \boldsymbol \beta^\text{T}\boldsymbol \beta\), which maps every thing back to the ridge regression form.

\hypertarget{example-alternative-view}{%
\section{Example: Alternative View}\label{example-alternative-view}}

This example is motivated from an alternative derivation provided by Prof.~Max Welling on his kernel ridge regression lecture note. This understanding matches the SVM primal to dual derivation, but is performed on a linear regression. We can then again switch things to the kernel version (through kernel trick).

Consider a linear regression

\[\underset{\boldsymbol \beta}{\text{minimize}} \,\, \frac{1}{n} \lVert \mathbf{y}- \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert \boldsymbol \beta\rVert^2\]

Introduce a new set of variables

\[z_i = y_i - \mathbf{x}_i^\text{T}\boldsymbol \beta,\]
for \(i = 1, \ldots, n\). Then The original problem becomes

\begin{align}
\underset{\boldsymbol \beta, \mathbf{z}}{\text{minimize}} \quad & \frac{1}{2n\lambda} \lVert \mathbf{z}\rVert^2 + \frac{1}{2} \lVert \boldsymbol \beta\rVert^2 \nonumber \\
\text{subj. to} \quad & z_i = y_i - \mathbf{x}_i^\text{T}\boldsymbol \beta, \,\, i = 1, \ldots, n.
\end{align}

If we use the same strategy from the SVM derivation, we have the Lagrangian

\[{\cal L} = \frac{1}{2n\lambda} \lVert \mathbf{z}\rVert^2 + \frac{1}{2} \lVert \boldsymbol \beta\rVert^2 + \sum_{i=1}^n \alpha_i (y_i - \mathbf{x}_i^\text{T}\boldsymbol \beta- z_i)\]
with \(\alpha_i \in \mathbb{R}\). Switching from primal to dual, by taking derivative w.r.t. \(\boldsymbol \beta\) and \(\mathbf{z}\), we have

\begin{align}
\frac{\partial \cal L}{\partial z_i} =&\, \frac{1}{n\lambda}z_i - \alpha_i = 0, \quad \text{for} \,\, i = 1, \ldots, n, \nonumber \\
\text{and}\,\, \frac{\partial \cal L}{\partial \boldsymbol \beta} =&\, \boldsymbol \beta- \sum_{i=1}^n \alpha_i \mathbf{x}_i = \mathbf{0}
\end{align}

Hence, we have, the estimated \(\widehat{\boldsymbol \beta}\) is \(\sum_{i=1}^n \alpha_i \mathbf{x}_i\) that matches our previous understanding. Also, if we view this as a linear kernel solution, the predicted value of at \(\mathbf{x}\) is

\begin{align}
f(\mathbf{x}) =& \,\, \mathbf{x}^\text{T}\boldsymbol \beta\nonumber \\
=& \sum_{i=1}^n \alpha_i \mathbf{x}^\text{T}\mathbf{x}_i \nonumber \\
=& \sum_{i=1}^n \alpha_i K(\mathbf{x}, \mathbf{x}_i).
\end{align}

Now, to complete our dual solution, we plugin these results, and have

\begin{align}
\underset{\boldsymbol \alpha}{\max} \underset{\mathbf{z}, \boldsymbol \beta}{\min} {\cal L} =& \frac{n\lambda}{2} \boldsymbol \alpha^\text{T}\boldsymbol \alpha+ \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j x_i^\text{T}x_j + \sum_{j} \alpha_j \big(y_j - x_j^\text{T}\sum_i \alpha_i \mathbf{x}_i - n\lambda \alpha_i \big) \nonumber \\
 =& - \frac{n\lambda}{2} \boldsymbol \alpha^\text{T}\boldsymbol \alpha- \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j \langle x_i, x_j \rangle + \sum_{i} \alpha_i y_i \nonumber \\
=& - \frac{n\lambda}{2} \boldsymbol \alpha^\text{T}\boldsymbol \alpha- \frac{1}{2} \boldsymbol \alpha^\text{T}\mathbf{K}\boldsymbol \alpha+ \boldsymbol \alpha^\text{T}\mathbf{y}
\end{align}

By again taking derivative w.r.t. \(\alpha\), we have

\[ - n\lambda \mathbf{I}\boldsymbol \alpha- \mathbf{K}\boldsymbol \alpha+ \mathbf{y}= \mathbf{0},\]
and the solution is the same as what we had before

\[\boldsymbol \alpha= (\mathbf{K}+ n\lambda \mathbf{I})^{-1} \mathbf{y}\]

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\cA{{\cal A}}
\def\cT{{\cal T}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{classification-and-regression-trees}{%
\chapter{Classification and Regression Trees}\label{classification-and-regression-trees}}

A tree model is very simple to fit and enjoys interpretability. It is also the core component of random forest and boosting. Both trees and random forests can be used for classification and regression problems, although trees are not ideal for regressions problems due to its large bias. There are two main stream of tree models, Classification and Regression Trees (CART, Breiman et al. (\protect\hyperlink{ref-breiman1984classification}{1984})) and C4.5 (\protect\hyperlink{ref-quinlan1993c4}{Quinlan 1993}), which is an improvement of the ID3 (Iterative Dichotomiser 3) algorithm. The main difference is to use binary or multiple splits and the criteria of the splitting rule. In fact the splitting rule criteria is probably the most essential part of a tree.

\hypertarget{example-classification-tree}{%
\section{Example: Classification Tree}\label{example-classification-tree}}

Let's generate a model with nonlinear classification rule.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{    n }\OtherTok{=} \DecValTok{500}
\NormalTok{    x1 }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{    x2 }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{    y }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{(n, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{ifelse}\NormalTok{(x1}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ x2}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{\textless{}} \FloatTok{0.6}\NormalTok{, }\FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{))}
    
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{))}
    \FunctionTok{plot}\NormalTok{(x1, x2, }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
    \FunctionTok{symbols}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\AttributeTok{circles =} \FunctionTok{sqrt}\NormalTok{(}\FloatTok{0.6}\NormalTok{), }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{inches =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-215-1} \end{center}

A classification tree model is recursively splitting the feature space such that eventually each region is dominated by one class. We will use \texttt{rpart} as an example to fit trees, which stands for recursively partitioning.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
    \FunctionTok{library}\NormalTok{(rpart)}
\NormalTok{    rpart.fit }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(y)}\SpecialCharTok{\textasciitilde{}}\NormalTok{x1}\SpecialCharTok{+}\NormalTok{x2, }\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(x1, x2, y))}
    
    \CommentTok{\# the tree structure    }
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{rep}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\DecValTok{4}\NormalTok{))}
    \FunctionTok{plot}\NormalTok{(rpart.fit)}
    \FunctionTok{text}\NormalTok{(rpart.fit)    }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-216-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

    \CommentTok{\# if you want to peek into the tree }
    \CommentTok{\# note that we set cp = 0.041, which is a tuning parameter}
    \CommentTok{\# we will discuss this later}
\NormalTok{    rpart.fit}\SpecialCharTok{$}\NormalTok{cptable}
\DocumentationTok{\#\#           CP nsplit rel error    xerror       xstd}
\DocumentationTok{\#\# 1 0.17040359      0 1.0000000 1.0000000 0.04984280}
\DocumentationTok{\#\# 2 0.14798206      3 0.4843049 0.7264574 0.04692735}
\DocumentationTok{\#\# 3 0.01121076      4 0.3363229 0.4484305 0.04010884}
\DocumentationTok{\#\# 4 0.01000000      7 0.3004484 0.4035874 0.03852329}
    \FunctionTok{prune}\NormalTok{(rpart.fit, }\AttributeTok{cp =} \FloatTok{0.041}\NormalTok{)}
\DocumentationTok{\#\# n= 500 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# node), split, n, loss, yval, (yprob)}
\DocumentationTok{\#\#       * denotes terminal node}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#  1) root 500 223 0 (0.55400000 0.44600000)  }
\DocumentationTok{\#\#    2) x2\textless{} {-}0.6444322 90   6 0 (0.93333333 0.06666667) *}
\DocumentationTok{\#\#    3) x2\textgreater{}={-}0.6444322 410 193 1 (0.47073171 0.52926829)  }
\DocumentationTok{\#\#      6) x1\textgreater{}=0.6941279 68   8 0 (0.88235294 0.11764706) *}
\DocumentationTok{\#\#      7) x1\textless{} 0.6941279 342 133 1 (0.38888889 0.61111111)  }
\DocumentationTok{\#\#       14) x2\textgreater{}=0.7484327 53   7 0 (0.86792453 0.13207547) *}
\DocumentationTok{\#\#       15) x2\textless{} 0.7484327 289  87 1 (0.30103806 0.69896194)  }
\DocumentationTok{\#\#         30) x1\textless{} {-}0.6903174 51   9 0 (0.82352941 0.17647059) *}
\DocumentationTok{\#\#         31) x1\textgreater{}={-}0.6903174 238  45 1 (0.18907563 0.81092437) *}
\end{Highlighting}
\end{Shaded}

The model proceed with the following steps. Note that steps 5 and 6 may not be really beneficial (consider that we know the true model).

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-217-1} \end{center}

Alternatively, there are many other packages that can perform the same analysis. For example, the \texttt{tree} package. However, be careful that this package uses a different splitting rule by default If you want to match the result, use \texttt{split\ =\ "gini"}. Note that this plot is very crowded because it will split until pretty much only one class in each terminal node. Hence, you can imaging that there will be a tuning parameter issue. We will discuss this later.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(tree)}
\NormalTok{    tree.fit }\OtherTok{=} \FunctionTok{tree}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(y)}\SpecialCharTok{\textasciitilde{}}\NormalTok{x1}\SpecialCharTok{+}\NormalTok{x2, }\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(x1, x2, y), }\AttributeTok{split =} \StringTok{"gini"}\NormalTok{)}
    \FunctionTok{plot}\NormalTok{(tree.fit)}
    \FunctionTok{text}\NormalTok{(tree.fit)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-219-1} \end{center}

\hypertarget{splitting-a-node}{%
\section{Splitting a Node}\label{splitting-a-node}}

In a tree model, the splitting mechanism performs in the following way, which is just comparing all possible splits on all variables. For simplicity, we will assume that a binary splitting rule is used, i.e., we split the current node into to two child nodes, and apply the procedure recursively.

\begin{itemize}
\tightlist
\item
  At the current node, go through each variable to find the best cut-off point that splits the node.
\item
  Compare all the best cut-off points across all variable and choose the best one to split the current node and then iterate.
\end{itemize}

So, what error criterion should we use to compare different cut-off points? There are three of them at least:

\begin{itemize}
\tightlist
\item
  Gini impurity (CART)
\item
  Shannon entropy (C4.5)
\item
  Mis-classification error
\end{itemize}

Gini impurity is used in CART, while ID3/C4.5 uses the Shannon entropy. These criteria have different effects than the mis-classifications error. They usually prefer more ``pure'' nodes, meaning that it is more likely to single out a set of pure class terminal node if we use Gini impurity and Shannon entropy. This is because their measures are nonlinear.

Suppose that we have a population (or a set of observations) with \(p_k\) proportion of class \(k\), for \(k = 1, \ldots, K\). Then, the Gini impurity is given by

\[ \text{Gini} = \sum_{k = 1}^K p_k (1 - p_k) = 1 - \sum_{k=1}^K p_k^2.\]
The Shannon theory is defined as

\[- \sum_{k=1}^K p_k \log(p_k).\]
And the classification error simply adds up all mis-classified portions if we predict the population into the most prevalent one:

\[ 1 - \underset{k = 1, \ldots, K}{\max} \,\, p_k\]
The following plot shows all three quantities as a function of \(p\), when there are only two classes, i.e., \(K = 2\).

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-220-1} \end{center}

For each quantity, smaller value means that the node is more ``pure'', hence, there is a higher certainty when we predict a new value. The idea of splitting a node is that, we want the two resulting child node to contain less variation. In other words, we want each child node to be as ``pure'' as possible. Hence, the idea is to calculate this error criterion both before and after the split and see what cut-off point gives us the best reduction of error. Of course, all of these quantities will be calculated based on the sample version, instead of the truth. For example, if we use the Gini impurity to compare different splits, we use the following quantity for an \textbf{internal node} \({\cal A}\):

\begin{align}
\text{score}(j, c) = \text{Gini}({\cal A}) - \left( \frac{N_{{\cal A}_L}}{N_{{\cal A}}} \text{Gini}({\cal A}_L) + \frac{N_{{\cal A}_R}}{N_{{\cal A}}} \text{Gini}({\cal A}_R)  \right).
\end{align}

Here, \({\cal A}_L\) (left child node) and \({\cal A}_R\) (right child node) denote the two child nodes resulted from a potential split on the \(j\)th variable at a cut-off point \(c\), such that

\[{\cal A}_L = \{\mathbf{x}: \mathbf{x}\in {\cal A}, \, x_j \leq c\}\]
and

\[{\cal A}_R = \{\mathbf{x}: \mathbf{x}\in {\cal A}, \, x_j > c\}.\]
Then \(N_{\cal A}\), \(N_{{\cal A}_L}\), \(N_{{\cal A}_R}\) are the number of observations in these nodes, respectively. The implication of this is quite intuitive: \(\text{Gini}({\cal A})\) calculates the uncertainty of the entire node \({\cal A}\), while the second quantity is a summary of the uncertainty of the two potential child nodes. Hence a larger score indicates a better split, and we may choose the best index \(j\) and cut-off point \(c\) to proceed,

\[\underset{j \, , \, c}{\arg\max} \,\, \text{score}(j, c)\]

and then work on each child node separately using the same procedure.

\hypertarget{regression-trees}{%
\section{Regression Trees}\label{regression-trees}}

The basic procedure for a regression tree is pretty much the same as a classification tree, except that we will use a different way to evaluate how good a potential split is. Note that the variance is a simple quantity to describe the noise within a node, we can use

\begin{align}
\text{score}(j, c) = \text{Var}({\cal A}) - \left( \frac{N_{{\cal A}_L}}{N_{{\cal A}}} \text{Var}({\cal A}_L) + \frac{N_{{\cal A}_R}}{N_{{\cal A}}} \text{Var}({\cal A}_R)  \right).
\end{align}

\hypertarget{predicting-a-target-point}{%
\section{Predicting a Target Point}\label{predicting-a-target-point}}

When we have a new target point \(\mathbf{x}_0\) to predict, the basic strategy is to ``drop it down the tree''. This is simply starting from the root node and following the splitting rule to see which terminal node it ends up with. Note that a fitted tree will have a collection of terminal nodes, say, \(\{{\cal A}_1, {\cal A}_2, \ldots, {\cal A}_M\}\), then suppose \(\mathbf{x}_0\) falls into terminal node \({\cal A}_m\), we use \(\bar{y}_{{\cal A}_m}\), the average of original training data that falls into this node, as the prediction. The final prediction can be written as

\begin{align}
\widehat{f}(\mathbf{x}_0) =& \sum_{m = 1}^M \bar{y}_{{\cal A}_m} \mathbf{1}\{\mathbf{x}_0 \in {\cal A}_m\} \\
=& \sum_{m = 1}^M \frac{\sum_{i=1}^n y_i \mathbf{1}\{\mathbf{x}_i \in {\cal A}_m\}}{\sum_{i=1}^n \mathbf{1}\{\mathbf{x}_i \in {\cal A}_m\}} \mathbf{1}\{\mathbf{x}_0 \in {\cal A}_m\}.
\end{align}

\hypertarget{tuning-a-tree-model}{%
\section{Tuning a Tree Model}\label{tuning-a-tree-model}}

Tree tuning is essentially about when to stop splitting. Or we could look at this reversely by first fitting a very large tree, then see if we could remove some branches of a tree to make it simpler without sacrificing much accuracy. One approach is called the \textbf{cost-complexity pruning}. This is another penalized framework that we use the accuracy as the loss function, and use the tree-size as the penalty part for complexity. Formally, if we have any tree model \({\cal T}\), consider this can be written as

\begin{align}
C_\alpha({\cal T}) =&~ \sum_{\text{all terminal nodes $t$ in ${\cal T}$}} N_t \cdot \text{Impurity}(t) + \alpha |{\cal T}| \nonumber \\
=&~ C({\cal T}) + \alpha |{\cal T}|
\end{align}

Now, we can start with a very large tree, say, fitted until all pure terminal nodes. Call this tree as \({\cal T}_\text{max}\). We can then exhaust all its sub-trees by pruning any branches, and calculate this \(C(\cdot)\) function of the sub-tree. Then the tree that gives the smallest value will be our best tree.

But this can be computationally too expensive. Hence, one compromise, instead of trying all possible sub-trees, is to use the \textbf{weakest-link cutting}. This means that, we cut the branch (essentially a certain split) that displays the weakest banefit towards the \(C(\cdot)\) function. The procedure is the following:

\begin{itemize}
\tightlist
\item
  Look at an internal node \(t\) of \({\cal T}_\text{max}\), and denote the entire branch starting from \(t\) as \({\cal T}_t\)
\item
  Compare: remove the entire branch (collapse \({\cal T}_t\) into a single terminal node) vs.~keep \(T_t\). To do this, calculate
  \[\alpha \leq \frac{C(t) - C({\cal T}_t)}{|T_t| - 1}\]
  Note that \(|{\cal T}_t| - 1\) is the size difference between the two trees.
\item
  Try all internal nodes \(t\), and cut the branch \(t\) that has the smallest value on the right hand side. This gives the smallest \(\alpha\) value to remove some branches. Then iterate the procedure based on this reduced tree.
\end{itemize}

Note that the \(\alpha\) values will get larger as we move more branches. Hence this produces a solution path. Now this is very similar to the Lasso solution path idea, and we could use cross-validation to select the best tuning. By default, the \texttt{rpart} function uses a 10-fold cross-validation. This can be controlled using the \texttt{rpart.control()} function and specify the \texttt{xval} argument. For details, please see the \href{https://cran.r-project.org/web/packages/rpart/rpart.pdf}{documentation}. The following plot using \texttt{plotcp()} in the \texttt{rpart} package gives a visualization of the relative cross-validation error. It also produces a horizontal line (the dotted line). It suggests the lowest (plus certain variation) that we could achieve. Hence, we will select the best \texttt{cp} value (\(alpha\)) that is above this line. The way that this is constructed is similar to the \texttt{lambda.1se} choice in \texttt{glmnet}.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# and the tuning parameter }
  \FunctionTok{plotcp}\NormalTok{(rpart.fit)  }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-221-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{printcp}\NormalTok{(rpart.fit)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Classification tree:}
\DocumentationTok{\#\# rpart(formula = as.factor(y) \textasciitilde{} x1 + x2, data = data.frame(x1, }
\DocumentationTok{\#\#     x2, y))}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Variables actually used in tree construction:}
\DocumentationTok{\#\# [1] x1 x2}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Root node error: 223/500 = 0.446}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# n= 500 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\#         CP nsplit rel error  xerror     xstd}
\DocumentationTok{\#\# 1 0.170404      0   1.00000 1.00000 0.049843}
\DocumentationTok{\#\# 2 0.147982      3   0.48430 0.72646 0.046927}
\DocumentationTok{\#\# 3 0.011211      4   0.33632 0.44843 0.040109}
\DocumentationTok{\#\# 4 0.010000      7   0.30045 0.40359 0.038523}
\end{Highlighting}
\end{Shaded}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{random-forests}{%
\chapter{Random Forests}\label{random-forests}}

Roughly speaking, random forests (\protect\hyperlink{ref-breiman2001random}{Breiman 2001}) are parallelly fitted CART models with some randomness. There are several main components:

\begin{itemize}
\tightlist
\item
  Bootstrapping of data for each tree using the Bagging idea (\protect\hyperlink{ref-breiman1996bagging}{Breiman 1996}), and use the averaged result (for regression) or majority voting (for classification) of all trees as the prediction.
\item
  At each internal node, we may not consider all variables. Instead, we consider a randomly selected \texttt{mtry} variables to search for the best split. This idea was inspired by Ho (\protect\hyperlink{ref-ho1998random}{1998}).
\item
  For each tree, we will not perform pruning. Instead, we simply stop when the internal node contains no more than \texttt{nodesize} number of observations.
\end{itemize}

Later on, there were various version of random forests that attempts to improve the performance, from both computational and theoretical prospective. We will introduce them later.

\hypertarget{bagging-predictors}{%
\section{Bagging Predictors}\label{bagging-predictors}}

CART models may be difficult when dealing with non-axis-aligned decision boundaries. This can be seen from the example below, in a two-dimensional case. The idea of Bagging is that we can fit many CART models, each from a Bootstrap sample, i.e., sample with replacement from the original \(n\) observations. The reason that Breiman considered bootstrap samples is because it can approximate the original distribution that generates the data. But the end result is that since each tree may be slightly different from each other, when we stack them, the decision bound can be more ``smooth''.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# generate some data }
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{  n }\OtherTok{=} \DecValTok{1000}
\NormalTok{  x1 }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  x2 }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  y }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{(n, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{ifelse}\NormalTok{((x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{\textgreater{}} \SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{\&}\NormalTok{ (x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{\textless{}} \FloatTok{0.5}\NormalTok{) , }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}
\NormalTok{  xgrid }\OtherTok{=} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{x1 =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\AttributeTok{x2 =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Let's compare the decision rule of CART and Bagging. For CART, the decision line has to be aligned to axis. For Bagging, we use a total of 200 trees, specified by \texttt{nbagg} in the \texttt{ipred} package.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# fit CART}
  \FunctionTok{library}\NormalTok{(rpart)}
\NormalTok{  rpart.fit }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(y)}\SpecialCharTok{\textasciitilde{}}\NormalTok{x1}\SpecialCharTok{+}\NormalTok{x2, }\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(x1, x2, y))}

  \CommentTok{\# we could fit a different tree using a bootstrap sample}
  \CommentTok{\# rpart.fit = rpart(as.factor(y)\textasciitilde{}x1+x2, data = data.frame(x1, x2, y)[sample(1:n, n, replace = TRUE), ])}

\NormalTok{  pred }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{predict}\NormalTok{(rpart.fit, xgrid, }\AttributeTok{type =} \StringTok{"class"}\NormalTok{) }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\DecValTok{201}\NormalTok{, }\DecValTok{201}\NormalTok{)}
  \FunctionTok{contour}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{), pred, }\AttributeTok{levels=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{labels=}\StringTok{""}\NormalTok{,}\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(x1, x2, }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{yaxt=}\StringTok{"n"}\NormalTok{, }\AttributeTok{xaxt =} \StringTok{"n"}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(xgrid, }\AttributeTok{pch=}\StringTok{"."}\NormalTok{, }\AttributeTok{cex=}\FloatTok{1.2}\NormalTok{, }\AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(pred, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{))}
  \FunctionTok{box}\NormalTok{()    }
  \FunctionTok{title}\NormalTok{(}\StringTok{"CART"}\NormalTok{)}
 
  \CommentTok{\# fit Bagging}
  \FunctionTok{library}\NormalTok{(ipred)}
\NormalTok{  bag.fit }\OtherTok{=} \FunctionTok{bagging}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(y)}\SpecialCharTok{\textasciitilde{}}\NormalTok{x1}\SpecialCharTok{+}\NormalTok{x2, }\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(x1, x2, y), }\AttributeTok{nbagg =} \DecValTok{200}\NormalTok{, }\AttributeTok{ns =} \DecValTok{400}\NormalTok{)}
\NormalTok{  pred }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{predict}\NormalTok{(}\FunctionTok{prune}\NormalTok{(bag.fit), xgrid) }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\DecValTok{201}\NormalTok{, }\DecValTok{201}\NormalTok{)}
  \FunctionTok{contour}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{), pred, }\AttributeTok{levels=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{labels=}\StringTok{""}\NormalTok{,}\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(x1, x2, }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{yaxt=}\StringTok{"n"}\NormalTok{, }\AttributeTok{xaxt =} \StringTok{"n"}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(xgrid, }\AttributeTok{pch=}\StringTok{"."}\NormalTok{, }\AttributeTok{cex=}\FloatTok{1.2}\NormalTok{, }\AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(pred, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{))}
  \FunctionTok{box}\NormalTok{()}
  \FunctionTok{title}\NormalTok{(}\StringTok{"Bagging"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-226-1} \end{center}

\hypertarget{random-forests-1}{%
\section{Random Forests}\label{random-forests-1}}

Random forests are equipped with this Bootstrapping strategy, but also with other things, which are mentioned previously. They are controlled by several key parameters:

\begin{itemize}
\tightlist
\item
  \texttt{ntree}: number of trees
\item
  \texttt{sampsize}: how many samples to use when fitting each tree
\item
  \texttt{mtry}: number of randomly sampled variable to consider at each internal node
\item
  \texttt{nodesize}: stop splitting when the node sample size is no larger than \texttt{nodesize}
\end{itemize}

Using the \texttt{randomForest} package, we can fit the model. It is difficult to visualize this when \texttt{p\ \textgreater{}\ 2}. But we can look at the testing error.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# generate some data with larger p}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{  n }\OtherTok{=} \DecValTok{1000}
\NormalTok{  p }\OtherTok{=} \DecValTok{10}
\NormalTok{  X }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{runif}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{p, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), n, p)}
\NormalTok{  x1 }\OtherTok{=}\NormalTok{ X[, }\DecValTok{1}\NormalTok{]}
\NormalTok{  x2 }\OtherTok{=}\NormalTok{ X[, }\DecValTok{2}\NormalTok{]}
\NormalTok{  y }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{(n, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{ifelse}\NormalTok{((x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{\textgreater{}} \SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{\&}\NormalTok{ (x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{\textless{}} \FloatTok{0.5}\NormalTok{), }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}
\NormalTok{  xgrid }\OtherTok{=} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{x1 =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\AttributeTok{x2 =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{))}

  \CommentTok{\# fit random forests with a selected tuning}
  \FunctionTok{library}\NormalTok{(randomForest)}
\DocumentationTok{\#\# randomForest 4.7{-}1.1}
\DocumentationTok{\#\# Type rfNews() to see new features/changes/bug fixes.}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Attaching package: \textquotesingle{}randomForest\textquotesingle{}}
\DocumentationTok{\#\# The following object is masked from \textquotesingle{}package:ggplot2\textquotesingle{}:}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#     margin}
\NormalTok{  rf.fit }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(X, }\FunctionTok{as.factor}\NormalTok{(y), }\AttributeTok{ntree =} \DecValTok{1000}\NormalTok{, }
                        \AttributeTok{mtry =} \DecValTok{7}\NormalTok{, }\AttributeTok{nodesize =} \DecValTok{10}\NormalTok{, }\AttributeTok{sampsize =} \DecValTok{800}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Instead of generating a set of testing samples labels, let's directly compare with the ``true'' decision rule, the Bayes rule.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# the testing data }
\NormalTok{  Xtest }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{runif}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{p, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), n, p)}
  
  \CommentTok{\# the Bayes rule}
\NormalTok{  BayesRule }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{((Xtest[, }\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ Xtest[, }\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textgreater{}} \SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{\&} 
\NormalTok{                     (Xtest[, }\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ Xtest[, }\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textless{}} \FloatTok{0.5}\NormalTok{), }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
  
  \FunctionTok{mean}\NormalTok{( (}\FunctionTok{predict}\NormalTok{(rf.fit, Xtest) }\SpecialCharTok{==} \StringTok{"1"}\NormalTok{) }\SpecialCharTok{==}\NormalTok{ BayesRule )}
\DocumentationTok{\#\# [1] 0.785}
\end{Highlighting}
\end{Shaded}

\hypertarget{effect-of-mtry}{%
\section{\texorpdfstring{Effect of \texttt{mtry}}{Effect of mtry}}\label{effect-of-mtry}}

In the two dimensional setting, we probably won't see much difference by using random forests, since the only effective change is \texttt{mtry\ =\ 1}, which is not really different than \texttt{mtry\ =\ 2} (the CART choice). You can try this by yourself.
However, the difference would be significant in higher dimensional settings, in our case \(p=10\). This is again an issue of bias-variance trade-off. The intuition is that, when we use a small \texttt{mtry}, and when \(p\) is large, we may by chance randomly select some irrelevant variables that has nothing to do with the outcome. Then this particular split would be wasted. Missing the true variable may cause larger bias. On the other hand, when we use a large \texttt{mtry}, we will be greedy for signals since we compare many different variables and pick the best one. But this is also as the risk of over-fitting. Hence, tuning is necessary.

Just as an example, let's try a small \texttt{mtry}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  rf.fit }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(X, }\FunctionTok{as.factor}\NormalTok{(y), }\AttributeTok{ntree =} \DecValTok{1000}\NormalTok{, }
                        \AttributeTok{mtry =} \DecValTok{1}\NormalTok{, }\AttributeTok{nodesize =} \DecValTok{10}\NormalTok{, }\AttributeTok{sampsize =} \DecValTok{800}\NormalTok{)}

  \FunctionTok{mean}\NormalTok{( (}\FunctionTok{predict}\NormalTok{(rf.fit, Xtest) }\SpecialCharTok{==} \StringTok{"1"}\NormalTok{) }\SpecialCharTok{==}\NormalTok{ BayesRule )}
\DocumentationTok{\#\# [1] 0.634}
\end{Highlighting}
\end{Shaded}

\hypertarget{effect-of-nodesize}{%
\section{\texorpdfstring{Effect of \texttt{nodesize}}{Effect of nodesize}}\label{effect-of-nodesize}}

When we use a small \texttt{nodesize}, we are at the risk of over-fitting. This is similar to the 1NN example. When we use large \texttt{nodesize}, there could be under-fitting.

\hypertarget{variable-importance}{%
\section{Variable Importance}\label{variable-importance}}

Random forests model provides a way to evaluate the importance of each variable. This can be done by specifying the \texttt{importance} argument. We usually use the \texttt{MeanDecreaseAccuracy} or \texttt{MeanDecreaseGini} column as the summary of the importance of each variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  rf.fit }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(X, }\FunctionTok{as.factor}\NormalTok{(y), }\AttributeTok{ntree =} \DecValTok{1000}\NormalTok{, }
                        \AttributeTok{mtry =} \DecValTok{7}\NormalTok{, }\AttributeTok{nodesize =} \DecValTok{10}\NormalTok{, }\AttributeTok{sampsize =} \DecValTok{800}\NormalTok{,}
                        \AttributeTok{importance=}\ConstantTok{TRUE}\NormalTok{)}

  \FunctionTok{importance}\NormalTok{(rf.fit)}
\DocumentationTok{\#\#             0         1 MeanDecreaseAccuracy MeanDecreaseGini}
\DocumentationTok{\#\# 1  39.1053057 39.823786           45.4232897         47.79065}
\DocumentationTok{\#\# 2  38.1820764 40.119387           45.1964485         54.89580}
\DocumentationTok{\#\# 3   3.2719270  1.461298            3.2274895         28.44828}
\DocumentationTok{\#\# 4  {-}0.2777943 {-}6.287430           {-}4.8470758         22.09006}
\DocumentationTok{\#\# 5   2.0937973  1.654224            2.5256400         28.57575}
\DocumentationTok{\#\# 6   2.2354984 {-}2.435663           {-}0.1297796         25.29836}
\DocumentationTok{\#\# 7   0.2083020  2.724449            2.0679184         24.28751}
\DocumentationTok{\#\# 8   0.2018946  3.350897            2.3962745         25.51630}
\DocumentationTok{\#\# 9  {-}1.6159803  2.150674            0.3912234         23.41498}
\DocumentationTok{\#\# 10  2.6081961  4.417256            4.8004480         27.80399}
\end{Highlighting}
\end{Shaded}

\hypertarget{kernel-view-of-random-forets}{%
\section{Kernel view of Random Forets}\label{kernel-view-of-random-forets}}

I wrote a small function that will extract the kernel weights from a random forests for predicting a testing point \(x\). This is essentially the counts for how many times a training data falls into the same terminal node as \(x\). Since the prediction on \(x\) are essentially the average of them in a weighted fashion, this is basically a kernel averaging approach. However, the kernel weights are adaptive to the true structure.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# generate the 2 dimensional case}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{  n }\OtherTok{=} \DecValTok{1000}
\NormalTok{  x1 }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  x2 }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  y }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{(n, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{ifelse}\NormalTok{((x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{\textgreater{}} \SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{\&}\NormalTok{ (x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{\textless{}} \FloatTok{0.5}\NormalTok{) , }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}
\NormalTok{  xgrid }\OtherTok{=} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{x1 =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\AttributeTok{x2 =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{))}
  
  \CommentTok{\# fit a random forest model}
\NormalTok{  rf.fit }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(x1, x2), }\FunctionTok{as.factor}\NormalTok{(y), }\AttributeTok{ntree =} \DecValTok{300}\NormalTok{, }
                        \AttributeTok{mtry =} \DecValTok{1}\NormalTok{, }\AttributeTok{nodesize =} \DecValTok{20}\NormalTok{, }\AttributeTok{keep.inbag =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  pred }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{predict}\NormalTok{(rf.fit, xgrid) }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\DecValTok{201}\NormalTok{, }\DecValTok{201}\NormalTok{)}
  
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}

  \CommentTok{\# check the kernel weight at different points}
  \FunctionTok{plotRFKernel}\NormalTok{(rf.fit, }\FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(x1, x2)), }\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.4}\NormalTok{))}
  \FunctionTok{plotRFKernel}\NormalTok{(rf.fit, }\FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(x1, x2)), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-233-1} \end{center}

As contrast, here is the regular Gaussian kernel weights (after some tuning). This effect will play an important role when \(p\) is large.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# Gaussian kernel weights}
\NormalTok{  onex }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.4}\NormalTok{)}
\NormalTok{  h }\OtherTok{=} \FloatTok{0.2}
\NormalTok{  wt }\OtherTok{=} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\FunctionTok{rowSums}\NormalTok{(}\FunctionTok{sweep}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(x1, x2), }\DecValTok{2}\NormalTok{, onex, }\AttributeTok{FUN =} \StringTok{"{-}"}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{/}\NormalTok{h}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
  \FunctionTok{contour}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{), pred, }
          \AttributeTok{levels=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{labels=}\StringTok{""}\NormalTok{,}\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(x1, x2, }\AttributeTok{cex =} \DecValTok{4}\SpecialCharTok{*}\NormalTok{wt}\SpecialCharTok{\^{}}\NormalTok{(}\DecValTok{2}\SpecialCharTok{/}\DecValTok{3}\NormalTok{), }\AttributeTok{pch =} \DecValTok{1}\NormalTok{, }\AttributeTok{cex.axis=}\FloatTok{1.25}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(x1, x2, }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{), }
         \AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.75}\NormalTok{, }\AttributeTok{yaxt=}\StringTok{"n"}\NormalTok{, }\AttributeTok{xaxt =} \StringTok{"n"}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(xgrid, }\AttributeTok{pch=}\StringTok{"."}\NormalTok{, }\AttributeTok{cex=}\FloatTok{1.2}\NormalTok{, }
         \AttributeTok{col=}\FunctionTok{ifelse}\NormalTok{(pred, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{))}
  \FunctionTok{points}\NormalTok{(onex[}\DecValTok{1}\NormalTok{], onex[}\DecValTok{2}\NormalTok{], }\AttributeTok{pch =} \DecValTok{4}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{cex =}\DecValTok{4}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{6}\NormalTok{)}
  \FunctionTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-235-1} \end{center}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{boosting}{%
\chapter{Boosting}\label{boosting}}

Boosting is another ensemble model, created in the form of

\[F_T(x) = \sum_{t = 1}^T \alpha_t f_t(x)\]

However, it is different from random forest, in which each \(f_t(x)\) is learned parallelly. These \(f_t(x)\)'s are called weak learners and are constructed \textbf{sequentially}, with coefficients \(\alpha_t\)'s to represent their weights. The most classical model, AdaBoost was proposed by Freund and Schapire (\protect\hyperlink{ref-freund1997decision}{1997}) for classification problems, and a more statically view of this model called gradient boosting machines (\protect\hyperlink{ref-friedman2001greedy}{J. H. Friedman 2001}) can handle any loss function we commonly use. We will first introduce AdaBoost and then discuss gradient boosting.

\hypertarget{adaboost}{%
\section{AdaBoost}\label{adaboost}}

Following our common notation, we observe a set of data \(\{\mathbf{x}_i, y_i\}_{i=1}^n\). Similar to SVM, we code \(y_i\)s as \(-1\) or \(1\). The AdaBoost works by creating \(F_T(x)\) sequentially and use \(\text{sign}(F_T(x))\) as the classification rule. The algorithm is given in the following:

\begin{itemize}
\tightlist
\item
  Initiate weights \(w_i^{(1)} = 1/n\), for \(i = 1, \ldots, n\)
\item
  For \(t = 1, \ldots, T\), do

  \begin{itemize}
  \tightlist
  \item
    Fit a classifier \(f_t(x)\) to the training data with subject weights \(w_i^{(t)}\)'s.
  \item
    Compute the weighed error rate
    \[\epsilon_t = \sum_{i=1}^n w_i^{(t)} \mathbf{1}\{y_i \neq f_t(x_i) \}\]
  \item
    Compute
    \[\alpha_t = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t}\]
  \item
    Update subject weights
    \[w_i^{(t + 1)} = \frac{1}{Z_t} w_i^{(t)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\}\]
    where \(Z_t\) is a normalizing constant make \(w_i^{(t + 1)}\)'s sum up to 1:
    \[Z_t = \sum_{i=1}^n w_i^{(t)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\}\]
  \end{itemize}
\item
  Output the final model
  \[F_T(x) = \sum_{t = 1}^T \alpha_t f_t(x)\]
  and the decision rule is \(\text{sign}(F_T(x))\).
\end{itemize}

An important mechanism in AdaBoost is the weight update step. We can notice that the weight is increased if \(\exp\big\{ - \alpha_t y_i f_t(x_i) \big\}\) is larger than 1. This is simply when \(y_i f_t(x_i)\) is negative, i.e., subject \(i\) got mis-classified by \(f_t\) at this iteration. Hence, during the next iteration \(t+1\), the model \(f_{(t+1)}\) will more likely to address this subject. Here, \(f_t\) can be any classification model, for example, we could use a tree model. The following figures demonstrate this idea of updating weights and aggregate the learners.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  x1 }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{  x2 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.7}\NormalTok{,}
         \FloatTok{0.8}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{)}
  
  \CommentTok{\# the data}
\NormalTok{  y }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }
        \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\NormalTok{  X }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\StringTok{"x1"} \OtherTok{=}\NormalTok{ x1, }\StringTok{"x2"} \OtherTok{=}\NormalTok{ x2)}
\NormalTok{  xgrid }\OtherTok{=} \FunctionTok{expand.grid}\NormalTok{(}\StringTok{"x1"} \OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.1}\NormalTok{, }\FloatTok{0.01}\NormalTok{), }\StringTok{"x2"} \OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.9}\NormalTok{, }\FloatTok{0.01}\NormalTok{))}
  
  \CommentTok{\# plot data}
  \FunctionTok{plot}\NormalTok{(X[, }\DecValTok{1}\NormalTok{], X[, }\DecValTok{2}\NormalTok{], }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.1}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{,}
       \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.9}\NormalTok{), }\AttributeTok{cex =} \DecValTok{3}\NormalTok{)}
  
  \CommentTok{\# fit gbm with 3 trees}
  \FunctionTok{library}\NormalTok{(gbm)}
\DocumentationTok{\#\# Loaded gbm 2.1.8.1}
\NormalTok{  gbm.fit }\OtherTok{=} \FunctionTok{gbm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\FunctionTok{data.frame}\NormalTok{(x1, x2, }\AttributeTok{y=} \FunctionTok{as.numeric}\NormalTok{(y }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)), }
                \AttributeTok{distribution=}\StringTok{"adaboost"}\NormalTok{, }\AttributeTok{interaction.depth =} \DecValTok{1}\NormalTok{, }
                \AttributeTok{n.minobsinnode =} \DecValTok{1}\NormalTok{, }\AttributeTok{n.trees =} \DecValTok{3}\NormalTok{, }
                \AttributeTok{shrinkage =} \DecValTok{1}\NormalTok{, }\AttributeTok{bag.fraction =} \DecValTok{1}\NormalTok{)}
  
  \CommentTok{\# you may peek into each tree}
  \FunctionTok{pretty.gbm.tree}\NormalTok{(gbm.fit, }\AttributeTok{i.tree =} \DecValTok{1}\NormalTok{)}
\DocumentationTok{\#\#   SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight Prediction}
\DocumentationTok{\#\# 0        0          0.25        1         2           3            2.5     10       0.00}
\DocumentationTok{\#\# 1       {-}1          1.00       {-}1        {-}1          {-}1            0.0      2       1.00}
\DocumentationTok{\#\# 2       {-}1         {-}0.25       {-}1        {-}1          {-}1            0.0      8      {-}0.25}
\DocumentationTok{\#\# 3       {-}1          0.00       {-}1        {-}1          {-}1            0.0     10       0.00}
  
  \CommentTok{\# we can view the predicted decision rule}
  \FunctionTok{plot}\NormalTok{(X[, }\DecValTok{1}\NormalTok{], X[, }\DecValTok{2}\NormalTok{], }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.1}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{,}
       \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.9}\NormalTok{), }\AttributeTok{cex =} \DecValTok{3}\NormalTok{)}
\NormalTok{  pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(gbm.fit, xgrid)}
\DocumentationTok{\#\# Using 3 trees...}
  \FunctionTok{points}\NormalTok{(xgrid, }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(pred }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{), }
         \AttributeTok{cex =} \FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-239-1} \end{center}

Here is a rundown of the algorithm. Let's initialize all weights as \(1/n\). We only used trees with a single split as weak learners. The first tree is splitting at \(X_1 = 0.25\). After the first split, we need to adjust the weights.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  w1 }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{  f1 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{ifelse}\NormalTok{(x[, }\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textless{}} \FloatTok{0.25}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\NormalTok{  e1 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(w1}\SpecialCharTok{*}\NormalTok{(}\FunctionTok{f1}\NormalTok{(X) }\SpecialCharTok{!=}\NormalTok{ y))}
\NormalTok{  a1 }\OtherTok{=} \FloatTok{0.5}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{((}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{e1)}\SpecialCharTok{/}\NormalTok{e1)}
  
\NormalTok{  w2 }\OtherTok{=}\NormalTok{ w1}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{ a1}\SpecialCharTok{*}\NormalTok{y}\SpecialCharTok{*}\FunctionTok{f1}\NormalTok{(X))}
\NormalTok{  w2 }\OtherTok{=}\NormalTok{ w2}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(w2)}
  
  \CommentTok{\# the first tree}
  \FunctionTok{plot}\NormalTok{(X[, }\DecValTok{1}\NormalTok{], X[, }\DecValTok{2}\NormalTok{], }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.1}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{,}
       \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.9}\NormalTok{), }\AttributeTok{cex =} \DecValTok{3}\NormalTok{)}
  
\NormalTok{  pred }\OtherTok{=} \FunctionTok{f1}\NormalTok{(xgrid)}
  \FunctionTok{points}\NormalTok{(xgrid, }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(pred }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{), }
         \AttributeTok{cex =} \FloatTok{0.2}\NormalTok{)}
  
  \CommentTok{\# weights after the first tree}
  \FunctionTok{plot}\NormalTok{(X[, }\DecValTok{1}\NormalTok{], X[, }\DecValTok{2}\NormalTok{], }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.1}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{,}
       \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.9}\NormalTok{), }\AttributeTok{cex =} \DecValTok{30}\SpecialCharTok{*}\NormalTok{w2)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-241-1} \end{center}

We can notice that the observations got correctly classified will decrease their weights while those mis-classified will increase the weights.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  f2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{ifelse}\NormalTok{(x[, }\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textgreater{}} \FloatTok{0.65}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\NormalTok{  e2 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(w2}\SpecialCharTok{*}\NormalTok{(}\FunctionTok{f2}\NormalTok{(X) }\SpecialCharTok{!=}\NormalTok{ y))}
\NormalTok{  a2 }\OtherTok{=} \FloatTok{0.5}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{((}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{e2)}\SpecialCharTok{/}\NormalTok{e2)}
  
\NormalTok{  w3 }\OtherTok{=}\NormalTok{ w2}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{ a2}\SpecialCharTok{*}\NormalTok{y}\SpecialCharTok{*}\FunctionTok{f2}\NormalTok{(X))}
\NormalTok{  w3 }\OtherTok{=}\NormalTok{ w3}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(w3)}
  
  \CommentTok{\# the second tree}
  \FunctionTok{plot}\NormalTok{(X[, }\DecValTok{1}\NormalTok{], X[, }\DecValTok{2}\NormalTok{], }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.1}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{,}
       \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.9}\NormalTok{), }\AttributeTok{cex =} \DecValTok{30}\SpecialCharTok{*}\NormalTok{w2)}
  
\NormalTok{  pred }\OtherTok{=} \FunctionTok{f2}\NormalTok{(xgrid)}
  \FunctionTok{points}\NormalTok{(xgrid, }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(pred }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{), }
         \AttributeTok{cex =} \FloatTok{0.2}\NormalTok{)}
  
  \CommentTok{\# weights after the second tree}
  \FunctionTok{plot}\NormalTok{(X[, }\DecValTok{1}\NormalTok{], X[, }\DecValTok{2}\NormalTok{], }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.1}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{,}
       \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.9}\NormalTok{), }\AttributeTok{cex =} \DecValTok{30}\SpecialCharTok{*}\NormalTok{w3)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-242-1} \end{center}

And then we have the third step. Combining all three steps and their decision function, we have the final classifier

\begin{align}
F_3(x) =& \sum_{t=1}^3 \alpha_t f_t(x) \nonumber \\
=& 0.4236 \cdot f_1(x) + 0.6496 \cdot f_2(x) + 0.9229 \cdot f_3(x)
\end{align}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  f3 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{ifelse}\NormalTok{(x[, }\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textless{}} \FloatTok{0.85}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\NormalTok{  e3 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(w3}\SpecialCharTok{*}\NormalTok{(}\FunctionTok{f3}\NormalTok{(X) }\SpecialCharTok{!=}\NormalTok{ y))}
\NormalTok{  a3 }\OtherTok{=} \FloatTok{0.5}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{((}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{e3)}\SpecialCharTok{/}\NormalTok{e3)}
  
  \CommentTok{\# the third tree}
  \FunctionTok{plot}\NormalTok{(X[, }\DecValTok{1}\NormalTok{], X[, }\DecValTok{2}\NormalTok{], }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.1}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{,}
       \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.9}\NormalTok{), }\AttributeTok{cex =} \DecValTok{30}\SpecialCharTok{*}\NormalTok{w3)}
  
\NormalTok{  pred }\OtherTok{=} \FunctionTok{f3}\NormalTok{(xgrid)}
  \FunctionTok{points}\NormalTok{(xgrid, }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(pred }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{), }
         \AttributeTok{cex =} \FloatTok{0.2}\NormalTok{)}
  
  \CommentTok{\# the final decision rule }
  \FunctionTok{plot}\NormalTok{(X[, }\DecValTok{1}\NormalTok{], X[, }\DecValTok{2}\NormalTok{], }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.1}\NormalTok{), }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{,}
       \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.9}\NormalTok{), }\AttributeTok{cex =} \DecValTok{3}\NormalTok{)}
  
\NormalTok{  pred }\OtherTok{=}\NormalTok{ a1}\SpecialCharTok{*}\FunctionTok{f1}\NormalTok{(xgrid) }\SpecialCharTok{+}\NormalTok{ a2}\SpecialCharTok{*}\FunctionTok{f2}\NormalTok{(xgrid) }\SpecialCharTok{+}\NormalTok{ a3}\SpecialCharTok{*}\FunctionTok{f3}\NormalTok{(xgrid)}
  \FunctionTok{points}\NormalTok{(xgrid, }\AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(pred }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{), }
         \AttributeTok{cex =} \FloatTok{0.2}\NormalTok{)}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \FloatTok{0.25}\NormalTok{) }\CommentTok{\# f1}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \FloatTok{0.65}\NormalTok{) }\CommentTok{\# f2}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \FloatTok{0.85}\NormalTok{) }\CommentTok{\# f3}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-243-1} \end{center}

\hypertarget{training-error-of-adaboost}{%
\section{Training Error of AdaBoost}\label{training-error-of-adaboost}}

There is an interesting property about the boosting algorithm that if we can always find a classifier that performs better than random guessing at each iteration \(t\), then the training error will eventually converge to zero. This works by analyzing the weight after the last iteration \(T\):

\begin{align}
w_i^{(T+1)} =& \frac{1}{Z_T} w_i^{(T)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\} \nonumber \\
=& \frac{1}{Z_1\cdots Z_T} w_i^{(1)} \prod_{t = 1}^T \exp\big\{ - \alpha_t y_i f_t(x_i) \big\} \nonumber \\
=& \frac{1}{Z_1\cdots Z_T} \frac{1}{n} \exp\Big\{ - y_i \sum_{t = 1}^T \alpha_t f_t(x_i) \Big\}
\end{align}

Since \(\sum_{t = 1}^T \alpha_t f_t(x_i)\) is just the model at the \(T\)-th iteration, we can write it as \(F_T(x_i)\). Noticing that they sum up to 1, we have

\[1 = \sum_{i = 1}^n w_i^{(T+1)} = \frac{1}{Z_1\cdots Z_T} \frac{1}{n} \sum_{i = 1}^n \exp\big\{ - y_i F_T(x_i) \big\}\]
and
\[Z_1\cdots Z_T = \frac{1}{n} \sum_{i = 1}^n \exp\big\{ - y_i F_T(x_i) \big\}\]
On the right-hand-side, this is the exponential loss after we fit the model. In fact, this quantity would bound above the 0/1 loss, since the exponential loss is \(\exp[ - y f(x) ]\),

\begin{itemize}
\tightlist
\item
  For correctly classified subjects, \(y f(x) > 0\), and \(\exp[ - y f(x) ] > 0\)
\item
  For incorrectly classified subjects, \(y f(x) < 0\) the exponential loss is larger than 1
\end{itemize}

This means that

\[Z_1\cdots Z_T > \frac{1}{n} \sum_{i = 1}^n \mathbf{1} \big\{ y_i \neq \text{sign}(F_T(x_i)) \big\}\]
Hence, if we want the final model to have low training error, we should bound above the \(Z_t\)'s. Recall that \(Z_t\) is used to normalize the weights, we have

\[Z_t = \sum_i^{n} w_i^{(t)} \exp[ - \alpha_t y_i f_t(x_i) ].\]
We have two cases at this iteration, \(y_i f(x_i) = 1\) for correct subjects, and \(y_i f(x_i) = -1\) for the incorrect ones, hence,
By our definition, \(\epsilon_t = \sum_i w_i^{(t)} \mathbf{1} \big\{ y_i \neq f_t(x_i) \big\}\) is the proportion of weights for mis-classified samples.
\begin{align}
Z_t =& \,\,\sum_{i=1}^n w_i^{(t)} \exp[ - \alpha_t y_i f_t(x_i)] \nonumber\\
=&\,\,\sum_{y_i = f_t(x_i)} w_i^{(t)} \exp[ - \alpha_t ] +  \sum_{y_i \neq f_t(x_i)} w_i^{(t)} \exp[ \alpha_t ] \nonumber\\
=& \,\, \exp[ - \alpha_t ] \sum_{y_i = f_t(x_i)} w_i^{(t)} + \exp[ \alpha_t ] \sum_{y_i \neq f_t(x_i)} w_i^{(t)}
\end{align}

So we have

\[ Z_t = (1 - \epsilon_t) \exp[ - \alpha_t ] + \epsilon_t \exp[ \alpha_t ].\]

If we want to minimize the product of all \(Z_t\)'s, we can consider minimizing each of them. Let's consider this as a function of \(\alpha_t\), then by taking a derivative with respect to \(\alpha_t\), we have

\[ - (1 - \epsilon_t) \exp[ - \alpha_t ] + \epsilon_t \exp[ \alpha_t ] = 0\]
and

\[\alpha_t = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t}.\]
Plugging this back into \(Z_t\), we have

\[Z_t = 2 \sqrt{\epsilon_t(1-\epsilon_t)}\]
Since \(\epsilon_t(1-\epsilon_t)\) can only attain maximum of \(1/4\), \(Z_t\) must be smaller than 1. This makes the product \(Z_1 \cdots Z_T\) converging to 0. If we look at this more closely, by defining \(\gamma_t = \frac{1}{2} - \epsilon_t\) as the improvement from a random model (with error \(1/2\)), then

\begin{align}
Z_t =& 2 \sqrt{\epsilon_t(1-\epsilon_t)} \nonumber \\
=& \sqrt{1 - 4 \gamma_t^2} \nonumber \\
\leq& \exp\big[ - 2 \gamma_t^2 \big]
\end{align}

The last equation is because by Taylor expansion, \(\exp\big[ - 4 \gamma_t^2 \big] \geq 1 - 4 \gamma_t^2\). Then, we can finally put all \(Z_t\)'s together:

\begin{align}
\text{Training Error} =& \sum_{i = 1}^n \mathbf{1} \big\{ y_i \neq \text{sign}(F_T(x_i)) \big\} \nonumber \\
=& \sum_{i = 1}^n \exp \big[ - y_i \neq F_T(x_i) \big] \nonumber \\
=& Z_1 \cdots Z_T \nonumber \\
\leq& \exp \big[ - 2 \sum_{t=1}^T \gamma_t^2 \big],
\end{align}

which converges to 0 as long as \(\sum_{t=1}^T \gamma_t^2\) accumulates up to infinite. But of course, in practice, it would increasing difficult find \(f_t(x)\) that reduces the training error greatly.

\hypertarget{tuning-the-number-of-trees}{%
\section{Tuning the Number of Trees}\label{tuning-the-number-of-trees}}

Although we can get really low training classification error, this is subject to overfitting. The following code demonstrates what an overfitted looks like.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# One{-}dimensional classification example}
\NormalTok{  n }\OtherTok{=} \DecValTok{1000}\NormalTok{; }\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  x }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n), }\FunctionTok{runif}\NormalTok{(n))}
\NormalTok{  py }\OtherTok{=}\NormalTok{ (}\FunctionTok{sin}\NormalTok{(}\DecValTok{4}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{x[, }\DecValTok{1}\NormalTok{]) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\DecValTok{2}
\NormalTok{  y }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, py)}
  
  \FunctionTok{plot}\NormalTok{(x[, }\DecValTok{1}\NormalTok{], y }\SpecialCharTok{+} \FunctionTok{runif}\NormalTok{(n, }\SpecialCharTok{{-}}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.05}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.05}\NormalTok{, }\FloatTok{1.05}\NormalTok{), }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
       \AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y}\SpecialCharTok{==}\DecValTok{1}\NormalTok{,}\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{), }\AttributeTok{xlab =} \StringTok{"x"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"P(Y=1 | X=x)"}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(x[, }\DecValTok{1}\NormalTok{], py, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-245-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
  
  \CommentTok{\# fit AdaBoost with bootstrapping, I am using a large shrinkage factor}
\NormalTok{  gbm.fit }\OtherTok{=} \FunctionTok{gbm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\FunctionTok{data.frame}\NormalTok{(x, y), }\AttributeTok{distribution=}\StringTok{"adaboost"}\NormalTok{, }\AttributeTok{n.minobsinnode =} \DecValTok{2}\NormalTok{, }
                \AttributeTok{n.trees=}\DecValTok{200}\NormalTok{, }\AttributeTok{shrinkage =} \DecValTok{1}\NormalTok{, }\AttributeTok{bag.fraction=}\FloatTok{0.8}\NormalTok{, }\AttributeTok{cv.folds =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# plot the decision function (Fx, not sign(Fx))}
\NormalTok{  size}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{)}

  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{)}
\NormalTok{  \{}
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{))}
    \FunctionTok{plot}\NormalTok{(x[, }\DecValTok{1}\NormalTok{], py, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"P(Y=1 | X=x)"}\NormalTok{, }\AttributeTok{col =} \StringTok{"gray"}\NormalTok{)}
    \FunctionTok{points}\NormalTok{(x[, }\DecValTok{1}\NormalTok{], y }\SpecialCharTok{+} \FunctionTok{runif}\NormalTok{(n, }\SpecialCharTok{{-}}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.05}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{ylim =}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.05}\NormalTok{, }\FloatTok{1.05}\NormalTok{),}
           \AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(y}\SpecialCharTok{==}\DecValTok{1}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{))}
\NormalTok{    Fx }\OtherTok{=} \FunctionTok{predict}\NormalTok{(gbm.fit, }\AttributeTok{n.trees=}\NormalTok{size[i]) }\CommentTok{\# this returns the fitted function, but not class}
    \FunctionTok{lines}\NormalTok{(x[, }\DecValTok{1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\SpecialCharTok{*}\NormalTok{Fx)), }\AttributeTok{lwd =} \DecValTok{1}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"\# of Iterations = "}\NormalTok{, size[i]))}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-247-1} \end{center}

Hence, selecting trees is necessary. For this purpose, we can use either the out-of-bag error to estimate the exponential upper bound, or simply do cross-validation.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# get the best number of trees from cross{-}validation (or oob if no cv is used)}
  \FunctionTok{gbm.perf}\NormalTok{(gbm.fit)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-249-1} \end{center}

\begin{verbatim}
## [1] 39
\end{verbatim}

\hypertarget{gradient-boosting}{%
\section{Gradient Boosting}\label{gradient-boosting}}

Let's take an alternative view of this problem, we use an additive structure to fit models

\[F_T(x) = \sum_{t = 1}^T \alpha_t f(x; \boldsymbol \theta_t)\]

by minimizing a loss function

\[\underset{\{\alpha_t, \boldsymbol \theta_t\}_{t=1}^T}{\min} \sum_{i=1}^n L\big(y_i, F_T(x_i)\big)\]
In this framework, we may choose a loss function \(L\) that is suitable for the problem, and also choose the base learner \(f(x; \boldsymbol \theta)\) with parameter \(\boldsymbol \theta\). Examples of this include linear function, spline, tree, etc.. While it maybe difficult to minimize over all parameters \(\{\alpha_t, \boldsymbol \theta_t\}_{t=1}^T\), we may consider doing this in a stage-wise fashion. The algorithm could work in the following way:

\begin{itemize}
\tightlist
\item
  Set \(F_0(x) = 0\)
\item
  For \(t = 1, \ldots, T\)

  \begin{itemize}
  \tightlist
  \item
    Choose \((\alpha_t, \boldsymbol \theta_t)\) to minimize the loss
    \[\underset{\alpha, \boldsymbol \theta}{\min} \,\, \sum_{i=1}^n L\big(y_i, F_{t-1}(x_i) + \alpha f(x_i; \boldsymbol \theta)\big)\]
  \item
    Update \(F_t(x) = F_{t-1}(x) + \alpha_t f(x; \boldsymbol \theta_t)\)
  \end{itemize}
\item
  Output \(F_T(x)\) as the final model
\end{itemize}

The previous AdaBoost example is using exponential loss function. Also, it doesn't pick an optimal \(f(x; \boldsymbol \theta)\) at each step. We just need a model that is better than random. The step size \(\alpha_t\) is optimized at each \(t\) given the fitted \(f(x; \boldsymbol \theta_t)\).

Another example is the forward stage-wise linear regression. In this case, we fit a single variable linear model at each step \(t\):

\[f(x, j) = \text{sign}\big(\text{Cor}(X_j, \mathbf{r})\big) X_j\]
* \(\mathbf{r}\) is the residual, as \(r_i = y_i - F_{t-1}(x_i)\)
* \(j\) is the index that has the largest absolute correlation with \(\mathbf{r}\)

Then we give a very small step size \(\alpha_t\), say, \(\alpha_t = 10^{-5}\), and with sign equal to the correlation between \(X_j\). In this case, \(F_t(x)\) is almost equivalent to the Lasso solution path, as \(t\) increases.

We may notice that \(r_i\) is in fact the negative gradient of the squared-error loss, as a function of the fitted function:

\[r_{it} = - \left[ \frac{\partial \, \big(y_i - F(x_i)\big)^2 }{\partial \, F(x_i)} \right]_{F(x_i) = F_{t-1}(x_i)}\]
and we are essentially fitting a weak leaner \(f_t(x)\) to the residuals and update the fitted model \(F_t(x)\). The following example shows the result of using a tree leaner as \(f_t(x)\):

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(gbm)}

  \CommentTok{\# a simple regression problem}
\NormalTok{  p }\OtherTok{=} \DecValTok{1}
\NormalTok{  x }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.001}\NormalTok{)}
\NormalTok{  fx }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) }\DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(}\DecValTok{3}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{x)}
\NormalTok{  y }\OtherTok{=} \FunctionTok{fx}\NormalTok{(x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x))}

  \FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"y"}\NormalTok{, }\AttributeTok{col =} \StringTok{"gray"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
  \CommentTok{\# plot the true regression line}
  \FunctionTok{lines}\NormalTok{(x, }\FunctionTok{fx}\NormalTok{(x), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-251-1} \end{center}

We can see that the fitted model progressively approaximates the true function.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# fit regression boosting}
  \CommentTok{\# I use a very large shrinkage value for demonstrating the functions}
  \CommentTok{\# in practice you should use 0.1 or even smaller values for stability}
\NormalTok{  gbm.fit }\OtherTok{=} \FunctionTok{gbm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x, }\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(x, y), }\AttributeTok{distribution =} \StringTok{"gaussian"}\NormalTok{,}
                \AttributeTok{n.trees=}\DecValTok{300}\NormalTok{, }\AttributeTok{shrinkage=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{bag.fraction=}\FloatTok{0.8}\NormalTok{)}

  \CommentTok{\# somehow, cross{-}validation for 1 dimensional problem creates error}
  \CommentTok{\# gbm(y \textasciitilde{} ., data = data.frame(x, y), cv.folds = 3) \# this produces an error  }
  
  \CommentTok{\# plot the fitted regression function at several iterations}
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{  size}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{50}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{300}\NormalTok{)}
  
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{)}
\NormalTok{  \{}
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{))}
    \FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"y"}\NormalTok{, }\AttributeTok{col =} \StringTok{"gray"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
    \FunctionTok{lines}\NormalTok{(x, }\FunctionTok{fx}\NormalTok{(x), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{)}
    
    \CommentTok{\# this returns the fitted function, but not class}
\NormalTok{    Fx }\OtherTok{=} \FunctionTok{predict}\NormalTok{(gbm.fit, }\AttributeTok{n.trees=}\NormalTok{size[i])}
    \FunctionTok{lines}\NormalTok{(x, Fx, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{)}
    \FunctionTok{title}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"\# of Iterations = "}\NormalTok{, size[i]))}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{SMLR_files/figure-latex/unnamed-chunk-252-1} \end{center}

This idea can be generalized to any loss function \(L\). This is the \textbf{gradient boosting} model:

\begin{itemize}
\tightlist
\item
  At each iteration \(t\), calculate ``pseudo-residuals'\,', i.e., the negative gradient for each observation
  \[g_{it} = - \left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F(x_i) = F_{t-1}(x_i)}\]
\item
  Fit \(f_t(x, \boldsymbol \theta_t)\) to pseudo-residual \(g_{it}\)'s
\item
  Search for the best \alert{step length}
  \[\alpha_t = \underset{\alpha}{\arg\min} \sum_{i=1}^n L\big(y_i, F_{t-1}(x_i) + \alpha f(x_i; \boldsymbol \theta_t)\big)\]
\item
  Update \(F_t(x) = F_{t-1}(x) + \alpha_t f(x; \boldsymbol \theta_t)\)
\end{itemize}

Hence, the only change when modeling different outcomes is to choose the loss function \(L\), and derive the pseudo-residuals

\begin{itemize}
\tightlist
\item
  For regression, the loss is \(\frac{1}{2} (y - f(x))^2\), and the pseudo-residual is \(y_i - f(x_i)\)
\item
  For quantile regression to model median, the loss is \(|y - f(x)|\), and the pseudo-residual is sign\((y_i - f(x_i))\) \textbackslash{}
\item
  For classification, we can use the deviance \(y\log(p) + (1-y)\log(1-p)\), and express \(p\) as the log-odds of a scale predictor, i.e., \(f = \log(p/(1-p))\). Then the pseudo-residual is \(y_i - p(x_i)\)
\end{itemize}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{part-unsupervised-learning}{%
\part{Unsupervised Learning}\label{part-unsupervised-learning}}

\hypertarget{k-means}{%
\chapter{K-Means}\label{k-means}}

\hypertarget{basic-concepts}{%
\section{Basic Concepts}\label{basic-concepts}}

The \(k\)-means clustering algorithm attempts to solve the following optimization problem:

\[ \underset{C, \, \{m_k\}_{k=1}^K}\min \sum_{k=1}^K \sum_{C(i) = k} \lVert x_i - m_k \rVert^2, \]
where \(C(\cdot): \{1, \ldots, n\} \rightarrow \{1, \ldots, K\}\) is a cluster assignment function, and \(m_k\)'s are the cluster means. To solve this problem, \(k\)-means uses an iterative approach that updates \(C(\cdot)\) and \(m_k\)'s alternatively. Suppose we have a set of six observations.

\begin{center}\includegraphics[width=0.35\linewidth]{SMLR_files/figure-latex/unnamed-chunk-255-1} \end{center}

We first randomly assign them into two clusters (initiate a random \(C\) function). Based on this cluster assignment, we can calculate the corresponding cluster mean \(m_k\)'s.

\begin{center}\includegraphics[width=0.7\linewidth]{SMLR_files/figure-latex/unnamed-chunk-257-1} \end{center}

Then we will assign each observation to the closest cluster mean. In this example, only the blue point on the top will be moved to a new cluster. Then the cluster means can then be recalculated.

\begin{center}\includegraphics[width=0.7\linewidth]{SMLR_files/figure-latex/unnamed-chunk-258-1} \end{center}

When there is nothing to move anymore, the algorithm stops. Keep in mind that we started with a random cluster assignment, and this objective function is not convex. Hence we may obtain different results if started with different values. The solution is to try different starting points and use the best final results. This can be tuned using the \texttt{nstart} parameter in the \texttt{kmeans()} function.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# some random data}
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{    mat }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{), }\DecValTok{50}\NormalTok{, }\DecValTok{20}\NormalTok{)}
    
    \CommentTok{\# if we use only one starting point}
    \FunctionTok{kmeans}\NormalTok{(mat, }\AttributeTok{centers =} \DecValTok{3}\NormalTok{, }\AttributeTok{nstart =} \DecValTok{1}\NormalTok{)}\SpecialCharTok{$}\NormalTok{tot.withinss}
\DocumentationTok{\#\# [1] 885.8913}
    
    \CommentTok{\# if we use multiple starting point and pick the best one}
    \FunctionTok{kmeans}\NormalTok{(mat, }\AttributeTok{centers =} \DecValTok{3}\NormalTok{, }\AttributeTok{nstart =} \DecValTok{100}\NormalTok{)}\SpecialCharTok{$}\NormalTok{tot.withinss}
\DocumentationTok{\#\# [1] 883.8241}
\end{Highlighting}
\end{Shaded}

\hypertarget{example-1-iris-data}{%
\section{\texorpdfstring{Example 1: \texttt{iris} data}{Example 1: iris data}}\label{example-1-iris-data}}

We use the classical \texttt{iris} data as an example. This dataset contains three different classes, but the goal here is to learn the clusters without knowing the class labels.

\begin{Shaded}
\begin{Highlighting}[]
    \CommentTok{\# plot the original data using two variables}
    \FunctionTok{head}\NormalTok{(iris)}
\DocumentationTok{\#\#   Sepal.Length Sepal.Width Petal.Length Petal.Width Species}
\DocumentationTok{\#\# 1          5.1         3.5          1.4         0.2  setosa}
\DocumentationTok{\#\# 2          4.9         3.0          1.4         0.2  setosa}
\DocumentationTok{\#\# 3          4.7         3.2          1.3         0.2  setosa}
\DocumentationTok{\#\# 4          4.6         3.1          1.5         0.2  setosa}
\DocumentationTok{\#\# 5          5.0         3.6          1.4         0.2  setosa}
\DocumentationTok{\#\# 6          5.4         3.9          1.7         0.4  setosa}
    \FunctionTok{library}\NormalTok{(ggplot2)}
    \FunctionTok{ggplot}\NormalTok{(iris, }\FunctionTok{aes}\NormalTok{(Petal.Length, Petal.Width, }\AttributeTok{color =}\NormalTok{ Species)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-260-1} \end{center}

The last two variables in the \texttt{iris} data carry more information on separating the three classes. Hence we will only use the \texttt{Petal.Length} and \texttt{Petal.Width}.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(colorspace)}
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{xpd =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    MASS}\SpecialCharTok{::}\FunctionTok{parcoord}\NormalTok{(iris[, }\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{], }\AttributeTok{col =} \FunctionTok{rainbow\_hcl}\NormalTok{(}\DecValTok{3}\NormalTok{)[iris}\SpecialCharTok{$}\NormalTok{Species], }
                   \AttributeTok{var.label =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
    \FunctionTok{legend}\NormalTok{(}\AttributeTok{x =} \FloatTok{1.2}\NormalTok{, }\AttributeTok{y =} \FloatTok{1.3}\NormalTok{, }\AttributeTok{cex =} \DecValTok{1}\NormalTok{,}
       \AttributeTok{legend =} \FunctionTok{as.character}\NormalTok{(}\FunctionTok{levels}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Species)),}
        \AttributeTok{fill =} \FunctionTok{rainbow\_hcl}\NormalTok{(}\DecValTok{3}\NormalTok{), }\AttributeTok{horiz =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-262-1} \end{center}

Let's perform the \(k\)-means clustering

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

  \CommentTok{\# k mean clustering}
\NormalTok{  iris.kmean }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(iris[, }\DecValTok{3}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\AttributeTok{centers =} \DecValTok{3}\NormalTok{, }\AttributeTok{nstart =} \DecValTok{20}\NormalTok{)}
  
  \CommentTok{\# the center of each class}
\NormalTok{  iris.kmean}\SpecialCharTok{$}\NormalTok{centers}
\DocumentationTok{\#\#   Petal.Length Petal.Width}
\DocumentationTok{\#\# 1     1.462000    0.246000}
\DocumentationTok{\#\# 2     5.595833    2.037500}
\DocumentationTok{\#\# 3     4.269231    1.342308}
  
  \CommentTok{\# the within cluster variation }
\NormalTok{  iris.kmean}\SpecialCharTok{$}\NormalTok{withinss}
\DocumentationTok{\#\# [1]  2.02200 16.29167 13.05769}
  
  \CommentTok{\# the between cluster variation }
\NormalTok{  iris.kmean}\SpecialCharTok{$}\NormalTok{betweenss}
\DocumentationTok{\#\# [1] 519.524}
  
  \CommentTok{\# plot the fitted clusters vs. the truth}
\NormalTok{  iris.kmean}\SpecialCharTok{$}\NormalTok{cluster }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(iris.kmean}\SpecialCharTok{$}\NormalTok{cluster)}
  
  \FunctionTok{ggplot}\NormalTok{(iris, }\FunctionTok{aes}\NormalTok{(Petal.Length, Petal.Width, }\AttributeTok{color =}\NormalTok{ Species)) }\SpecialCharTok{+} \CommentTok{\# true cluster}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{size =} \FloatTok{3.5}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}green\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"red"}\NormalTok{)[iris.kmean}\SpecialCharTok{$}\NormalTok{cluster]) }\CommentTok{\# fitted cluster }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-263-1} \end{center}

\hypertarget{example-2-clustering-of-image-pixels}{%
\section{Example 2: clustering of image pixels}\label{example-2-clustering-of-image-pixels}}

Let's first load and plot an image of Leo.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(jpeg)}
\NormalTok{    img}\OtherTok{\textless{}{-}}\FunctionTok{readJPEG}\NormalTok{(}\StringTok{"data/leo.jpg"}\NormalTok{)}
    
    \CommentTok{\# generate a blank image}
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{rep}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\DecValTok{4}\NormalTok{))}
    \FunctionTok{plot}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{400}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{500}\NormalTok{), }\AttributeTok{xaxt =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{yaxt =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }
         \AttributeTok{bty =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{pch =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, }\AttributeTok{xlab =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}

    \FunctionTok{rasterImage}\NormalTok{(img, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{400}\NormalTok{, }\DecValTok{500}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.35\linewidth]{SMLR_files/figure-latex/unnamed-chunk-264-1} \end{center}

For a \texttt{jpg} file, each pixel is stored as a vector with 3 elements --- representing red, green and blue intensities. However, by the way, that this objective \texttt{img} being constructed, it is stored as a 3d array. The first two dimensions are the height and width of the figure. We need to vectorize them and treat each pixel as an observation.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{dim}\NormalTok{(img)}
\DocumentationTok{\#\# [1] 500 400   3}
    
    \CommentTok{\# this apply function applies vecterization to each layer (r/g/b) of the image. }
\NormalTok{    img\_expand }\OtherTok{=} \FunctionTok{apply}\NormalTok{(img, }\DecValTok{3}\NormalTok{, c)}

    \CommentTok{\# and now we have the desired data matrix}
    \FunctionTok{dim}\NormalTok{(img\_expand)}
\DocumentationTok{\#\# [1] 200000      3}
\end{Highlighting}
\end{Shaded}

Before performing the \(k\)-mean clustering, let's have a quick peek at the data in a 3d view. Since there are too many observations, we randomly sample a few.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(scatterplot3d)}
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{    sub\_pixels }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(img\_expand), }\DecValTok{1000}\NormalTok{)}
\NormalTok{    sub\_img\_expand }\OtherTok{=}\NormalTok{ img\_expand[sub\_pixels, ]}
    
    \FunctionTok{scatterplot3d}\NormalTok{(sub\_img\_expand, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }
                  \AttributeTok{xlab =} \StringTok{"red"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"green"}\NormalTok{, }\AttributeTok{zlab =} \StringTok{"blue"}\NormalTok{, }
                  \AttributeTok{color =} \FunctionTok{rgb}\NormalTok{(sub\_img\_expand[,}\DecValTok{1}\NormalTok{], sub\_img\_expand[,}\DecValTok{2}\NormalTok{],}
\NormalTok{                              sub\_img\_expand[,}\DecValTok{3}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.55\linewidth]{SMLR_files/figure-latex/unnamed-chunk-266-1} \end{center}

The next step is to perform the \(k\)-mean and obtain the cluster label. For example, let's try 5 clusters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  kmeanfit }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(img\_expand, }\DecValTok{5}\NormalTok{)}

  \CommentTok{\# to produce the new graph, we simply replicate the cluster mean }
  \CommentTok{\# for all observations in the same cluster}
\NormalTok{  new\_img\_expand }\OtherTok{=}\NormalTok{ kmeanfit}\SpecialCharTok{$}\NormalTok{centers[kmeanfit}\SpecialCharTok{$}\NormalTok{cluster, ]}
  
  \CommentTok{\# now we need to convert this back to the array that can be plotted as an image. }
  \CommentTok{\# this is a lazy way to do it, but get the job done}
\NormalTok{  new\_img }\OtherTok{=}\NormalTok{ img}
\NormalTok{  new\_img[, , }\DecValTok{1}\NormalTok{] }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(new\_img\_expand[,}\DecValTok{1}\NormalTok{], }\DecValTok{500}\NormalTok{, }\DecValTok{400}\NormalTok{)}
\NormalTok{  new\_img[, , }\DecValTok{2}\NormalTok{] }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(new\_img\_expand[,}\DecValTok{2}\NormalTok{], }\DecValTok{500}\NormalTok{, }\DecValTok{400}\NormalTok{)}
\NormalTok{  new\_img[, , }\DecValTok{3}\NormalTok{] }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(new\_img\_expand[,}\DecValTok{3}\NormalTok{], }\DecValTok{500}\NormalTok{, }\DecValTok{400}\NormalTok{)}

  \CommentTok{\# plot the new image}
  \FunctionTok{plot}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{400}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{500}\NormalTok{), }\AttributeTok{xaxt =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{yaxt =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{bty =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }
       \AttributeTok{pch =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, }\AttributeTok{xlab =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}

  \FunctionTok{rasterImage}\NormalTok{(new\_img, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{400}\NormalTok{, }\DecValTok{500}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.35\linewidth]{SMLR_files/figure-latex/unnamed-chunk-267-1} \end{center}

With this technique, we can easily reproduce results with different \(k\) values. Apparently, as \(k\) increases, we get better resolution. \(k = 30\) seems to recover the original image fairly well.

\begin{verbatim}
## Warning: did not converge in 10 iterations
\end{verbatim}

\begin{center}\includegraphics[width=1\linewidth]{SMLR_files/figure-latex/unnamed-chunk-269-1} \end{center}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{hierarchical-clustering}{%
\chapter{Hierarchical Clustering}\label{hierarchical-clustering}}

\hypertarget{basic-concepts-1}{%
\section{Basic Concepts}\label{basic-concepts-1}}

Suppose we have a set of six observations:

\begin{center}\includegraphics[width=0.3\linewidth]{SMLR_files/figure-latex/unnamed-chunk-273-1} \end{center}

The goal is to progressively group them together until there is only one group. During this process, we will always choose the closest two groups (some may be individuals) to merge.

\begin{center}\includegraphics[width=1\linewidth]{SMLR_files/figure-latex/unnamed-chunk-275-1} \end{center}

If we evaluate the distance between two observations, that would be very easy. For example, the Euclidean distance and Hamming distance can be used. But what about the distance between two groups? Suppose we have two groups of observations \(G\) and \(H\), then several distance metric can be considered:

\begin{itemize}
\tightlist
\item
  \textbf{Complete linkage}: the furthest pair
  \[d(G, H) = \underset{i \in G, \, j \in G}{\max} d(x_i, x_j)\]
\item
  \textbf{Single linkage}: the closest pair
  \[d(G, H) = \underset{i \in G, \, j \in G}{\min} d(x_i, x_j)\]
\item
  \textbf{Average linkage}: average distance
  \[d(G, H) = \frac{1}{n_G n_H} \sum_{i \in G} \sum_{i \in H} d(x_i, x_j)\]
\end{itemize}

The \texttt{R} function \texttt{hclust()} uses the complete linkage as default. To perform a hierarchical clustering, we need to know all the pair-wise distances, i.e., \(d(x_i, x_j)\). Let's consider the Euclidean distance.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# the Euclidean distance can be computed using dist()}
  \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{dist}\NormalTok{(x))}
\DocumentationTok{\#\#          1         2         3         4         5         6}
\DocumentationTok{\#\# 1 0.000000 1.2294164 1.7864196 1.1971565 1.4246185 1.5698349}
\DocumentationTok{\#\# 2 1.229416 0.0000000 2.3996575 0.8727261 1.9243764 2.2708670}
\DocumentationTok{\#\# 3 1.786420 2.3996575 0.0000000 2.8586738 0.4782442 0.2448835}
\DocumentationTok{\#\# 4 1.197156 0.8727261 2.8586738 0.0000000 2.4219048 2.6741260}
\DocumentationTok{\#\# 5 1.424618 1.9243764 0.4782442 2.4219048 0.0000000 0.4204479}
\DocumentationTok{\#\# 6 1.569835 2.2708670 0.2448835 2.6741260 0.4204479 0.0000000}
\end{Highlighting}
\end{Shaded}

We use this distance matrix in the hierarchical clustering algorithm \texttt{hclust()}. The \texttt{plot()} function will display the merging process. This should be exactly the same as we demonstrated previously.

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-278-1} \end{center}

The height of each split represents how separated the two subsets are (the distance when they are merged). Selecting the number of clusters is still a tricky problem. Usually, we pick a cutoff where the height of the next split is short. Hence, the above example fits well with two clusters.

\hypertarget{example-1-iris-data-1}{%
\section{\texorpdfstring{Example 1: \texttt{iris} data}{Example 1: iris data}}\label{example-1-iris-data-1}}

The \texttt{iris} data contains three clusters and four variables. We use all variables in the distance calculation and use the default complete linkage.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  iris\_hc }\OtherTok{\textless{}{-}} \FunctionTok{hclust}\NormalTok{(}\FunctionTok{dist}\NormalTok{(iris[, }\DecValTok{3}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]))}
  \FunctionTok{plot}\NormalTok{(iris\_hc)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{SMLR_files/figure-latex/unnamed-chunk-279-1} \end{center}

This does not seem to perform very well, considering that we know the true number of classes is three. This shows that, in practice, the detected clusters can heavily depend on the variables you use. Let's try some other linkage functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  iris\_hc }\OtherTok{\textless{}{-}} \FunctionTok{hclust}\NormalTok{(}\FunctionTok{dist}\NormalTok{(iris[, }\DecValTok{3}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]), }\AttributeTok{method =} \StringTok{"average"}\NormalTok{)}
  \FunctionTok{plot}\NormalTok{(iris\_hc, }\AttributeTok{hang =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{SMLR_files/figure-latex/unnamed-chunk-280-1} \end{center}

This looks better, at least more consistent with the truth. Now we can also consider using other package to plot this result. For example, the \texttt{ape} package provides some interesting choices.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(ape)}
  \FunctionTok{plot}\NormalTok{(}\FunctionTok{as.phylo}\NormalTok{(iris\_hc), }\AttributeTok{type =} \StringTok{"unrooted"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{no.margin =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-281-1} \end{center}

We can also add the true class colors to the plot. This plot is motivated by the \texttt{dendextend} package vignettes. Of course in a realistic situation, we wouldn't know what the true class is.

\begin{center}\includegraphics[width=0.6\linewidth]{SMLR_files/figure-latex/unnamed-chunk-282-1} \end{center}

\hypertarget{example-2-rna-expression-data}{%
\section{Example 2: RNA Expression Data}\label{example-2-rna-expression-data}}

We use a tissue gene expression dataset from the \texttt{tissuesGeneExpression} library, available from bioconductor. I prepared the data to include only 100 genes. You can download the data from the course website. In this first step, we simply plot the data using a heatmap. By default, a heatmap uses red to denote higher values, and yellow for lower values. Note that we first plot the data without organizing the columns or rows. The data is also standardized based on columns (genes).

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{load}\NormalTok{(}\StringTok{"data/tissue.Rda"}\NormalTok{)}
    \FunctionTok{dim}\NormalTok{(expression)}
\DocumentationTok{\#\# [1] 189 100}
    \FunctionTok{table}\NormalTok{(tissue)}
\DocumentationTok{\#\# tissue}
\DocumentationTok{\#\#  cerebellum       colon endometrium hippocampus      kidney       liver    placenta }
\DocumentationTok{\#\#          38          34          15          31          39          26           6}
    \FunctionTok{head}\NormalTok{(expression[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\DocumentationTok{\#\#                 211298\_s\_at 203540\_at 211357\_s\_at}
\DocumentationTok{\#\# GSM11805.CEL.gz    7.710426  5.856596   12.618471}
\DocumentationTok{\#\# GSM11814.CEL.gz    4.741010  5.813841    5.116707}
\DocumentationTok{\#\# GSM11823.CEL.gz   11.730652  5.986338   13.206078}
\DocumentationTok{\#\# GSM11830.CEL.gz    5.061337  6.316815    9.780614}
\DocumentationTok{\#\# GSM12067.CEL.gz    4.955245  6.561705    8.589003}
\DocumentationTok{\#\# GSM12075.CEL.gz   10.469501  5.880740   13.050554}
    \FunctionTok{heatmap}\NormalTok{(}\FunctionTok{scale}\NormalTok{(expression), }\AttributeTok{Rowv =} \ConstantTok{NA}\NormalTok{, }\AttributeTok{Colv =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-283-1} \end{center}

Hierarchical clustering may help us discover interesting patterns. If we reorganize the columns and rows based on the clusters, then it may reveal underlying subclasses of issues, or subgroups of genes.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{heatmap}\NormalTok{(}\FunctionTok{scale}\NormalTok{(expression))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-284-1} \end{center}

Note that there are many other \texttt{R} packages that produce more interesting plots. For example, you can try the \href{https://cran.r-project.org/web/packages/heatmaply/vignettes/heatmaply.html}{heatmaply} package.

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{principle-component-analysis}{%
\chapter{Principle Component Analysis}\label{principle-component-analysis}}

\hypertarget{basic-concepts-2}{%
\section{Basic Concepts}\label{basic-concepts-2}}

Principle Component Analysis (PCA) is arguably the most commonly used approach for dimension reduction and visualization. The idea is to capture major signals of variation in a dataset. A nice demonstration of the search of direction is provided at this \href{https://www.r-bloggers.com/principal-component-analysis-in-r/}{r-bloggers} site:

\includegraphics{images/PCA2.gif}

Let's look at a two-dimensional case, we are trying to find a line (direction) on this plain, such that if all points are projected onto this line, their coordinates have the largest variance, compared with any other line. The following code is used to generate a set of observations.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# generate some random data from a 2{-}dimensional normal distribution. }
  \FunctionTok{library}\NormalTok{(MASS)}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
  
\NormalTok{  n }\OtherTok{=} \DecValTok{100}
\NormalTok{  Sigma }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.65}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.65}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{  x\_org }\OtherTok{=} \FunctionTok{mvrnorm}\NormalTok{(n, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), Sigma)}
\NormalTok{  x }\OtherTok{=} \FunctionTok{scale}\NormalTok{(x\_org, }\AttributeTok{scale =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{center =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{plot}\NormalTok{(x, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{ylim=} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }
       \AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.75}\NormalTok{)}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{SMLR_files/figure-latex/unnamed-chunk-288-1} \end{center}

Let' start with finding a direction to project all the observations onto. And we want this projection to have the largest variation. Of course the direction that goes along the spread of the data would be the best choice for the purpose of large variance.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{plot}\NormalTok{(x, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{ylim=} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.75}\NormalTok{)}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

  \CommentTok{\# This line is obtained from performing PCA}
\NormalTok{  pc1 }\OtherTok{=} \FunctionTok{princomp}\NormalTok{(x)}\SpecialCharTok{$}\NormalTok{loadings[,}\DecValTok{1}\NormalTok{]}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{a =} \DecValTok{0}\NormalTok{, }\AttributeTok{b =}\NormalTok{ pc1[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{/}\NormalTok{pc1[}\DecValTok{1}\NormalTok{], }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{SMLR_files/figure-latex/unnamed-chunk-289-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
  
  \CommentTok{\# The direction }
\NormalTok{  pc1}
\DocumentationTok{\#\# [1]  0.5659608 {-}0.8244322}
\end{Highlighting}
\end{Shaded}

Once we have the first direction, we can also remove the projection from the original covariates, and search for a direction \(\mathbf{v}_2\) that is orthogonal to \(\mathbf{v}_1\), with \(\mathbf{v}_1^\text{T}\mathbf{v}_2 = 0\), such that it contains large variation of \(\mathbf{X}\).

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.3}\NormalTok{))}
  \FunctionTok{plot}\NormalTok{(x, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{3.5}\NormalTok{, }\FloatTok{3.5}\NormalTok{), }\AttributeTok{ylim=} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{3.5}\NormalTok{, }\FloatTok{3.5}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

  \CommentTok{\# largest PC }
\NormalTok{  pc1 }\OtherTok{=} \FunctionTok{princomp}\NormalTok{(x)}\SpecialCharTok{$}\NormalTok{loadings[,}\DecValTok{1}\NormalTok{]}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{a =} \DecValTok{0}\NormalTok{, }\AttributeTok{b =}\NormalTok{ pc1[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{/}\NormalTok{pc1[}\DecValTok{1}\NormalTok{], }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}

  \CommentTok{\# second largest PC}
\NormalTok{  pc2 }\OtherTok{=} \FunctionTok{princomp}\NormalTok{(x)}\SpecialCharTok{$}\NormalTok{loadings[,}\DecValTok{2}\NormalTok{]}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{a =} \DecValTok{0}\NormalTok{, }\AttributeTok{b =}\NormalTok{ pc2[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{/}\NormalTok{pc2[}\DecValTok{1}\NormalTok{], }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{SMLR_files/figure-latex/unnamed-chunk-290-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
  
\NormalTok{  pc2}
\DocumentationTok{\#\# [1] 0.8244322 0.5659608}
  \FunctionTok{t}\NormalTok{(pc1) }\SpecialCharTok{\%*\%}\NormalTok{ pc2}
\DocumentationTok{\#\#      [,1]}
\DocumentationTok{\#\# [1,]    0}
\end{Highlighting}
\end{Shaded}

We can also see how much variation these two directions accounts for in the original data. The following shows the corresponding standard deviation.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{princomp}\NormalTok{(x)}\SpecialCharTok{$}\NormalTok{sdev}
\DocumentationTok{\#\#    Comp.1    Comp.2 }
\DocumentationTok{\#\# 1.0748243 0.2206133}
\end{Highlighting}
\end{Shaded}

Formally, we can generalized this to \(\mathbf{X}\) with any dimensions. And the key tool is to perform the singular value decomposition (SVD):

\[\mathbf{X}= \mathbf{U}\mathbf{D}\mathbf{V}^\text{T}\]
The \(\mathbf{V}\) matrix here corresponds to the directions we found. Hence, \(\mathbf{v}_1\) is its first column, \(\mathbf{v}_1\) is its second column, etc.. \(\mathbf{D}\) is a diagonal matrix ordered from the largest to the smallest values, correspond to the standard deviation of the spreads. And \(\mathbf{U}\) represents the coordinates once we project \(\mathbf{X}\) onto those directions. An alternative way to understand this is by matrix approximation, if we want to find a rank-1 matrix that best approximate \(\mathbf{X}\) with the Frobenius norm, we optimize

\[\text{minimize} \quad \lVert \mathbf{X}- \mathbf{u}_1 d_1 \mathbf{v}_1^\text{T}\rVert_2^2\]

This can be generalized into any dimensional problem. Another alternative formulation is to use eigen-decomposition of \(\mathbf{X}^\text{T}\mathbf{X}\), which can be written as

\[\mathbf{X}^\text{T}\mathbf{X}= \mathbf{V}\mathbf{D}\mathbf{U}^\text{T}\mathbf{U}\mathbf{D}\mathbf{V}^\text{T}= \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}\]

But one thing we usually need to take care of is the centering issue. This is why we used \texttt{scale()} function at the beginning. However, we only center, but not scale the data. If we do not center, then the first principle component (PC) could be a direction that points to the center of the data. Note that when \(\mathbf{X}\) is already centered, \(\mathbf{X}^\text{T}\mathbf{X}\) is the covariance matrix. Hence PCA is also performing eigen-decomposition to the covariance matrix.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{plot}\NormalTok{(x\_org, }\AttributeTok{main =} \StringTok{"Before Centering"}\NormalTok{, }
         \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), }\AttributeTok{ylim=} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
    
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.3}\NormalTok{))}
    \FunctionTok{plot}\NormalTok{(x, }\AttributeTok{main =} \StringTok{"After Centering"}\NormalTok{, }
         \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), }\AttributeTok{ylim=} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{), }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)    }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{SMLR_files/figure-latex/unnamed-chunk-293-1} \end{center}

Finally, for any dimensional data \(\mathbf{X}\), we usually visualize them in the first two directions, or three. Note that the coordinates on the PC's can be obtained using either the \texttt{scores} (\(\mathbf{U}\)) in the fitted object of \texttt{princomp}, or simply multiply the original data matrix by the loading matrix \(\mathbf{V}\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    pcafit }\OtherTok{\textless{}{-}} \FunctionTok{princomp}\NormalTok{(x)}

    \CommentTok{\# the new coordinates on PC\textquotesingle{}s}
    \FunctionTok{head}\NormalTok{(pcafit}\SpecialCharTok{$}\NormalTok{scores)}
\DocumentationTok{\#\#           Comp.1      Comp.2}
\DocumentationTok{\#\# [1,]  0.88434533  0.13503607}
\DocumentationTok{\#\# [2,] {-}0.08990294 {-}0.01851954}
\DocumentationTok{\#\# [3,]  1.13589963  0.20234582}
\DocumentationTok{\#\# [4,] {-}1.78763374 {-}0.04571218}
\DocumentationTok{\#\# [5,] {-}0.26536435  0.14271170}
\DocumentationTok{\#\# [6,]  1.11779894 {-}0.41759594}
    
    \CommentTok{\# direct calculation based on projection }
    \FunctionTok{head}\NormalTok{(x }\SpecialCharTok{\%*\%}\NormalTok{ pcafit}\SpecialCharTok{$}\NormalTok{loadings)}
\DocumentationTok{\#\#           Comp.1      Comp.2}
\DocumentationTok{\#\# [1,]  0.88434533  0.13503607}
\DocumentationTok{\#\# [2,] {-}0.08990294 {-}0.01851954}
\DocumentationTok{\#\# [3,]  1.13589963  0.20234582}
\DocumentationTok{\#\# [4,] {-}1.78763374 {-}0.04571218}
\DocumentationTok{\#\# [5,] {-}0.26536435  0.14271170}
\DocumentationTok{\#\# [6,]  1.11779894 {-}0.41759594}

    \CommentTok{\# visualize the data on the PCs}
    \CommentTok{\# Note that the both axies are scaled }
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{4.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.3}\NormalTok{))}
    \FunctionTok{plot}\NormalTok{(pcafit}\SpecialCharTok{$}\NormalTok{scores[,}\DecValTok{1}\NormalTok{], pcafit}\SpecialCharTok{$}\NormalTok{scores[,}\DecValTok{2}\NormalTok{], }\AttributeTok{xlab =} \StringTok{"First PC"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Second PC"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex.lab =} \FloatTok{1.5}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"deepskyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
    \FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{SMLR_files/figure-latex/unnamed-chunk-295-1} \end{center}

There are many different functions in \texttt{R} that performs PCA. \texttt{princomp} and \texttt{prcomp} are the most popular ones.

\hypertarget{note-scaling}{%
\subsection{Note: Scaling}\label{note-scaling}}

You should always center the variables when performing PCA, however, whether to use scaling (force each variable to have a standard deviation of 1) depends on the particular application. When you have variables that are extremely disproportionate, e.g., age vs.~RNA expression, scaling should be used. This is to prevent some variables from dominating the PC loadings due to their large scales. When all the variables are of the similar type, e.g., color intensities of pixels in a figure, it is better to use the original scale. This is because the variables with larger variations may carry more signal. Scaling may lose that information.

\hypertarget{example-1-iris-data-2}{%
\section{\texorpdfstring{Example 1: \texttt{iris} Data}{Example 1: iris Data}}\label{example-1-iris-data-2}}

We use the \texttt{iris} data again. All four variables are considered in this analysis. We plot the first and second PC directions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    iris\_pc }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(iris[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{])}
    \FunctionTok{library}\NormalTok{(ggplot2)}
    \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(iris\_pc}\SpecialCharTok{$}\NormalTok{x), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{PC1, }\AttributeTok{y=}\NormalTok{PC2)) }\SpecialCharTok{+} 
        \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\FunctionTok{c}\NormalTok{(}\StringTok{"chartreuse4"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{)[iris}\SpecialCharTok{$}\NormalTok{Species], }\AttributeTok{size =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-296-1} \end{center}

One may be interested in plotting all pair-wise direction to see if lower PC's provide useful information.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{pairs}\NormalTok{(iris\_pc}\SpecialCharTok{$}\NormalTok{x, }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"chartreuse4"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{)[iris}\SpecialCharTok{$}\NormalTok{Species], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-297-1} \end{center}

However, usually, the lower PC's are less informative. This can also be speculated from the eigenvalue plot, which shows how influential each PC is.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{plot}\NormalTok{(iris\_pc, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{main =} \StringTok{"Iris PCA Variance"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-298-1} \end{center}

Feature contributions to the PC can be accessed through the magnitude of the loadings. This table shows that \texttt{Petal.Length} is the most influential variable on the first PC, with loading \(\approx 0.8567\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    iris\_pc}\SpecialCharTok{$}\NormalTok{rotation}
\DocumentationTok{\#\#                      PC1         PC2         PC3        PC4}
\DocumentationTok{\#\# Sepal.Length  0.36138659 {-}0.65658877  0.58202985  0.3154872}
\DocumentationTok{\#\# Sepal.Width  {-}0.08452251 {-}0.73016143 {-}0.59791083 {-}0.3197231}
\DocumentationTok{\#\# Petal.Length  0.85667061  0.17337266 {-}0.07623608 {-}0.4798390}
\DocumentationTok{\#\# Petal.Width   0.35828920  0.07548102 {-}0.54583143  0.7536574}
\end{Highlighting}
\end{Shaded}

We can further visualize this on a plot. This can be helpful when the number of variables is large.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    features }\OtherTok{=} \FunctionTok{row.names}\NormalTok{(iris\_pc}\SpecialCharTok{$}\NormalTok{rotation)}
    \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(iris\_pc}\SpecialCharTok{$}\NormalTok{rotation), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{PC1, }\AttributeTok{y=}\NormalTok{PC2, }\AttributeTok{label=}\NormalTok{features,}\AttributeTok{color=}\NormalTok{features)) }\SpecialCharTok{+} 
        \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-300-1} \end{center}

\hypertarget{example-2-handwritten-digits}{%
\section{Example 2: Handwritten Digits}\label{example-2-handwritten-digits}}

The handwritten zip code digits data contains 7291 training data and 2007 testing data. Each image is a \(16 \times 16\)-pixel gray-scale image. Hence they are converted to a vector of 256 variables.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(ElemStatLearn)}
    \CommentTok{\# Handwritten Digit Recognition Data}
    \CommentTok{\# the first column is the true digit}
    \FunctionTok{dim}\NormalTok{(zip.train)}
\DocumentationTok{\#\# [1] 7291  257}
\end{Highlighting}
\end{Shaded}

Here is a sample of some images:

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-302-1} \end{center}

Let's do a simpler task, using just three letters: 1, 4 and 8.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    zip.sub }\OtherTok{=}\NormalTok{ zip.train[zip.train[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{8}\NormalTok{), }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{    zip.sub.truth }\OtherTok{=} \FunctionTok{as.factor}\NormalTok{(zip.train[zip.train[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{8}\NormalTok{), }\DecValTok{1}\NormalTok{])}
    \FunctionTok{dim}\NormalTok{(zip.sub)}
\DocumentationTok{\#\# [1] 2199  256}
\NormalTok{    zip\_pc }\OtherTok{=} \FunctionTok{prcomp}\NormalTok{(zip.sub)}
    \FunctionTok{plot}\NormalTok{(zip\_pc, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{main =} \StringTok{"Digits 1, 4, and 8: PCA Variance"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-303-1} \end{center}

The eigenvalue results suggest that the first two principal components are much more influential than the rest. A pair-wise PC plot of the first four PC's may further confirm that speculation.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{pairs}\NormalTok{(zip\_pc}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"chartreuse4"}\NormalTok{, }\StringTok{"darkorange"}\NormalTok{, }\StringTok{"deepskyblue"}\NormalTok{)[zip.sub.truth], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-304-1} \end{center}

Let's look at the first two PCs more closely. Even without knowing the true class (no colors) we can still vaguely see 3 clusters.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(ggplot2)}
    \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(zip\_pc}\SpecialCharTok{$}\NormalTok{x), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{PC1, }\AttributeTok{y=}\NormalTok{PC2)) }\SpecialCharTok{+} 
        \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-305-1} \end{center}

Finally, let's briefly look at the results of PCA for all 10 different digits. Of course, more PC's are needed for this task. You can also plot other PC's to get more information.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(colorspace)}

\NormalTok{    zip\_pc }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(zip.train)}

    \FunctionTok{plot}\NormalTok{(zip\_pc, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{main =} \StringTok{"All Digits: PCA Variance"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-306-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

    \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{prcomp}\NormalTok{(zip.train)}\SpecialCharTok{$}\NormalTok{x), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{PC1, }\AttributeTok{y=}\NormalTok{PC2)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =} \FunctionTok{rainbow\_hcl}\NormalTok{(}\DecValTok{10}\NormalTok{)[zip.train[,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{], }\AttributeTok{size =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-306-2} \end{center}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{self-organizing-map}{%
\chapter{Self-Organizing Map}\label{self-organizing-map}}

\hypertarget{basic-concepts-3}{%
\section{Basic Concepts}\label{basic-concepts-3}}

I found the best demonstration of the Self-Organizing Map algorithm is the following graph that displays it over iterations. It is available at \href{https://annalyzin.wordpress.com/2017/11/02/self-organizing-map/}{this website}:

\includegraphics[width=4.16667in,height=\textheight]{images/SOM2.gif}

Let's understand this by pairing it with the algorithm. There are several different algorithms available, but one of the most popular ones is proposed by Kohonen (\protect\hyperlink{ref-kohonen1990self}{1990}). Here, we present a SOM with a 2-dimensional output. The following are the inputs:

\begin{itemize}
\tightlist
\item
  \(\{x_i\}_{i=1}^n\) is a set of \(n\) observations, with dimension \(p\) (the yellow and green dots in the figure).
\item
  \(w_{ij}\), \(i = 1, \ldots p\), \(j = 1, \ldots q\) are a grid of centers (the connected black dots). They are similar to the centers in a k-mean algorithm. However, they also preserve some geometric relationships among \(w_{ij}\)'s, meaning that \(w_{ij}\)'s are closer if their indices \({i, j}\) are closer (connected in the figure).
\item
  \(\alpha\) this is a learning rate between \([0, 1]\). This controls how fast the \(w_{ij}\)'s are updated.
\item
  \(r\) is also a tuning parameter. This controls how many \(w_{ij}\)'s will be updated at each iteration
\end{itemize}

Now, we look at the algorithm. This is different from \(k\)-means because we do not use all the observations immediately. The algorithm works by stream-in the observations one-by-one. Whenever a new observation \(x_k\), \(k = 1, \ldots, n\) comes in, we will update the centers \(w_{ij}\)'s by the following:

\begin{itemize}
\tightlist
\item
  For all \(w_{ij}\), calculate the distance between each \(w_{ij}\) and \(x_k\). Let \(d_{ij} = \lVert x_k - w_{ij} \rVert\). By default, we use Euclidean distance.
\item
  Select the closest \(w_{ij}\), denoted as \(w_{\ast}\)
\item
  Update each \(w_{ij}\) based on the fomular \(w_{ij} = w_{ij} + \alpha \, h(w_\ast, w_{ij}, r) \, \lVert x_k - w_{ij} \rVert\)
\end{itemize}

After each iteration (updating with one more observation), we will decrease the value of \(\alpha\) and \(r\). In the \texttt{kohonen} package, the \(\alpha\) starts at 0.05, and gradually decreases to 0.01, while \(r\) is chosen to be 2/3 of all cluster means at the first iteration.

Using the \texttt{kohonen} package, we perform a SOM on the Handwritten Digit Recognition Data. The heatmap shows how each \(w_{ij}\) is away from it's neighboring \(w_{ij}\)'s. The extreme bright one means that the center is quite isolated by itself.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(kohonen)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Attaching package: \textquotesingle{}kohonen\textquotesingle{}}
\DocumentationTok{\#\# The following object is masked from \textquotesingle{}package:class\textquotesingle{}:}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#     somgrid}

  \CommentTok{\# Handwritten Digit Recognition Data}
  \FunctionTok{library}\NormalTok{(ElemStatLearn)}

  \CommentTok{\# the first column is the true digit}
  \FunctionTok{dim}\NormalTok{(zip.train)}
\DocumentationTok{\#\# [1] 7291  257}
  
  \CommentTok{\# for speed concern, I only use a few variables (pixels)}
\NormalTok{  zip.SOM }\OtherTok{\textless{}{-}} \FunctionTok{som}\NormalTok{(zip.train[, }\FunctionTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{257}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{10}\NormalTok{)], }
                 \AttributeTok{grid =} \FunctionTok{somgrid}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{, }\StringTok{"rectangular"}\NormalTok{))}
  \FunctionTok{plot}\NormalTok{(zip.SOM, }\AttributeTok{type =} \StringTok{"dist.neighbours"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-309-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{plot}\NormalTok{(zip.SOM, }\AttributeTok{type =} \StringTok{"mapping"}\NormalTok{, }\AttributeTok{pchs =} \DecValTok{20}\NormalTok{, }
       \AttributeTok{main =} \StringTok{"Mapping Type SOM"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-309-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# plot(zip.SOM, main = "Default SOM Plot")}
  
  \CommentTok{\# you can try using all the pixels}
  \CommentTok{\# zip.SOM \textless{}{-} som(zip.train[, 2:257], }
  \CommentTok{\#            grid = somgrid(10, 10, "rectangular"))}
  \CommentTok{\# plot(zip.SOM, type = "dist.neighbours")}
\end{Highlighting}
\end{Shaded}

We could also look at the class labels (digits) coming out of the SOM. Particularly the plot on the right-hand side shows the proportion of subjects with each label for the subjects in each cluster (using a pie chart).

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{    zip.SOM2 }\OtherTok{\textless{}{-}} \FunctionTok{xyf}\NormalTok{(zip.train[, }\FunctionTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{257}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{10}\NormalTok{)], }
                    \FunctionTok{classvec2classmat}\NormalTok{(zip.train[, }\DecValTok{1}\NormalTok{]),}
                    \AttributeTok{grid =} \FunctionTok{somgrid}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{, }\StringTok{"hexagonal"}\NormalTok{), }\AttributeTok{rlen =} \DecValTok{300}\NormalTok{)}
    \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
    \FunctionTok{plot}\NormalTok{(zip.SOM2, }\AttributeTok{type =} \StringTok{"codes"}\NormalTok{, }\AttributeTok{main =} \FunctionTok{c}\NormalTok{(}\StringTok{"Codes X"}\NormalTok{, }\StringTok{"Codes Y"}\NormalTok{))}
\DocumentationTok{\#\# Warning in par(opar): argument 1 does not name a graphical parameter}
\NormalTok{    zip.SOM2.hc }\OtherTok{\textless{}{-}} \FunctionTok{cutree}\NormalTok{(}\FunctionTok{hclust}\NormalTok{(}\FunctionTok{dist}\NormalTok{(zip.SOM2}\SpecialCharTok{$}\NormalTok{codes[[}\DecValTok{2}\NormalTok{]])), }\DecValTok{10}\NormalTok{)}
    \FunctionTok{add.cluster.boundaries}\NormalTok{(zip.SOM2, zip.SOM2.hc)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-310-1} \end{center}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{spectral-clustering}{%
\chapter{Spectral Clustering}\label{spectral-clustering}}

Spectral clustering aims at clustering observations based on their proximity information. It essentially consists of two steps. The first step is a feature embedding, or dimension reduction. We first construct the graph Laplacian \(\mathbf{L}\) (or normalized version), which represent the proximity information, and perform eigen-decomposition of the matrix. This allows us to use a lower dimensional matrix to represent the proximity information of the original data. Once we have the low-dimensional data, we can perform the regular clustering algorithm, i.e., \(k\)-means on the new dataset.

\hypertarget{an-example}{%
\section{An Example}\label{an-example}}

Let's look at an example that regular \(k\)-means would fail.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\NormalTok{  n }\OtherTok{=} \DecValTok{200}
  
\NormalTok{  r }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, n), }\FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{, n), }\FunctionTok{rep}\NormalTok{(}\DecValTok{3}\NormalTok{, n)) }\SpecialCharTok{+} \FunctionTok{runif}\NormalTok{(n}\SpecialCharTok{*}\DecValTok{3}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{  theta }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n) }\SpecialCharTok{*} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ pi}

\NormalTok{  x1 }\OtherTok{=}\NormalTok{ r }\SpecialCharTok{*} \FunctionTok{cos}\NormalTok{(theta)}
\NormalTok{  x2 }\OtherTok{=}\NormalTok{ r }\SpecialCharTok{*} \FunctionTok{sin}\NormalTok{(theta)}
\NormalTok{  X }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(x1, x2)}
  
  \FunctionTok{plot}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-313-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{  kmeanfit }\OtherTok{=} \FunctionTok{kmeans}\NormalTok{(X, }\AttributeTok{centers =} \DecValTok{3}\NormalTok{)}
  
  \FunctionTok{plot}\NormalTok{(X, }\AttributeTok{col =}\NormalTok{ kmeanfit}\SpecialCharTok{$}\NormalTok{cluster }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-313-2} \end{center}

Since \(k\)-means use Euclidean distance, it is not appropriate for such problems.

\hypertarget{adjacency-matrix}{%
\section{Adjacency Matrix}\label{adjacency-matrix}}

Maybe we should use a nonlinear way to describe the distance/closeness between subjects. For example, let's define two sample to be close if they are within the k-nearest neighbors of each other. We use \(k = 10\), and create an adjacency matrix \(\mathbf{W}\).

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(FNN)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Attaching package: \textquotesingle{}FNN\textquotesingle{}}
\DocumentationTok{\#\# The following objects are masked from \textquotesingle{}package:class\textquotesingle{}:}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#     knn, knn.cv}
  
\NormalTok{  W }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\SpecialCharTok{*}\NormalTok{n, }\DecValTok{3}\SpecialCharTok{*}\NormalTok{n)}
  
  \CommentTok{\# get neighbor index for each observation}
\NormalTok{  nn }\OtherTok{=} \FunctionTok{get.knn}\NormalTok{(X, }\AttributeTok{k=}\DecValTok{10}\NormalTok{)}

  \CommentTok{\# write into W}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(}\DecValTok{3}\SpecialCharTok{*}\NormalTok{n))}
\NormalTok{    W[i, nn}\SpecialCharTok{$}\NormalTok{nn.index[i, ]] }\OtherTok{=} \DecValTok{1}
  
  \CommentTok{\# W is not necessary symmetric}
\NormalTok{  W }\OtherTok{=} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{(W }\SpecialCharTok{+} \FunctionTok{t}\NormalTok{(W))}
  \CommentTok{\# we may also use}
  \CommentTok{\# W = pmax(W, t(W))}
\end{Highlighting}
\end{Shaded}

Let's use a heatmap to display what the adjacency information look like. Please note that our data are ordered with clusters 1, 2 and 3.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# plot the adjacency matrix}
  \FunctionTok{heatmap}\NormalTok{(W, }\AttributeTok{Rowv =} \ConstantTok{NA}\NormalTok{, }\AttributeTok{Colv=}\ConstantTok{NA}\NormalTok{, }\AttributeTok{symm =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{revC =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-315-1} \end{center}

\hypertarget{laplacian-matrix}{%
\section{Laplacian Matrix}\label{laplacian-matrix}}

The next step is to calculate the Laplacian

\[\mathbf{L} = \mathbf{D} - \mathbf{W},\]
where \(\mathbf{D}\) is a diagonal matrix with its elements equation to the row sums (or column sums) of \(\mathbf{W}\). We will then perform eigen-decomposition on \(\mathbf{L}\) to extract/define some underlying features.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# compute the degree of each vertex}
\NormalTok{  d }\OtherTok{=} \FunctionTok{colSums}\NormalTok{(W)}
  
  \CommentTok{\# the laplacian matrix}
\NormalTok{  L }\OtherTok{=} \FunctionTok{diag}\NormalTok{(d) }\SpecialCharTok{{-}}\NormalTok{ W}
  
  \CommentTok{\# eigen{-}decomposition}
\NormalTok{  f }\OtherTok{=} \FunctionTok{eigen}\NormalTok{(L, }\AttributeTok{symmetric =} \ConstantTok{TRUE}\NormalTok{)}
  
  \CommentTok{\# plot the smallest eigen{-}values }
  \CommentTok{\# the smallest one will be zero {-}{-}{-} why?}
  \FunctionTok{plot}\NormalTok{(}\FunctionTok{rev}\NormalTok{(f}\SpecialCharTok{$}\NormalTok{values)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"eigen{-}values"}\NormalTok{, }
       \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\DecValTok{3}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\DecValTok{17}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-316-1} \end{center}

\hypertarget{feature-embedding}{%
\section{Feature Embedding}\label{feature-embedding}}

In fact the smallest eigen-value will always be zero. However, we can use the eigen-vectors associated with the second and third smallest eigen-values. They define some feature embedding or dimension reduction to represent the original data. Since we know the underlying model, two dimensions are enough (to separate three clusters). However, based on the eigen-value plot, there is a big gap between the third and the fourth one. Hence, we only need three. Further removing the smallest one, only two are needed.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
  \FunctionTok{plot}\NormalTok{(f}\SpecialCharTok{$}\NormalTok{vectors[, }\FunctionTok{length}\NormalTok{(f}\SpecialCharTok{$}\NormalTok{values)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{], }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"eigen{-}values"}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{))}
  \FunctionTok{plot}\NormalTok{(f}\SpecialCharTok{$}\NormalTok{vectors[, }\FunctionTok{length}\NormalTok{(f}\SpecialCharTok{$}\NormalTok{values)}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{], }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"eigen{-}values"}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{SMLR_files/figure-latex/unnamed-chunk-317-1} \end{center}

\hypertarget{clustering-with-embedded-features}{%
\section{Clustering with Embedded Features}\label{clustering-with-embedded-features}}

We can then perform \(k\)-means on these two new features. And it will give us the correct clustering.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{  scfit }\OtherTok{=} \FunctionTok{kmeans}\NormalTok{(f}\SpecialCharTok{$}\NormalTok{vectors[, (}\FunctionTok{length}\NormalTok{(f}\SpecialCharTok{$}\NormalTok{values)}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{:}\NormalTok{ (}\FunctionTok{length}\NormalTok{(f}\SpecialCharTok{$}\NormalTok{values)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) ], }\AttributeTok{centers =} \DecValTok{3}\NormalTok{, }\AttributeTok{nstart =} \DecValTok{20}\NormalTok{)}
  \FunctionTok{plot}\NormalTok{(X, }\AttributeTok{col =}\NormalTok{ scfit}\SpecialCharTok{$}\NormalTok{cluster }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-318-1} \end{center}

\hypertarget{normalized-graph-laplacian}{%
\section{Normalized Graph Laplacian}\label{normalized-graph-laplacian}}

There are other choices of the Laplacian matrix. For example, a normalized graph Laplacian is defined as
\[\mathbf{L}_\text{sym} = \mathbf{I} - \mathbf{D^{-1/2} \mathbf{W} D^{-1/2}}\]

For our problem, it achieves pretty much the same effect.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# the normed laplacian matrix}
\NormalTok{  L }\OtherTok{=} \FunctionTok{diag}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(W)) }\SpecialCharTok{{-}} \FunctionTok{diag}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(d)) }\SpecialCharTok{\%*\%}\NormalTok{ W }\SpecialCharTok{\%*\%} \FunctionTok{diag}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(d))}
  
  \CommentTok{\# eigen{-}decomposition}
\NormalTok{  f }\OtherTok{=} \FunctionTok{eigen}\NormalTok{(L, }\AttributeTok{symmetric =} \ConstantTok{TRUE}\NormalTok{)}
  
  \CommentTok{\# perform clustering}
\NormalTok{  scfit }\OtherTok{=} \FunctionTok{kmeans}\NormalTok{(f}\SpecialCharTok{$}\NormalTok{vectors[, (}\FunctionTok{length}\NormalTok{(f}\SpecialCharTok{$}\NormalTok{values)}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{:}\NormalTok{ (}\FunctionTok{length}\NormalTok{(f}\SpecialCharTok{$}\NormalTok{values)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) ], }\AttributeTok{centers =} \DecValTok{3}\NormalTok{, }\AttributeTok{nstart =} \DecValTok{20}\NormalTok{)}
  \FunctionTok{plot}\NormalTok{(X, }\AttributeTok{col =}\NormalTok{ scfit}\SpecialCharTok{$}\NormalTok{cluster }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-319-1} \end{center}

\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\def\argmin{\arg\min}
\def\argmax{\arg\max}

\hypertarget{uniform-manifold-approximation-and-projection}{%
\chapter{Uniform Manifold Approximation and Projection}\label{uniform-manifold-approximation-and-projection}}

Uniform Manifold Approximation and Projection (UMAP, McInnes, Healy, and Melville (\protect\hyperlink{ref-mcinnes2018umap}{2018})) becomes a very popular feature embedding / dimension reduction algorithm. If you have finished the spectral clustering section, then these concepts shouldn't be new. In fact, PCA is also a similar approach, but its just linear in terms of the original features.

There are two methods that are worth to mention here, t-SNE (\protect\hyperlink{ref-van2008visualizing}{Van der Maaten and Hinton 2008}) and spectral embedding (the embedding step in spectral clustering). All of these methods are graph-based, meaning that they are trying to learn an embedding space such that the pair-wise geometric distances among subjects in this embedding space is ``similar'' to the graph defined in the original data. Here, an example of the graph is the k-nearest neighbor graph (for spectral clustering), which counts 1 if two subjects are within each others neighbors. But for t-SNE, the graph values are proportional to the kernel density function between two points with a t-distribution density function.

The difference among these method is mainly on how do they define ``similar''. In UMAP, this similarity is defined by a type of cross-entropy, while for spectral clustering, its the eigen-values, meaning the matrix approximation, and for t-SNE, its based on the Kullback-Leibler divergence.

\hypertarget{an-example-1}{%
\section{An Example}\label{an-example-1}}

Let's consider our example from the spectral clustering lecture.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\NormalTok{  n }\OtherTok{=} \DecValTok{200}
  
\NormalTok{  r }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, n), }\FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{, n), }\FunctionTok{rep}\NormalTok{(}\DecValTok{3}\NormalTok{, n)) }\SpecialCharTok{+} \FunctionTok{runif}\NormalTok{(n}\SpecialCharTok{*}\DecValTok{3}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{  theta }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n) }\SpecialCharTok{*} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ pi}

\NormalTok{  x1 }\OtherTok{=}\NormalTok{ r }\SpecialCharTok{*} \FunctionTok{cos}\NormalTok{(theta)}
\NormalTok{  x2 }\OtherTok{=}\NormalTok{ r }\SpecialCharTok{*} \FunctionTok{sin}\NormalTok{(theta)}
\NormalTok{  X }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(x1, x2)}
  
  \FunctionTok{plot}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-322-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  circle.labels }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, n), }\FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{, n), }\FunctionTok{rep}\NormalTok{(}\DecValTok{3}\NormalTok{, n))}
\end{Highlighting}
\end{Shaded}

We can perform UMAP using the default tuning. This will create a two-dimensional embedding.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(umap)}
\NormalTok{  circles.umap }\OtherTok{=} \FunctionTok{umap}\NormalTok{(X)}
\NormalTok{  circles.umap}
\DocumentationTok{\#\# umap embedding of 600 items in 2 dimensions}
\DocumentationTok{\#\# object components: layout, data, knn, config}
  
  \FunctionTok{plot}\NormalTok{(circles.umap}\SpecialCharTok{$}\NormalTok{layout, }\AttributeTok{col =}\NormalTok{ circle.labels)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-323-1} \end{center}

We can see that UMAP learns these new features, which groups similar observations together. Its reasonable to expect that if we perform any clustering algorithm on these new embedded features, we will recover the truth.

\hypertarget{tuning}{%
\section{Tuning}\label{tuning}}

UMAP involves a lot of tuning parameters and the most significant one concerns about how we create the (KNN) graph in the first step. You can see the summary of all tuning parameters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  umap.defaults}
\DocumentationTok{\#\# umap configuration parameters}
\DocumentationTok{\#\#            n\_neighbors: 15}
\DocumentationTok{\#\#           n\_components: 2}
\DocumentationTok{\#\#                 metric: euclidean}
\DocumentationTok{\#\#               n\_epochs: 200}
\DocumentationTok{\#\#                  input: data}
\DocumentationTok{\#\#                   init: spectral}
\DocumentationTok{\#\#               min\_dist: 0.1}
\DocumentationTok{\#\#       set\_op\_mix\_ratio: 1}
\DocumentationTok{\#\#     local\_connectivity: 1}
\DocumentationTok{\#\#              bandwidth: 1}
\DocumentationTok{\#\#                  alpha: 1}
\DocumentationTok{\#\#                  gamma: 1}
\DocumentationTok{\#\#   negative\_sample\_rate: 5}
\DocumentationTok{\#\#                      a: NA}
\DocumentationTok{\#\#                      b: NA}
\DocumentationTok{\#\#                 spread: 1}
\DocumentationTok{\#\#           random\_state: NA}
\DocumentationTok{\#\#        transform\_state: NA}
\DocumentationTok{\#\#                    knn: NA}
\DocumentationTok{\#\#            knn\_repeats: 1}
\DocumentationTok{\#\#                verbose: FALSE}
\DocumentationTok{\#\#        umap\_learn\_args: NA}
\end{Highlighting}
\end{Shaded}

To change the default value, we can do the following

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  myumap.tuning }\OtherTok{=}\NormalTok{ umap.defaults}
\NormalTok{  umap.defaults}\SpecialCharTok{$}\NormalTok{n\_neighbors }\OtherTok{=} \DecValTok{5}
  
\NormalTok{  circles.umap }\OtherTok{=} \FunctionTok{umap}\NormalTok{(X, umap.defaults)}

  \FunctionTok{plot}\NormalTok{(circles.umap}\SpecialCharTok{$}\NormalTok{layout, }\AttributeTok{col =}\NormalTok{ circle.labels)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-325-1} \end{center}

You can see that the result is not as perfect as we wanted. It seems that there are more groups, although each group only involves one type of data. There are other parameter you may consider tuning. For example \texttt{n\_components} controls how many dimensions you reduced data should have. Usually we don't use values larger than three, but this is very problem specific.

\hypertarget{another-example}{%
\section{Another Example}\label{another-example}}

Let's use UMAP on a larger data, the hand written digit data. We will perform clustering on just the pixels. We can also predict the embedding feature values for future observations. We can see that both recovers the true labels pretty well.

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(ElemStatLearn)}
  \FunctionTok{dim}\NormalTok{(zip.train)}
\DocumentationTok{\#\# [1] 7291  257}
  \FunctionTok{dim}\NormalTok{(zip.test)}
\DocumentationTok{\#\# [1] 2007  257}
  
\NormalTok{  zip.umap }\OtherTok{=} \FunctionTok{umap}\NormalTok{(zip.train[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{])}
\NormalTok{  zip.pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(zip.umap, zip.test[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{])}
  
  \FunctionTok{plot}\NormalTok{(zip.umap}\SpecialCharTok{$}\NormalTok{layout, }\AttributeTok{col =}\NormalTok{ zip.train[, }\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-326-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{plot}\NormalTok{(zip.pred, }\AttributeTok{col =}\NormalTok{ zip.test[, }\DecValTok{1}\NormalTok{]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.45\linewidth]{SMLR_files/figure-latex/unnamed-chunk-326-2} \end{center}

\hypertarget{part-reference}{%
\part{Reference}\label{part-reference}}

\hypertarget{reference}{%
\chapter{Reference}\label{reference}}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-aronszajn1950theory}{}}%
Aronszajn, Nachman. 1950. {``Theory of Reproducing Kernels.''} \emph{Transactions of the American Mathematical Society} 68 (3): 337--404.

\leavevmode\vadjust pre{\hypertarget{ref-boser1992training}{}}%
Boser, Bernhard E, Isabelle M Guyon, and Vladimir N Vapnik. 1992. {``A Training Algorithm for Optimal Margin Classifiers.''} In \emph{Proceedings of the Fifth Annual Workshop on Computational Learning Theory}, 144--52.

\leavevmode\vadjust pre{\hypertarget{ref-boyd2004convex}{}}%
Boyd, Stephen, and Lieven Vandenberghe. 2004. \emph{Convex Optimization}. Cambridge university press.

\leavevmode\vadjust pre{\hypertarget{ref-breiman1996bagging}{}}%
Breiman, Leo. 1996. {``Bagging Predictors.''} \emph{Machine Learning} 24 (2): 123--40.

\leavevmode\vadjust pre{\hypertarget{ref-breiman2001random}{}}%
---------. 2001. {``Random Forests.''} \emph{Machine Learning} 45 (1): 5--32.

\leavevmode\vadjust pre{\hypertarget{ref-breiman1984classification}{}}%
Breiman, Leo, Jerome H Friedman, Richard A Olshen, and Charles J Stone. 1984. \emph{Classification and Regression Trees}. Monterey, CA: Wadsworth \& Brooks/Cole Advanced Books \& Software.

\leavevmode\vadjust pre{\hypertarget{ref-cayton2005algorithms}{}}%
Cayton, Lawrence. 2005. {``Algorithms for Manifold Learning.''} \emph{Univ. Of California at San Diego Tech. Rep} 12 (1-17): 1.

\leavevmode\vadjust pre{\hypertarget{ref-cortes1995support}{}}%
Cortes, Corinna, and Vladimir Vapnik. 1995. {``Support-Vector Networks.''} \emph{Machine Learning} 20 (3): 273--97.

\leavevmode\vadjust pre{\hypertarget{ref-efron2004least}{}}%
Efron, Bradley, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. 2004. {``Least Angle Regression.''} \emph{The Annals of Statistics} 32 (2): 407--99.

\leavevmode\vadjust pre{\hypertarget{ref-freund1997decision}{}}%
Freund, Yoav, and Robert E Schapire. 1997. {``A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.''} \emph{Journal of Computer and System Sciences} 55 (1): 119--39.

\leavevmode\vadjust pre{\hypertarget{ref-friedman2001greedy}{}}%
Friedman, Jerome H. 2001. {``Greedy Function Approximation: A Gradient Boosting Machine.''} \emph{Annals of Statistics}, 1189--1232.

\leavevmode\vadjust pre{\hypertarget{ref-friedman2010regularization}{}}%
Friedman, Jerome, Trevor Hastie, and Rob Tibshirani. 2010. {``Regularization Paths for Generalized Linear Models via Coordinate Descent.''} \emph{Journal of Statistical Software} 33 (1): 1.

\leavevmode\vadjust pre{\hypertarget{ref-golub1979generalized}{}}%
Golub, Gene H, Michael Heath, and Grace Wahba. 1979. {``Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter.''} \emph{Technometrics} 21 (2): 215--23.

\leavevmode\vadjust pre{\hypertarget{ref-hastie2001elements}{}}%
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. \emph{The Elements of Statistical Learning}. Vol. 1. Springer series in statistics New York.

\leavevmode\vadjust pre{\hypertarget{ref-ho1998random}{}}%
Ho, Tin Kam. 1998. {``The Random Subspace Method for Constructing Decision Forests.''} \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence} 20 (8): 832--44.

\leavevmode\vadjust pre{\hypertarget{ref-hoerl1970ridge}{}}%
Hoerl, Arthur E, and Robert W Kennard. 1970. {``Ridge Regression: Biased Estimation for Nonorthogonal Problems.''} \emph{Technometrics} 12 (1): 55--67.

\leavevmode\vadjust pre{\hypertarget{ref-james2013introduction}{}}%
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. \emph{An Introduction to Statistical Learning}. Vol. 112. Springer.

\leavevmode\vadjust pre{\hypertarget{ref-kimeldorf1970correspondence}{}}%
Kimeldorf, George S, and Grace Wahba. 1970. {``A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines.''} \emph{The Annals of Mathematical Statistics} 41 (2): 495--502.

\leavevmode\vadjust pre{\hypertarget{ref-kohonen1990self}{}}%
Kohonen, Teuvo. 1990. {``The Self-Organizing Map.''} \emph{Proceedings of the IEEE} 78 (9): 1464--80.

\leavevmode\vadjust pre{\hypertarget{ref-li1991sliced}{}}%
Li, Ker-Chau. 1991. {``Sliced Inverse Regression for Dimension Reduction.''} \emph{Journal of the American Statistical Association} 86 (414): 316--27.

\leavevmode\vadjust pre{\hypertarget{ref-mcinnes2018umap}{}}%
McInnes, Leland, John Healy, and James Melville. 2018. {``Umap: Uniform Manifold Approximation and Projection for Dimension Reduction.''} \emph{arXiv Preprint arXiv:1802.03426}.

\leavevmode\vadjust pre{\hypertarget{ref-mercer1909xvi}{}}%
Mercer, James. 1909. {``Xvi. Functions of Positive and Negative Type, and Their Connection the Theory of Integral Equations.''} \emph{Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character} 209 (441-458): 415--46.

\leavevmode\vadjust pre{\hypertarget{ref-nocedal2006numerical}{}}%
Nocedal, Jorge, and Stephen Wright. 2006. \emph{Numerical Optimization}. Springer Science \& Business Media.

\leavevmode\vadjust pre{\hypertarget{ref-quinlan1993c4}{}}%
Quinlan, J Ross. 1993. \emph{C4. 5: Programs for Machine Learning}. Elsevier.

\leavevmode\vadjust pre{\hypertarget{ref-tibshirani1996regression}{}}%
Tibshirani, Robert. 1996. {``Regression Shrinkage and Selection via the Lasso.''} \emph{Journal of the Royal Statistical Society: Series B (Methodological)} 58 (1): 267--88.

\leavevmode\vadjust pre{\hypertarget{ref-van2008visualizing}{}}%
Van der Maaten, Laurens, and Geoffrey Hinton. 2008. {``Visualizing Data Using t-SNE.''} \emph{Journal of Machine Learning Research} 9 (11).

\leavevmode\vadjust pre{\hypertarget{ref-yeh2018building}{}}%
Yeh, I-Cheng, and Tzu-Kuang Hsu. 2018. {``Building Real Estate Valuation Models with Comparative Approach Through Case-Based Reasoning.''} \emph{Applied Soft Computing} 65: 260--71.

\leavevmode\vadjust pre{\hypertarget{ref-zou2005regularization}{}}%
Zou, Hui, and Trevor Hastie. 2005. {``Regularization and Variable Selection via the Elastic Net.''} \emph{Journal of the Royal Statistical Society: Series B (Statistical Methodology)} 67 (2): 301--20.

\end{CSLReferences}

\end{document}
