<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Statistical Learning and Machine Learning with R</title>
  <meta name="description" content="A textbook for STAT 542 and 432 at UIUC" />
  <meta name="generator" content="bookdown #bookdown:version# and GitBook 2.6.7" />

  <meta property="og:title" content="Statistical Learning and Machine Learning with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A textbook for STAT 542 and 432 at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistical Learning and Machine Learning with R" />
  
  <meta name="twitter:description" content="A textbook for STAT 542 and 432 at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2022-11-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.26/datatables.js"></script>
<link href="libs/dt-core-1.12.1/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.12.1/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.12.1/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.10.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">Statistical Learning and Machine Learning with R</h1>
<p class="author"><em><a href="https://sites.google.com/site/teazrq/">Ruoqing Zhu, PhD</a></em></p>
<p class="date"><em>2022-11-09</em></p>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#preface" id="toc-preface">Preface<span></span></a>
<ul>
<li><a href="#target-audience" id="toc-target-audience">Target Audience<span></span></a></li>
<li><a href="#whats-covered" id="toc-whats-covered">What’s Covered?<span></span></a></li>
<li><a href="#acknowledgements" id="toc-acknowledgements">Acknowledgements<span></span></a></li>
<li><a href="#license" id="toc-license">License<span></span></a></li>
</ul></li>
<li><a href="#part-basics-knowledge" id="toc-part-basics-knowledge">(PART) Basics Knowledge<span></span></a></li>
<li><a href="#r-and-rstudio" id="toc-r-and-rstudio"><span class="toc-section-number">1</span> R and RStudio<span></span></a>
<ul>
<li><a href="#installing-r-and-rstudio" id="toc-installing-r-and-rstudio"><span class="toc-section-number">1.1</span> Installing R and RStudio<span></span></a></li>
<li><a href="#r-basic" id="toc-r-basic"><span class="toc-section-number">1.2</span> Resources and Guides<span></span></a></li>
<li><a href="#basic-mathematical-operations" id="toc-basic-mathematical-operations"><span class="toc-section-number">1.3</span> Basic Mathematical Operations<span></span></a></li>
<li><a href="#data-objects" id="toc-data-objects"><span class="toc-section-number">1.4</span> Data Objects<span></span></a></li>
<li><a href="#readin-and-save-data" id="toc-readin-and-save-data"><span class="toc-section-number">1.5</span> Readin and save data<span></span></a></li>
<li><a href="#using-and-defining-functions" id="toc-using-and-defining-functions"><span class="toc-section-number">1.6</span> Using and defining functions<span></span></a></li>
<li><a href="#distribution-and-random-numbers" id="toc-distribution-and-random-numbers"><span class="toc-section-number">1.7</span> Distribution and random numbers<span></span></a></li>
<li><a href="#using-packages-and-other-resources" id="toc-using-packages-and-other-resources"><span class="toc-section-number">1.8</span> Using packages and other resources<span></span></a></li>
<li><a href="#practice-questions" id="toc-practice-questions"><span class="toc-section-number">1.9</span> Practice questions<span></span></a></li>
</ul></li>
<li><a href="#rmarkdown" id="toc-rmarkdown"><span class="toc-section-number">2</span> RMarkdown<span></span></a>
<ul>
<li><a href="#basics-and-resources" id="toc-basics-and-resources"><span class="toc-section-number">2.1</span> Basics and Resources<span></span></a></li>
<li><a href="#formatting-text" id="toc-formatting-text"><span class="toc-section-number">2.2</span> Formatting Text<span></span></a></li>
<li><a href="#adding-r-code" id="toc-adding-r-code"><span class="toc-section-number">2.3</span> Adding <code>R</code> Code<span></span></a>
<ul>
<li><a href="#r-chunks" id="toc-r-chunks"><span class="toc-section-number">2.3.1</span> <code>R</code> Chunks<span></span></a></li>
<li><a href="#inline-r" id="toc-inline-r"><span class="toc-section-number">2.3.2</span> Inline <code>R</code><span></span></a></li>
</ul></li>
<li><a href="#importing-data" id="toc-importing-data"><span class="toc-section-number">2.4</span> Importing Data<span></span></a></li>
<li><a href="#working-directory" id="toc-working-directory"><span class="toc-section-number">2.5</span> Working Directory<span></span></a></li>
<li><a href="#plotting" id="toc-plotting"><span class="toc-section-number">2.6</span> Plotting<span></span></a></li>
<li><a href="#chunk-options" id="toc-chunk-options"><span class="toc-section-number">2.7</span> Chunk Options<span></span></a></li>
<li><a href="#adding-math-with-latex" id="toc-adding-math-with-latex"><span class="toc-section-number">2.8</span> Adding Math with LaTeX<span></span></a>
<ul>
<li><a href="#displaystyle-latex" id="toc-displaystyle-latex"><span class="toc-section-number">2.8.1</span> Displaystyle LaTeX<span></span></a></li>
<li><a href="#inline-latex" id="toc-inline-latex"><span class="toc-section-number">2.8.2</span> Inline LaTex<span></span></a></li>
</ul></li>
<li><a href="#output-options" id="toc-output-options"><span class="toc-section-number">2.9</span> Output Options<span></span></a></li>
<li><a href="#try-it" id="toc-try-it"><span class="toc-section-number">2.10</span> Try It!<span></span></a></li>
</ul></li>
<li><a href="#linear-algebra-basics" id="toc-linear-algebra-basics"><span class="toc-section-number">3</span> Linear Algebra Basics<span></span></a>
<ul>
<li><a href="#definition" id="toc-definition"><span class="toc-section-number">3.1</span> Definition<span></span></a></li>
</ul></li>
<li><a href="#optimization-basics" id="toc-optimization-basics"><span class="toc-section-number">4</span> Optimization Basics<span></span></a>
<ul>
<li><a href="#basic-concept" id="toc-basic-concept"><span class="toc-section-number">4.1</span> Basic Concept<span></span></a></li>
<li><a href="#global_local" id="toc-global_local"><span class="toc-section-number">4.2</span> Global vs. Local Optima<span></span></a></li>
<li><a href="#example-linear-regression-using-optim" id="toc-example-linear-regression-using-optim"><span class="toc-section-number">4.3</span> Example: Linear Regression using <code>optim()</code><span></span></a></li>
<li><a href="#first-and-second-order-properties" id="toc-first-and-second-order-properties"><span class="toc-section-number">4.4</span> First and Second Order Properties<span></span></a></li>
<li><a href="#algorithm" id="toc-algorithm"><span class="toc-section-number">4.5</span> Algorithm<span></span></a></li>
<li><a href="#second-order-methods" id="toc-second-order-methods"><span class="toc-section-number">4.6</span> Second-order Methods<span></span></a>
<ul>
<li><a href="#newtons-method" id="toc-newtons-method"><span class="toc-section-number">4.6.1</span> Newton’s Method<span></span></a></li>
<li><a href="#quasi-newton-methods" id="toc-quasi-newton-methods"><span class="toc-section-number">4.6.2</span> Quasi-Newton Methods<span></span></a></li>
</ul></li>
<li><a href="#first-order-methods" id="toc-first-order-methods"><span class="toc-section-number">4.7</span> First-order Methods<span></span></a>
<ul>
<li><a href="#gradient-descent" id="toc-gradient-descent"><span class="toc-section-number">4.7.1</span> Gradient Descent<span></span></a></li>
<li><a href="#gradient-descent-example-linear-regression" id="toc-gradient-descent-example-linear-regression"><span class="toc-section-number">4.7.2</span> Gradient Descent Example: Linear Regression<span></span></a></li>
</ul></li>
<li><a href="#coordinate" id="toc-coordinate"><span class="toc-section-number">4.8</span> Coordinate Descent<span></span></a>
<ul>
<li><a href="#coordinate-descent-example-linear-regression" id="toc-coordinate-descent-example-linear-regression"><span class="toc-section-number">4.8.1</span> Coordinate Descent Example: Linear Regression<span></span></a></li>
</ul></li>
<li><a href="#stocastic-gradient-descent" id="toc-stocastic-gradient-descent"><span class="toc-section-number">4.9</span> Stocastic Gradient Descent<span></span></a>
<ul>
<li><a href="#mini-batch-stocastic-gradient-descent" id="toc-mini-batch-stocastic-gradient-descent"><span class="toc-section-number">4.9.1</span> Mini-batch Stocastic Gradient Descent<span></span></a></li>
</ul></li>
<li><a href="#lagrangian-multiplier-for-constrained-problems" id="toc-lagrangian-multiplier-for-constrained-problems"><span class="toc-section-number">4.10</span> Lagrangian Multiplier for Constrained Problems<span></span></a></li>
</ul></li>
<li><a href="#part-linear-and-penalized-linear-models" id="toc-part-linear-and-penalized-linear-models">(PART) Linear and Penalized Linear Models<span></span></a></li>
<li><a href="#linear-regression-and-model-selection" id="toc-linear-regression-and-model-selection"><span class="toc-section-number">5</span> Linear Regression and Model Selection<span></span></a>
<ul>
<li><a href="#example-real-estate-data" id="toc-example-real-estate-data"><span class="toc-section-number">5.1</span> Example: real estate data<span></span></a></li>
<li><a href="#notation-and-basic-properties" id="toc-notation-and-basic-properties"><span class="toc-section-number">5.2</span> Notation and Basic Properties<span></span></a></li>
<li><a href="#using-the-lm-function" id="toc-using-the-lm-function"><span class="toc-section-number">5.3</span> Using the <code>lm()</code> Function<span></span></a>
<ul>
<li><a href="#adding-covariates" id="toc-adding-covariates"><span class="toc-section-number">5.3.1</span> Adding Covariates<span></span></a></li>
<li><a href="#categorical-variables" id="toc-categorical-variables"><span class="toc-section-number">5.3.2</span> Categorical Variables<span></span></a></li>
</ul></li>
<li><a href="#model-selection-criteria" id="toc-model-selection-criteria"><span class="toc-section-number">5.4</span> Model Selection Criteria<span></span></a>
<ul>
<li><a href="#using-marrows-c_p" id="toc-using-marrows-c_p"><span class="toc-section-number">5.4.1</span> Using Marrows’ <span class="math inline">\(C_p\)</span><span></span></a></li>
<li><a href="#using-aic-and-bic" id="toc-using-aic-and-bic"><span class="toc-section-number">5.4.2</span> Using AIC and BIC<span></span></a></li>
</ul></li>
<li><a href="#model-selection-algorithms" id="toc-model-selection-algorithms"><span class="toc-section-number">5.5</span> Model Selection Algorithms<span></span></a>
<ul>
<li><a href="#best-subset-selection-with-leaps" id="toc-best-subset-selection-with-leaps"><span class="toc-section-number">5.5.1</span> Best Subset Selection with <code>leaps</code><span></span></a></li>
<li><a href="#step-wise-regression-using-step" id="toc-step-wise-regression-using-step"><span class="toc-section-number">5.5.2</span> Step-wise regression using <code>step()</code><span></span></a></li>
</ul></li>
<li><a href="#marrows-cp" id="toc-marrows-cp"><span class="toc-section-number">5.6</span> Derivation of Marrows’ <span class="math inline">\(C_p\)</span><span></span></a></li>
</ul></li>
<li><a href="#ridge-regression" id="toc-ridge-regression"><span class="toc-section-number">6</span> Ridge Regression<span></span></a>
<ul>
<li><a href="#motivation-correlated-variables-and-convexity" id="toc-motivation-correlated-variables-and-convexity"><span class="toc-section-number">6.1</span> Motivation: Correlated Variables and Convexity<span></span></a></li>
<li><a href="#ridge-penalty-and-the-reduced-variation" id="toc-ridge-penalty-and-the-reduced-variation"><span class="toc-section-number">6.2</span> Ridge Penalty and the Reduced Variation<span></span></a></li>
<li><a href="#bias-and-variance-of-ridge-regression" id="toc-bias-and-variance-of-ridge-regression"><span class="toc-section-number">6.3</span> Bias and Variance of Ridge Regression<span></span></a></li>
<li><a href="#degrees-of-freedom" id="toc-degrees-of-freedom"><span class="toc-section-number">6.4</span> Degrees of Freedom<span></span></a></li>
<li><a href="#using-the-lm.ridge-function" id="toc-using-the-lm.ridge-function"><span class="toc-section-number">6.5</span> Using the <code>lm.ridge()</code> function<span></span></a>
<ul>
<li><a href="#scaling-issue" id="toc-scaling-issue"><span class="toc-section-number">6.5.1</span> Scaling Issue<span></span></a></li>
<li><a href="#multiple-lambda-values" id="toc-multiple-lambda-values"><span class="toc-section-number">6.5.2</span> Multiple <span class="math inline">\(\lambda\)</span> values<span></span></a></li>
</ul></li>
<li><a href="#cross-validation" id="toc-cross-validation"><span class="toc-section-number">6.6</span> Cross-validation<span></span></a></li>
<li><a href="#leave-one-out-cross-validation" id="toc-leave-one-out-cross-validation"><span class="toc-section-number">6.7</span> Leave-one-out cross-validation<span></span></a>
<ul>
<li><a href="#generalized-cross-validation" id="toc-generalized-cross-validation"><span class="toc-section-number">6.7.1</span> Generalized cross-validation<span></span></a></li>
</ul></li>
<li><a href="#the-glmnet-package" id="toc-the-glmnet-package"><span class="toc-section-number">6.8</span> The <code>glmnet</code> package<span></span></a>
<ul>
<li><a href="#scaling-issue-1" id="toc-scaling-issue-1"><span class="toc-section-number">6.8.1</span> Scaling Issue<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="#lasso" id="toc-lasso"><span class="toc-section-number">7</span> Lasso<span></span></a>
<ul>
<li><a href="#one-variable-lasso-and-shrinkage" id="toc-one-variable-lasso-and-shrinkage"><span class="toc-section-number">7.1</span> One-Variable Lasso and Shrinkage<span></span></a></li>
<li><a href="#constrained-optimization-view" id="toc-constrained-optimization-view"><span class="toc-section-number">7.2</span> Constrained Optimization View<span></span></a></li>
<li><a href="#the-solution-path" id="toc-the-solution-path"><span class="toc-section-number">7.3</span> The Solution Path<span></span></a></li>
<li><a href="#path-wise-coordinate-descent" id="toc-path-wise-coordinate-descent"><span class="toc-section-number">7.4</span> Path-wise Coordinate Descent<span></span></a></li>
<li><a href="#using-the-glmnet-package" id="toc-using-the-glmnet-package"><span class="toc-section-number">7.5</span> Using the <code>glmnet</code> package<span></span></a></li>
<li><a href="#elastic-net" id="toc-elastic-net"><span class="toc-section-number">7.6</span> Elastic-Net<span></span></a></li>
</ul></li>
<li><a href="#part-nonparametric-models" id="toc-part-nonparametric-models">(PART) Nonparametric Models<span></span></a></li>
<li><a href="#spline" id="toc-spline"><span class="toc-section-number">8</span> Spline<span></span></a>
<ul>
<li><a href="#from-linear-to-nonlinear" id="toc-from-linear-to-nonlinear"><span class="toc-section-number">8.1</span> From Linear to Nonlinear<span></span></a></li>
<li><a href="#a-motivating-example-and-polynomials" id="toc-a-motivating-example-and-polynomials"><span class="toc-section-number">8.2</span> A Motivating Example and Polynomials<span></span></a></li>
<li><a href="#piecewise-polynomials" id="toc-piecewise-polynomials"><span class="toc-section-number">8.3</span> Piecewise Polynomials<span></span></a></li>
<li><a href="#splines" id="toc-splines"><span class="toc-section-number">8.4</span> Splines<span></span></a></li>
<li><a href="#spline-basis" id="toc-spline-basis"><span class="toc-section-number">8.5</span> Spline Basis<span></span></a></li>
<li><a href="#natural-cubic-spline" id="toc-natural-cubic-spline"><span class="toc-section-number">8.6</span> Natural Cubic Spline<span></span></a></li>
<li><a href="#smoothing-spline" id="toc-smoothing-spline"><span class="toc-section-number">8.7</span> Smoothing Spline<span></span></a></li>
<li><a href="#fitting-smoothing-splines" id="toc-fitting-smoothing-splines"><span class="toc-section-number">8.8</span> Fitting Smoothing Splines<span></span></a></li>
<li><a href="#extending-splines-to-multiple-varibles" id="toc-extending-splines-to-multiple-varibles"><span class="toc-section-number">8.9</span> Extending Splines to Multiple Varibles<span></span></a></li>
</ul></li>
<li><a href="#k-neariest-neighber" id="toc-k-neariest-neighber"><span class="toc-section-number">9</span> K-Neariest Neighber<span></span></a>
<ul>
<li><a href="#definition-1" id="toc-definition-1"><span class="toc-section-number">9.1</span> Definition<span></span></a></li>
<li><a href="#tuning-k" id="toc-tuning-k"><span class="toc-section-number">9.2</span> Tuning <span class="math inline">\(k\)</span><span></span></a></li>
<li><a href="#the-bias-variance-trade-off" id="toc-the-bias-variance-trade-off"><span class="toc-section-number">9.3</span> The Bias-variance Trade-off<span></span></a></li>
<li><a href="#knn-for-classification" id="toc-knn-for-classification"><span class="toc-section-number">9.4</span> KNN for Classification<span></span></a></li>
<li><a href="#example-1-an-artificial-data" id="toc-example-1-an-artificial-data"><span class="toc-section-number">9.5</span> Example 1: An artificial data<span></span></a></li>
<li><a href="#tuning-with-the-caret-package" id="toc-tuning-with-the-caret-package"><span class="toc-section-number">9.6</span> Tuning with the <code>caret</code> Package<span></span></a></li>
<li><a href="#distance-measures" id="toc-distance-measures"><span class="toc-section-number">9.7</span> Distance Measures<span></span></a></li>
<li><a href="#nn-error-bound" id="toc-nn-error-bound"><span class="toc-section-number">9.8</span> 1NN Error Bound<span></span></a></li>
<li><a href="#example-2-handwritten-digit-data" id="toc-example-2-handwritten-digit-data"><span class="toc-section-number">9.9</span> Example 2: Handwritten Digit Data<span></span></a></li>
<li><a href="#curse-of-dimensionality" id="toc-curse-of-dimensionality"><span class="toc-section-number">9.10</span> Curse of Dimensionality<span></span></a></li>
</ul></li>
<li><a href="#kernel-smoothing" id="toc-kernel-smoothing"><span class="toc-section-number">10</span> Kernel Smoothing<span></span></a>
<ul>
<li><a href="#knn-vs.-kernel" id="toc-knn-vs.-kernel"><span class="toc-section-number">10.1</span> KNN vs. Kernel<span></span></a></li>
<li><a href="#kernel-density-estimations" id="toc-kernel-density-estimations"><span class="toc-section-number">10.2</span> Kernel Density Estimations<span></span></a></li>
<li><a href="#bias-variance-trade-off" id="toc-bias-variance-trade-off"><span class="toc-section-number">10.3</span> Bias-variance trade-off<span></span></a></li>
<li><a href="#gaussian-kernel-regression" id="toc-gaussian-kernel-regression"><span class="toc-section-number">10.4</span> Gaussian Kernel Regression<span></span></a>
<ul>
<li><a href="#bias-variance-trade-off-1" id="toc-bias-variance-trade-off-1"><span class="toc-section-number">10.4.1</span> Bias-variance Trade-off<span></span></a></li>
</ul></li>
<li><a href="#choice-of-kernel-functions" id="toc-choice-of-kernel-functions"><span class="toc-section-number">10.5</span> Choice of Kernel Functions<span></span></a></li>
<li><a href="#local-linear-regression" id="toc-local-linear-regression"><span class="toc-section-number">10.6</span> Local Linear Regression<span></span></a></li>
<li><a href="#local-polynomial-regression" id="toc-local-polynomial-regression"><span class="toc-section-number">10.7</span> Local Polynomial Regression<span></span></a></li>
<li><a href="#r-implementations" id="toc-r-implementations"><span class="toc-section-number">10.8</span> R Implementations<span></span></a></li>
</ul></li>
<li><a href="#part-classification-models" id="toc-part-classification-models">(PART) Classification Models<span></span></a></li>
<li><a href="#logistic-regression" id="toc-logistic-regression"><span class="toc-section-number">11</span> Logistic Regression<span></span></a>
<ul>
<li><a href="#modeling-binary-outcomes" id="toc-modeling-binary-outcomes"><span class="toc-section-number">11.1</span> Modeling Binary Outcomes<span></span></a></li>
<li><a href="#example-cleveland-clinic-heart-disease-data" id="toc-example-cleveland-clinic-heart-disease-data"><span class="toc-section-number">11.2</span> Example: Cleveland Clinic Heart Disease Data<span></span></a></li>
<li><a href="#interpretation-of-the-parameters" id="toc-interpretation-of-the-parameters"><span class="toc-section-number">11.3</span> Interpretation of the Parameters<span></span></a></li>
<li><a href="#solving-a-logistic-regression" id="toc-solving-a-logistic-regression"><span class="toc-section-number">11.4</span> Solving a Logistic Regression<span></span></a></li>
<li><a href="#example-south-africa-heart-data" id="toc-example-south-africa-heart-data"><span class="toc-section-number">11.5</span> Example: South Africa Heart Data<span></span></a></li>
<li><a href="#penalized-logistic-regression" id="toc-penalized-logistic-regression"><span class="toc-section-number">11.6</span> Penalized Logistic Regression<span></span></a></li>
</ul></li>
<li><a href="#discriminant-analysis" id="toc-discriminant-analysis"><span class="toc-section-number">12</span> Discriminant Analysis<span></span></a>
<ul>
<li><a href="#bayes-rule" id="toc-bayes-rule"><span class="toc-section-number">12.1</span> Bayes Rule<span></span></a></li>
<li><a href="#example-linear-discriminant-analysis-lda" id="toc-example-linear-discriminant-analysis-lda"><span class="toc-section-number">12.2</span> Example: Linear Discriminant Analysis (LDA)<span></span></a></li>
<li><a href="#linear-discriminant-analysis" id="toc-linear-discriminant-analysis"><span class="toc-section-number">12.3</span> Linear Discriminant Analysis<span></span></a></li>
<li><a href="#example-quadratic-discriminant-analysis-qda" id="toc-example-quadratic-discriminant-analysis-qda"><span class="toc-section-number">12.4</span> Example: Quadratic Discriminant Analysis (QDA)<span></span></a></li>
<li><a href="#quadratic-discriminant-analysis" id="toc-quadratic-discriminant-analysis"><span class="toc-section-number">12.5</span> Quadratic Discriminant Analysis<span></span></a></li>
<li><a href="#example-the-hand-written-digit-data" id="toc-example-the-hand-written-digit-data"><span class="toc-section-number">12.6</span> Example: the Hand Written Digit Data<span></span></a></li>
</ul></li>
<li><a href="#part-machine-learning-algorithms" id="toc-part-machine-learning-algorithms">(PART) Machine Learning Algorithms<span></span></a></li>
<li><a href="#support-vector-machines" id="toc-support-vector-machines"><span class="toc-section-number">13</span> Support Vector Machines<span></span></a>
<ul>
<li><a href="#maximum-margin-classifier" id="toc-maximum-margin-classifier"><span class="toc-section-number">13.1</span> Maximum-margin Classifier<span></span></a></li>
<li><a href="#linearly-separable-svm" id="toc-linearly-separable-svm"><span class="toc-section-number">13.2</span> Linearly Separable SVM<span></span></a>
<ul>
<li><a href="#from-primal-to-dual" id="toc-from-primal-to-dual"><span class="toc-section-number">13.2.1</span> From Primal to Dual<span></span></a></li>
</ul></li>
<li><a href="#linearly-non-separable-svm-with-slack-variables" id="toc-linearly-non-separable-svm-with-slack-variables"><span class="toc-section-number">13.3</span> Linearly Non-separable SVM with Slack Variables<span></span></a></li>
<li><a href="#example-saheart-data" id="toc-example-saheart-data"><span class="toc-section-number">13.4</span> Example: <code>SAheart</code> Data<span></span></a></li>
<li><a href="#nonlinear-svm-via-kernel-trick" id="toc-nonlinear-svm-via-kernel-trick"><span class="toc-section-number">13.5</span> Nonlinear SVM via Kernel Trick<span></span></a></li>
<li><a href="#example-mixture.example-data" id="toc-example-mixture.example-data"><span class="toc-section-number">13.6</span> Example: <code>mixture.example</code> Data<span></span></a></li>
<li><a href="#svm-as-a-penalized-model" id="toc-svm-as-a-penalized-model"><span class="toc-section-number">13.7</span> SVM as a Penalized Model<span></span></a></li>
<li><a href="#kernel-and-feature-maps-another-example" id="toc-kernel-and-feature-maps-another-example"><span class="toc-section-number">13.8</span> Kernel and Feature Maps: Another Example<span></span></a></li>
</ul></li>
<li><a href="#reproducing-kernel-hilbert-space" id="toc-reproducing-kernel-hilbert-space"><span class="toc-section-number">14</span> Reproducing Kernel Hilbert Space<span></span></a>
<ul>
<li><a href="#constructing-the-rkhs" id="toc-constructing-the-rkhs"><span class="toc-section-number">14.1</span> Constructing the RKHS<span></span></a></li>
<li><a href="#properties-of-rkhs" id="toc-properties-of-rkhs"><span class="toc-section-number">14.2</span> Properties of RKHS<span></span></a></li>
<li><a href="#the-representer-theorem" id="toc-the-representer-theorem"><span class="toc-section-number">14.3</span> The Representer Theorem<span></span></a></li>
</ul></li>
<li><a href="#kernel-ridge-regression" id="toc-kernel-ridge-regression"><span class="toc-section-number">15</span> Kernel Ridge Regression<span></span></a>
<ul>
<li><a href="#example-linear-kernel-and-ridge-regression" id="toc-example-linear-kernel-and-ridge-regression"><span class="toc-section-number">15.1</span> Example: Linear Kernel and Ridge Regression<span></span></a></li>
<li><a href="#example-alternative-view" id="toc-example-alternative-view"><span class="toc-section-number">15.2</span> Example: Alternative View<span></span></a></li>
</ul></li>
<li><a href="#classification-and-regression-trees" id="toc-classification-and-regression-trees"><span class="toc-section-number">16</span> Classification and Regression Trees<span></span></a>
<ul>
<li><a href="#example-classification-tree" id="toc-example-classification-tree"><span class="toc-section-number">16.1</span> Example: Classification Tree<span></span></a></li>
<li><a href="#splitting-a-node" id="toc-splitting-a-node"><span class="toc-section-number">16.2</span> Splitting a Node<span></span></a></li>
<li><a href="#regression-trees" id="toc-regression-trees"><span class="toc-section-number">16.3</span> Regression Trees<span></span></a></li>
<li><a href="#predicting-a-target-point" id="toc-predicting-a-target-point"><span class="toc-section-number">16.4</span> Predicting a Target Point<span></span></a></li>
<li><a href="#tuning-a-tree-model" id="toc-tuning-a-tree-model"><span class="toc-section-number">16.5</span> Tuning a Tree Model<span></span></a></li>
</ul></li>
<li><a href="#random-forests" id="toc-random-forests"><span class="toc-section-number">17</span> Random Forests<span></span></a>
<ul>
<li><a href="#bagging-predictors" id="toc-bagging-predictors"><span class="toc-section-number">17.1</span> Bagging Predictors<span></span></a></li>
<li><a href="#random-forests-1" id="toc-random-forests-1"><span class="toc-section-number">17.2</span> Random Forests<span></span></a></li>
<li><a href="#effect-of-mtry" id="toc-effect-of-mtry"><span class="toc-section-number">17.3</span> Effect of <code>mtry</code><span></span></a></li>
<li><a href="#effect-of-nodesize" id="toc-effect-of-nodesize"><span class="toc-section-number">17.4</span> Effect of <code>nodesize</code><span></span></a></li>
<li><a href="#variable-importance" id="toc-variable-importance"><span class="toc-section-number">17.5</span> Variable Importance<span></span></a></li>
<li><a href="#kernel-view-of-random-forets" id="toc-kernel-view-of-random-forets"><span class="toc-section-number">17.6</span> Kernel view of Random Forets<span></span></a></li>
</ul></li>
<li><a href="#boosting" id="toc-boosting"><span class="toc-section-number">18</span> Boosting<span></span></a>
<ul>
<li><a href="#adaboost" id="toc-adaboost"><span class="toc-section-number">18.1</span> AdaBoost<span></span></a></li>
<li><a href="#training-error-of-adaboost" id="toc-training-error-of-adaboost"><span class="toc-section-number">18.2</span> Training Error of AdaBoost<span></span></a></li>
<li><a href="#gradient-boosting" id="toc-gradient-boosting"><span class="toc-section-number">18.3</span> Gradient Boosting<span></span></a></li>
</ul></li>
<li><a href="#part-unsupervised-learning" id="toc-part-unsupervised-learning">(PART) Unsupervised Learning<span></span></a></li>
<li><a href="#k-means" id="toc-k-means"><span class="toc-section-number">19</span> K-Means<span></span></a>
<ul>
<li><a href="#basic-concepts" id="toc-basic-concepts"><span class="toc-section-number">19.1</span> Basic Concepts<span></span></a></li>
<li><a href="#example-1-iris-data" id="toc-example-1-iris-data"><span class="toc-section-number">19.2</span> Example 1: <code>iris</code> data<span></span></a></li>
<li><a href="#example-2-clustering-of-image-pixels" id="toc-example-2-clustering-of-image-pixels"><span class="toc-section-number">19.3</span> Example 2: clustering of image pixels<span></span></a></li>
</ul></li>
<li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering"><span class="toc-section-number">20</span> Hierarchical Clustering<span></span></a>
<ul>
<li><a href="#basic-concepts-1" id="toc-basic-concepts-1"><span class="toc-section-number">20.1</span> Basic Concepts<span></span></a></li>
<li><a href="#example-1-iris-data-1" id="toc-example-1-iris-data-1"><span class="toc-section-number">20.2</span> Example 1: <code>iris</code> data<span></span></a></li>
<li><a href="#example-2-rna-expression-data" id="toc-example-2-rna-expression-data"><span class="toc-section-number">20.3</span> Example 2: RNA Expression Data<span></span></a></li>
</ul></li>
<li><a href="#principle-component-analysis" id="toc-principle-component-analysis"><span class="toc-section-number">21</span> Principle Component Analysis<span></span></a>
<ul>
<li><a href="#basic-concepts-2" id="toc-basic-concepts-2"><span class="toc-section-number">21.1</span> Basic Concepts<span></span></a>
<ul>
<li><a href="#note-scaling" id="toc-note-scaling"><span class="toc-section-number">21.1.1</span> Note: Scaling<span></span></a></li>
</ul></li>
<li><a href="#example-1-iris-data-2" id="toc-example-1-iris-data-2"><span class="toc-section-number">21.2</span> Example 1: <code>iris</code> Data<span></span></a></li>
<li><a href="#example-2-handwritten-digits" id="toc-example-2-handwritten-digits"><span class="toc-section-number">21.3</span> Example 2: Handwritten Digits<span></span></a></li>
</ul></li>
<li><a href="#self-organizing-map" id="toc-self-organizing-map"><span class="toc-section-number">22</span> Self-Organizing Map<span></span></a>
<ul>
<li><a href="#basic-concepts-3" id="toc-basic-concepts-3"><span class="toc-section-number">22.1</span> Basic Concepts<span></span></a></li>
</ul></li>
<li><a href="#spectral-clustering" id="toc-spectral-clustering"><span class="toc-section-number">23</span> Spectral Clustering<span></span></a>
<ul>
<li><a href="#basic-concepts-4" id="toc-basic-concepts-4"><span class="toc-section-number">23.1</span> Basic Concepts<span></span></a></li>
</ul></li>
<li><a href="#part-reference" id="toc-part-reference">(PART) Reference<span></span></a></li>
<li><a href="#reference" id="toc-reference"><span class="toc-section-number">24</span> Reference<span></span></a></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning and Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<div id="preface" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Preface<a href="#preface" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Welcome to <em>Statistical Learning and Machine Learning with R</em>! I started this project during the summer of 2018 when I was preparing for the Stat 432 course. At that time, our faculty member <a href="https://daviddalpiaz.com/teaching.html">Dr. David Dalpiaz</a>, had decided to move to The Ohio State University (although he moved back to UIUC later on). David introduced to me this awesome way of publishing website on GitHub, which is a very efficient approach for developing courses. Since I have also taught Stat 542 (Statistical Learning) for several years, I figured it could be beneficial to integrate what I have to this <a href="https://daviddalpiaz.github.io/r4sl/">existing book</a> by David and use it as the R material for both courses. For Stat 542, the main focus is to learn the numerical optimization behind these learning algorithms, and also be familiar with the theoretical background. As you can tell, I am not being very creative on the name, so <code>SMLR</code> it is. You can find the source file of this book on my <a href="https://teazrq.github.io/SMLR/">GitHub</a>.</p>
<div id="target-audience" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Target Audience<a href="#target-audience" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This book can be suitable for students ranging from advanced undergraduate to first/second year Ph.D students who have prior knowledge in statistics. Although a student at the masters level will likely benefit most from the material. Previous experience with both basic mathematics (mainly linear algebra), statistical modeling (such as linear regressions) and R are assumed.</p>
</div>
<div id="whats-covered" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">What’s Covered?<a href="#whats-covered" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This book currently covers the following topics:</p>
<ol style="list-style-type: decimal">
<li>Basic Knowledge
<ul>
<li>R, R Studio and R Markdown</li>
<li>Linear regression and linear algebra</li>
<li>Numerical optimization</li>
</ul></li>
<li>Penalized linear models and model selection</li>
<li>Nonlinear and Nonparametric Models
<ul>
<li>Spline</li>
<li>K-nearest neighbor</li>
<li>Kernel smoothing</li>
</ul></li>
<li>Classification models
<ul>
<li>Logistic regression</li>
<li>Discriminant analysis</li>
</ul></li>
<li>Machine Learning Models
<ul>
<li>Support vector machine</li>
<li>Kernel ridge regression</li>
<li>Tree models</li>
<li>Random forest</li>
<li>Boosting</li>
</ul></li>
<li>Unsupervised Learning
<ul>
<li>K-means</li>
<li>Hierarchical clustering</li>
<li>PCA</li>
<li>self-organizing map</li>
<li>Spectral clustering</li>
</ul></li>
</ol>
<p>The goal of this book is to introduce not only how to run some of the popular statistical learning models in <code>R</code>, know the algorithms and programming techniques for solving these models and also understand some of the fundamental statistical theory behind them. For example, for graduate students, these topics will be discuss in more detail:</p>
<ul>
<li>Optimization
<ul>
<li>Lagrangian</li>
<li>Primal vs. dual</li>
</ul></li>
<li>EM and MM algorithm</li>
<li>Bias-variance trade-off in
<ul>
<li>Linear regression</li>
<li>KNN</li>
<li>Kernel density estimation</li>
</ul></li>
<li>Kernel Trick and RKHS</li>
<li>Representer Theorem
<ul>
<li>SVM</li>
<li>Spline</li>
</ul></li>
</ul>
<p>For each section, the difficulty will gradually increase from an undergraduate level to a graduate level.</p>
<p>It will be served as a supplement to <a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a> <span class="citation">(<a href="#ref-james2013introduction" role="doc-biblioref">James et al. 2013</a>)</span> for <a href="https://go.illinois.edu/stat432">STAT 432 - Basics of Statistical Learning</a> and to <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of
Statistical Learning: Data Mining, Inference, and Prediction</a> <span class="citation">(<a href="#ref-hastie2001elements" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2001</a>)</span> for <a href="https://go.illinois.edu/stat542">STAT 542 - Statistical Learning</a> at the <a href="http://illinois.edu/">University of Illinois at Urbana-Champaign</a>.</p>
<p><strong>This book is under active development</strong>. Hence, you may encounter errors ranging from typos to broken code, to poorly explained topics. If you do, please let me know! Simply send an email and I will make the changes as soon as possible (<code>rqzhu AT illinois DOT edu</code>). Or, if you know <code>R Markdown</code> and are familiar with GitHub, <a href="https://github.com/teazrq/SLWR">make a pull request and fix an issue yourself</a>! These contributions will be acknowledged.</p>
</div>
<div id="acknowledgements" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Acknowledgements<a href="#acknowledgements" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The initial contents are derived from Dr. David Dalpiaz’s book. My STAT 542 course materials are also inspired by <a href="https://stat.illinois.edu/directory/profile/liangf">Dr. Feng Liang</a> and <a href="https://stat.illinois.edu/directory/profile/jimarden">Dr. John Marden</a> who developed earlier versions of this course. And I also incorporated many online resources, which I cannot put into a comprehensive list. If you think I missed some references, please let me know.</p>
</div>
<div id="license" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">License<a href="#license" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="figure">
<img src="images/cc.png" style="width:15.0%" alt="" />
<p class="caption">This work is licensed under a <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</p>
</div>
<!--chapter:end:index.Rmd-->
</div>
</div>
<div id="part-basics-knowledge" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART) Basics Knowledge<a href="#part-basics-knowledge" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="r-and-rstudio" class="section level1 hasAnchor" number="1">
<h1 class="hasAnchor"><span class="header-section-number">1</span> R and RStudio<a href="#r-and-rstudio" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="installing-r-and-rstudio" class="section level2 hasAnchor" number="1.1">
<h2 class="hasAnchor"><span class="header-section-number">1.1</span> Installing R and RStudio<a href="#installing-r-and-rstudio" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The first step is to download and install <a href="https://www.r-project.org/">R</a> and <a href="https://www.rstudio.com/products/rstudio/download/#download">RStudio</a>. Most steps should be self-explanatory. You can also find many online guides for step-by-step instruction, such as <a href="https://www.youtube.com/watch?v=cX532N_XLIs&amp;t=19s/">this YouTube video</a>. However, be aware that some details may have been changed over the years.</p>
<p>After installing both, open your RStudio, you should see four panes, which can be seen below:</p>
<ul>
<li>Source pane on top-left where you write code in to files</li>
<li>Console on bottom-left where the code is inputted into R</li>
<li>Environment (and other tabs) on top-right where you can see current variables and objects you defined</li>
<li>File (and other tabs) on bottom-right which is essentially a file borrower</li>
</ul>
<center>
<img src="images/RStudio.png" style="width:100.0%" />
</center>
<p>We will mainly use the left two panes. You can either directly input code into the console to run for results, or edit your code in a file and run them in chunks or as a whole.</p>
</div>
<div id="r-basic" class="section level2 hasAnchor" number="1.2">
<h2 class="hasAnchor"><span class="header-section-number">1.2</span> Resources and Guides<a href="#r-basic" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are many online resources for how to use R, RStudio. For example, David Dalpiaz’s other online book <a href="http://daviddalpiaz.github.io/appliedstats/">Applied Statistics with R</a> contains an introduction to using them. There are also other online documentation such as</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=cX532N_XLIs&amp;t=19s/">Install R and RStudio</a></li>
<li><a href="http://www.r-tutor.com/r-introduction">R tutorial</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLBgxzZMu3GpPojVSoriMTWQCUno_3hjNi">Data in R Play-list (video)</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLBgxzZMu3GpMjYhX7jLm5B9gEV7AOOJ5w">R and RStudio Play-list (video)</a></li>
</ul>
<p>It is worth to mention that once you become an advanced user, and possibly a developer of R packages using <code>C/C++</code> (add-on of R for performing specific tasks), and you also happen to use Windows like I do, you will have to install <a href="https://cran.r-project.org/bin/windows/Rtools/">Rtools</a> that contains the gcc compilers. This is also needed if you want to install any R package from a “source” (<code>.tar.gz</code>) file instead of using the so-called “binaries” (<code>.zip</code> files).</p>
</div>
<div id="basic-mathematical-operations" class="section level2 hasAnchor" number="1.3">
<h2 class="hasAnchor"><span class="header-section-number">1.3</span> Basic Mathematical Operations<a href="#basic-mathematical-operations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Basic R calculations and operations should be self-explanatory. Try to type-in the following commands into your R console and start to explore yourself. Lines with a <code>#</code> in the front are comments, which will not be executed. Lines with <code>##</code> in the front are outputs you should expect.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Basic mathematical operations</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span> <span class="sc">+</span> <span class="dv">3</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 4</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span> <span class="sc">-</span> <span class="dv">3</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -2</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span> <span class="sc">*</span> <span class="dv">3</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 3</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span> <span class="sc">/</span> <span class="dv">3</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.3333333</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="dv">3</span><span class="sc">^</span><span class="dv">5</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 243</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="dv">4</span><span class="sc">^</span>(<span class="sc">-</span><span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.5</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    pi</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 3.141593</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># some math functions</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sqrt</span>(<span class="dv">4</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">exp</span>(<span class="dv">1</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2.718282</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">log</span>(<span class="dv">3</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1.098612</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">log2</span>(<span class="dv">16</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 4</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">log</span>(<span class="dv">15</span>, <span class="at">base =</span> <span class="dv">3</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2.464974</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="fu">factorial</span>(<span class="dv">5</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 120</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sin</span>(pi)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1.224606e-16</span></span></code></pre></div>
<p>If you want to see more information about a particular function or operator in R, the easiest way is to get the reference document. Put a question mark in front of a function name:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># In a default R console window, this will open up a web browser.</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># In RStudio, this will be displayed at the ‘Help’ window at the bottom-right penal (Help tab). </span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    ?log10</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    ?cos</span></code></pre></div>
</div>
<div id="data-objects" class="section level2 hasAnchor" number="1.4">
<h2 class="hasAnchor"><span class="header-section-number">1.4</span> Data Objects<a href="#data-objects" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Data objects can be a complicated topic for people who never used R before. The most common data objects are <code>vector</code>, <code>matrix</code>, <code>list</code>, and <code>data.frame</code>. They are defined using a specific syntax. To define a vector, we use <code>c</code> followed by <code>()</code>, where the elements within the parenthesis are separated using comma. You can save the vector and name as something else. For example</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># creating a vector</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1 2 3 4</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>, <span class="st">&quot;c&quot;</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot;</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define a new vector object, called `x`</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)</span></code></pre></div>
<p>After defining this object <code>x</code>, it should also appear on your top-right environment pane. To access elements in an object, we use the <code>[]</code> operator, like a <code>C</code> programming reference style.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># getting the second element in x</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    x[<span class="dv">2</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># getting the second to the fourth element in x</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    x[<span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>]</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1 1 0</span></span></code></pre></div>
<p>Similarly, we can create and access elements in a matrix:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create a matrix by providing all of its elements</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the elements are filled to the matrix by column</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="do">##      [,1] [,2]</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]    1    3</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]    2    4</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create a matrix by column-bind vectors</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cbind</span>(x, y)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="do">##      x y</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,] 1 1</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,] 1 0</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,] 1 1</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,] 0 0</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,] 0 1</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,] 0 0</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># access elements in a matrix</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Note that in R, upper and lower cases are treated as two different objects</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">16</span>), <span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    X</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="do">##      [,1] [,2] [,3] [,4]</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]    1    5    9   13</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]    2    6   10   14</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]    3    7   11   15</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,]    4    8   12   16</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    X[<span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 10</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    X[<span class="dv">1</span>, ]</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="do">## [1]  1  5  9 13</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># getting a sub-matrix of X</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    X[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>]</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="do">##      [,1] [,2]</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]    9   13</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]   10   14</span></span></code></pre></div>
<p>Mathematical operations on vectors and matrices are, by default, element-wise. For matrix multiplications, you should use <code>%*%</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># adding two vectors</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    (x <span class="sc">+</span> y)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 4 1 4 0 1 0</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># getting the length of a vector</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">length</span>(x)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 6</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matrix multiplication</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    X <span class="sc">%*%</span> X</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="do">##      [,1] [,2] [,3] [,4]</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]   90  202  314  426</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,]  100  228  356  484</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]  110  254  398  542</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,]  120  280  440  600</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># getting the dimension of a matrix</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">dim</span>(X)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 4 4</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># A warning will be issued when R detects something wrong</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Results may still be produced however</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    y <span class="sc">+</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning in y + c(1, 2, 3, 4): longer object length is not a multiple of shorter object length</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2 2 4 4 2 2</span></span></code></pre></div>
<p><code>list()</code> creates a list of objects (of any type). However, some operators cannot be directly applied to a list in a similar way as to vectors or matrices. Model fitting results in R are usually stored as a list. For example, the <code>lm()</code> function, which will be introduced later.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># creating a list</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="st">&quot;hello&quot;</span>, <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>), <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># accessing its elements using double brackets `[[]]` </span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    x[[<span class="dv">1</span>]]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1 2</span></span></code></pre></div>
<p><code>data.frame()</code> creates a list of vectors of equal length, and display them as a matrix-like object, in which each vector is a column of the matrix. It is mainly used for storing data. This will be our most frequently used data object for analysis. For example, in the famous <code>iris</code> data, the first four columns are numerical variables, while the last column is a categorical variable with three levels: <code>setosa</code>, <code>versicolor</code>, and <code>virginica</code>:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The iris data is included with base R, so we can use them directly</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This will create a copy of the data into your environment</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data</span>(iris)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the head function peeks the first several rows of the dataset </span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">head</span>(iris, <span class="at">n =</span> <span class="dv">3</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="do">##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 1          5.1         3.5          1.4         0.2  setosa</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 2          4.9         3.0          1.4         0.2  setosa</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 3          4.7         3.2          1.3         0.2  setosa</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># each column usually contains a column (variable) name </span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">colnames</span>(iris)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot;  &quot;Petal.Length&quot; &quot;Petal.Width&quot;  &quot;Species&quot;</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># data frame can be called by each individual column, which will be a vector</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># iris$Species</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    iris<span class="sc">$</span>Species[<span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>]</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] setosa setosa setosa</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Levels: setosa versicolor virginica</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the summary function can be used to view summary statistics of all variables</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summary</span>(iris)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="do">##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width          Species  </span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="do">##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100   setosa    :50  </span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="do">##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300   versicolor:50  </span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="do">##  Median :5.800   Median :3.000   Median :4.350   Median :1.300   virginica :50  </span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="do">##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199                  </span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="do">##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800                  </span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="do">##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500</span></span></code></pre></div>
<p><code>factor</code> is a special type of vector. It is frequently used to store a categorical variable with more than two categories. The last column of the iris data is a factor. You need to be a little bit careful when dealing with factor variables when during modeling since some functions do not take care of them automatically or they do it in a different way than you thought. For example, changing a factor variable into numerical ones will ignore any potential relationship among different categories.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">levels</span>(iris<span class="sc">$</span>Species)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">as.numeric</span>(iris<span class="sc">$</span>Species)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="do">##  [48] 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="do">##  [95] 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [142] 3 3 3 3 3 3 3 3 3</span></span></code></pre></div>
</div>
<div id="readin-and-save-data" class="section level2 hasAnchor" number="1.5">
<h2 class="hasAnchor"><span class="header-section-number">1.5</span> Readin and save data<a href="#readin-and-save-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Data can be imported from a variety of sources. More commonly, a dataset can be stored in <code>.txt</code> and <code>.csv</code> files. Such data reading methods require specific structures in the source file: the first row should contain column names, and there should be equal number of elements in each row. Hence you should always check your file before reading them in.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># read-in data</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    birthrate <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/birthrate.csv&quot;</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">head</span>(birthrate)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   Year Birthrate</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 1917     183.1</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 1918     183.9</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 3 1919     163.1</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 1920     179.5</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 5 1921     181.4</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 6 1922     173.4</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to see how many observations (rows) and variables (columns) in a dataset</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">dim</span>(birthrate)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 87  2</span></span></code></pre></div>
<p>R data can also be saved into other formats. The more efficient way, assuming that you are going to load these file back to R in the future, is to save them as <code>.RData</code> file. Usually, for a larger dataset, this reduces the time spend on reading the data.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># saving a object to .RData file</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">save</span>(birthrate, <span class="at">file =</span> <span class="st">&quot;mydata.RData&quot;</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># you can specify multiple objects to be saved into the same file</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">save</span>(birthrate, iris, <span class="at">file =</span> <span class="st">&quot;mydata.RData&quot;</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># load the data again back to your environment</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">load</span>(<span class="st">&quot;mydata.RData&quot;</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># alternatively, you can also save data to a .csv file</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">write.csv</span>(birthrate, <span class="at">file =</span> <span class="st">&quot;mydata.csv&quot;</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># you can notice that this .csv file contains an extra column of &quot;ID number&quot;, without a column name</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hence, when you read this file back into R, you should specify `row.names = 1` to indicate that.</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Otherwise this will produce an error</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">&quot;mydata.csv&quot;</span>, <span class="at">row.names =</span> <span class="dv">1</span>)</span></code></pre></div>
</div>
<div id="using-and-defining-functions" class="section level2 hasAnchor" number="1.6">
<h2 class="hasAnchor"><span class="header-section-number">1.6</span> Using and defining functions<a href="#using-and-defining-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have already used many functions. You can also define your own functions, and even build them into packages (more on this later) for other people to use. This is the main advantage of R. For example, let’s consider writing a function that returns the minimum and maximum of a vector. Suppose we already know the <code>min()</code> and <code>max()</code> functions.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>    myrange <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="co"># x is the argument that your function takes in</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>      <span class="fu">return</span>(<span class="fu">c</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x))) <span class="co"># return a vector that contains two elements</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">myrange</span>(x)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1]  1 10</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># R already has this function</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">range</span>(x)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1]  1 10</span></span></code></pre></div>
</div>
<div id="distribution-and-random-numbers" class="section level2 hasAnchor" number="1.7">
<h2 class="hasAnchor"><span class="header-section-number">1.7</span> Distribution and random numbers<a href="#distribution-and-random-numbers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Three distributions that are most frequently used in this course are Bernoulli, Gaussian (normal), and <span class="math inline">\(t\)</span> distributions. Bernoulli distributions can be used to describe binary variables, while Gaussian distribution is often used to describe continuous ones. The following code generates some random variables</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># read the documentation of rbinom() using ?rbinom</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> <span class="fu">rbinom</span>(<span class="dv">100</span>, <span class="dv">1</span>, <span class="fl">0.4</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">table</span>(x)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="do">## x</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="do">##  0  1 </span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 62 38</span></span></code></pre></div>
<p>However, this result cannot be replicated by others, since the next time we run this code, the random numbers will be different. Hence it is important to set and keep the random seed when a random algorithm is involved. The following code will always generate the same result</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> <span class="fu">rbinom</span>(<span class="dv">100</span>, <span class="dv">1</span>, <span class="fl">0.4</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">100</span>) <span class="co"># by default, this is mean 0 and variance 1</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">table</span>(x)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="do">## x</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  0  1 </span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 57 43</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">hist</span>(y)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-19-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">boxplot</span>(y <span class="sc">~</span> x)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-19-2.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="using-packages-and-other-resources" class="section level2 hasAnchor" number="1.8">
<h2 class="hasAnchor"><span class="header-section-number">1.8</span> Using packages and other resources<a href="#using-packages-and-other-resources" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Packages are written and contributed to R by individuals. They provide additional features (functions or data) that serve particular needs. For example, the <code>ggplot2</code> package is developed by the RStudio team that provides nice features to plot data. We will have more examples of this later on, but first, let’s install and load the package so that we can use these features. More details will be provided in the data visualization section.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to install a package</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">install.packages</span>(<span class="st">&quot;ggplot2&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to load the package</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(ggplot2)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: package &#39;ggplot2&#39; was built under R version 4.2.2</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use the ggplot() function to produce a plot</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sepal.Length is the horizontal axis</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sepal.Width is the vertical axis</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Species labels are used as color</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(iris, <span class="fu">aes</span>(Sepal.Length, Sepal.Width, <span class="at">colour =</span> Species)) <span class="sc">+</span> </span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_point</span>()  </span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-21-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>You may also noticed that in our previous examples, all tables only displayed the first several rows. One may be interested in looking at the entire dataset, however, it would take too much space to display the whole table. Here is a package that would allow you to display it in a compact window. It also provides searching and sorting tools. You can integrate this into your R Markdown reports.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(DT)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: package &#39;DT&#39; was built under R version 4.2.2</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">datatable</span>(iris, <span class="at">filter =</span> <span class="st">&quot;top&quot;</span>, <span class="at">rownames =</span> <span class="cn">FALSE</span>,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">5</span>))</span></code></pre></div>
<div id="htmlwidget-78481a280eb8253d8583" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-78481a280eb8253d8583">{"x":{"filter":"top","vertical":false,"filterHTML":"<tr>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"4.3\" data-max=\"7.9\" data-scale=\"1\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"2\" data-max=\"4.4\" data-scale=\"1\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"1\" data-max=\"6.9\" data-scale=\"1\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"0.1\" data-max=\"2.5\" data-scale=\"1\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"factor\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"width: 100%; display: none;\">\n      <select multiple=\"multiple\" style=\"width: 100%;\" data-options=\"[&quot;setosa&quot;,&quot;versicolor&quot;,&quot;virginica&quot;]\"><\/select>\n    <\/div>\n  <\/td>\n<\/tr>","data":[[5.1,4.9,4.7,4.6,5,5.4,4.6,5,4.4,4.9,5.4,4.8,4.8,4.3,5.8,5.7,5.4,5.1,5.7,5.1,5.4,5.1,4.6,5.1,4.8,5,5,5.2,5.2,4.7,4.8,5.4,5.2,5.5,4.9,5,5.5,4.9,4.4,5.1,5,4.5,4.4,5,5.1,4.8,5.1,4.6,5.3,5,7,6.4,6.9,5.5,6.5,5.7,6.3,4.9,6.6,5.2,5,5.9,6,6.1,5.6,6.7,5.6,5.8,6.2,5.6,5.9,6.1,6.3,6.1,6.4,6.6,6.8,6.7,6,5.7,5.5,5.5,5.8,6,5.4,6,6.7,6.3,5.6,5.5,5.5,6.1,5.8,5,5.6,5.7,5.7,6.2,5.1,5.7,6.3,5.8,7.1,6.3,6.5,7.6,4.9,7.3,6.7,7.2,6.5,6.4,6.8,5.7,5.8,6.4,6.5,7.7,7.7,6,6.9,5.6,7.7,6.3,6.7,7.2,6.2,6.1,6.4,7.2,7.4,7.9,6.4,6.3,6.1,7.7,6.3,6.4,6,6.9,6.7,6.9,5.8,6.8,6.7,6.7,6.3,6.5,6.2,5.9],[3.5,3,3.2,3.1,3.6,3.9,3.4,3.4,2.9,3.1,3.7,3.4,3,3,4,4.4,3.9,3.5,3.8,3.8,3.4,3.7,3.6,3.3,3.4,3,3.4,3.5,3.4,3.2,3.1,3.4,4.1,4.2,3.1,3.2,3.5,3.6,3,3.4,3.5,2.3,3.2,3.5,3.8,3,3.8,3.2,3.7,3.3,3.2,3.2,3.1,2.3,2.8,2.8,3.3,2.4,2.9,2.7,2,3,2.2,2.9,2.9,3.1,3,2.7,2.2,2.5,3.2,2.8,2.5,2.8,2.9,3,2.8,3,2.9,2.6,2.4,2.4,2.7,2.7,3,3.4,3.1,2.3,3,2.5,2.6,3,2.6,2.3,2.7,3,2.9,2.9,2.5,2.8,3.3,2.7,3,2.9,3,3,2.5,2.9,2.5,3.6,3.2,2.7,3,2.5,2.8,3.2,3,3.8,2.6,2.2,3.2,2.8,2.8,2.7,3.3,3.2,2.8,3,2.8,3,2.8,3.8,2.8,2.8,2.6,3,3.4,3.1,3,3.1,3.1,3.1,2.7,3.2,3.3,3,2.5,3,3.4,3],[1.4,1.4,1.3,1.5,1.4,1.7,1.4,1.5,1.4,1.5,1.5,1.6,1.4,1.1,1.2,1.5,1.3,1.4,1.7,1.5,1.7,1.5,1,1.7,1.9,1.6,1.6,1.5,1.4,1.6,1.6,1.5,1.5,1.4,1.5,1.2,1.3,1.4,1.3,1.5,1.3,1.3,1.3,1.6,1.9,1.4,1.6,1.4,1.5,1.4,4.7,4.5,4.9,4,4.6,4.5,4.7,3.3,4.6,3.9,3.5,4.2,4,4.7,3.6,4.4,4.5,4.1,4.5,3.9,4.8,4,4.9,4.7,4.3,4.4,4.8,5,4.5,3.5,3.8,3.7,3.9,5.1,4.5,4.5,4.7,4.4,4.1,4,4.4,4.6,4,3.3,4.2,4.2,4.2,4.3,3,4.1,6,5.1,5.9,5.6,5.8,6.6,4.5,6.3,5.8,6.1,5.1,5.3,5.5,5,5.1,5.3,5.5,6.7,6.9,5,5.7,4.9,6.7,4.9,5.7,6,4.8,4.9,5.6,5.8,6.1,6.4,5.6,5.1,5.6,6.1,5.6,5.5,4.8,5.4,5.6,5.1,5.1,5.9,5.7,5.2,5,5.2,5.4,5.1],[0.2,0.2,0.2,0.2,0.2,0.4,0.3,0.2,0.2,0.1,0.2,0.2,0.1,0.1,0.2,0.4,0.4,0.3,0.3,0.3,0.2,0.4,0.2,0.5,0.2,0.2,0.4,0.2,0.2,0.2,0.2,0.4,0.1,0.2,0.2,0.2,0.2,0.1,0.2,0.2,0.3,0.3,0.2,0.6,0.4,0.3,0.2,0.2,0.2,0.2,1.4,1.5,1.5,1.3,1.5,1.3,1.6,1,1.3,1.4,1,1.5,1,1.4,1.3,1.4,1.5,1,1.5,1.1,1.8,1.3,1.5,1.2,1.3,1.4,1.4,1.7,1.5,1,1.1,1,1.2,1.6,1.5,1.6,1.5,1.3,1.3,1.3,1.2,1.4,1.2,1,1.3,1.2,1.3,1.3,1.1,1.3,2.5,1.9,2.1,1.8,2.2,2.1,1.7,1.8,1.8,2.5,2,1.9,2.1,2,2.4,2.3,1.8,2.2,2.3,1.5,2.3,2,2,1.8,2.1,1.8,1.8,1.8,2.1,1.6,1.9,2,2.2,1.5,1.4,2.3,2.4,1.8,1.8,2.1,2.4,2.3,1.9,2.3,2.5,2.3,1.9,2,2.3,1.8],["setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","setosa","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","versicolor","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica","virginica"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>Sepal.Length<\/th>\n      <th>Sepal.Width<\/th>\n      <th>Petal.Length<\/th>\n      <th>Petal.Width<\/th>\n      <th>Species<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"columnDefs":[{"className":"dt-right","targets":[0,1,2,3]}],"order":[],"autoWidth":false,"orderClasses":false,"orderCellsTop":true,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
<p>Often times, you may want to perform a new task and you don’t know what function can be used to achieve that. Google Search or Stack Overflow are probably your best friends. I used to encounter this problem: I have a list of objects, and each of them is a vector. I then need to extract the first element of all these vectors. However, doing this using a for-loop can be slow, and I am also interested in a cleaner code. So I found <a href="https://stackoverflow.com/questions/44176908/r-list-get-first-item-of-each-element">this post</a>, which provided a simple answer:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create the list</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    a <span class="ot">=</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>), <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># extract the first element in each vector of the list</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sapply</span>(a, <span class="st">&quot;[[&quot;</span>, <span class="dv">1</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1 2 3</span></span></code></pre></div>
</div>
<div id="practice-questions" class="section level2 hasAnchor" number="1.9">
<h2 class="hasAnchor"><span class="header-section-number">1.9</span> Practice questions<a href="#practice-questions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Attach a new numerical column to the <code>iris</code> data, as the product of <code>Petal.Length</code> and <code>Petal.Width</code> and name the column as <code>Petal.Prod</code>.</li>
</ol>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>  iris <span class="ot">=</span> <span class="fu">cbind</span>(iris, <span class="st">&quot;Petal.Prod&quot;</span> <span class="ot">=</span> iris<span class="sc">$</span>Petal.Length<span class="sc">*</span>iris<span class="sc">$</span>Petal.Width)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>(iris)</span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Attach a new numerical column to the <code>iris</code> data, with value 1 if the observation is <code>setosa</code>, 2 for <code>versicolor</code> and 3 for <code>virginica</code>, and name the column as <code>Species.Num</code>.</li>
</ol>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>  iris <span class="ot">=</span> <span class="fu">cbind</span>(iris, <span class="st">&quot;Species.Num&quot;</span> <span class="ot">=</span> <span class="fu">as.numeric</span>(iris<span class="sc">$</span>Species))</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>(iris)</span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Change <code>Species.Num</code> to a factor variable such that it takes value “Type1” if the observation is <code>setosa</code> and “NA” otherwise.</li>
</ol>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>  iris<span class="sc">$</span>Species.Num <span class="ot">=</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(iris<span class="sc">$</span>Species <span class="sc">==</span> <span class="st">&quot;setosa&quot;</span>, <span class="st">&quot;Type1&quot;</span>, <span class="st">&quot;NA&quot;</span>))</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>(iris)</span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>Define a function that takes in a numerical vector, and output the mean of that vector. Do this without using the <code>mean()</code> and <code>sum()</code> function.</li>
</ol>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>  mymean <span class="ot">&lt;-</span> <span class="cf">function</span>(x)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    sum <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(x))</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>      sum <span class="ot">=</span> sum <span class="sc">+</span> x[i]</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="at">sum =</span> sum <span class="sc">/</span> <span class="fu">length</span>(x))</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mymean</span>(x)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(x)</span></code></pre></div>
<!--chapter:end:01.1-r-basics.Rmd-->
</div>
</div>
<div id="rmarkdown" class="section level1 hasAnchor" number="2">
<h1 class="hasAnchor"><span class="header-section-number">2</span> RMarkdown<a href="#rmarkdown" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="basics-and-resources" class="section level2 hasAnchor" number="2.1">
<h2 class="hasAnchor"><span class="header-section-number">2.1</span> Basics and Resources<a href="#basics-and-resources" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>R Markdown is a built-in feature of RStudio. It integrates plain text with chunks of R code in to a single file, which is extremely useful when constructing class notes or building a website. A <code>.rmd</code> file can be compiled into nice-looking <code>.html</code>, <code>.pdf</code>, and <code>.docx</code> file. For example, this entire guide is created using R Markdown. With RStudio, you can install R Markdown from R console using the following code. Note that this should be automatically done the first time you create and compile a <code>.rmd</code> file in RStudio.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Install R Markdown from CRAN</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">install.packages</span>(<span class="st">&quot;rmarkdown&quot;</span>)</span></code></pre></div>
<p>Again there are many online guides for R Markdown, and these may not be the best ones.</p>
<ul>
<li><a href="https://bookdown.org/yihui/rmarkdown/" target="_blank">R Markdown: The Definitive Guide</a></li>
<li><a href="https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf" target="_blank">R Markdown Cheat Sheet</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLBgxzZMu3GpNgd07DwmS-2odHtMO6MWGH" target="_blank">R Markdown Play-list (video)</a></li>
</ul>
<p>To get started, create an R Markdown template file by clicking <code>File</code> -&gt; <code>New File</code> -&gt; <code>R Markdown...</code></p>
<center>
<img src="images/Create.png" style="width:70.0%" />
</center>
<p>You can then <code>Knit</code> the template file and start to explore its features.</p>
<center>
<img src="images/knit.png" style="width:70.0%" />
</center>
<p>Please note that this guide is provided in the <code>.html</code> format. However, your homework report should be in <code>.pdf</code> format. This can be done by selecting the <code>Knit to PDF</code> option from the Knit button. Again there are many online guides, and these may not be the best ones.</p>
<ul>
<li><a href="https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf">R Markdown Cheat Sheet</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLBgxzZMu3GpNgd07DwmS-2odHtMO6MWGH">R Markdown Play-list (video)</a></li>
</ul>
</div>
<div id="formatting-text" class="section level2 hasAnchor" number="2.2">
<h2 class="hasAnchor"><span class="header-section-number">2.2</span> Formatting Text<a href="#formatting-text" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Formatting text is easy. Bold can be done using <code>**</code> or <code>__</code> before and after the text. Italics can be done using <code>*</code> or <code>_</code> before and after the text. For example, <strong>This is bold.</strong> <em>This is italics.</em> and <strong><em>this is bold italics</em></strong>. <code>This text appears as monospaced.</code></p>
<ul>
<li>Unordered list element 1.</li>
<li>Unordered list element 2.</li>
<li>Unordered list element 3.</li>
</ul>
<ol style="list-style-type: decimal">
<li>Ordered list element 1.</li>
<li>Ordered list element 2.</li>
<li>Ordered list element 3.</li>
</ol>
<p>We could mix lists and links. Note that a link can be constructed in the format <code>[display text](http link)</code>. If colors are desired, we can customize it using, for example, <code>[\textcolor{blue}{display text}](http link)</code>. But this only works in <code>.pdf</code> format. For <code>.html</code>, use <code>&lt;span style="color: red;"&gt;text&lt;/span&gt;</code>.</p>
<ul>
<li>A default link: <a href="http://rmarkdown.rstudio.com/" target="_blank">RMarkdown Documentation</a></li>
<li>colored link 1: <a href="https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf" target="_blank"></a> (Not shown because it only works in PDF)</li>
<li>colored link 2: <a href="http://www.tablesgenerator.com/markdown_tables" target="_blank"><span style="color: red;">Table Generator</span></a> (only works in HTML)</li>
</ul>
<p>Tables are sometimes tricky using Markdown. See the above link for a helpful Markdown table generator. Note that you can also adjust the alignment by using a <code>:</code> sign.</p>
<table>
<thead>
<tr class="header">
<th align="center">A</th>
<th align="left">B</th>
<th align="left">C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="left">2</td>
<td align="left">3</td>
</tr>
<tr class="even">
<td align="center">Middle</td>
<td align="left">Left</td>
<td align="left">Right</td>
</tr>
</tbody>
</table>
</div>
<div id="adding-r-code" class="section level2 hasAnchor" number="2.3">
<h2 class="hasAnchor"><span class="header-section-number">2.3</span> Adding <code>R</code> Code<a href="#adding-r-code" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far we have only used Markdown to create the text part. This is useful by itself, but the real power of RMarkdown comes when we add <code>R</code>. There are two ways we can do this. We can use <code>R</code> code chunks, or run <code>R</code> inline.</p>
<div id="r-chunks" class="section level3 hasAnchor" number="2.3.1">
<h3 class="hasAnchor"><span class="header-section-number">2.3.1</span> <code>R</code> Chunks<a href="#r-chunks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following is an example of an <code>R</code> code chunk. Start the chunk with <code>```{r}</code> and end with <code>```</code>:</p>
<p><code>```{r}</code></p>
<p><span class="math inline">\(\quad\)</span> <code>set.seed(123)</code></p>
<p><span class="math inline">\(\quad\)</span> <code>rnorm(5)</code></p>
<p><code>```</code></p>
<p>This generates five random observations from the standard normal distribution. We also set the seed so that the results can be later on replicated. The result looks like the following</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rnorm</span>(<span class="dv">5</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774</span></span></code></pre></div>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define function</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    get_sd <span class="ot">=</span> <span class="cf">function</span>(x, <span class="at">biased =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>      n <span class="ot">=</span> <span class="fu">length</span>(x) <span class="sc">-</span> <span class="dv">1</span> <span class="sc">*</span> <span class="sc">!</span>biased</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>      <span class="fu">sqrt</span>((<span class="dv">1</span> <span class="sc">/</span> n) <span class="sc">*</span> <span class="fu">sum</span>((x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate random sample data</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    (<span class="at">test_sample =</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">10</span>, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">5</span>))</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1]  8.8547922 -0.8234909  3.8156421  5.1643130  4.0213416  1.4693774  9.5576100  1.5267048</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="do">##  [9] 12.0921186  1.6864295</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run function on generated data</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">get_sd</span>(test_sample)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 4.177244</span></span></code></pre></div>
<p>There is a lot going on here. In the <code>.Rmd</code> file, notice the syntax that creates and ends the chunk. Also note that <code>example_chunk</code> is the chunk name. Everything between the start and end syntax must be valid <code>R</code> code. Chunk names are not necessary, but can become useful as your documents grow in size.</p>
<p>In this example, we define a function, generate some random data in a reproducible manner, displayed the data, then ran our function.</p>
</div>
<div id="inline-r" class="section level3 hasAnchor" number="2.3.2">
<h3 class="hasAnchor"><span class="header-section-number">2.3.2</span> Inline <code>R</code><a href="#inline-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><code>R</code> can also be run in the middle of the exposition. For example, the mean of the data we generated is 4.7364838.</p>
</div>
</div>
<div id="importing-data" class="section level2 hasAnchor" number="2.4">
<h2 class="hasAnchor"><span class="header-section-number">2.4</span> Importing Data<a href="#importing-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When using RMarkdown, any time you <em>knit</em> your document to its final form, say <code>.html</code>, a number of programs run in the background. Your current <code>R</code> environment seen in RStudio will be reset. Any objects you created while working interactively inside RStudio will be ignored. Essentially a new <code>R</code> session will be spawned in the background and the code in your document is run there from start to finish. For this reason, things such as importing data must be explicitly coded into your document.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(readr)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    example_data <span class="ot">=</span> <span class="fu">read_table</span>(<span class="st">&quot;data/skincancer.txt&quot;</span>)</span></code></pre></div>
<p>The above loads the online file. In many cases, you will load a file that is locally stored in your own computer. In that case, you can either specify the full file path, or simply use, for example <code>read_csv("filename.csv")</code> if that file is stored at your <strong>working directory</strong>. The <strong>working directory</strong> will usually be the directory that contains your <code>.Rmd</code> file. You are recommended to reference data in this manner. Note that we use the newer <code>read_csv()</code> from the <code>readr</code> package instead of the default <code>read.csv()</code>.</p>
</div>
<div id="working-directory" class="section level2 hasAnchor" number="2.5">
<h2 class="hasAnchor"><span class="header-section-number">2.5</span> Working Directory<a href="#working-directory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Whenever <code>R</code> code is run, there is always a current working directory. This allows for relative references to external files, in addition to absolute references. Since the working directory when knitting a file is always the directory that contains the <code>.Rmd</code> file, it can be helpful to set the working directory inside RStudio to match while working interactively.</p>
<p>To do so, select <code>Session &gt; Set Working Directory &gt; To Source File Location</code> while editing a <code>.Rmd</code> file. This will set the working directory to the path that contains the <code>.Rmd</code>. You can also use <code>getwd()</code> and <code>setwd()</code> to manipulate your working directory programmatically. These should only be used interactively. Using them inside an RMarkdown document would likely result in lessened reproducibility.</p>
<p><strong>As of recent RStudio updates, this practice is not always necessary when working interactively.</strong> If lines of code are being “Output Inline,” then the working directory is automatically the directory which contains the <code>.Rmd</code> file.</p>
</div>
<div id="plotting" class="section level2 hasAnchor" number="2.6">
<h2 class="hasAnchor"><span class="header-section-number">2.6</span> Plotting<a href="#plotting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The following generates a simple plot, which displays the skin cancer mortality. By default, the figure is aligned on the left, with size 3 by 5 inches.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(Mort <span class="sc">~</span> Lat, <span class="at">data =</span> example_data)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-34-1.png" width="50%" style="display: block; margin: auto auto auto 0;" /></p>
<p>In our R introduction, we used <code>ggplot2</code> to create a more interesting plot. You may also polish a plot with basic functions. Notice it is <em>huge</em> in the resulting document, since we have modified some <em>chunk options</em> (<code>fig.height = 6, fig.width = 8</code>) in the RMarkdown file to manipulate its size.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(Mort <span class="sc">~</span> Lat, <span class="at">data =</span> example_data,</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;Latitude&quot;</span>,</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">ylab =</span> <span class="st">&quot;Skin Cancer Mortality Rate&quot;</span>,</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">main =</span> <span class="st">&quot;Skin Cancer Mortality vs. State Latitude&quot;</span>,</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">pch  =</span> <span class="dv">19</span>,</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex  =</span> <span class="fl">1.5</span>,</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">col  =</span> <span class="st">&quot;deepskyblue&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-35-1.png" width="50%" style="display: block; margin: auto auto auto 0;" /></p>
<p>But you can also notice that the labels and the plots becomes disproportional when the figure size is set too small. This can be resolved using a scaling option such as <code>out.width = '60%</code>, but enlarge the original figure size. We also align the figure at the center using <code>fig.align = 'center'</code></p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-36-1.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="chunk-options" class="section level2 hasAnchor" number="2.7">
<h2 class="hasAnchor"><span class="header-section-number">2.7</span> Chunk Options<a href="#chunk-options" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have already seen chunk options <code>fig.height</code>, <code>fig.width</code>, and <code>out.width</code> which modified the size of plots from a particular chunk. There are many <a href="http://yihui.name/knitr/options/">chunk options</a>, but we will discuss some others which are frequently used including; <code>eval</code>, <code>echo</code>, <code>message</code>, and <code>warning</code>. If you noticed, the plot above was displayed without showing the code.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">install.packages</span>(<span class="st">&quot;rmarkdown&quot;</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    ?log</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">View</span>(mpg)</span></code></pre></div>
<p>Using <code>eval = FALSE</code> the above chunk displays the code, but it is not run. We’ve already discussed not wanting install code to run. The <code>?</code> code pulls up documentation of a function. This will spawn a browser window when knitting, or potentially crash during knitting. Similarly, using <code>View()</code> is an issue with RMarkdown. Inside RStudio, this would pull up a window which displays the data. However, when knitting, <code>R</code> runs in the background and RStudio is not modifying the <code>View()</code> function. This, on OSX especially, usually causes knitting to fail.</p>
<pre><code>## [1] &quot;Hello World!&quot;</code></pre>
<p>Above, we see output, but no code! This is done using <code>echo = FALSE</code>, which is often useful.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summary</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x))</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = y ~ x)</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="do">##        Min         1Q     Median         3Q        Max </span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="do">## -5.661e-16 -1.157e-16  4.273e-17  2.153e-16  4.167e-16 </span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error   t value Pr(&gt;|t|)    </span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 1.123e-15  2.458e-16 4.571e+00  0.00182 ** </span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="do">## x           1.000e+00  3.961e-17 2.525e+16  &lt; 2e-16 ***</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 3.598e-16 on 8 degrees of freedom</span></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:      1,  Adjusted R-squared:      1 </span></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 6.374e+32 on 1 and 8 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>The above code produces a warning, for reasons we will discuss later. Sometimes, in final reports, it is nice to hide these, which we have done here. <code>message = FALSE</code> and <code>warning = FALSE</code> can be used to do so. Messages are often created when loading packages to give the user information about the effects of loading the package. These should be suppressed in final reports. Be careful about suppressing these messages and warnings too early in an analysis as you could potentially miss important information!</p>
</div>
<div id="adding-math-with-latex" class="section level2 hasAnchor" number="2.8">
<h2 class="hasAnchor"><span class="header-section-number">2.8</span> Adding Math with LaTeX<a href="#adding-math-with-latex" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Another benefit of RMarkdown is the ability to add <a href="https://www.latex-project.org/about/">Latex for mathematics typesetting</a>. Like <code>R</code> code, there are two ways we can include Latex; displaystyle and inline.</p>
<p>Note that use of LaTeX is somewhat dependent on the resulting file format. For example, it cannot be used at all with <code>.docx</code>. To use it with <code>.pdf</code> you must have LaTeX installed on your machine.</p>
<p>With <code>.html</code> the LaTeX is not actually rendered during knitting, but actually rendered in your browser using MathJax.</p>
<div id="displaystyle-latex" class="section level3 hasAnchor" number="2.8.1">
<h3 class="hasAnchor"><span class="header-section-number">2.8.1</span> Displaystyle LaTeX<a href="#displaystyle-latex" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Displaystyle is used for larger equations which appear centered on their own line. This is done by putting <code>$$</code> before and after the mathematical equation.</p>
<p><span class="math display">\[
\widehat \sigma = \sqrt{\frac{1}{n - 1}\sum_{i=1}^{n}(x_i - \bar{x})^2}
\]</span></p>
</div>
<div id="inline-latex" class="section level3 hasAnchor" number="2.8.2">
<h3 class="hasAnchor"><span class="header-section-number">2.8.2</span> Inline LaTex<a href="#inline-latex" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We could mix LaTeX commands in the middle of exposition, for example: <span class="math inline">\(t = 2\)</span>. We could actually mix <code>R</code> with Latex as well! For example: <span class="math inline">\(\bar{x} = 4.7364838\)</span>.</p>
</div>
</div>
<div id="output-options" class="section level2 hasAnchor" number="2.9">
<h2 class="hasAnchor"><span class="header-section-number">2.9</span> Output Options<a href="#output-options" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>At the beginning of the document, there is a code which describes some metadata and settings of the document. The default code looks like</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>    title<span class="sc">:</span> <span class="st">&quot;R Notebook&quot;</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    output<span class="sc">:</span> html_notebook</span></code></pre></div>
<p>You can easily add your name and date to it, and add a Table of Contents, using <code>toc: yes</code>. Note that the following code would specify the theme of an <code>html</code> file.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>    title<span class="sc">:</span> <span class="st">&quot;My RMarkdown Template&quot;</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    author<span class="sc">:</span> <span class="st">&quot;Your Name&quot;</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    date<span class="sc">:</span> <span class="st">&quot;Aug 26, 2021&quot;</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    output<span class="sc">:</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>      html_document<span class="sc">:</span> </span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        toc<span class="sc">:</span> yes</span></code></pre></div>
<p>You can edit this yourself, or click the settings button at the top of the document and select <code>Output Options...</code>. Here you can explore other themes and syntax highlighting options, as well as many additional options. Using this method will automatically modify this information in the document.</p>
</div>
<div id="try-it" class="section level2 hasAnchor" number="2.10">
<h2 class="hasAnchor"><span class="header-section-number">2.10</span> Try It!<a href="#try-it" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Be sure to play with this document! Change it. Break it. Fix it. The best way to learn RMarkdown (or really almost anything) is to try, fail, then find out what you did wrong.</p>
<p>RStudio has provided a number of <a href="http://rmarkdown.rstudio.com/lesson-1.html">beginner tutorials</a> which have been greatly improved recently and detail many of the specifics potentially not covered in this document. RMarkdown is continually improving, and this document covers only the very basics.</p>
<!--chapter:end:01.2-rmd-basics.Rmd-->
</div>
</div>
<div id="linear-algebra-basics" class="section level1 hasAnchor" number="3">
<h1 class="hasAnchor"><span class="header-section-number">3</span> Linear Algebra Basics<a href="#linear-algebra-basics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>You should already be familiar with some basic linear algebra concepts such as matrix and vector multiplications. Here we review some basic concepts and properties that will be used in this course. For the most part, they are used in deriving linear regression results.</p>
<div id="definition" class="section level2 hasAnchor" number="3.1">
<h2 class="hasAnchor"><span class="header-section-number">3.1</span> Definition<a href="#definition" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We usually use <span class="math inline">\(\mathbf{X}\)</span> to denote an <span class="math inline">\(n \times p\)</span> dimensional design matrix, where <span class="math inline">\(n\)</span> is the number of observations and <span class="math inline">\(p\)</span> is the number of variables. The columns of <span class="math inline">\(\mathbf{x}\)</span> are denoted as <span class="math inline">\(\mathbf{x}_1, \ldots, \mathbf{x}_p\)</span>:</p>
<p><span class="math display">\[
\mathbf{X}= \begin{pmatrix}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{m1} &amp; x_{n2} &amp; \cdots &amp; x_{np}\\
\end{pmatrix} =
\begin{pmatrix}
\mathbf{x}_1 &amp; \mathbf{x}_2 &amp; \cdots &amp; \mathbf{x}_p\\
\end{pmatrix}
\]</span>
The <strong>column space</strong>, <span class="math inline">\(\cal{C}(\mathbf{X})\)</span> of <span class="math inline">\(\mathbf{X}\)</span> is the set of all linear combinations of <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_p\)</span>, i.e.,</p>
<p><span class="math display">\[c_1 \mathbf{x}_1 + c_2 \mathbf{x}_2 + \cdots c_p \mathbf{x}_p.\]</span>
This is also called the <strong>span</strong> of these vectors, <span class="math inline">\(\text{span}(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_p)\)</span>. Its orthogonal space is</p>
<p><span class="math display">\[\{\mathbf{v}: \mathbf{X}^\text{T}\mathbf{v}= 0\}.\]</span>
A projection matrix <span class="math inline">\(\bP\)</span> is a matrix such that</p>
<!--chapter:end:01.3-linearalgebra.Rmd-->
</div>
</div>
<div id="optimization-basics" class="section level1 hasAnchor" number="4">
<h1 class="hasAnchor"><span class="header-section-number">4</span> Optimization Basics<a href="#optimization-basics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Optimization is heavily involved in statistics and machine learning. Almost all methods introduced in this book can be viewed as some form of optimization. It would be good to have some prior knowledge of it so that later chapters can use these concepts without difficulties. Especially, one should be familiar with concepts such as constrains, gradient methods, and be able to implement them using existing R functions. Since optimization is such a broad topic, we refer readers to <span class="citation">Boyd and Vandenberghe (<a href="#ref-boyd2004convex" role="doc-biblioref">2004</a>)</span> and <span class="citation">Nocedal and Wright (<a href="#ref-nocedal2006numerical" role="doc-biblioref">2006</a>)</span> for more further reading.</p>
<p>We will use a slightly different set of notations in this Chapter so that we are consistent with the literature. This means that for the most part, we will use <span class="math inline">\(x\)</span> as our parameter of interest and optimize a function <span class="math inline">\(f(x)\)</span>. This is in contrast to optimizing <span class="math inline">\(\theta\)</span> in a statistical model <span class="math inline">\(f_\theta(x)\)</span> where <span class="math inline">\(x\)</span> is the observed data. However, in the example of linear regression, we may again switch back to the regular notation of <span class="math inline">\(x^\text{T} \boldsymbol \beta\)</span>. These transitions will only happen under clear context and should not create ambiguity.</p>
<div id="basic-concept" class="section level2 hasAnchor" number="4.1">
<h2 class="hasAnchor"><span class="header-section-number">4.1</span> Basic Concept<a href="#basic-concept" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We usually consider a convex optimization problem (non-convex problems are a bit too involving although we will also see some examples of that), meaning that we optimize (minimize) a convex function in a convex domain. A <strong><em>convex function</em></strong> <span class="math inline">\(f(\mathbf{x})\)</span> maps some subset <span class="math inline">\(C \in \mathbb{R}^p\)</span> into <span class="math inline">\(\mathbb{R}^p\)</span>, but enjoys the property that</p>
<p><span class="math display">\[ f(t \mathbf{x}_1 + (1 - t) \mathbf{x}_2) \leq t f(\mathbf{x}_1) + ( 1- t) f(\mathbf{x}_2), \]</span>
for all <span class="math inline">\(t \in [0, 1]\)</span> and any two points <span class="math inline">\(\mathbf{x}_1\)</span>, <span class="math inline">\(\mathbf{x}_2\)</span> in the domain of <span class="math inline">\(f\)</span>.</p>
<center>
<div class="figure">
<img src="images/ConvexFunction.png" style="width:55.0%" alt="" />
<p class="caption">An example of convex function, <a href="https://en.wikipedia.org/wiki/Convex_function">from wikipedia</a></p>
</div>
</center>
<p>Note that if you have a concave function (the bowl faces downwards) then <span class="math inline">\(-f(\mathbf{x})\)</span> would be convex. Examples of convex functions:</p>
<ul>
<li>Univariate functions: <span class="math inline">\(x^2\)</span>, <span class="math inline">\(\exp(x)\)</span>, <span class="math inline">\(-log(x)\)</span></li>
<li>Affine map: <span class="math inline">\(a^\text{T}\mathbf{x}+ b\)</span> is both convex and concave</li>
<li>A quadratic function <span class="math inline">\(\frac{1}{2}\mathbf{x}^\text{T}\mathbf{A}\mathbf{x}+ b^\text{T}\mathbf{x}+ c\)</span>, if <span class="math inline">\(\mathbf{A}\)</span> is positive semidefinite</li>
<li>All <span class="math inline">\(p\)</span> norms are convex, following the Triangle inequality and properties of a norm.</li>
<li>A sin function is neither convex or concave</li>
</ul>
<p>On the other hand, a <strong><em>convex set</em></strong> <span class="math inline">\(C\)</span> means that if we have two points <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> in <span class="math inline">\(C\)</span>, the line segment joining these two points has to lie within <span class="math inline">\(C\)</span>, i.e.,</p>
<p><span class="math display">\[\mathbf{x}_1, \mathbf{x}_2 \in C \quad \Longrightarrow \quad t \mathbf{x}_1 + (1 - t) \mathbf{x}_2 \in C,\]</span>
for all <span class="math inline">\(t \in [0, 1]\)</span>.</p>
<center>
<div class="figure">
<img src="images/ConvexSet.png" style="width:55.0%" alt="" />
<p class="caption">An example of convex set</p>
</div>
</center>
<p>Examples of convex set include</p>
<ul>
<li>Real line: <span class="math inline">\(\mathbb{R}\)</span></li>
<li>Norm ball: <span class="math inline">\(\{ \mathbf{x}: \lVert \mathbf{x}\rVert \leq r \}\)</span></li>
<li>Hyperplane: <span class="math inline">\(\{ \mathbf{x}: a^\text{T}\mathbf{x}= b \}\)</span></li>
</ul>
<p>Consider a simple optimization problem:</p>
<p><span class="math display">\[ \text{minimize} \quad f(x_1, x_2) = x_1^2 + x_2^2\]</span></p>
<p>Clearly, <span class="math inline">\(f(x_1, x_2)\)</span> is a convex function, and we know that the solution of this problem is <span class="math inline">\(x_1 = x_2 = 0\)</span>. However, the problem might be a bit more complicated if we restrict that in a certain (convex) region, for example,</p>
<p><span class="math display">\[\begin{align}
&amp;\underset{x_1, x_2}{\text{minimize}} &amp; \quad f(x_1, x_2) &amp;= x_1^2 + x_2^2 \\
&amp;\text{subject to} &amp; x_1 + x_2 &amp;\leq -1 \\
&amp; &amp; x_1 + x_2 &amp;&gt; -2
\end{align}\]</span></p>
<p>Here the convex set <span class="math inline">\(C = \{x_1, x_2 \in \mathbb{R}: x_1 + x_2 \leq -1 \,\, \text{and} \,\, x_1 + x_2 &gt; -2\}\)</span>. And our problem looks like the following, which attains it minimum at <span class="math inline">\((-0.5, -0.5)\)</span>.</p>
<pre><code>## Warning: package &#39;plotly&#39; was built under R version 4.2.2</code></pre>
<div id="htmlwidget-227c624c68b846981eed" style="width:75%;height:576px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-227c624c68b846981eed">{"x":{"visdat":{"46f4185f1684":["function () ","plotlyVisDat"],"46f470c65cd6":["function () ","data"]},"cur_data":"46f470c65cd6","attrs":{"46f4185f1684":{"x":[-1.5,-1.49,-1.48,-1.47,-1.46,-1.45,-1.44,-1.43,-1.42,-1.41,-1.4,-1.39,-1.38,-1.37,-1.36,-1.35,-1.34,-1.33,-1.32,-1.31,-1.3,-1.29,-1.28,-1.27,-1.26,-1.25,-1.24,-1.23,-1.22,-1.21,-1.2,-1.19,-1.18,-1.17,-1.16,-1.15,-1.14,-1.13,-1.12,-1.11,-1.1,-1.09,-1.08,-1.07,-1.06,-1.05,-1.04,-1.03,-1.02,-1.01,-1,-0.99,-0.98,-0.97,-0.96,-0.95,-0.94,-0.93,-0.92,-0.91,-0.9,-0.89,-0.88,-0.87,-0.86,-0.85,-0.84,-0.83,-0.82,-0.81,-0.8,-0.79,-0.78,-0.77,-0.76,-0.75,-0.74,-0.73,-0.72,-0.71,-0.7,-0.69,-0.68,-0.67,-0.66,-0.65,-0.64,-0.63,-0.62,-0.61,-0.6,-0.59,-0.58,-0.57,-0.56,-0.55,-0.54,-0.53,-0.52,-0.51,-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.0999999999999999,-0.0900000000000001,-0.0800000000000001,-0.0700000000000001,-0.0600000000000001,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.0800000000000001,0.0900000000000001,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1],"y":[-1.5,-1.49,-1.48,-1.47,-1.46,-1.45,-1.44,-1.43,-1.42,-1.41,-1.4,-1.39,-1.38,-1.37,-1.36,-1.35,-1.34,-1.33,-1.32,-1.31,-1.3,-1.29,-1.28,-1.27,-1.26,-1.25,-1.24,-1.23,-1.22,-1.21,-1.2,-1.19,-1.18,-1.17,-1.16,-1.15,-1.14,-1.13,-1.12,-1.11,-1.1,-1.09,-1.08,-1.07,-1.06,-1.05,-1.04,-1.03,-1.02,-1.01,-1,-0.99,-0.98,-0.97,-0.96,-0.95,-0.94,-0.93,-0.92,-0.91,-0.9,-0.89,-0.88,-0.87,-0.86,-0.85,-0.84,-0.83,-0.82,-0.81,-0.8,-0.79,-0.78,-0.77,-0.76,-0.75,-0.74,-0.73,-0.72,-0.71,-0.7,-0.69,-0.68,-0.67,-0.66,-0.65,-0.64,-0.63,-0.62,-0.61,-0.6,-0.59,-0.58,-0.57,-0.56,-0.55,-0.54,-0.53,-0.52,-0.51,-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.0999999999999999,-0.0900000000000001,-0.0800000000000001,-0.0700000000000001,-0.0600000000000001,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.0800000000000001,0.0900000000000001,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.4901,2.4804,2.4709,2.4616,2.4525,2.4436,2.4349,2.4264,2.4181,2.41,2.4021,2.3944,2.3869,2.3796,2.3725,2.3656,2.3589,2.3524,2.3461,2.34,2.3341,2.3284,2.3229,2.3176,2.3125,2.3076,2.3029,2.2984,2.2941,2.29,2.2861,2.2824,2.2789,2.2756,2.2725,2.2696,2.2669,2.2644,2.2621,2.26,2.2581,2.2564,2.2549,2.2536,2.2525,2.2516,2.2509,2.2504,2.2501,2.25,2.2501,2.2504,2.2509,2.2516,2.2525,2.2536,2.2549,2.2564,2.2581,2.26,2.2621,2.2644,2.2669,2.2696,2.2725,2.2756,2.2789,2.2824,2.2861,2.29,2.2941,2.2984,2.3029,2.3076,2.3125,2.3176,2.3229,2.3284,2.3341,2.34,2.3461,2.3524,2.3589,2.3656,2.3725,2.3796,2.3869,2.3944,2.4021,2.41,2.4181,2.4264,2.4349,2.4436,2.4525,2.4616,2.4709,2.4804,2.4901,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.4701,2.4602,2.4505,2.441,2.4317,2.4226,2.4137,2.405,2.3965,2.3882,2.3801,2.3722,2.3645,2.357,2.3497,2.3426,2.3357,2.329,2.3225,2.3162,2.3101,2.3042,2.2985,2.293,2.2877,2.2826,2.2777,2.273,2.2685,2.2642,2.2601,2.2562,2.2525,2.249,2.2457,2.2426,2.2397,2.237,2.2345,2.2322,2.2301,2.2282,2.2265,2.225,2.2237,2.2226,2.2217,2.221,2.2205,2.2202,2.2201,2.2202,2.2205,2.221,2.2217,2.2226,2.2237,2.225,2.2265,2.2282,2.2301,2.2322,2.2345,2.237,2.2397,2.2426,2.2457,2.249,2.2525,2.2562,2.2601,2.2642,2.2685,2.273,2.2777,2.2826,2.2877,2.293,2.2985,2.3042,2.3101,2.3162,2.3225,2.329,2.3357,2.3426,2.3497,2.357,2.3645,2.3722,2.3801,2.3882,2.3965,2.405,2.4137,2.4226,2.4317,2.441,2.4505,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.4505,2.4404,2.4305,2.4208,2.4113,2.402,2.3929,2.384,2.3753,2.3668,2.3585,2.3504,2.3425,2.3348,2.3273,2.32,2.3129,2.306,2.2993,2.2928,2.2865,2.2804,2.2745,2.2688,2.2633,2.258,2.2529,2.248,2.2433,2.2388,2.2345,2.2304,2.2265,2.2228,2.2193,2.216,2.2129,2.21,2.2073,2.2048,2.2025,2.2004,2.1985,2.1968,2.1953,2.194,2.1929,2.192,2.1913,2.1908,2.1905,2.1904,2.1905,2.1908,2.1913,2.192,2.1929,2.194,2.1953,2.1968,2.1985,2.2004,2.2025,2.2048,2.2073,2.21,2.2129,2.216,2.2193,2.2228,2.2265,2.2304,2.2345,2.2388,2.2433,2.248,2.2529,2.258,2.2633,2.2688,2.2745,2.2804,2.2865,2.2928,2.2993,2.306,2.3129,2.32,2.3273,2.3348,2.3425,2.3504,2.3585,2.3668,2.3753,2.384,2.3929,2.402,2.4113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.4313,2.421,2.4109,2.401,2.3913,2.3818,2.3725,2.3634,2.3545,2.3458,2.3373,2.329,2.3209,2.313,2.3053,2.2978,2.2905,2.2834,2.2765,2.2698,2.2633,2.257,2.2509,2.245,2.2393,2.2338,2.2285,2.2234,2.2185,2.2138,2.2093,2.205,2.2009,2.197,2.1933,2.1898,2.1865,2.1834,2.1805,2.1778,2.1753,2.173,2.1709,2.169,2.1673,2.1658,2.1645,2.1634,2.1625,2.1618,2.1613,2.161,2.1609,2.161,2.1613,2.1618,2.1625,2.1634,2.1645,2.1658,2.1673,2.169,2.1709,2.173,2.1753,2.1778,2.1805,2.1834,2.1865,2.1898,2.1933,2.197,2.2009,2.205,2.2093,2.2138,2.2185,2.2234,2.2285,2.2338,2.2393,2.245,2.2509,2.257,2.2633,2.2698,2.2765,2.2834,2.2905,2.2978,2.3053,2.313,2.3209,2.329,2.3373,2.3458,2.3545,2.3634,2.3725,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.4125,2.402,2.3917,2.3816,2.3717,2.362,2.3525,2.3432,2.3341,2.3252,2.3165,2.308,2.2997,2.2916,2.2837,2.276,2.2685,2.2612,2.2541,2.2472,2.2405,2.234,2.2277,2.2216,2.2157,2.21,2.2045,2.1992,2.1941,2.1892,2.1845,2.18,2.1757,2.1716,2.1677,2.164,2.1605,2.1572,2.1541,2.1512,2.1485,2.146,2.1437,2.1416,2.1397,2.138,2.1365,2.1352,2.1341,2.1332,2.1325,2.132,2.1317,2.1316,2.1317,2.132,2.1325,2.1332,2.1341,2.1352,2.1365,2.138,2.1397,2.1416,2.1437,2.146,2.1485,2.1512,2.1541,2.1572,2.1605,2.164,2.1677,2.1716,2.1757,2.18,2.1845,2.1892,2.1941,2.1992,2.2045,2.21,2.2157,2.2216,2.2277,2.234,2.2405,2.2472,2.2541,2.2612,2.2685,2.276,2.2837,2.2916,2.2997,2.308,2.3165,2.3252,2.3341,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.3941,2.3834,2.3729,2.3626,2.3525,2.3426,2.3329,2.3234,2.3141,2.305,2.2961,2.2874,2.2789,2.2706,2.2625,2.2546,2.2469,2.2394,2.2321,2.225,2.2181,2.2114,2.2049,2.1986,2.1925,2.1866,2.1809,2.1754,2.1701,2.165,2.1601,2.1554,2.1509,2.1466,2.1425,2.1386,2.1349,2.1314,2.1281,2.125,2.1221,2.1194,2.1169,2.1146,2.1125,2.1106,2.1089,2.1074,2.1061,2.105,2.1041,2.1034,2.1029,2.1026,2.1025,2.1026,2.1029,2.1034,2.1041,2.105,2.1061,2.1074,2.1089,2.1106,2.1125,2.1146,2.1169,2.1194,2.1221,2.125,2.1281,2.1314,2.1349,2.1386,2.1425,2.1466,2.1509,2.1554,2.1601,2.165,2.1701,2.1754,2.1809,2.1866,2.1925,2.1986,2.2049,2.2114,2.2181,2.225,2.2321,2.2394,2.2469,2.2546,2.2625,2.2706,2.2789,2.2874,2.2961,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.3761,2.3652,2.3545,2.344,2.3337,2.3236,2.3137,2.304,2.2945,2.2852,2.2761,2.2672,2.2585,2.25,2.2417,2.2336,2.2257,2.218,2.2105,2.2032,2.1961,2.1892,2.1825,2.176,2.1697,2.1636,2.1577,2.152,2.1465,2.1412,2.1361,2.1312,2.1265,2.122,2.1177,2.1136,2.1097,2.106,2.1025,2.0992,2.0961,2.0932,2.0905,2.088,2.0857,2.0836,2.0817,2.08,2.0785,2.0772,2.0761,2.0752,2.0745,2.074,2.0737,2.0736,2.0737,2.074,2.0745,2.0752,2.0761,2.0772,2.0785,2.08,2.0817,2.0836,2.0857,2.088,2.0905,2.0932,2.0961,2.0992,2.1025,2.106,2.1097,2.1136,2.1177,2.122,2.1265,2.1312,2.1361,2.1412,2.1465,2.152,2.1577,2.1636,2.1697,2.176,2.1825,2.1892,2.1961,2.2032,2.2105,2.218,2.2257,2.2336,2.2417,2.25,2.2585,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.3585,2.3474,2.3365,2.3258,2.3153,2.305,2.2949,2.285,2.2753,2.2658,2.2565,2.2474,2.2385,2.2298,2.2213,2.213,2.2049,2.197,2.1893,2.1818,2.1745,2.1674,2.1605,2.1538,2.1473,2.141,2.1349,2.129,2.1233,2.1178,2.1125,2.1074,2.1025,2.0978,2.0933,2.089,2.0849,2.081,2.0773,2.0738,2.0705,2.0674,2.0645,2.0618,2.0593,2.057,2.0549,2.053,2.0513,2.0498,2.0485,2.0474,2.0465,2.0458,2.0453,2.045,2.0449,2.045,2.0453,2.0458,2.0465,2.0474,2.0485,2.0498,2.0513,2.053,2.0549,2.057,2.0593,2.0618,2.0645,2.0674,2.0705,2.0738,2.0773,2.081,2.0849,2.089,2.0933,2.0978,2.1025,2.1074,2.1125,2.1178,2.1233,2.129,2.1349,2.141,2.1473,2.1538,2.1605,2.1674,2.1745,2.1818,2.1893,2.197,2.2049,2.213,2.2213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.3413,2.33,2.3189,2.308,2.2973,2.2868,2.2765,2.2664,2.2565,2.2468,2.2373,2.228,2.2189,2.21,2.2013,2.1928,2.1845,2.1764,2.1685,2.1608,2.1533,2.146,2.1389,2.132,2.1253,2.1188,2.1125,2.1064,2.1005,2.0948,2.0893,2.084,2.0789,2.074,2.0693,2.0648,2.0605,2.0564,2.0525,2.0488,2.0453,2.042,2.0389,2.036,2.0333,2.0308,2.0285,2.0264,2.0245,2.0228,2.0213,2.02,2.0189,2.018,2.0173,2.0168,2.0165,2.0164,2.0165,2.0168,2.0173,2.018,2.0189,2.02,2.0213,2.0228,2.0245,2.0264,2.0285,2.0308,2.0333,2.036,2.0389,2.042,2.0453,2.0488,2.0525,2.0564,2.0605,2.0648,2.0693,2.074,2.0789,2.084,2.0893,2.0948,2.1005,2.1064,2.1125,2.1188,2.1253,2.132,2.1389,2.146,2.1533,2.1608,2.1685,2.1764,2.1845,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.3245,2.313,2.3017,2.2906,2.2797,2.269,2.2585,2.2482,2.2381,2.2282,2.2185,2.209,2.1997,2.1906,2.1817,2.173,2.1645,2.1562,2.1481,2.1402,2.1325,2.125,2.1177,2.1106,2.1037,2.097,2.0905,2.0842,2.0781,2.0722,2.0665,2.061,2.0557,2.0506,2.0457,2.041,2.0365,2.0322,2.0281,2.0242,2.0205,2.017,2.0137,2.0106,2.0077,2.005,2.0025,2.0002,1.9981,1.9962,1.9945,1.993,1.9917,1.9906,1.9897,1.989,1.9885,1.9882,1.9881,1.9882,1.9885,1.989,1.9897,1.9906,1.9917,1.993,1.9945,1.9962,1.9981,2.0002,2.0025,2.005,2.0077,2.0106,2.0137,2.017,2.0205,2.0242,2.0281,2.0322,2.0365,2.041,2.0457,2.0506,2.0557,2.061,2.0665,2.0722,2.0781,2.0842,2.0905,2.097,2.1037,2.1106,2.1177,2.125,2.1325,2.1402,2.1481,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.3081,2.2964,2.2849,2.2736,2.2625,2.2516,2.2409,2.2304,2.2201,2.21,2.2001,2.1904,2.1809,2.1716,2.1625,2.1536,2.1449,2.1364,2.1281,2.12,2.1121,2.1044,2.0969,2.0896,2.0825,2.0756,2.0689,2.0624,2.0561,2.05,2.0441,2.0384,2.0329,2.0276,2.0225,2.0176,2.0129,2.0084,2.0041,2,1.9961,1.9924,1.9889,1.9856,1.9825,1.9796,1.9769,1.9744,1.9721,1.97,1.9681,1.9664,1.9649,1.9636,1.9625,1.9616,1.9609,1.9604,1.9601,1.96,1.9601,1.9604,1.9609,1.9616,1.9625,1.9636,1.9649,1.9664,1.9681,1.97,1.9721,1.9744,1.9769,1.9796,1.9825,1.9856,1.9889,1.9924,1.9961,2,2.0041,2.0084,2.0129,2.0176,2.0225,2.0276,2.0329,2.0384,2.0441,2.05,2.0561,2.0624,2.0689,2.0756,2.0825,2.0896,2.0969,2.1044,2.1121,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2921,2.2802,2.2685,2.257,2.2457,2.2346,2.2237,2.213,2.2025,2.1922,2.1821,2.1722,2.1625,2.153,2.1437,2.1346,2.1257,2.117,2.1085,2.1002,2.0921,2.0842,2.0765,2.069,2.0617,2.0546,2.0477,2.041,2.0345,2.0282,2.0221,2.0162,2.0105,2.005,1.9997,1.9946,1.9897,1.985,1.9805,1.9762,1.9721,1.9682,1.9645,1.961,1.9577,1.9546,1.9517,1.949,1.9465,1.9442,1.9421,1.9402,1.9385,1.937,1.9357,1.9346,1.9337,1.933,1.9325,1.9322,1.9321,1.9322,1.9325,1.933,1.9337,1.9346,1.9357,1.937,1.9385,1.9402,1.9421,1.9442,1.9465,1.949,1.9517,1.9546,1.9577,1.961,1.9645,1.9682,1.9721,1.9762,1.9805,1.985,1.9897,1.9946,1.9997,2.005,2.0105,2.0162,2.0221,2.0282,2.0345,2.041,2.0477,2.0546,2.0617,2.069,2.0765,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2765,2.2644,2.2525,2.2408,2.2293,2.218,2.2069,2.196,2.1853,2.1748,2.1645,2.1544,2.1445,2.1348,2.1253,2.116,2.1069,2.098,2.0893,2.0808,2.0725,2.0644,2.0565,2.0488,2.0413,2.034,2.0269,2.02,2.0133,2.0068,2.0005,1.9944,1.9885,1.9828,1.9773,1.972,1.9669,1.962,1.9573,1.9528,1.9485,1.9444,1.9405,1.9368,1.9333,1.93,1.9269,1.924,1.9213,1.9188,1.9165,1.9144,1.9125,1.9108,1.9093,1.908,1.9069,1.906,1.9053,1.9048,1.9045,1.9044,1.9045,1.9048,1.9053,1.906,1.9069,1.908,1.9093,1.9108,1.9125,1.9144,1.9165,1.9188,1.9213,1.924,1.9269,1.93,1.9333,1.9368,1.9405,1.9444,1.9485,1.9528,1.9573,1.962,1.9669,1.972,1.9773,1.9828,1.9885,1.9944,2.0005,2.0068,2.0133,2.02,2.0269,2.034,2.0413,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2613,2.249,2.2369,2.225,2.2133,2.2018,2.1905,2.1794,2.1685,2.1578,2.1473,2.137,2.1269,2.117,2.1073,2.0978,2.0885,2.0794,2.0705,2.0618,2.0533,2.045,2.0369,2.029,2.0213,2.0138,2.0065,1.9994,1.9925,1.9858,1.9793,1.973,1.9669,1.961,1.9553,1.9498,1.9445,1.9394,1.9345,1.9298,1.9253,1.921,1.9169,1.913,1.9093,1.9058,1.9025,1.8994,1.8965,1.8938,1.8913,1.889,1.8869,1.885,1.8833,1.8818,1.8805,1.8794,1.8785,1.8778,1.8773,1.877,1.8769,1.877,1.8773,1.8778,1.8785,1.8794,1.8805,1.8818,1.8833,1.885,1.8869,1.889,1.8913,1.8938,1.8965,1.8994,1.9025,1.9058,1.9093,1.913,1.9169,1.921,1.9253,1.9298,1.9345,1.9394,1.9445,1.9498,1.9553,1.961,1.9669,1.973,1.9793,1.9858,1.9925,1.9994,2.0065,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2465,2.234,2.2217,2.2096,2.1977,2.186,2.1745,2.1632,2.1521,2.1412,2.1305,2.12,2.1097,2.0996,2.0897,2.08,2.0705,2.0612,2.0521,2.0432,2.0345,2.026,2.0177,2.0096,2.0017,1.994,1.9865,1.9792,1.9721,1.9652,1.9585,1.952,1.9457,1.9396,1.9337,1.928,1.9225,1.9172,1.9121,1.9072,1.9025,1.898,1.8937,1.8896,1.8857,1.882,1.8785,1.8752,1.8721,1.8692,1.8665,1.864,1.8617,1.8596,1.8577,1.856,1.8545,1.8532,1.8521,1.8512,1.8505,1.85,1.8497,1.8496,1.8497,1.85,1.8505,1.8512,1.8521,1.8532,1.8545,1.856,1.8577,1.8596,1.8617,1.864,1.8665,1.8692,1.8721,1.8752,1.8785,1.882,1.8857,1.8896,1.8937,1.898,1.9025,1.9072,1.9121,1.9172,1.9225,1.928,1.9337,1.9396,1.9457,1.952,1.9585,1.9652,1.9721,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2321,2.2194,2.2069,2.1946,2.1825,2.1706,2.1589,2.1474,2.1361,2.125,2.1141,2.1034,2.0929,2.0826,2.0725,2.0626,2.0529,2.0434,2.0341,2.025,2.0161,2.0074,1.9989,1.9906,1.9825,1.9746,1.9669,1.9594,1.9521,1.945,1.9381,1.9314,1.9249,1.9186,1.9125,1.9066,1.9009,1.8954,1.8901,1.885,1.8801,1.8754,1.8709,1.8666,1.8625,1.8586,1.8549,1.8514,1.8481,1.845,1.8421,1.8394,1.8369,1.8346,1.8325,1.8306,1.8289,1.8274,1.8261,1.825,1.8241,1.8234,1.8229,1.8226,1.8225,1.8226,1.8229,1.8234,1.8241,1.825,1.8261,1.8274,1.8289,1.8306,1.8325,1.8346,1.8369,1.8394,1.8421,1.845,1.8481,1.8514,1.8549,1.8586,1.8625,1.8666,1.8709,1.8754,1.8801,1.885,1.8901,1.8954,1.9009,1.9066,1.9125,1.9186,1.9249,1.9314,1.9381,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2181,2.2052,2.1925,2.18,2.1677,2.1556,2.1437,2.132,2.1205,2.1092,2.0981,2.0872,2.0765,2.066,2.0557,2.0456,2.0357,2.026,2.0165,2.0072,1.9981,1.9892,1.9805,1.972,1.9637,1.9556,1.9477,1.94,1.9325,1.9252,1.9181,1.9112,1.9045,1.898,1.8917,1.8856,1.8797,1.874,1.8685,1.8632,1.8581,1.8532,1.8485,1.844,1.8397,1.8356,1.8317,1.828,1.8245,1.8212,1.8181,1.8152,1.8125,1.81,1.8077,1.8056,1.8037,1.802,1.8005,1.7992,1.7981,1.7972,1.7965,1.796,1.7957,1.7956,1.7957,1.796,1.7965,1.7972,1.7981,1.7992,1.8005,1.802,1.8037,1.8056,1.8077,1.81,1.8125,1.8152,1.8181,1.8212,1.8245,1.828,1.8317,1.8356,1.8397,1.844,1.8485,1.8532,1.8581,1.8632,1.8685,1.874,1.8797,1.8856,1.8917,1.898,1.9045,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2045,2.1914,2.1785,2.1658,2.1533,2.141,2.1289,2.117,2.1053,2.0938,2.0825,2.0714,2.0605,2.0498,2.0393,2.029,2.0189,2.009,1.9993,1.9898,1.9805,1.9714,1.9625,1.9538,1.9453,1.937,1.9289,1.921,1.9133,1.9058,1.8985,1.8914,1.8845,1.8778,1.8713,1.865,1.8589,1.853,1.8473,1.8418,1.8365,1.8314,1.8265,1.8218,1.8173,1.813,1.8089,1.805,1.8013,1.7978,1.7945,1.7914,1.7885,1.7858,1.7833,1.781,1.7789,1.777,1.7753,1.7738,1.7725,1.7714,1.7705,1.7698,1.7693,1.769,1.7689,1.769,1.7693,1.7698,1.7705,1.7714,1.7725,1.7738,1.7753,1.777,1.7789,1.781,1.7833,1.7858,1.7885,1.7914,1.7945,1.7978,1.8013,1.805,1.8089,1.813,1.8173,1.8218,1.8265,1.8314,1.8365,1.8418,1.8473,1.853,1.8589,1.865,1.8713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1913,2.178,2.1649,2.152,2.1393,2.1268,2.1145,2.1024,2.0905,2.0788,2.0673,2.056,2.0449,2.034,2.0233,2.0128,2.0025,1.9924,1.9825,1.9728,1.9633,1.954,1.9449,1.936,1.9273,1.9188,1.9105,1.9024,1.8945,1.8868,1.8793,1.872,1.8649,1.858,1.8513,1.8448,1.8385,1.8324,1.8265,1.8208,1.8153,1.81,1.8049,1.8,1.7953,1.7908,1.7865,1.7824,1.7785,1.7748,1.7713,1.768,1.7649,1.762,1.7593,1.7568,1.7545,1.7524,1.7505,1.7488,1.7473,1.746,1.7449,1.744,1.7433,1.7428,1.7425,1.7424,1.7425,1.7428,1.7433,1.744,1.7449,1.746,1.7473,1.7488,1.7505,1.7524,1.7545,1.7568,1.7593,1.762,1.7649,1.768,1.7713,1.7748,1.7785,1.7824,1.7865,1.7908,1.7953,1.8,1.8049,1.81,1.8153,1.8208,1.8265,1.8324,1.8385,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1785,2.165,2.1517,2.1386,2.1257,2.113,2.1005,2.0882,2.0761,2.0642,2.0525,2.041,2.0297,2.0186,2.0077,1.997,1.9865,1.9762,1.9661,1.9562,1.9465,1.937,1.9277,1.9186,1.9097,1.901,1.8925,1.8842,1.8761,1.8682,1.8605,1.853,1.8457,1.8386,1.8317,1.825,1.8185,1.8122,1.8061,1.8002,1.7945,1.789,1.7837,1.7786,1.7737,1.769,1.7645,1.7602,1.7561,1.7522,1.7485,1.745,1.7417,1.7386,1.7357,1.733,1.7305,1.7282,1.7261,1.7242,1.7225,1.721,1.7197,1.7186,1.7177,1.717,1.7165,1.7162,1.7161,1.7162,1.7165,1.717,1.7177,1.7186,1.7197,1.721,1.7225,1.7242,1.7261,1.7282,1.7305,1.733,1.7357,1.7386,1.7417,1.745,1.7485,1.7522,1.7561,1.7602,1.7645,1.769,1.7737,1.7786,1.7837,1.789,1.7945,1.8002,1.8061,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1661,2.1524,2.1389,2.1256,2.1125,2.0996,2.0869,2.0744,2.0621,2.05,2.0381,2.0264,2.0149,2.0036,1.9925,1.9816,1.9709,1.9604,1.9501,1.94,1.9301,1.9204,1.9109,1.9016,1.8925,1.8836,1.8749,1.8664,1.8581,1.85,1.8421,1.8344,1.8269,1.8196,1.8125,1.8056,1.7989,1.7924,1.7861,1.78,1.7741,1.7684,1.7629,1.7576,1.7525,1.7476,1.7429,1.7384,1.7341,1.73,1.7261,1.7224,1.7189,1.7156,1.7125,1.7096,1.7069,1.7044,1.7021,1.7,1.6981,1.6964,1.6949,1.6936,1.6925,1.6916,1.6909,1.6904,1.6901,1.69,1.6901,1.6904,1.6909,1.6916,1.6925,1.6936,1.6949,1.6964,1.6981,1.7,1.7021,1.7044,1.7069,1.7096,1.7125,1.7156,1.7189,1.7224,1.7261,1.73,1.7341,1.7384,1.7429,1.7476,1.7525,1.7576,1.7629,1.7684,1.7741,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1541,2.1402,2.1265,2.113,2.0997,2.0866,2.0737,2.061,2.0485,2.0362,2.0241,2.0122,2.0005,1.989,1.9777,1.9666,1.9557,1.945,1.9345,1.9242,1.9141,1.9042,1.8945,1.885,1.8757,1.8666,1.8577,1.849,1.8405,1.8322,1.8241,1.8162,1.8085,1.801,1.7937,1.7866,1.7797,1.773,1.7665,1.7602,1.7541,1.7482,1.7425,1.737,1.7317,1.7266,1.7217,1.717,1.7125,1.7082,1.7041,1.7002,1.6965,1.693,1.6897,1.6866,1.6837,1.681,1.6785,1.6762,1.6741,1.6722,1.6705,1.669,1.6677,1.6666,1.6657,1.665,1.6645,1.6642,1.6641,1.6642,1.6645,1.665,1.6657,1.6666,1.6677,1.669,1.6705,1.6722,1.6741,1.6762,1.6785,1.681,1.6837,1.6866,1.6897,1.693,1.6965,1.7002,1.7041,1.7082,1.7125,1.717,1.7217,1.7266,1.7317,1.737,1.7425,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1425,2.1284,2.1145,2.1008,2.0873,2.074,2.0609,2.048,2.0353,2.0228,2.0105,1.9984,1.9865,1.9748,1.9633,1.952,1.9409,1.93,1.9193,1.9088,1.8985,1.8884,1.8785,1.8688,1.8593,1.85,1.8409,1.832,1.8233,1.8148,1.8065,1.7984,1.7905,1.7828,1.7753,1.768,1.7609,1.754,1.7473,1.7408,1.7345,1.7284,1.7225,1.7168,1.7113,1.706,1.7009,1.696,1.6913,1.6868,1.6825,1.6784,1.6745,1.6708,1.6673,1.664,1.6609,1.658,1.6553,1.6528,1.6505,1.6484,1.6465,1.6448,1.6433,1.642,1.6409,1.64,1.6393,1.6388,1.6385,1.6384,1.6385,1.6388,1.6393,1.64,1.6409,1.642,1.6433,1.6448,1.6465,1.6484,1.6505,1.6528,1.6553,1.658,1.6609,1.664,1.6673,1.6708,1.6745,1.6784,1.6825,1.6868,1.6913,1.696,1.7009,1.706,1.7113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1313,2.117,2.1029,2.089,2.0753,2.0618,2.0485,2.0354,2.0225,2.0098,1.9973,1.985,1.9729,1.961,1.9493,1.9378,1.9265,1.9154,1.9045,1.8938,1.8833,1.873,1.8629,1.853,1.8433,1.8338,1.8245,1.8154,1.8065,1.7978,1.7893,1.781,1.7729,1.765,1.7573,1.7498,1.7425,1.7354,1.7285,1.7218,1.7153,1.709,1.7029,1.697,1.6913,1.6858,1.6805,1.6754,1.6705,1.6658,1.6613,1.657,1.6529,1.649,1.6453,1.6418,1.6385,1.6354,1.6325,1.6298,1.6273,1.625,1.6229,1.621,1.6193,1.6178,1.6165,1.6154,1.6145,1.6138,1.6133,1.613,1.6129,1.613,1.6133,1.6138,1.6145,1.6154,1.6165,1.6178,1.6193,1.621,1.6229,1.625,1.6273,1.6298,1.6325,1.6354,1.6385,1.6418,1.6453,1.649,1.6529,1.657,1.6613,1.6658,1.6705,1.6754,1.6805,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1205,2.106,2.0917,2.0776,2.0637,2.05,2.0365,2.0232,2.0101,1.9972,1.9845,1.972,1.9597,1.9476,1.9357,1.924,1.9125,1.9012,1.8901,1.8792,1.8685,1.858,1.8477,1.8376,1.8277,1.818,1.8085,1.7992,1.7901,1.7812,1.7725,1.764,1.7557,1.7476,1.7397,1.732,1.7245,1.7172,1.7101,1.7032,1.6965,1.69,1.6837,1.6776,1.6717,1.666,1.6605,1.6552,1.6501,1.6452,1.6405,1.636,1.6317,1.6276,1.6237,1.62,1.6165,1.6132,1.6101,1.6072,1.6045,1.602,1.5997,1.5976,1.5957,1.594,1.5925,1.5912,1.5901,1.5892,1.5885,1.588,1.5877,1.5876,1.5877,1.588,1.5885,1.5892,1.5901,1.5912,1.5925,1.594,1.5957,1.5976,1.5997,1.602,1.6045,1.6072,1.6101,1.6132,1.6165,1.62,1.6237,1.6276,1.6317,1.636,1.6405,1.6452,1.6501,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1101,2.0954,2.0809,2.0666,2.0525,2.0386,2.0249,2.0114,1.9981,1.985,1.9721,1.9594,1.9469,1.9346,1.9225,1.9106,1.8989,1.8874,1.8761,1.865,1.8541,1.8434,1.8329,1.8226,1.8125,1.8026,1.7929,1.7834,1.7741,1.765,1.7561,1.7474,1.7389,1.7306,1.7225,1.7146,1.7069,1.6994,1.6921,1.685,1.6781,1.6714,1.6649,1.6586,1.6525,1.6466,1.6409,1.6354,1.6301,1.625,1.6201,1.6154,1.6109,1.6066,1.6025,1.5986,1.5949,1.5914,1.5881,1.585,1.5821,1.5794,1.5769,1.5746,1.5725,1.5706,1.5689,1.5674,1.5661,1.565,1.5641,1.5634,1.5629,1.5626,1.5625,1.5626,1.5629,1.5634,1.5641,1.565,1.5661,1.5674,1.5689,1.5706,1.5725,1.5746,1.5769,1.5794,1.5821,1.585,1.5881,1.5914,1.5949,1.5986,1.6025,1.6066,1.6109,1.6154,1.6201,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1001,2.0852,2.0705,2.056,2.0417,2.0276,2.0137,2,1.9865,1.9732,1.9601,1.9472,1.9345,1.922,1.9097,1.8976,1.8857,1.874,1.8625,1.8512,1.8401,1.8292,1.8185,1.808,1.7977,1.7876,1.7777,1.768,1.7585,1.7492,1.7401,1.7312,1.7225,1.714,1.7057,1.6976,1.6897,1.682,1.6745,1.6672,1.6601,1.6532,1.6465,1.64,1.6337,1.6276,1.6217,1.616,1.6105,1.6052,1.6001,1.5952,1.5905,1.586,1.5817,1.5776,1.5737,1.57,1.5665,1.5632,1.5601,1.5572,1.5545,1.552,1.5497,1.5476,1.5457,1.544,1.5425,1.5412,1.5401,1.5392,1.5385,1.538,1.5377,1.5376,1.5377,1.538,1.5385,1.5392,1.5401,1.5412,1.5425,1.544,1.5457,1.5476,1.5497,1.552,1.5545,1.5572,1.5601,1.5632,1.5665,1.57,1.5737,1.5776,1.5817,1.586,1.5905,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0905,2.0754,2.0605,2.0458,2.0313,2.017,2.0029,1.989,1.9753,1.9618,1.9485,1.9354,1.9225,1.9098,1.8973,1.885,1.8729,1.861,1.8493,1.8378,1.8265,1.8154,1.8045,1.7938,1.7833,1.773,1.7629,1.753,1.7433,1.7338,1.7245,1.7154,1.7065,1.6978,1.6893,1.681,1.6729,1.665,1.6573,1.6498,1.6425,1.6354,1.6285,1.6218,1.6153,1.609,1.6029,1.597,1.5913,1.5858,1.5805,1.5754,1.5705,1.5658,1.5613,1.557,1.5529,1.549,1.5453,1.5418,1.5385,1.5354,1.5325,1.5298,1.5273,1.525,1.5229,1.521,1.5193,1.5178,1.5165,1.5154,1.5145,1.5138,1.5133,1.513,1.5129,1.513,1.5133,1.5138,1.5145,1.5154,1.5165,1.5178,1.5193,1.521,1.5229,1.525,1.5273,1.5298,1.5325,1.5354,1.5385,1.5418,1.5453,1.549,1.5529,1.557,1.5613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0813,2.066,2.0509,2.036,2.0213,2.0068,1.9925,1.9784,1.9645,1.9508,1.9373,1.924,1.9109,1.898,1.8853,1.8728,1.8605,1.8484,1.8365,1.8248,1.8133,1.802,1.7909,1.78,1.7693,1.7588,1.7485,1.7384,1.7285,1.7188,1.7093,1.7,1.6909,1.682,1.6733,1.6648,1.6565,1.6484,1.6405,1.6328,1.6253,1.618,1.6109,1.604,1.5973,1.5908,1.5845,1.5784,1.5725,1.5668,1.5613,1.556,1.5509,1.546,1.5413,1.5368,1.5325,1.5284,1.5245,1.5208,1.5173,1.514,1.5109,1.508,1.5053,1.5028,1.5005,1.4984,1.4965,1.4948,1.4933,1.492,1.4909,1.49,1.4893,1.4888,1.4885,1.4884,1.4885,1.4888,1.4893,1.49,1.4909,1.492,1.4933,1.4948,1.4965,1.4984,1.5005,1.5028,1.5053,1.508,1.5109,1.514,1.5173,1.5208,1.5245,1.5284,1.5325,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0725,2.057,2.0417,2.0266,2.0117,1.997,1.9825,1.9682,1.9541,1.9402,1.9265,1.913,1.8997,1.8866,1.8737,1.861,1.8485,1.8362,1.8241,1.8122,1.8005,1.789,1.7777,1.7666,1.7557,1.745,1.7345,1.7242,1.7141,1.7042,1.6945,1.685,1.6757,1.6666,1.6577,1.649,1.6405,1.6322,1.6241,1.6162,1.6085,1.601,1.5937,1.5866,1.5797,1.573,1.5665,1.5602,1.5541,1.5482,1.5425,1.537,1.5317,1.5266,1.5217,1.517,1.5125,1.5082,1.5041,1.5002,1.4965,1.493,1.4897,1.4866,1.4837,1.481,1.4785,1.4762,1.4741,1.4722,1.4705,1.469,1.4677,1.4666,1.4657,1.465,1.4645,1.4642,1.4641,1.4642,1.4645,1.465,1.4657,1.4666,1.4677,1.469,1.4705,1.4722,1.4741,1.4762,1.4785,1.481,1.4837,1.4866,1.4897,1.493,1.4965,1.5002,1.5041,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0641,2.0484,2.0329,2.0176,2.0025,1.9876,1.9729,1.9584,1.9441,1.93,1.9161,1.9024,1.8889,1.8756,1.8625,1.8496,1.8369,1.8244,1.8121,1.8,1.7881,1.7764,1.7649,1.7536,1.7425,1.7316,1.7209,1.7104,1.7001,1.69,1.6801,1.6704,1.6609,1.6516,1.6425,1.6336,1.6249,1.6164,1.6081,1.6,1.5921,1.5844,1.5769,1.5696,1.5625,1.5556,1.5489,1.5424,1.5361,1.53,1.5241,1.5184,1.5129,1.5076,1.5025,1.4976,1.4929,1.4884,1.4841,1.48,1.4761,1.4724,1.4689,1.4656,1.4625,1.4596,1.4569,1.4544,1.4521,1.45,1.4481,1.4464,1.4449,1.4436,1.4425,1.4416,1.4409,1.4404,1.4401,1.44,1.4401,1.4404,1.4409,1.4416,1.4425,1.4436,1.4449,1.4464,1.4481,1.45,1.4521,1.4544,1.4569,1.4596,1.4625,1.4656,1.4689,1.4724,1.4761,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0561,2.0402,2.0245,2.009,1.9937,1.9786,1.9637,1.949,1.9345,1.9202,1.9061,1.8922,1.8785,1.865,1.8517,1.8386,1.8257,1.813,1.8005,1.7882,1.7761,1.7642,1.7525,1.741,1.7297,1.7186,1.7077,1.697,1.6865,1.6762,1.6661,1.6562,1.6465,1.637,1.6277,1.6186,1.6097,1.601,1.5925,1.5842,1.5761,1.5682,1.5605,1.553,1.5457,1.5386,1.5317,1.525,1.5185,1.5122,1.5061,1.5002,1.4945,1.489,1.4837,1.4786,1.4737,1.469,1.4645,1.4602,1.4561,1.4522,1.4485,1.445,1.4417,1.4386,1.4357,1.433,1.4305,1.4282,1.4261,1.4242,1.4225,1.421,1.4197,1.4186,1.4177,1.417,1.4165,1.4162,1.4161,1.4162,1.4165,1.417,1.4177,1.4186,1.4197,1.421,1.4225,1.4242,1.4261,1.4282,1.4305,1.433,1.4357,1.4386,1.4417,1.445,1.4485,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0485,2.0324,2.0165,2.0008,1.9853,1.97,1.9549,1.94,1.9253,1.9108,1.8965,1.8824,1.8685,1.8548,1.8413,1.828,1.8149,1.802,1.7893,1.7768,1.7645,1.7524,1.7405,1.7288,1.7173,1.706,1.6949,1.684,1.6733,1.6628,1.6525,1.6424,1.6325,1.6228,1.6133,1.604,1.5949,1.586,1.5773,1.5688,1.5605,1.5524,1.5445,1.5368,1.5293,1.522,1.5149,1.508,1.5013,1.4948,1.4885,1.4824,1.4765,1.4708,1.4653,1.46,1.4549,1.45,1.4453,1.4408,1.4365,1.4324,1.4285,1.4248,1.4213,1.418,1.4149,1.412,1.4093,1.4068,1.4045,1.4024,1.4005,1.3988,1.3973,1.396,1.3949,1.394,1.3933,1.3928,1.3925,1.3924,1.3925,1.3928,1.3933,1.394,1.3949,1.396,1.3973,1.3988,1.4005,1.4024,1.4045,1.4068,1.4093,1.412,1.4149,1.418,1.4213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0413,2.025,2.0089,1.993,1.9773,1.9618,1.9465,1.9314,1.9165,1.9018,1.8873,1.873,1.8589,1.845,1.8313,1.8178,1.8045,1.7914,1.7785,1.7658,1.7533,1.741,1.7289,1.717,1.7053,1.6938,1.6825,1.6714,1.6605,1.6498,1.6393,1.629,1.6189,1.609,1.5993,1.5898,1.5805,1.5714,1.5625,1.5538,1.5453,1.537,1.5289,1.521,1.5133,1.5058,1.4985,1.4914,1.4845,1.4778,1.4713,1.465,1.4589,1.453,1.4473,1.4418,1.4365,1.4314,1.4265,1.4218,1.4173,1.413,1.4089,1.405,1.4013,1.3978,1.3945,1.3914,1.3885,1.3858,1.3833,1.381,1.3789,1.377,1.3753,1.3738,1.3725,1.3714,1.3705,1.3698,1.3693,1.369,1.3689,1.369,1.3693,1.3698,1.3705,1.3714,1.3725,1.3738,1.3753,1.377,1.3789,1.381,1.3833,1.3858,1.3885,1.3914,1.3945,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0345,2.018,2.0017,1.9856,1.9697,1.954,1.9385,1.9232,1.9081,1.8932,1.8785,1.864,1.8497,1.8356,1.8217,1.808,1.7945,1.7812,1.7681,1.7552,1.7425,1.73,1.7177,1.7056,1.6937,1.682,1.6705,1.6592,1.6481,1.6372,1.6265,1.616,1.6057,1.5956,1.5857,1.576,1.5665,1.5572,1.5481,1.5392,1.5305,1.522,1.5137,1.5056,1.4977,1.49,1.4825,1.4752,1.4681,1.4612,1.4545,1.448,1.4417,1.4356,1.4297,1.424,1.4185,1.4132,1.4081,1.4032,1.3985,1.394,1.3897,1.3856,1.3817,1.378,1.3745,1.3712,1.3681,1.3652,1.3625,1.36,1.3577,1.3556,1.3537,1.352,1.3505,1.3492,1.3481,1.3472,1.3465,1.346,1.3457,1.3456,1.3457,1.346,1.3465,1.3472,1.3481,1.3492,1.3505,1.352,1.3537,1.3556,1.3577,1.36,1.3625,1.3652,1.3681,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0281,2.0114,1.9949,1.9786,1.9625,1.9466,1.9309,1.9154,1.9001,1.885,1.8701,1.8554,1.8409,1.8266,1.8125,1.7986,1.7849,1.7714,1.7581,1.745,1.7321,1.7194,1.7069,1.6946,1.6825,1.6706,1.6589,1.6474,1.6361,1.625,1.6141,1.6034,1.5929,1.5826,1.5725,1.5626,1.5529,1.5434,1.5341,1.525,1.5161,1.5074,1.4989,1.4906,1.4825,1.4746,1.4669,1.4594,1.4521,1.445,1.4381,1.4314,1.4249,1.4186,1.4125,1.4066,1.4009,1.3954,1.3901,1.385,1.3801,1.3754,1.3709,1.3666,1.3625,1.3586,1.3549,1.3514,1.3481,1.345,1.3421,1.3394,1.3369,1.3346,1.3325,1.3306,1.3289,1.3274,1.3261,1.325,1.3241,1.3234,1.3229,1.3226,1.3225,1.3226,1.3229,1.3234,1.3241,1.325,1.3261,1.3274,1.3289,1.3306,1.3325,1.3346,1.3369,1.3394,1.3421,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0221,2.0052,1.9885,1.972,1.9557,1.9396,1.9237,1.908,1.8925,1.8772,1.8621,1.8472,1.8325,1.818,1.8037,1.7896,1.7757,1.762,1.7485,1.7352,1.7221,1.7092,1.6965,1.684,1.6717,1.6596,1.6477,1.636,1.6245,1.6132,1.6021,1.5912,1.5805,1.57,1.5597,1.5496,1.5397,1.53,1.5205,1.5112,1.5021,1.4932,1.4845,1.476,1.4677,1.4596,1.4517,1.444,1.4365,1.4292,1.4221,1.4152,1.4085,1.402,1.3957,1.3896,1.3837,1.378,1.3725,1.3672,1.3621,1.3572,1.3525,1.348,1.3437,1.3396,1.3357,1.332,1.3285,1.3252,1.3221,1.3192,1.3165,1.314,1.3117,1.3096,1.3077,1.306,1.3045,1.3032,1.3021,1.3012,1.3005,1.3,1.2997,1.2996,1.2997,1.3,1.3005,1.3012,1.3021,1.3032,1.3045,1.306,1.3077,1.3096,1.3117,1.314,1.3165,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0165,1.9994,1.9825,1.9658,1.9493,1.933,1.9169,1.901,1.8853,1.8698,1.8545,1.8394,1.8245,1.8098,1.7953,1.781,1.7669,1.753,1.7393,1.7258,1.7125,1.6994,1.6865,1.6738,1.6613,1.649,1.6369,1.625,1.6133,1.6018,1.5905,1.5794,1.5685,1.5578,1.5473,1.537,1.5269,1.517,1.5073,1.4978,1.4885,1.4794,1.4705,1.4618,1.4533,1.445,1.4369,1.429,1.4213,1.4138,1.4065,1.3994,1.3925,1.3858,1.3793,1.373,1.3669,1.361,1.3553,1.3498,1.3445,1.3394,1.3345,1.3298,1.3253,1.321,1.3169,1.313,1.3093,1.3058,1.3025,1.2994,1.2965,1.2938,1.2913,1.289,1.2869,1.285,1.2833,1.2818,1.2805,1.2794,1.2785,1.2778,1.2773,1.277,1.2769,1.277,1.2773,1.2778,1.2785,1.2794,1.2805,1.2818,1.2833,1.285,1.2869,1.289,1.2913,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0113,1.994,1.9769,1.96,1.9433,1.9268,1.9105,1.8944,1.8785,1.8628,1.8473,1.832,1.8169,1.802,1.7873,1.7728,1.7585,1.7444,1.7305,1.7168,1.7033,1.69,1.6769,1.664,1.6513,1.6388,1.6265,1.6144,1.6025,1.5908,1.5793,1.568,1.5569,1.546,1.5353,1.5248,1.5145,1.5044,1.4945,1.4848,1.4753,1.466,1.4569,1.448,1.4393,1.4308,1.4225,1.4144,1.4065,1.3988,1.3913,1.384,1.3769,1.37,1.3633,1.3568,1.3505,1.3444,1.3385,1.3328,1.3273,1.322,1.3169,1.312,1.3073,1.3028,1.2985,1.2944,1.2905,1.2868,1.2833,1.28,1.2769,1.274,1.2713,1.2688,1.2665,1.2644,1.2625,1.2608,1.2593,1.258,1.2569,1.256,1.2553,1.2548,1.2545,1.2544,1.2545,1.2548,1.2553,1.256,1.2569,1.258,1.2593,1.2608,1.2625,1.2644,1.2665,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0065,1.989,1.9717,1.9546,1.9377,1.921,1.9045,1.8882,1.8721,1.8562,1.8405,1.825,1.8097,1.7946,1.7797,1.765,1.7505,1.7362,1.7221,1.7082,1.6945,1.681,1.6677,1.6546,1.6417,1.629,1.6165,1.6042,1.5921,1.5802,1.5685,1.557,1.5457,1.5346,1.5237,1.513,1.5025,1.4922,1.4821,1.4722,1.4625,1.453,1.4437,1.4346,1.4257,1.417,1.4085,1.4002,1.3921,1.3842,1.3765,1.369,1.3617,1.3546,1.3477,1.341,1.3345,1.3282,1.3221,1.3162,1.3105,1.305,1.2997,1.2946,1.2897,1.285,1.2805,1.2762,1.2721,1.2682,1.2645,1.261,1.2577,1.2546,1.2517,1.249,1.2465,1.2442,1.2421,1.2402,1.2385,1.237,1.2357,1.2346,1.2337,1.233,1.2325,1.2322,1.2321,1.2322,1.2325,1.233,1.2337,1.2346,1.2357,1.237,1.2385,1.2402,1.2421,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0021,1.9844,1.9669,1.9496,1.9325,1.9156,1.8989,1.8824,1.8661,1.85,1.8341,1.8184,1.8029,1.7876,1.7725,1.7576,1.7429,1.7284,1.7141,1.7,1.6861,1.6724,1.6589,1.6456,1.6325,1.6196,1.6069,1.5944,1.5821,1.57,1.5581,1.5464,1.5349,1.5236,1.5125,1.5016,1.4909,1.4804,1.4701,1.46,1.4501,1.4404,1.4309,1.4216,1.4125,1.4036,1.3949,1.3864,1.3781,1.37,1.3621,1.3544,1.3469,1.3396,1.3325,1.3256,1.3189,1.3124,1.3061,1.3,1.2941,1.2884,1.2829,1.2776,1.2725,1.2676,1.2629,1.2584,1.2541,1.25,1.2461,1.2424,1.2389,1.2356,1.2325,1.2296,1.2269,1.2244,1.2221,1.22,1.2181,1.2164,1.2149,1.2136,1.2125,1.2116,1.2109,1.2104,1.2101,1.21,1.2101,1.2104,1.2109,1.2116,1.2125,1.2136,1.2149,1.2164,1.2181,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9981,1.9802,1.9625,1.945,1.9277,1.9106,1.8937,1.877,1.8605,1.8442,1.8281,1.8122,1.7965,1.781,1.7657,1.7506,1.7357,1.721,1.7065,1.6922,1.6781,1.6642,1.6505,1.637,1.6237,1.6106,1.5977,1.585,1.5725,1.5602,1.5481,1.5362,1.5245,1.513,1.5017,1.4906,1.4797,1.469,1.4585,1.4482,1.4381,1.4282,1.4185,1.409,1.3997,1.3906,1.3817,1.373,1.3645,1.3562,1.3481,1.3402,1.3325,1.325,1.3177,1.3106,1.3037,1.297,1.2905,1.2842,1.2781,1.2722,1.2665,1.261,1.2557,1.2506,1.2457,1.241,1.2365,1.2322,1.2281,1.2242,1.2205,1.217,1.2137,1.2106,1.2077,1.205,1.2025,1.2002,1.1981,1.1962,1.1945,1.193,1.1917,1.1906,1.1897,1.189,1.1885,1.1882,1.1881,1.1882,1.1885,1.189,1.1897,1.1906,1.1917,1.193,1.1945,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9945,1.9764,1.9585,1.9408,1.9233,1.906,1.8889,1.872,1.8553,1.8388,1.8225,1.8064,1.7905,1.7748,1.7593,1.744,1.7289,1.714,1.6993,1.6848,1.6705,1.6564,1.6425,1.6288,1.6153,1.602,1.5889,1.576,1.5633,1.5508,1.5385,1.5264,1.5145,1.5028,1.4913,1.48,1.4689,1.458,1.4473,1.4368,1.4265,1.4164,1.4065,1.3968,1.3873,1.378,1.3689,1.36,1.3513,1.3428,1.3345,1.3264,1.3185,1.3108,1.3033,1.296,1.2889,1.282,1.2753,1.2688,1.2625,1.2564,1.2505,1.2448,1.2393,1.234,1.2289,1.224,1.2193,1.2148,1.2105,1.2064,1.2025,1.1988,1.1953,1.192,1.1889,1.186,1.1833,1.1808,1.1785,1.1764,1.1745,1.1728,1.1713,1.17,1.1689,1.168,1.1673,1.1668,1.1665,1.1664,1.1665,1.1668,1.1673,1.168,1.1689,1.17,1.1713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9913,1.973,1.9549,1.937,1.9193,1.9018,1.8845,1.8674,1.8505,1.8338,1.8173,1.801,1.7849,1.769,1.7533,1.7378,1.7225,1.7074,1.6925,1.6778,1.6633,1.649,1.6349,1.621,1.6073,1.5938,1.5805,1.5674,1.5545,1.5418,1.5293,1.517,1.5049,1.493,1.4813,1.4698,1.4585,1.4474,1.4365,1.4258,1.4153,1.405,1.3949,1.385,1.3753,1.3658,1.3565,1.3474,1.3385,1.3298,1.3213,1.313,1.3049,1.297,1.2893,1.2818,1.2745,1.2674,1.2605,1.2538,1.2473,1.241,1.2349,1.229,1.2233,1.2178,1.2125,1.2074,1.2025,1.1978,1.1933,1.189,1.1849,1.181,1.1773,1.1738,1.1705,1.1674,1.1645,1.1618,1.1593,1.157,1.1549,1.153,1.1513,1.1498,1.1485,1.1474,1.1465,1.1458,1.1453,1.145,1.1449,1.145,1.1453,1.1458,1.1465,1.1474,1.1485,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9885,1.97,1.9517,1.9336,1.9157,1.898,1.8805,1.8632,1.8461,1.8292,1.8125,1.796,1.7797,1.7636,1.7477,1.732,1.7165,1.7012,1.6861,1.6712,1.6565,1.642,1.6277,1.6136,1.5997,1.586,1.5725,1.5592,1.5461,1.5332,1.5205,1.508,1.4957,1.4836,1.4717,1.46,1.4485,1.4372,1.4261,1.4152,1.4045,1.394,1.3837,1.3736,1.3637,1.354,1.3445,1.3352,1.3261,1.3172,1.3085,1.3,1.2917,1.2836,1.2757,1.268,1.2605,1.2532,1.2461,1.2392,1.2325,1.226,1.2197,1.2136,1.2077,1.202,1.1965,1.1912,1.1861,1.1812,1.1765,1.172,1.1677,1.1636,1.1597,1.156,1.1525,1.1492,1.1461,1.1432,1.1405,1.138,1.1357,1.1336,1.1317,1.13,1.1285,1.1272,1.1261,1.1252,1.1245,1.124,1.1237,1.1236,1.1237,1.124,1.1245,1.1252,1.1261,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9861,1.9674,1.9489,1.9306,1.9125,1.8946,1.8769,1.8594,1.8421,1.825,1.8081,1.7914,1.7749,1.7586,1.7425,1.7266,1.7109,1.6954,1.6801,1.665,1.6501,1.6354,1.6209,1.6066,1.5925,1.5786,1.5649,1.5514,1.5381,1.525,1.5121,1.4994,1.4869,1.4746,1.4625,1.4506,1.4389,1.4274,1.4161,1.405,1.3941,1.3834,1.3729,1.3626,1.3525,1.3426,1.3329,1.3234,1.3141,1.305,1.2961,1.2874,1.2789,1.2706,1.2625,1.2546,1.2469,1.2394,1.2321,1.225,1.2181,1.2114,1.2049,1.1986,1.1925,1.1866,1.1809,1.1754,1.1701,1.165,1.1601,1.1554,1.1509,1.1466,1.1425,1.1386,1.1349,1.1314,1.1281,1.125,1.1221,1.1194,1.1169,1.1146,1.1125,1.1106,1.1089,1.1074,1.1061,1.105,1.1041,1.1034,1.1029,1.1026,1.1025,1.1026,1.1029,1.1034,1.1041,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9841,1.9652,1.9465,1.928,1.9097,1.8916,1.8737,1.856,1.8385,1.8212,1.8041,1.7872,1.7705,1.754,1.7377,1.7216,1.7057,1.69,1.6745,1.6592,1.6441,1.6292,1.6145,1.6,1.5857,1.5716,1.5577,1.544,1.5305,1.5172,1.5041,1.4912,1.4785,1.466,1.4537,1.4416,1.4297,1.418,1.4065,1.3952,1.3841,1.3732,1.3625,1.352,1.3417,1.3316,1.3217,1.312,1.3025,1.2932,1.2841,1.2752,1.2665,1.258,1.2497,1.2416,1.2337,1.226,1.2185,1.2112,1.2041,1.1972,1.1905,1.184,1.1777,1.1716,1.1657,1.16,1.1545,1.1492,1.1441,1.1392,1.1345,1.13,1.1257,1.1216,1.1177,1.114,1.1105,1.1072,1.1041,1.1012,1.0985,1.096,1.0937,1.0916,1.0897,1.088,1.0865,1.0852,1.0841,1.0832,1.0825,1.082,1.0817,1.0816,1.0817,1.082,1.0825,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9825,1.9634,1.9445,1.9258,1.9073,1.889,1.8709,1.853,1.8353,1.8178,1.8005,1.7834,1.7665,1.7498,1.7333,1.717,1.7009,1.685,1.6693,1.6538,1.6385,1.6234,1.6085,1.5938,1.5793,1.565,1.5509,1.537,1.5233,1.5098,1.4965,1.4834,1.4705,1.4578,1.4453,1.433,1.4209,1.409,1.3973,1.3858,1.3745,1.3634,1.3525,1.3418,1.3313,1.321,1.3109,1.301,1.2913,1.2818,1.2725,1.2634,1.2545,1.2458,1.2373,1.229,1.2209,1.213,1.2053,1.1978,1.1905,1.1834,1.1765,1.1698,1.1633,1.157,1.1509,1.145,1.1393,1.1338,1.1285,1.1234,1.1185,1.1138,1.1093,1.105,1.1009,1.097,1.0933,1.0898,1.0865,1.0834,1.0805,1.0778,1.0753,1.073,1.0709,1.069,1.0673,1.0658,1.0645,1.0634,1.0625,1.0618,1.0613,1.061,1.0609,1.061,1.0613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9813,1.962,1.9429,1.924,1.9053,1.8868,1.8685,1.8504,1.8325,1.8148,1.7973,1.78,1.7629,1.746,1.7293,1.7128,1.6965,1.6804,1.6645,1.6488,1.6333,1.618,1.6029,1.588,1.5733,1.5588,1.5445,1.5304,1.5165,1.5028,1.4893,1.476,1.4629,1.45,1.4373,1.4248,1.4125,1.4004,1.3885,1.3768,1.3653,1.354,1.3429,1.332,1.3213,1.3108,1.3005,1.2904,1.2805,1.2708,1.2613,1.252,1.2429,1.234,1.2253,1.2168,1.2085,1.2004,1.1925,1.1848,1.1773,1.17,1.1629,1.156,1.1493,1.1428,1.1365,1.1304,1.1245,1.1188,1.1133,1.108,1.1029,1.098,1.0933,1.0888,1.0845,1.0804,1.0765,1.0728,1.0693,1.066,1.0629,1.06,1.0573,1.0548,1.0525,1.0504,1.0485,1.0468,1.0453,1.044,1.0429,1.042,1.0413,1.0408,1.0405,1.0404,1.0405,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9805,1.961,1.9417,1.9226,1.9037,1.885,1.8665,1.8482,1.8301,1.8122,1.7945,1.777,1.7597,1.7426,1.7257,1.709,1.6925,1.6762,1.6601,1.6442,1.6285,1.613,1.5977,1.5826,1.5677,1.553,1.5385,1.5242,1.5101,1.4962,1.4825,1.469,1.4557,1.4426,1.4297,1.417,1.4045,1.3922,1.3801,1.3682,1.3565,1.345,1.3337,1.3226,1.3117,1.301,1.2905,1.2802,1.2701,1.2602,1.2505,1.241,1.2317,1.2226,1.2137,1.205,1.1965,1.1882,1.1801,1.1722,1.1645,1.157,1.1497,1.1426,1.1357,1.129,1.1225,1.1162,1.1101,1.1042,1.0985,1.093,1.0877,1.0826,1.0777,1.073,1.0685,1.0642,1.0601,1.0562,1.0525,1.049,1.0457,1.0426,1.0397,1.037,1.0345,1.0322,1.0301,1.0282,1.0265,1.025,1.0237,1.0226,1.0217,1.021,1.0205,1.0202,1.0201,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9801,1.9604,1.9409,1.9216,1.9025,1.8836,1.8649,1.8464,1.8281,1.81,1.7921,1.7744,1.7569,1.7396,1.7225,1.7056,1.6889,1.6724,1.6561,1.64,1.6241,1.6084,1.5929,1.5776,1.5625,1.5476,1.5329,1.5184,1.5041,1.49,1.4761,1.4624,1.4489,1.4356,1.4225,1.4096,1.3969,1.3844,1.3721,1.36,1.3481,1.3364,1.3249,1.3136,1.3025,1.2916,1.2809,1.2704,1.2601,1.25,1.2401,1.2304,1.2209,1.2116,1.2025,1.1936,1.1849,1.1764,1.1681,1.16,1.1521,1.1444,1.1369,1.1296,1.1225,1.1156,1.1089,1.1024,1.0961,1.09,1.0841,1.0784,1.0729,1.0676,1.0625,1.0576,1.0529,1.0484,1.0441,1.04,1.0361,1.0324,1.0289,1.0256,1.0225,1.0196,1.0169,1.0144,1.0121,1.01,1.0081,1.0064,1.0049,1.0036,1.0025,1.0016,1.0009,1.0004,1.0001,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9801,1.9602,1.9405,1.921,1.9017,1.8826,1.8637,1.845,1.8265,1.8082,1.7901,1.7722,1.7545,1.737,1.7197,1.7026,1.6857,1.669,1.6525,1.6362,1.6201,1.6042,1.5885,1.573,1.5577,1.5426,1.5277,1.513,1.4985,1.4842,1.4701,1.4562,1.4425,1.429,1.4157,1.4026,1.3897,1.377,1.3645,1.3522,1.3401,1.3282,1.3165,1.305,1.2937,1.2826,1.2717,1.261,1.2505,1.2402,1.2301,1.2202,1.2105,1.201,1.1917,1.1826,1.1737,1.165,1.1565,1.1482,1.1401,1.1322,1.1245,1.117,1.1097,1.1026,1.0957,1.089,1.0825,1.0762,1.0701,1.0642,1.0585,1.053,1.0477,1.0426,1.0377,1.033,1.0285,1.0242,1.0201,1.0162,1.0125,1.009,1.0057,1.0026,0.9997,0.997,0.9945,0.9922,0.9901,0.9882,0.9865,0.985,0.9837,0.9826,0.9817,0.981,0.9805,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9805,1.9604,1.9405,1.9208,1.9013,1.882,1.8629,1.844,1.8253,1.8068,1.7885,1.7704,1.7525,1.7348,1.7173,1.7,1.6829,1.666,1.6493,1.6328,1.6165,1.6004,1.5845,1.5688,1.5533,1.538,1.5229,1.508,1.4933,1.4788,1.4645,1.4504,1.4365,1.4228,1.4093,1.396,1.3829,1.37,1.3573,1.3448,1.3325,1.3204,1.3085,1.2968,1.2853,1.274,1.2629,1.252,1.2413,1.2308,1.2205,1.2104,1.2005,1.1908,1.1813,1.172,1.1629,1.154,1.1453,1.1368,1.1285,1.1204,1.1125,1.1048,1.0973,1.09,1.0829,1.076,1.0693,1.0628,1.0565,1.0504,1.0445,1.0388,1.0333,1.028,1.0229,1.018,1.0133,1.0088,1.0045,1.0004,0.9965,0.9928,0.9893,0.986,0.9829,0.98,0.9773,0.9748,0.9725,0.9704,0.9685,0.9668,0.9653,0.964,0.9629,0.962,0.9613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9813,1.961,1.9409,1.921,1.9013,1.8818,1.8625,1.8434,1.8245,1.8058,1.7873,1.769,1.7509,1.733,1.7153,1.6978,1.6805,1.6634,1.6465,1.6298,1.6133,1.597,1.5809,1.565,1.5493,1.5338,1.5185,1.5034,1.4885,1.4738,1.4593,1.445,1.4309,1.417,1.4033,1.3898,1.3765,1.3634,1.3505,1.3378,1.3253,1.313,1.3009,1.289,1.2773,1.2658,1.2545,1.2434,1.2325,1.2218,1.2113,1.201,1.1909,1.181,1.1713,1.1618,1.1525,1.1434,1.1345,1.1258,1.1173,1.109,1.1009,1.093,1.0853,1.0778,1.0705,1.0634,1.0565,1.0498,1.0433,1.037,1.0309,1.025,1.0193,1.0138,1.0085,1.0034,0.9985,0.9938,0.9893,0.985,0.9809,0.977,0.9733,0.9698,0.9665,0.9634,0.9605,0.9578,0.9553,0.953,0.9509,0.949,0.9473,0.9458,0.9445,0.9434,0.9425,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9825,1.962,1.9417,1.9216,1.9017,1.882,1.8625,1.8432,1.8241,1.8052,1.7865,1.768,1.7497,1.7316,1.7137,1.696,1.6785,1.6612,1.6441,1.6272,1.6105,1.594,1.5777,1.5616,1.5457,1.53,1.5145,1.4992,1.4841,1.4692,1.4545,1.44,1.4257,1.4116,1.3977,1.384,1.3705,1.3572,1.3441,1.3312,1.3185,1.306,1.2937,1.2816,1.2697,1.258,1.2465,1.2352,1.2241,1.2132,1.2025,1.192,1.1817,1.1716,1.1617,1.152,1.1425,1.1332,1.1241,1.1152,1.1065,1.098,1.0897,1.0816,1.0737,1.066,1.0585,1.0512,1.0441,1.0372,1.0305,1.024,1.0177,1.0116,1.0057,1,0.9945,0.9892,0.9841,0.9792,0.9745,0.97,0.9657,0.9616,0.9577,0.954,0.9505,0.9472,0.9441,0.9412,0.9385,0.936,0.9337,0.9316,0.9297,0.928,0.9265,0.9252,0.9241,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9841,1.9634,1.9429,1.9226,1.9025,1.8826,1.8629,1.8434,1.8241,1.805,1.7861,1.7674,1.7489,1.7306,1.7125,1.6946,1.6769,1.6594,1.6421,1.625,1.6081,1.5914,1.5749,1.5586,1.5425,1.5266,1.5109,1.4954,1.4801,1.465,1.4501,1.4354,1.4209,1.4066,1.3925,1.3786,1.3649,1.3514,1.3381,1.325,1.3121,1.2994,1.2869,1.2746,1.2625,1.2506,1.2389,1.2274,1.2161,1.205,1.1941,1.1834,1.1729,1.1626,1.1525,1.1426,1.1329,1.1234,1.1141,1.105,1.0961,1.0874,1.0789,1.0706,1.0625,1.0546,1.0469,1.0394,1.0321,1.025,1.0181,1.0114,1.0049,0.9986,0.9925,0.9866,0.9809,0.9754,0.9701,0.965,0.9601,0.9554,0.9509,0.9466,0.9425,0.9386,0.9349,0.9314,0.9281,0.925,0.9221,0.9194,0.9169,0.9146,0.9125,0.9106,0.9089,0.9074,0.9061,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9861,1.9652,1.9445,1.924,1.9037,1.8836,1.8637,1.844,1.8245,1.8052,1.7861,1.7672,1.7485,1.73,1.7117,1.6936,1.6757,1.658,1.6405,1.6232,1.6061,1.5892,1.5725,1.556,1.5397,1.5236,1.5077,1.492,1.4765,1.4612,1.4461,1.4312,1.4165,1.402,1.3877,1.3736,1.3597,1.346,1.3325,1.3192,1.3061,1.2932,1.2805,1.268,1.2557,1.2436,1.2317,1.22,1.2085,1.1972,1.1861,1.1752,1.1645,1.154,1.1437,1.1336,1.1237,1.114,1.1045,1.0952,1.0861,1.0772,1.0685,1.06,1.0517,1.0436,1.0357,1.028,1.0205,1.0132,1.0061,0.9992,0.9925,0.986,0.9797,0.9736,0.9677,0.962,0.9565,0.9512,0.9461,0.9412,0.9365,0.932,0.9277,0.9236,0.9197,0.916,0.9125,0.9092,0.9061,0.9032,0.9005,0.898,0.8957,0.8936,0.8917,0.89,0.8885,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9885,1.9674,1.9465,1.9258,1.9053,1.885,1.8649,1.845,1.8253,1.8058,1.7865,1.7674,1.7485,1.7298,1.7113,1.693,1.6749,1.657,1.6393,1.6218,1.6045,1.5874,1.5705,1.5538,1.5373,1.521,1.5049,1.489,1.4733,1.4578,1.4425,1.4274,1.4125,1.3978,1.3833,1.369,1.3549,1.341,1.3273,1.3138,1.3005,1.2874,1.2745,1.2618,1.2493,1.237,1.2249,1.213,1.2013,1.1898,1.1785,1.1674,1.1565,1.1458,1.1353,1.125,1.1149,1.105,1.0953,1.0858,1.0765,1.0674,1.0585,1.0498,1.0413,1.033,1.0249,1.017,1.0093,1.0018,0.9945,0.9874,0.9805,0.9738,0.9673,0.961,0.9549,0.949,0.9433,0.9378,0.9325,0.9274,0.9225,0.9178,0.9133,0.909,0.9049,0.901,0.8973,0.8938,0.8905,0.8874,0.8845,0.8818,0.8793,0.877,0.8749,0.873,0.8713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9913,1.97,1.9489,1.928,1.9073,1.8868,1.8665,1.8464,1.8265,1.8068,1.7873,1.768,1.7489,1.73,1.7113,1.6928,1.6745,1.6564,1.6385,1.6208,1.6033,1.586,1.5689,1.552,1.5353,1.5188,1.5025,1.4864,1.4705,1.4548,1.4393,1.424,1.4089,1.394,1.3793,1.3648,1.3505,1.3364,1.3225,1.3088,1.2953,1.282,1.2689,1.256,1.2433,1.2308,1.2185,1.2064,1.1945,1.1828,1.1713,1.16,1.1489,1.138,1.1273,1.1168,1.1065,1.0964,1.0865,1.0768,1.0673,1.058,1.0489,1.04,1.0313,1.0228,1.0145,1.0064,0.9985,0.9908,0.9833,0.976,0.9689,0.962,0.9553,0.9488,0.9425,0.9364,0.9305,0.9248,0.9193,0.914,0.9089,0.904,0.8993,0.8948,0.8905,0.8864,0.8825,0.8788,0.8753,0.872,0.8689,0.866,0.8633,0.8608,0.8585,0.8564,0.8545,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9945,1.973,1.9517,1.9306,1.9097,1.889,1.8685,1.8482,1.8281,1.8082,1.7885,1.769,1.7497,1.7306,1.7117,1.693,1.6745,1.6562,1.6381,1.6202,1.6025,1.585,1.5677,1.5506,1.5337,1.517,1.5005,1.4842,1.4681,1.4522,1.4365,1.421,1.4057,1.3906,1.3757,1.361,1.3465,1.3322,1.3181,1.3042,1.2905,1.277,1.2637,1.2506,1.2377,1.225,1.2125,1.2002,1.1881,1.1762,1.1645,1.153,1.1417,1.1306,1.1197,1.109,1.0985,1.0882,1.0781,1.0682,1.0585,1.049,1.0397,1.0306,1.0217,1.013,1.0045,0.9962,0.9881,0.9802,0.9725,0.965,0.9577,0.9506,0.9437,0.937,0.9305,0.9242,0.9181,0.9122,0.9065,0.901,0.8957,0.8906,0.8857,0.881,0.8765,0.8722,0.8681,0.8642,0.8605,0.857,0.8537,0.8506,0.8477,0.845,0.8425,0.8402,0.8381,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9981,1.9764,1.9549,1.9336,1.9125,1.8916,1.8709,1.8504,1.8301,1.81,1.7901,1.7704,1.7509,1.7316,1.7125,1.6936,1.6749,1.6564,1.6381,1.62,1.6021,1.5844,1.5669,1.5496,1.5325,1.5156,1.4989,1.4824,1.4661,1.45,1.4341,1.4184,1.4029,1.3876,1.3725,1.3576,1.3429,1.3284,1.3141,1.3,1.2861,1.2724,1.2589,1.2456,1.2325,1.2196,1.2069,1.1944,1.1821,1.17,1.1581,1.1464,1.1349,1.1236,1.1125,1.1016,1.0909,1.0804,1.0701,1.06,1.0501,1.0404,1.0309,1.0216,1.0125,1.0036,0.9949,0.9864,0.9781,0.97,0.9621,0.9544,0.9469,0.9396,0.9325,0.9256,0.9189,0.9124,0.9061,0.9,0.8941,0.8884,0.8829,0.8776,0.8725,0.8676,0.8629,0.8584,0.8541,0.85,0.8461,0.8424,0.8389,0.8356,0.8325,0.8296,0.8269,0.8244,0.8221,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0021,1.9802,1.9585,1.937,1.9157,1.8946,1.8737,1.853,1.8325,1.8122,1.7921,1.7722,1.7525,1.733,1.7137,1.6946,1.6757,1.657,1.6385,1.6202,1.6021,1.5842,1.5665,1.549,1.5317,1.5146,1.4977,1.481,1.4645,1.4482,1.4321,1.4162,1.4005,1.385,1.3697,1.3546,1.3397,1.325,1.3105,1.2962,1.2821,1.2682,1.2545,1.241,1.2277,1.2146,1.2017,1.189,1.1765,1.1642,1.1521,1.1402,1.1285,1.117,1.1057,1.0946,1.0837,1.073,1.0625,1.0522,1.0421,1.0322,1.0225,1.013,1.0037,0.9946,0.9857,0.977,0.9685,0.9602,0.9521,0.9442,0.9365,0.929,0.9217,0.9146,0.9077,0.901,0.8945,0.8882,0.8821,0.8762,0.8705,0.865,0.8597,0.8546,0.8497,0.845,0.8405,0.8362,0.8321,0.8282,0.8245,0.821,0.8177,0.8146,0.8117,0.809,0.8065,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0065,1.9844,1.9625,1.9408,1.9193,1.898,1.8769,1.856,1.8353,1.8148,1.7945,1.7744,1.7545,1.7348,1.7153,1.696,1.6769,1.658,1.6393,1.6208,1.6025,1.5844,1.5665,1.5488,1.5313,1.514,1.4969,1.48,1.4633,1.4468,1.4305,1.4144,1.3985,1.3828,1.3673,1.352,1.3369,1.322,1.3073,1.2928,1.2785,1.2644,1.2505,1.2368,1.2233,1.21,1.1969,1.184,1.1713,1.1588,1.1465,1.1344,1.1225,1.1108,1.0993,1.088,1.0769,1.066,1.0553,1.0448,1.0345,1.0244,1.0145,1.0048,0.9953,0.986,0.9769,0.968,0.9593,0.9508,0.9425,0.9344,0.9265,0.9188,0.9113,0.904,0.8969,0.89,0.8833,0.8768,0.8705,0.8644,0.8585,0.8528,0.8473,0.842,0.8369,0.832,0.8273,0.8228,0.8185,0.8144,0.8105,0.8068,0.8033,0.8,0.7969,0.794,0.7913,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0113,1.989,1.9669,1.945,1.9233,1.9018,1.8805,1.8594,1.8385,1.8178,1.7973,1.777,1.7569,1.737,1.7173,1.6978,1.6785,1.6594,1.6405,1.6218,1.6033,1.585,1.5669,1.549,1.5313,1.5138,1.4965,1.4794,1.4625,1.4458,1.4293,1.413,1.3969,1.381,1.3653,1.3498,1.3345,1.3194,1.3045,1.2898,1.2753,1.261,1.2469,1.233,1.2193,1.2058,1.1925,1.1794,1.1665,1.1538,1.1413,1.129,1.1169,1.105,1.0933,1.0818,1.0705,1.0594,1.0485,1.0378,1.0273,1.017,1.0069,0.997,0.9873,0.9778,0.9685,0.9594,0.9505,0.9418,0.9333,0.925,0.9169,0.909,0.9013,0.8938,0.8865,0.8794,0.8725,0.8658,0.8593,0.853,0.8469,0.841,0.8353,0.8298,0.8245,0.8194,0.8145,0.8098,0.8053,0.801,0.7969,0.793,0.7893,0.7858,0.7825,0.7794,0.7765,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0165,1.994,1.9717,1.9496,1.9277,1.906,1.8845,1.8632,1.8421,1.8212,1.8005,1.78,1.7597,1.7396,1.7197,1.7,1.6805,1.6612,1.6421,1.6232,1.6045,1.586,1.5677,1.5496,1.5317,1.514,1.4965,1.4792,1.4621,1.4452,1.4285,1.412,1.3957,1.3796,1.3637,1.348,1.3325,1.3172,1.3021,1.2872,1.2725,1.258,1.2437,1.2296,1.2157,1.202,1.1885,1.1752,1.1621,1.1492,1.1365,1.124,1.1117,1.0996,1.0877,1.076,1.0645,1.0532,1.0421,1.0312,1.0205,1.01,0.9997,0.9896,0.9797,0.97,0.9605,0.9512,0.9421,0.9332,0.9245,0.916,0.9077,0.8996,0.8917,0.884,0.8765,0.8692,0.8621,0.8552,0.8485,0.842,0.8357,0.8296,0.8237,0.818,0.8125,0.8072,0.8021,0.7972,0.7925,0.788,0.7837,0.7796,0.7757,0.772,0.7685,0.7652,0.7621,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0221,1.9994,1.9769,1.9546,1.9325,1.9106,1.8889,1.8674,1.8461,1.825,1.8041,1.7834,1.7629,1.7426,1.7225,1.7026,1.6829,1.6634,1.6441,1.625,1.6061,1.5874,1.5689,1.5506,1.5325,1.5146,1.4969,1.4794,1.4621,1.445,1.4281,1.4114,1.3949,1.3786,1.3625,1.3466,1.3309,1.3154,1.3001,1.285,1.2701,1.2554,1.2409,1.2266,1.2125,1.1986,1.1849,1.1714,1.1581,1.145,1.1321,1.1194,1.1069,1.0946,1.0825,1.0706,1.0589,1.0474,1.0361,1.025,1.0141,1.0034,0.9929,0.9826,0.9725,0.9626,0.9529,0.9434,0.9341,0.925,0.9161,0.9074,0.8989,0.8906,0.8825,0.8746,0.8669,0.8594,0.8521,0.845,0.8381,0.8314,0.8249,0.8186,0.8125,0.8066,0.8009,0.7954,0.7901,0.785,0.7801,0.7754,0.7709,0.7666,0.7625,0.7586,0.7549,0.7514,0.7481,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0281,2.0052,1.9825,1.96,1.9377,1.9156,1.8937,1.872,1.8505,1.8292,1.8081,1.7872,1.7665,1.746,1.7257,1.7056,1.6857,1.666,1.6465,1.6272,1.6081,1.5892,1.5705,1.552,1.5337,1.5156,1.4977,1.48,1.4625,1.4452,1.4281,1.4112,1.3945,1.378,1.3617,1.3456,1.3297,1.314,1.2985,1.2832,1.2681,1.2532,1.2385,1.224,1.2097,1.1956,1.1817,1.168,1.1545,1.1412,1.1281,1.1152,1.1025,1.09,1.0777,1.0656,1.0537,1.042,1.0305,1.0192,1.0081,0.9972,0.9865,0.976,0.9657,0.9556,0.9457,0.936,0.9265,0.9172,0.9081,0.8992,0.8905,0.882,0.8737,0.8656,0.8577,0.85,0.8425,0.8352,0.8281,0.8212,0.8145,0.808,0.8017,0.7956,0.7897,0.784,0.7785,0.7732,0.7681,0.7632,0.7585,0.754,0.7497,0.7456,0.7417,0.738,0.7345,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0345,2.0114,1.9885,1.9658,1.9433,1.921,1.8989,1.877,1.8553,1.8338,1.8125,1.7914,1.7705,1.7498,1.7293,1.709,1.6889,1.669,1.6493,1.6298,1.6105,1.5914,1.5725,1.5538,1.5353,1.517,1.4989,1.481,1.4633,1.4458,1.4285,1.4114,1.3945,1.3778,1.3613,1.345,1.3289,1.313,1.2973,1.2818,1.2665,1.2514,1.2365,1.2218,1.2073,1.193,1.1789,1.165,1.1513,1.1378,1.1245,1.1114,1.0985,1.0858,1.0733,1.061,1.0489,1.037,1.0253,1.0138,1.0025,0.9914,0.9805,0.9698,0.9593,0.949,0.9389,0.929,0.9193,0.9098,0.9005,0.8914,0.8825,0.8738,0.8653,0.857,0.8489,0.841,0.8333,0.8258,0.8185,0.8114,0.8045,0.7978,0.7913,0.785,0.7789,0.773,0.7673,0.7618,0.7565,0.7514,0.7465,0.7418,0.7373,0.733,0.7289,0.725,0.7213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0413,2.018,1.9949,1.972,1.9493,1.9268,1.9045,1.8824,1.8605,1.8388,1.8173,1.796,1.7749,1.754,1.7333,1.7128,1.6925,1.6724,1.6525,1.6328,1.6133,1.594,1.5749,1.556,1.5373,1.5188,1.5005,1.4824,1.4645,1.4468,1.4293,1.412,1.3949,1.378,1.3613,1.3448,1.3285,1.3124,1.2965,1.2808,1.2653,1.25,1.2349,1.22,1.2053,1.1908,1.1765,1.1624,1.1485,1.1348,1.1213,1.108,1.0949,1.082,1.0693,1.0568,1.0445,1.0324,1.0205,1.0088,0.9973,0.986,0.9749,0.964,0.9533,0.9428,0.9325,0.9224,0.9125,0.9028,0.8933,0.884,0.8749,0.866,0.8573,0.8488,0.8405,0.8324,0.8245,0.8168,0.8093,0.802,0.7949,0.788,0.7813,0.7748,0.7685,0.7624,0.7565,0.7508,0.7453,0.74,0.7349,0.73,0.7253,0.7208,0.7165,0.7124,0.7085,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0485,2.025,2.0017,1.9786,1.9557,1.933,1.9105,1.8882,1.8661,1.8442,1.8225,1.801,1.7797,1.7586,1.7377,1.717,1.6965,1.6762,1.6561,1.6362,1.6165,1.597,1.5777,1.5586,1.5397,1.521,1.5025,1.4842,1.4661,1.4482,1.4305,1.413,1.3957,1.3786,1.3617,1.345,1.3285,1.3122,1.2961,1.2802,1.2645,1.249,1.2337,1.2186,1.2037,1.189,1.1745,1.1602,1.1461,1.1322,1.1185,1.105,1.0917,1.0786,1.0657,1.053,1.0405,1.0282,1.0161,1.0042,0.9925,0.981,0.9697,0.9586,0.9477,0.937,0.9265,0.9162,0.9061,0.8962,0.8865,0.877,0.8677,0.8586,0.8497,0.841,0.8325,0.8242,0.8161,0.8082,0.8005,0.793,0.7857,0.7786,0.7717,0.765,0.7585,0.7522,0.7461,0.7402,0.7345,0.729,0.7237,0.7186,0.7137,0.709,0.7045,0.7002,0.6961,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0561,2.0324,2.0089,1.9856,1.9625,1.9396,1.9169,1.8944,1.8721,1.85,1.8281,1.8064,1.7849,1.7636,1.7425,1.7216,1.7009,1.6804,1.6601,1.64,1.6201,1.6004,1.5809,1.5616,1.5425,1.5236,1.5049,1.4864,1.4681,1.45,1.4321,1.4144,1.3969,1.3796,1.3625,1.3456,1.3289,1.3124,1.2961,1.28,1.2641,1.2484,1.2329,1.2176,1.2025,1.1876,1.1729,1.1584,1.1441,1.13,1.1161,1.1024,1.0889,1.0756,1.0625,1.0496,1.0369,1.0244,1.0121,1,0.9881,0.9764,0.9649,0.9536,0.9425,0.9316,0.9209,0.9104,0.9001,0.89,0.8801,0.8704,0.8609,0.8516,0.8425,0.8336,0.8249,0.8164,0.8081,0.8,0.7921,0.7844,0.7769,0.7696,0.7625,0.7556,0.7489,0.7424,0.7361,0.73,0.7241,0.7184,0.7129,0.7076,0.7025,0.6976,0.6929,0.6884,0.6841,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0641,2.0402,2.0165,1.993,1.9697,1.9466,1.9237,1.901,1.8785,1.8562,1.8341,1.8122,1.7905,1.769,1.7477,1.7266,1.7057,1.685,1.6645,1.6442,1.6241,1.6042,1.5845,1.565,1.5457,1.5266,1.5077,1.489,1.4705,1.4522,1.4341,1.4162,1.3985,1.381,1.3637,1.3466,1.3297,1.313,1.2965,1.2802,1.2641,1.2482,1.2325,1.217,1.2017,1.1866,1.1717,1.157,1.1425,1.1282,1.1141,1.1002,1.0865,1.073,1.0597,1.0466,1.0337,1.021,1.0085,0.9962,0.9841,0.9722,0.9605,0.949,0.9377,0.9266,0.9157,0.905,0.8945,0.8842,0.8741,0.8642,0.8545,0.845,0.8357,0.8266,0.8177,0.809,0.8005,0.7922,0.7841,0.7762,0.7685,0.761,0.7537,0.7466,0.7397,0.733,0.7265,0.7202,0.7141,0.7082,0.7025,0.697,0.6917,0.6866,0.6817,0.677,0.6725,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0725,2.0484,2.0245,2.0008,1.9773,1.954,1.9309,1.908,1.8853,1.8628,1.8405,1.8184,1.7965,1.7748,1.7533,1.732,1.7109,1.69,1.6693,1.6488,1.6285,1.6084,1.5885,1.5688,1.5493,1.53,1.5109,1.492,1.4733,1.4548,1.4365,1.4184,1.4005,1.3828,1.3653,1.348,1.3309,1.314,1.2973,1.2808,1.2645,1.2484,1.2325,1.2168,1.2013,1.186,1.1709,1.156,1.1413,1.1268,1.1125,1.0984,1.0845,1.0708,1.0573,1.044,1.0309,1.018,1.0053,0.9928,0.9805,0.9684,0.9565,0.9448,0.9333,0.922,0.9109,0.9,0.8893,0.8788,0.8685,0.8584,0.8485,0.8388,0.8293,0.82,0.8109,0.802,0.7933,0.7848,0.7765,0.7684,0.7605,0.7528,0.7453,0.738,0.7309,0.724,0.7173,0.7108,0.7045,0.6984,0.6925,0.6868,0.6813,0.676,0.6709,0.666,0.6613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0813,2.057,2.0329,2.009,1.9853,1.9618,1.9385,1.9154,1.8925,1.8698,1.8473,1.825,1.8029,1.781,1.7593,1.7378,1.7165,1.6954,1.6745,1.6538,1.6333,1.613,1.5929,1.573,1.5533,1.5338,1.5145,1.4954,1.4765,1.4578,1.4393,1.421,1.4029,1.385,1.3673,1.3498,1.3325,1.3154,1.2985,1.2818,1.2653,1.249,1.2329,1.217,1.2013,1.1858,1.1705,1.1554,1.1405,1.1258,1.1113,1.097,1.0829,1.069,1.0553,1.0418,1.0285,1.0154,1.0025,0.9898,0.9773,0.965,0.9529,0.941,0.9293,0.9178,0.9065,0.8954,0.8845,0.8738,0.8633,0.853,0.8429,0.833,0.8233,0.8138,0.8045,0.7954,0.7865,0.7778,0.7693,0.761,0.7529,0.745,0.7373,0.7298,0.7225,0.7154,0.7085,0.7018,0.6953,0.689,0.6829,0.677,0.6713,0.6658,0.6605,0.6554,0.6505,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0905,2.066,2.0417,2.0176,1.9937,1.97,1.9465,1.9232,1.9001,1.8772,1.8545,1.832,1.8097,1.7876,1.7657,1.744,1.7225,1.7012,1.6801,1.6592,1.6385,1.618,1.5977,1.5776,1.5577,1.538,1.5185,1.4992,1.4801,1.4612,1.4425,1.424,1.4057,1.3876,1.3697,1.352,1.3345,1.3172,1.3001,1.2832,1.2665,1.25,1.2337,1.2176,1.2017,1.186,1.1705,1.1552,1.1401,1.1252,1.1105,1.096,1.0817,1.0676,1.0537,1.04,1.0265,1.0132,1.0001,0.9872,0.9745,0.962,0.9497,0.9376,0.9257,0.914,0.9025,0.8912,0.8801,0.8692,0.8585,0.848,0.8377,0.8276,0.8177,0.808,0.7985,0.7892,0.7801,0.7712,0.7625,0.754,0.7457,0.7376,0.7297,0.722,0.7145,0.7072,0.7001,0.6932,0.6865,0.68,0.6737,0.6676,0.6617,0.656,0.6505,0.6452,0.6401,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1001,2.0754,2.0509,2.0266,2.0025,1.9786,1.9549,1.9314,1.9081,1.885,1.8621,1.8394,1.8169,1.7946,1.7725,1.7506,1.7289,1.7074,1.6861,1.665,1.6441,1.6234,1.6029,1.5826,1.5625,1.5426,1.5229,1.5034,1.4841,1.465,1.4461,1.4274,1.4089,1.3906,1.3725,1.3546,1.3369,1.3194,1.3021,1.285,1.2681,1.2514,1.2349,1.2186,1.2025,1.1866,1.1709,1.1554,1.1401,1.125,1.1101,1.0954,1.0809,1.0666,1.0525,1.0386,1.0249,1.0114,0.9981,0.985,0.9721,0.9594,0.9469,0.9346,0.9225,0.9106,0.8989,0.8874,0.8761,0.865,0.8541,0.8434,0.8329,0.8226,0.8125,0.8026,0.7929,0.7834,0.7741,0.765,0.7561,0.7474,0.7389,0.7306,0.7225,0.7146,0.7069,0.6994,0.6921,0.685,0.6781,0.6714,0.6649,0.6586,0.6525,0.6466,0.6409,0.6354,0.6301,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1101,2.0852,2.0605,2.036,2.0117,1.9876,1.9637,1.94,1.9165,1.8932,1.8701,1.8472,1.8245,1.802,1.7797,1.7576,1.7357,1.714,1.6925,1.6712,1.6501,1.6292,1.6085,1.588,1.5677,1.5476,1.5277,1.508,1.4885,1.4692,1.4501,1.4312,1.4125,1.394,1.3757,1.3576,1.3397,1.322,1.3045,1.2872,1.2701,1.2532,1.2365,1.22,1.2037,1.1876,1.1717,1.156,1.1405,1.1252,1.1101,1.0952,1.0805,1.066,1.0517,1.0376,1.0237,1.01,0.9965,0.9832,0.9701,0.9572,0.9445,0.932,0.9197,0.9076,0.8957,0.884,0.8725,0.8612,0.8501,0.8392,0.8285,0.818,0.8077,0.7976,0.7877,0.778,0.7685,0.7592,0.7501,0.7412,0.7325,0.724,0.7157,0.7076,0.6997,0.692,0.6845,0.6772,0.6701,0.6632,0.6565,0.65,0.6437,0.6376,0.6317,0.626,0.6205,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1205,2.0954,2.0705,2.0458,2.0213,1.997,1.9729,1.949,1.9253,1.9018,1.8785,1.8554,1.8325,1.8098,1.7873,1.765,1.7429,1.721,1.6993,1.6778,1.6565,1.6354,1.6145,1.5938,1.5733,1.553,1.5329,1.513,1.4933,1.4738,1.4545,1.4354,1.4165,1.3978,1.3793,1.361,1.3429,1.325,1.3073,1.2898,1.2725,1.2554,1.2385,1.2218,1.2053,1.189,1.1729,1.157,1.1413,1.1258,1.1105,1.0954,1.0805,1.0658,1.0513,1.037,1.0229,1.009,0.9953,0.9818,0.9685,0.9554,0.9425,0.9298,0.9173,0.905,0.8929,0.881,0.8693,0.8578,0.8465,0.8354,0.8245,0.8138,0.8033,0.793,0.7829,0.773,0.7633,0.7538,0.7445,0.7354,0.7265,0.7178,0.7093,0.701,0.6929,0.685,0.6773,0.6698,0.6625,0.6554,0.6485,0.6418,0.6353,0.629,0.6229,0.617,0.6113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1313,2.106,2.0809,2.056,2.0313,2.0068,1.9825,1.9584,1.9345,1.9108,1.8873,1.864,1.8409,1.818,1.7953,1.7728,1.7505,1.7284,1.7065,1.6848,1.6633,1.642,1.6209,1.6,1.5793,1.5588,1.5385,1.5184,1.4985,1.4788,1.4593,1.44,1.4209,1.402,1.3833,1.3648,1.3465,1.3284,1.3105,1.2928,1.2753,1.258,1.2409,1.224,1.2073,1.1908,1.1745,1.1584,1.1425,1.1268,1.1113,1.096,1.0809,1.066,1.0513,1.0368,1.0225,1.0084,0.9945,0.9808,0.9673,0.954,0.9409,0.928,0.9153,0.9028,0.8905,0.8784,0.8665,0.8548,0.8433,0.832,0.8209,0.81,0.7993,0.7888,0.7785,0.7684,0.7585,0.7488,0.7393,0.73,0.7209,0.712,0.7033,0.6948,0.6865,0.6784,0.6705,0.6628,0.6553,0.648,0.6409,0.634,0.6273,0.6208,0.6145,0.6084,0.6025,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1425,2.117,2.0917,2.0666,2.0417,2.017,1.9925,1.9682,1.9441,1.9202,1.8965,1.873,1.8497,1.8266,1.8037,1.781,1.7585,1.7362,1.7141,1.6922,1.6705,1.649,1.6277,1.6066,1.5857,1.565,1.5445,1.5242,1.5041,1.4842,1.4645,1.445,1.4257,1.4066,1.3877,1.369,1.3505,1.3322,1.3141,1.2962,1.2785,1.261,1.2437,1.2266,1.2097,1.193,1.1765,1.1602,1.1441,1.1282,1.1125,1.097,1.0817,1.0666,1.0517,1.037,1.0225,1.0082,0.9941,0.9802,0.9665,0.953,0.9397,0.9266,0.9137,0.901,0.8885,0.8762,0.8641,0.8522,0.8405,0.829,0.8177,0.8066,0.7957,0.785,0.7745,0.7642,0.7541,0.7442,0.7345,0.725,0.7157,0.7066,0.6977,0.689,0.6805,0.6722,0.6641,0.6562,0.6485,0.641,0.6337,0.6266,0.6197,0.613,0.6065,0.6002,0.5941,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1541,2.1284,2.1029,2.0776,2.0525,2.0276,2.0029,1.9784,1.9541,1.93,1.9061,1.8824,1.8589,1.8356,1.8125,1.7896,1.7669,1.7444,1.7221,1.7,1.6781,1.6564,1.6349,1.6136,1.5925,1.5716,1.5509,1.5304,1.5101,1.49,1.4701,1.4504,1.4309,1.4116,1.3925,1.3736,1.3549,1.3364,1.3181,1.3,1.2821,1.2644,1.2469,1.2296,1.2125,1.1956,1.1789,1.1624,1.1461,1.13,1.1141,1.0984,1.0829,1.0676,1.0525,1.0376,1.0229,1.0084,0.9941,0.98,0.9661,0.9524,0.9389,0.9256,0.9125,0.8996,0.8869,0.8744,0.8621,0.85,0.8381,0.8264,0.8149,0.8036,0.7925,0.7816,0.7709,0.7604,0.7501,0.74,0.7301,0.7204,0.7109,0.7016,0.6925,0.6836,0.6749,0.6664,0.6581,0.65,0.6421,0.6344,0.6269,0.6196,0.6125,0.6056,0.5989,0.5924,0.5861,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1661,2.1402,2.1145,2.089,2.0637,2.0386,2.0137,1.989,1.9645,1.9402,1.9161,1.8922,1.8685,1.845,1.8217,1.7986,1.7757,1.753,1.7305,1.7082,1.6861,1.6642,1.6425,1.621,1.5997,1.5786,1.5577,1.537,1.5165,1.4962,1.4761,1.4562,1.4365,1.417,1.3977,1.3786,1.3597,1.341,1.3225,1.3042,1.2861,1.2682,1.2505,1.233,1.2157,1.1986,1.1817,1.165,1.1485,1.1322,1.1161,1.1002,1.0845,1.069,1.0537,1.0386,1.0237,1.009,0.9945,0.9802,0.9661,0.9522,0.9385,0.925,0.9117,0.8986,0.8857,0.873,0.8605,0.8482,0.8361,0.8242,0.8125,0.801,0.7897,0.7786,0.7677,0.757,0.7465,0.7362,0.7261,0.7162,0.7065,0.697,0.6877,0.6786,0.6697,0.661,0.6525,0.6442,0.6361,0.6282,0.6205,0.613,0.6057,0.5986,0.5917,0.585,0.5785,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1785,2.1524,2.1265,2.1008,2.0753,2.05,2.0249,2,1.9753,1.9508,1.9265,1.9024,1.8785,1.8548,1.8313,1.808,1.7849,1.762,1.7393,1.7168,1.6945,1.6724,1.6505,1.6288,1.6073,1.586,1.5649,1.544,1.5233,1.5028,1.4825,1.4624,1.4425,1.4228,1.4033,1.384,1.3649,1.346,1.3273,1.3088,1.2905,1.2724,1.2545,1.2368,1.2193,1.202,1.1849,1.168,1.1513,1.1348,1.1185,1.1024,1.0865,1.0708,1.0553,1.04,1.0249,1.01,0.9953,0.9808,0.9665,0.9524,0.9385,0.9248,0.9113,0.898,0.8849,0.872,0.8593,0.8468,0.8345,0.8224,0.8105,0.7988,0.7873,0.776,0.7649,0.754,0.7433,0.7328,0.7225,0.7124,0.7025,0.6928,0.6833,0.674,0.6649,0.656,0.6473,0.6388,0.6305,0.6224,0.6145,0.6068,0.5993,0.592,0.5849,0.578,0.5713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1913,2.165,2.1389,2.113,2.0873,2.0618,2.0365,2.0114,1.9865,1.9618,1.9373,1.913,1.8889,1.865,1.8413,1.8178,1.7945,1.7714,1.7485,1.7258,1.7033,1.681,1.6589,1.637,1.6153,1.5938,1.5725,1.5514,1.5305,1.5098,1.4893,1.469,1.4489,1.429,1.4093,1.3898,1.3705,1.3514,1.3325,1.3138,1.2953,1.277,1.2589,1.241,1.2233,1.2058,1.1885,1.1714,1.1545,1.1378,1.1213,1.105,1.0889,1.073,1.0573,1.0418,1.0265,1.0114,0.9965,0.9818,0.9673,0.953,0.9389,0.925,0.9113,0.8978,0.8845,0.8714,0.8585,0.8458,0.8333,0.821,0.8089,0.797,0.7853,0.7738,0.7625,0.7514,0.7405,0.7298,0.7193,0.709,0.6989,0.689,0.6793,0.6698,0.6605,0.6514,0.6425,0.6338,0.6253,0.617,0.6089,0.601,0.5933,0.5858,0.5785,0.5714,0.5645,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2045,2.178,2.1517,2.1256,2.0997,2.074,2.0485,2.0232,1.9981,1.9732,1.9485,1.924,1.8997,1.8756,1.8517,1.828,1.8045,1.7812,1.7581,1.7352,1.7125,1.69,1.6677,1.6456,1.6237,1.602,1.5805,1.5592,1.5381,1.5172,1.4965,1.476,1.4557,1.4356,1.4157,1.396,1.3765,1.3572,1.3381,1.3192,1.3005,1.282,1.2637,1.2456,1.2277,1.21,1.1925,1.1752,1.1581,1.1412,1.1245,1.108,1.0917,1.0756,1.0597,1.044,1.0285,1.0132,0.9981,0.9832,0.9685,0.954,0.9397,0.9256,0.9117,0.898,0.8845,0.8712,0.8581,0.8452,0.8325,0.82,0.8077,0.7956,0.7837,0.772,0.7605,0.7492,0.7381,0.7272,0.7165,0.706,0.6957,0.6856,0.6757,0.666,0.6565,0.6472,0.6381,0.6292,0.6205,0.612,0.6037,0.5956,0.5877,0.58,0.5725,0.5652,0.5581,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2181,2.1914,2.1649,2.1386,2.1125,2.0866,2.0609,2.0354,2.0101,1.985,1.9601,1.9354,1.9109,1.8866,1.8625,1.8386,1.8149,1.7914,1.7681,1.745,1.7221,1.6994,1.6769,1.6546,1.6325,1.6106,1.5889,1.5674,1.5461,1.525,1.5041,1.4834,1.4629,1.4426,1.4225,1.4026,1.3829,1.3634,1.3441,1.325,1.3061,1.2874,1.2689,1.2506,1.2325,1.2146,1.1969,1.1794,1.1621,1.145,1.1281,1.1114,1.0949,1.0786,1.0625,1.0466,1.0309,1.0154,1.0001,0.985,0.9701,0.9554,0.9409,0.9266,0.9125,0.8986,0.8849,0.8714,0.8581,0.845,0.8321,0.8194,0.8069,0.7946,0.7825,0.7706,0.7589,0.7474,0.7361,0.725,0.7141,0.7034,0.6929,0.6826,0.6725,0.6626,0.6529,0.6434,0.6341,0.625,0.6161,0.6074,0.5989,0.5906,0.5825,0.5746,0.5669,0.5594,0.5521,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2321,2.2052,2.1785,2.152,2.1257,2.0996,2.0737,2.048,2.0225,1.9972,1.9721,1.9472,1.9225,1.898,1.8737,1.8496,1.8257,1.802,1.7785,1.7552,1.7321,1.7092,1.6865,1.664,1.6417,1.6196,1.5977,1.576,1.5545,1.5332,1.5121,1.4912,1.4705,1.45,1.4297,1.4096,1.3897,1.37,1.3505,1.3312,1.3121,1.2932,1.2745,1.256,1.2377,1.2196,1.2017,1.184,1.1665,1.1492,1.1321,1.1152,1.0985,1.082,1.0657,1.0496,1.0337,1.018,1.0025,0.9872,0.9721,0.9572,0.9425,0.928,0.9137,0.8996,0.8857,0.872,0.8585,0.8452,0.8321,0.8192,0.8065,0.794,0.7817,0.7696,0.7577,0.746,0.7345,0.7232,0.7121,0.7012,0.6905,0.68,0.6697,0.6596,0.6497,0.64,0.6305,0.6212,0.6121,0.6032,0.5945,0.586,0.5777,0.5696,0.5617,0.554,0.5465,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2465,2.2194,2.1925,2.1658,2.1393,2.113,2.0869,2.061,2.0353,2.0098,1.9845,1.9594,1.9345,1.9098,1.8853,1.861,1.8369,1.813,1.7893,1.7658,1.7425,1.7194,1.6965,1.6738,1.6513,1.629,1.6069,1.585,1.5633,1.5418,1.5205,1.4994,1.4785,1.4578,1.4373,1.417,1.3969,1.377,1.3573,1.3378,1.3185,1.2994,1.2805,1.2618,1.2433,1.225,1.2069,1.189,1.1713,1.1538,1.1365,1.1194,1.1025,1.0858,1.0693,1.053,1.0369,1.021,1.0053,0.9898,0.9745,0.9594,0.9445,0.9298,0.9153,0.901,0.8869,0.873,0.8593,0.8458,0.8325,0.8194,0.8065,0.7938,0.7813,0.769,0.7569,0.745,0.7333,0.7218,0.7105,0.6994,0.6885,0.6778,0.6673,0.657,0.6469,0.637,0.6273,0.6178,0.6085,0.5994,0.5905,0.5818,0.5733,0.565,0.5569,0.549,0.5413,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,2.2613,2.234,2.2069,2.18,2.1533,2.1268,2.1005,2.0744,2.0485,2.0228,1.9973,1.972,1.9469,1.922,1.8973,1.8728,1.8485,1.8244,1.8005,1.7768,1.7533,1.73,1.7069,1.684,1.6613,1.6388,1.6165,1.5944,1.5725,1.5508,1.5293,1.508,1.4869,1.466,1.4453,1.4248,1.4045,1.3844,1.3645,1.3448,1.3253,1.306,1.2869,1.268,1.2493,1.2308,1.2125,1.1944,1.1765,1.1588,1.1413,1.124,1.1069,1.09,1.0733,1.0568,1.0405,1.0244,1.0085,0.9928,0.9773,0.962,0.9469,0.932,0.9173,0.9028,0.8885,0.8744,0.8605,0.8468,0.8333,0.82,0.8069,0.794,0.7813,0.7688,0.7565,0.7444,0.7325,0.7208,0.7093,0.698,0.6869,0.676,0.6653,0.6548,0.6445,0.6344,0.6245,0.6148,0.6053,0.596,0.5869,0.578,0.5693,0.5608,0.5525,0.5444,0.5365,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,2.2765,2.249,2.2217,2.1946,2.1677,2.141,2.1145,2.0882,2.0621,2.0362,2.0105,1.985,1.9597,1.9346,1.9097,1.885,1.8605,1.8362,1.8121,1.7882,1.7645,1.741,1.7177,1.6946,1.6717,1.649,1.6265,1.6042,1.5821,1.5602,1.5385,1.517,1.4957,1.4746,1.4537,1.433,1.4125,1.3922,1.3721,1.3522,1.3325,1.313,1.2937,1.2746,1.2557,1.237,1.2185,1.2002,1.1821,1.1642,1.1465,1.129,1.1117,1.0946,1.0777,1.061,1.0445,1.0282,1.0121,0.9962,0.9805,0.965,0.9497,0.9346,0.9197,0.905,0.8905,0.8762,0.8621,0.8482,0.8345,0.821,0.8077,0.7946,0.7817,0.769,0.7565,0.7442,0.7321,0.7202,0.7085,0.697,0.6857,0.6746,0.6637,0.653,0.6425,0.6322,0.6221,0.6122,0.6025,0.593,0.5837,0.5746,0.5657,0.557,0.5485,0.5402,0.5321,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,2.2921,2.2644,2.2369,2.2096,2.1825,2.1556,2.1289,2.1024,2.0761,2.05,2.0241,1.9984,1.9729,1.9476,1.9225,1.8976,1.8729,1.8484,1.8241,1.8,1.7761,1.7524,1.7289,1.7056,1.6825,1.6596,1.6369,1.6144,1.5921,1.57,1.5481,1.5264,1.5049,1.4836,1.4625,1.4416,1.4209,1.4004,1.3801,1.36,1.3401,1.3204,1.3009,1.2816,1.2625,1.2436,1.2249,1.2064,1.1881,1.17,1.1521,1.1344,1.1169,1.0996,1.0825,1.0656,1.0489,1.0324,1.0161,1,0.9841,0.9684,0.9529,0.9376,0.9225,0.9076,0.8929,0.8784,0.8641,0.85,0.8361,0.8224,0.8089,0.7956,0.7825,0.7696,0.7569,0.7444,0.7321,0.72,0.7081,0.6964,0.6849,0.6736,0.6625,0.6516,0.6409,0.6304,0.6201,0.61,0.6001,0.5904,0.5809,0.5716,0.5625,0.5536,0.5449,0.5364,0.5281,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,2.3081,2.2802,2.2525,2.225,2.1977,2.1706,2.1437,2.117,2.0905,2.0642,2.0381,2.0122,1.9865,1.961,1.9357,1.9106,1.8857,1.861,1.8365,1.8122,1.7881,1.7642,1.7405,1.717,1.6937,1.6706,1.6477,1.625,1.6025,1.5802,1.5581,1.5362,1.5145,1.493,1.4717,1.4506,1.4297,1.409,1.3885,1.3682,1.3481,1.3282,1.3085,1.289,1.2697,1.2506,1.2317,1.213,1.1945,1.1762,1.1581,1.1402,1.1225,1.105,1.0877,1.0706,1.0537,1.037,1.0205,1.0042,0.9881,0.9722,0.9565,0.941,0.9257,0.9106,0.8957,0.881,0.8665,0.8522,0.8381,0.8242,0.8105,0.797,0.7837,0.7706,0.7577,0.745,0.7325,0.7202,0.7081,0.6962,0.6845,0.673,0.6617,0.6506,0.6397,0.629,0.6185,0.6082,0.5981,0.5882,0.5785,0.569,0.5597,0.5506,0.5417,0.533,0.5245,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,2.3245,2.2964,2.2685,2.2408,2.2133,2.186,2.1589,2.132,2.1053,2.0788,2.0525,2.0264,2.0005,1.9748,1.9493,1.924,1.8989,1.874,1.8493,1.8248,1.8005,1.7764,1.7525,1.7288,1.7053,1.682,1.6589,1.636,1.6133,1.5908,1.5685,1.5464,1.5245,1.5028,1.4813,1.46,1.4389,1.418,1.3973,1.3768,1.3565,1.3364,1.3165,1.2968,1.2773,1.258,1.2389,1.22,1.2013,1.1828,1.1645,1.1464,1.1285,1.1108,1.0933,1.076,1.0589,1.042,1.0253,1.0088,0.9925,0.9764,0.9605,0.9448,0.9293,0.914,0.8989,0.884,0.8693,0.8548,0.8405,0.8264,0.8125,0.7988,0.7853,0.772,0.7589,0.746,0.7333,0.7208,0.7085,0.6964,0.6845,0.6728,0.6613,0.65,0.6389,0.628,0.6173,0.6068,0.5965,0.5864,0.5765,0.5668,0.5573,0.548,0.5389,0.53,0.5213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,2.3413,2.313,2.2849,2.257,2.2293,2.2018,2.1745,2.1474,2.1205,2.0938,2.0673,2.041,2.0149,1.989,1.9633,1.9378,1.9125,1.8874,1.8625,1.8378,1.8133,1.789,1.7649,1.741,1.7173,1.6938,1.6705,1.6474,1.6245,1.6018,1.5793,1.557,1.5349,1.513,1.4913,1.4698,1.4485,1.4274,1.4065,1.3858,1.3653,1.345,1.3249,1.305,1.2853,1.2658,1.2465,1.2274,1.2085,1.1898,1.1713,1.153,1.1349,1.117,1.0993,1.0818,1.0645,1.0474,1.0305,1.0138,0.9973,0.981,0.9649,0.949,0.9333,0.9178,0.9025,0.8874,0.8725,0.8578,0.8433,0.829,0.8149,0.801,0.7873,0.7738,0.7605,0.7474,0.7345,0.7218,0.7093,0.697,0.6849,0.673,0.6613,0.6498,0.6385,0.6274,0.6165,0.6058,0.5953,0.585,0.5749,0.565,0.5553,0.5458,0.5365,0.5274,0.5185,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,2.3585,2.33,2.3017,2.2736,2.2457,2.218,2.1905,2.1632,2.1361,2.1092,2.0825,2.056,2.0297,2.0036,1.9777,1.952,1.9265,1.9012,1.8761,1.8512,1.8265,1.802,1.7777,1.7536,1.7297,1.706,1.6825,1.6592,1.6361,1.6132,1.5905,1.568,1.5457,1.5236,1.5017,1.48,1.4585,1.4372,1.4161,1.3952,1.3745,1.354,1.3337,1.3136,1.2937,1.274,1.2545,1.2352,1.2161,1.1972,1.1785,1.16,1.1417,1.1236,1.1057,1.088,1.0705,1.0532,1.0361,1.0192,1.0025,0.986,0.9697,0.9536,0.9377,0.922,0.9065,0.8912,0.8761,0.8612,0.8465,0.832,0.8177,0.8036,0.7897,0.776,0.7625,0.7492,0.7361,0.7232,0.7105,0.698,0.6857,0.6736,0.6617,0.65,0.6385,0.6272,0.6161,0.6052,0.5945,0.584,0.5737,0.5636,0.5537,0.544,0.5345,0.5252,0.5161,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,2.3761,2.3474,2.3189,2.2906,2.2625,2.2346,2.2069,2.1794,2.1521,2.125,2.0981,2.0714,2.0449,2.0186,1.9925,1.9666,1.9409,1.9154,1.8901,1.865,1.8401,1.8154,1.7909,1.7666,1.7425,1.7186,1.6949,1.6714,1.6481,1.625,1.6021,1.5794,1.5569,1.5346,1.5125,1.4906,1.4689,1.4474,1.4261,1.405,1.3841,1.3634,1.3429,1.3226,1.3025,1.2826,1.2629,1.2434,1.2241,1.205,1.1861,1.1674,1.1489,1.1306,1.1125,1.0946,1.0769,1.0594,1.0421,1.025,1.0081,0.9914,0.9749,0.9586,0.9425,0.9266,0.9109,0.8954,0.8801,0.865,0.8501,0.8354,0.8209,0.8066,0.7925,0.7786,0.7649,0.7514,0.7381,0.725,0.7121,0.6994,0.6869,0.6746,0.6625,0.6506,0.6389,0.6274,0.6161,0.605,0.5941,0.5834,0.5729,0.5626,0.5525,0.5426,0.5329,0.5234,0.5141,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,2.3941,2.3652,2.3365,2.308,2.2797,2.2516,2.2237,2.196,2.1685,2.1412,2.1141,2.0872,2.0605,2.034,2.0077,1.9816,1.9557,1.93,1.9045,1.8792,1.8541,1.8292,1.8045,1.78,1.7557,1.7316,1.7077,1.684,1.6605,1.6372,1.6141,1.5912,1.5685,1.546,1.5237,1.5016,1.4797,1.458,1.4365,1.4152,1.3941,1.3732,1.3525,1.332,1.3117,1.2916,1.2717,1.252,1.2325,1.2132,1.1941,1.1752,1.1565,1.138,1.1197,1.1016,1.0837,1.066,1.0485,1.0312,1.0141,0.9972,0.9805,0.964,0.9477,0.9316,0.9157,0.9,0.8845,0.8692,0.8541,0.8392,0.8245,0.81,0.7957,0.7816,0.7677,0.754,0.7405,0.7272,0.7141,0.7012,0.6885,0.676,0.6637,0.6516,0.6397,0.628,0.6165,0.6052,0.5941,0.5832,0.5725,0.562,0.5517,0.5416,0.5317,0.522,0.5125,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,2.4125,2.3834,2.3545,2.3258,2.2973,2.269,2.2409,2.213,2.1853,2.1578,2.1305,2.1034,2.0765,2.0498,2.0233,1.997,1.9709,1.945,1.9193,1.8938,1.8685,1.8434,1.8185,1.7938,1.7693,1.745,1.7209,1.697,1.6733,1.6498,1.6265,1.6034,1.5805,1.5578,1.5353,1.513,1.4909,1.469,1.4473,1.4258,1.4045,1.3834,1.3625,1.3418,1.3213,1.301,1.2809,1.261,1.2413,1.2218,1.2025,1.1834,1.1645,1.1458,1.1273,1.109,1.0909,1.073,1.0553,1.0378,1.0205,1.0034,0.9865,0.9698,0.9533,0.937,0.9209,0.905,0.8893,0.8738,0.8585,0.8434,0.8285,0.8138,0.7993,0.785,0.7709,0.757,0.7433,0.7298,0.7165,0.7034,0.6905,0.6778,0.6653,0.653,0.6409,0.629,0.6173,0.6058,0.5945,0.5834,0.5725,0.5618,0.5513,0.541,0.5309,0.521,0.5113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,2.4313,2.402,2.3729,2.344,2.3153,2.2868,2.2585,2.2304,2.2025,2.1748,2.1473,2.12,2.0929,2.066,2.0393,2.0128,1.9865,1.9604,1.9345,1.9088,1.8833,1.858,1.8329,1.808,1.7833,1.7588,1.7345,1.7104,1.6865,1.6628,1.6393,1.616,1.5929,1.57,1.5473,1.5248,1.5025,1.4804,1.4585,1.4368,1.4153,1.394,1.3729,1.352,1.3313,1.3108,1.2905,1.2704,1.2505,1.2308,1.2113,1.192,1.1729,1.154,1.1353,1.1168,1.0985,1.0804,1.0625,1.0448,1.0273,1.01,0.9929,0.976,0.9593,0.9428,0.9265,0.9104,0.8945,0.8788,0.8633,0.848,0.8329,0.818,0.8033,0.7888,0.7745,0.7604,0.7465,0.7328,0.7193,0.706,0.6929,0.68,0.6673,0.6548,0.6425,0.6304,0.6185,0.6068,0.5953,0.584,0.5729,0.562,0.5513,0.5408,0.5305,0.5204,0.5105,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,2.4505,2.421,2.3917,2.3626,2.3337,2.305,2.2765,2.2482,2.2201,2.1922,2.1645,2.137,2.1097,2.0826,2.0557,2.029,2.0025,1.9762,1.9501,1.9242,1.8985,1.873,1.8477,1.8226,1.7977,1.773,1.7485,1.7242,1.7001,1.6762,1.6525,1.629,1.6057,1.5826,1.5597,1.537,1.5145,1.4922,1.4701,1.4482,1.4265,1.405,1.3837,1.3626,1.3417,1.321,1.3005,1.2802,1.2601,1.2402,1.2205,1.201,1.1817,1.1626,1.1437,1.125,1.1065,1.0882,1.0701,1.0522,1.0345,1.017,0.9997,0.9826,0.9657,0.949,0.9325,0.9162,0.9001,0.8842,0.8685,0.853,0.8377,0.8226,0.8077,0.793,0.7785,0.7642,0.7501,0.7362,0.7225,0.709,0.6957,0.6826,0.6697,0.657,0.6445,0.6322,0.6201,0.6082,0.5965,0.585,0.5737,0.5626,0.5517,0.541,0.5305,0.5202,0.5101,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,2.4701,2.4404,2.4109,2.3816,2.3525,2.3236,2.2949,2.2664,2.2381,2.21,2.1821,2.1544,2.1269,2.0996,2.0725,2.0456,2.0189,1.9924,1.9661,1.94,1.9141,1.8884,1.8629,1.8376,1.8125,1.7876,1.7629,1.7384,1.7141,1.69,1.6661,1.6424,1.6189,1.5956,1.5725,1.5496,1.5269,1.5044,1.4821,1.46,1.4381,1.4164,1.3949,1.3736,1.3525,1.3316,1.3109,1.2904,1.2701,1.25,1.2301,1.2104,1.1909,1.1716,1.1525,1.1336,1.1149,1.0964,1.0781,1.06,1.0421,1.0244,1.0069,0.9896,0.9725,0.9556,0.9389,0.9224,0.9061,0.89,0.8741,0.8584,0.8429,0.8276,0.8125,0.7976,0.7829,0.7684,0.7541,0.74,0.7261,0.7124,0.6989,0.6856,0.6725,0.6596,0.6469,0.6344,0.6221,0.61,0.5981,0.5864,0.5749,0.5636,0.5525,0.5416,0.5309,0.5204,0.5101,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4901,2.4602,2.4305,2.401,2.3717,2.3426,2.3137,2.285,2.2565,2.2282,2.2001,2.1722,2.1445,2.117,2.0897,2.0626,2.0357,2.009,1.9825,1.9562,1.9301,1.9042,1.8785,1.853,1.8277,1.8026,1.7777,1.753,1.7285,1.7042,1.6801,1.6562,1.6325,1.609,1.5857,1.5626,1.5397,1.517,1.4945,1.4722,1.4501,1.4282,1.4065,1.385,1.3637,1.3426,1.3217,1.301,1.2805,1.2602,1.2401,1.2202,1.2005,1.181,1.1617,1.1426,1.1237,1.105,1.0865,1.0682,1.0501,1.0322,1.0145,0.997,0.9797,0.9626,0.9457,0.929,0.9125,0.8962,0.8801,0.8642,0.8485,0.833,0.8177,0.8026,0.7877,0.773,0.7585,0.7442,0.7301,0.7162,0.7025,0.689,0.6757,0.6626,0.6497,0.637,0.6245,0.6122,0.6001,0.5882,0.5765,0.565,0.5537,0.5426,0.5317,0.521,0.5105,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4804,2.4505,2.4208,2.3913,2.362,2.3329,2.304,2.2753,2.2468,2.2185,2.1904,2.1625,2.1348,2.1073,2.08,2.0529,2.026,1.9993,1.9728,1.9465,1.9204,1.8945,1.8688,1.8433,1.818,1.7929,1.768,1.7433,1.7188,1.6945,1.6704,1.6465,1.6228,1.5993,1.576,1.5529,1.53,1.5073,1.4848,1.4625,1.4404,1.4185,1.3968,1.3753,1.354,1.3329,1.312,1.2913,1.2708,1.2505,1.2304,1.2105,1.1908,1.1713,1.152,1.1329,1.114,1.0953,1.0768,1.0585,1.0404,1.0225,1.0048,0.9873,0.97,0.9529,0.936,0.9193,0.9028,0.8865,0.8704,0.8545,0.8388,0.8233,0.808,0.7929,0.778,0.7633,0.7488,0.7345,0.7204,0.7065,0.6928,0.6793,0.666,0.6529,0.64,0.6273,0.6148,0.6025,0.5904,0.5785,0.5668,0.5553,0.544,0.5329,0.522,0.5113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4709,2.441,2.4113,2.3818,2.3525,2.3234,2.2945,2.2658,2.2373,2.209,2.1809,2.153,2.1253,2.0978,2.0705,2.0434,2.0165,1.9898,1.9633,1.937,1.9109,1.885,1.8593,1.8338,1.8085,1.7834,1.7585,1.7338,1.7093,1.685,1.6609,1.637,1.6133,1.5898,1.5665,1.5434,1.5205,1.4978,1.4753,1.453,1.4309,1.409,1.3873,1.3658,1.3445,1.3234,1.3025,1.2818,1.2613,1.241,1.2209,1.201,1.1813,1.1618,1.1425,1.1234,1.1045,1.0858,1.0673,1.049,1.0309,1.013,0.9953,0.9778,0.9605,0.9434,0.9265,0.9098,0.8933,0.877,0.8609,0.845,0.8293,0.8138,0.7985,0.7834,0.7685,0.7538,0.7393,0.725,0.7109,0.697,0.6833,0.6698,0.6565,0.6434,0.6305,0.6178,0.6053,0.593,0.5809,0.569,0.5573,0.5458,0.5345,0.5234,0.5125,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4616,2.4317,2.402,2.3725,2.3432,2.3141,2.2852,2.2565,2.228,2.1997,2.1716,2.1437,2.116,2.0885,2.0612,2.0341,2.0072,1.9805,1.954,1.9277,1.9016,1.8757,1.85,1.8245,1.7992,1.7741,1.7492,1.7245,1.7,1.6757,1.6516,1.6277,1.604,1.5805,1.5572,1.5341,1.5112,1.4885,1.466,1.4437,1.4216,1.3997,1.378,1.3565,1.3352,1.3141,1.2932,1.2725,1.252,1.2317,1.2116,1.1917,1.172,1.1525,1.1332,1.1141,1.0952,1.0765,1.058,1.0397,1.0216,1.0037,0.986,0.9685,0.9512,0.9341,0.9172,0.9005,0.884,0.8677,0.8516,0.8357,0.82,0.8045,0.7892,0.7741,0.7592,0.7445,0.73,0.7157,0.7016,0.6877,0.674,0.6605,0.6472,0.6341,0.6212,0.6085,0.596,0.5837,0.5716,0.5597,0.548,0.5365,0.5252,0.5141,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4525,2.4226,2.3929,2.3634,2.3341,2.305,2.2761,2.2474,2.2189,2.1906,2.1625,2.1346,2.1069,2.0794,2.0521,2.025,1.9981,1.9714,1.9449,1.9186,1.8925,1.8666,1.8409,1.8154,1.7901,1.765,1.7401,1.7154,1.6909,1.6666,1.6425,1.6186,1.5949,1.5714,1.5481,1.525,1.5021,1.4794,1.4569,1.4346,1.4125,1.3906,1.3689,1.3474,1.3261,1.305,1.2841,1.2634,1.2429,1.2226,1.2025,1.1826,1.1629,1.1434,1.1241,1.105,1.0861,1.0674,1.0489,1.0306,1.0125,0.9946,0.9769,0.9594,0.9421,0.925,0.9081,0.8914,0.8749,0.8586,0.8425,0.8266,0.8109,0.7954,0.7801,0.765,0.7501,0.7354,0.7209,0.7066,0.6925,0.6786,0.6649,0.6514,0.6381,0.625,0.6121,0.5994,0.5869,0.5746,0.5625,0.5506,0.5389,0.5274,0.5161,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4436,2.4137,2.384,2.3545,2.3252,2.2961,2.2672,2.2385,2.21,2.1817,2.1536,2.1257,2.098,2.0705,2.0432,2.0161,1.9892,1.9625,1.936,1.9097,1.8836,1.8577,1.832,1.8065,1.7812,1.7561,1.7312,1.7065,1.682,1.6577,1.6336,1.6097,1.586,1.5625,1.5392,1.5161,1.4932,1.4705,1.448,1.4257,1.4036,1.3817,1.36,1.3385,1.3172,1.2961,1.2752,1.2545,1.234,1.2137,1.1936,1.1737,1.154,1.1345,1.1152,1.0961,1.0772,1.0585,1.04,1.0217,1.0036,0.9857,0.968,0.9505,0.9332,0.9161,0.8992,0.8825,0.866,0.8497,0.8336,0.8177,0.802,0.7865,0.7712,0.7561,0.7412,0.7265,0.712,0.6977,0.6836,0.6697,0.656,0.6425,0.6292,0.6161,0.6032,0.5905,0.578,0.5657,0.5536,0.5417,0.53,0.5185,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4349,2.405,2.3753,2.3458,2.3165,2.2874,2.2585,2.2298,2.2013,2.173,2.1449,2.117,2.0893,2.0618,2.0345,2.0074,1.9805,1.9538,1.9273,1.901,1.8749,1.849,1.8233,1.7978,1.7725,1.7474,1.7225,1.6978,1.6733,1.649,1.6249,1.601,1.5773,1.5538,1.5305,1.5074,1.4845,1.4618,1.4393,1.417,1.3949,1.373,1.3513,1.3298,1.3085,1.2874,1.2665,1.2458,1.2253,1.205,1.1849,1.165,1.1453,1.1258,1.1065,1.0874,1.0685,1.0498,1.0313,1.013,0.9949,0.977,0.9593,0.9418,0.9245,0.9074,0.8905,0.8738,0.8573,0.841,0.8249,0.809,0.7933,0.7778,0.7625,0.7474,0.7325,0.7178,0.7033,0.689,0.6749,0.661,0.6473,0.6338,0.6205,0.6074,0.5945,0.5818,0.5693,0.557,0.5449,0.533,0.5213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4264,2.3965,2.3668,2.3373,2.308,2.2789,2.25,2.2213,2.1928,2.1645,2.1364,2.1085,2.0808,2.0533,2.026,1.9989,1.972,1.9453,1.9188,1.8925,1.8664,1.8405,1.8148,1.7893,1.764,1.7389,1.714,1.6893,1.6648,1.6405,1.6164,1.5925,1.5688,1.5453,1.522,1.4989,1.476,1.4533,1.4308,1.4085,1.3864,1.3645,1.3428,1.3213,1.3,1.2789,1.258,1.2373,1.2168,1.1965,1.1764,1.1565,1.1368,1.1173,1.098,1.0789,1.06,1.0413,1.0228,1.0045,0.9864,0.9685,0.9508,0.9333,0.916,0.8989,0.882,0.8653,0.8488,0.8325,0.8164,0.8005,0.7848,0.7693,0.754,0.7389,0.724,0.7093,0.6948,0.6805,0.6664,0.6525,0.6388,0.6253,0.612,0.5989,0.586,0.5733,0.5608,0.5485,0.5364,0.5245,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4181,2.3882,2.3585,2.329,2.2997,2.2706,2.2417,2.213,2.1845,2.1562,2.1281,2.1002,2.0725,2.045,2.0177,1.9906,1.9637,1.937,1.9105,1.8842,1.8581,1.8322,1.8065,1.781,1.7557,1.7306,1.7057,1.681,1.6565,1.6322,1.6081,1.5842,1.5605,1.537,1.5137,1.4906,1.4677,1.445,1.4225,1.4002,1.3781,1.3562,1.3345,1.313,1.2917,1.2706,1.2497,1.229,1.2085,1.1882,1.1681,1.1482,1.1285,1.109,1.0897,1.0706,1.0517,1.033,1.0145,0.9962,0.9781,0.9602,0.9425,0.925,0.9077,0.8906,0.8737,0.857,0.8405,0.8242,0.8081,0.7922,0.7765,0.761,0.7457,0.7306,0.7157,0.701,0.6865,0.6722,0.6581,0.6442,0.6305,0.617,0.6037,0.5906,0.5777,0.565,0.5525,0.5402,0.5281,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.41,2.3801,2.3504,2.3209,2.2916,2.2625,2.2336,2.2049,2.1764,2.1481,2.12,2.0921,2.0644,2.0369,2.0096,1.9825,1.9556,1.9289,1.9024,1.8761,1.85,1.8241,1.7984,1.7729,1.7476,1.7225,1.6976,1.6729,1.6484,1.6241,1.6,1.5761,1.5524,1.5289,1.5056,1.4825,1.4596,1.4369,1.4144,1.3921,1.37,1.3481,1.3264,1.3049,1.2836,1.2625,1.2416,1.2209,1.2004,1.1801,1.16,1.1401,1.1204,1.1009,1.0816,1.0625,1.0436,1.0249,1.0064,0.9881,0.97,0.9521,0.9344,0.9169,0.8996,0.8825,0.8656,0.8489,0.8324,0.8161,0.8,0.7841,0.7684,0.7529,0.7376,0.7225,0.7076,0.6929,0.6784,0.6641,0.65,0.6361,0.6224,0.6089,0.5956,0.5825,0.5696,0.5569,0.5444,0.5321,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4021,2.3722,2.3425,2.313,2.2837,2.2546,2.2257,2.197,2.1685,2.1402,2.1121,2.0842,2.0565,2.029,2.0017,1.9746,1.9477,1.921,1.8945,1.8682,1.8421,1.8162,1.7905,1.765,1.7397,1.7146,1.6897,1.665,1.6405,1.6162,1.5921,1.5682,1.5445,1.521,1.4977,1.4746,1.4517,1.429,1.4065,1.3842,1.3621,1.3402,1.3185,1.297,1.2757,1.2546,1.2337,1.213,1.1925,1.1722,1.1521,1.1322,1.1125,1.093,1.0737,1.0546,1.0357,1.017,0.9985,0.9802,0.9621,0.9442,0.9265,0.909,0.8917,0.8746,0.8577,0.841,0.8245,0.8082,0.7921,0.7762,0.7605,0.745,0.7297,0.7146,0.6997,0.685,0.6705,0.6562,0.6421,0.6282,0.6145,0.601,0.5877,0.5746,0.5617,0.549,0.5365,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3944,2.3645,2.3348,2.3053,2.276,2.2469,2.218,2.1893,2.1608,2.1325,2.1044,2.0765,2.0488,2.0213,1.994,1.9669,1.94,1.9133,1.8868,1.8605,1.8344,1.8085,1.7828,1.7573,1.732,1.7069,1.682,1.6573,1.6328,1.6085,1.5844,1.5605,1.5368,1.5133,1.49,1.4669,1.444,1.4213,1.3988,1.3765,1.3544,1.3325,1.3108,1.2893,1.268,1.2469,1.226,1.2053,1.1848,1.1645,1.1444,1.1245,1.1048,1.0853,1.066,1.0469,1.028,1.0093,0.9908,0.9725,0.9544,0.9365,0.9188,0.9013,0.884,0.8669,0.85,0.8333,0.8168,0.8005,0.7844,0.7685,0.7528,0.7373,0.722,0.7069,0.692,0.6773,0.6628,0.6485,0.6344,0.6205,0.6068,0.5933,0.58,0.5669,0.554,0.5413,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3869,2.357,2.3273,2.2978,2.2685,2.2394,2.2105,2.1818,2.1533,2.125,2.0969,2.069,2.0413,2.0138,1.9865,1.9594,1.9325,1.9058,1.8793,1.853,1.8269,1.801,1.7753,1.7498,1.7245,1.6994,1.6745,1.6498,1.6253,1.601,1.5769,1.553,1.5293,1.5058,1.4825,1.4594,1.4365,1.4138,1.3913,1.369,1.3469,1.325,1.3033,1.2818,1.2605,1.2394,1.2185,1.1978,1.1773,1.157,1.1369,1.117,1.0973,1.0778,1.0585,1.0394,1.0205,1.0018,0.9833,0.965,0.9469,0.929,0.9113,0.8938,0.8765,0.8594,0.8425,0.8258,0.8093,0.793,0.7769,0.761,0.7453,0.7298,0.7145,0.6994,0.6845,0.6698,0.6553,0.641,0.6269,0.613,0.5993,0.5858,0.5725,0.5594,0.5465,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3796,2.3497,2.32,2.2905,2.2612,2.2321,2.2032,2.1745,2.146,2.1177,2.0896,2.0617,2.034,2.0065,1.9792,1.9521,1.9252,1.8985,1.872,1.8457,1.8196,1.7937,1.768,1.7425,1.7172,1.6921,1.6672,1.6425,1.618,1.5937,1.5696,1.5457,1.522,1.4985,1.4752,1.4521,1.4292,1.4065,1.384,1.3617,1.3396,1.3177,1.296,1.2745,1.2532,1.2321,1.2112,1.1905,1.17,1.1497,1.1296,1.1097,1.09,1.0705,1.0512,1.0321,1.0132,0.9945,0.976,0.9577,0.9396,0.9217,0.904,0.8865,0.8692,0.8521,0.8352,0.8185,0.802,0.7857,0.7696,0.7537,0.738,0.7225,0.7072,0.6921,0.6772,0.6625,0.648,0.6337,0.6196,0.6057,0.592,0.5785,0.5652,0.5521,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3725,2.3426,2.3129,2.2834,2.2541,2.225,2.1961,2.1674,2.1389,2.1106,2.0825,2.0546,2.0269,1.9994,1.9721,1.945,1.9181,1.8914,1.8649,1.8386,1.8125,1.7866,1.7609,1.7354,1.7101,1.685,1.6601,1.6354,1.6109,1.5866,1.5625,1.5386,1.5149,1.4914,1.4681,1.445,1.4221,1.3994,1.3769,1.3546,1.3325,1.3106,1.2889,1.2674,1.2461,1.225,1.2041,1.1834,1.1629,1.1426,1.1225,1.1026,1.0829,1.0634,1.0441,1.025,1.0061,0.9874,0.9689,0.9506,0.9325,0.9146,0.8969,0.8794,0.8621,0.845,0.8281,0.8114,0.7949,0.7786,0.7625,0.7466,0.7309,0.7154,0.7001,0.685,0.6701,0.6554,0.6409,0.6266,0.6125,0.5986,0.5849,0.5714,0.5581,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3656,2.3357,2.306,2.2765,2.2472,2.2181,2.1892,2.1605,2.132,2.1037,2.0756,2.0477,2.02,1.9925,1.9652,1.9381,1.9112,1.8845,1.858,1.8317,1.8056,1.7797,1.754,1.7285,1.7032,1.6781,1.6532,1.6285,1.604,1.5797,1.5556,1.5317,1.508,1.4845,1.4612,1.4381,1.4152,1.3925,1.37,1.3477,1.3256,1.3037,1.282,1.2605,1.2392,1.2181,1.1972,1.1765,1.156,1.1357,1.1156,1.0957,1.076,1.0565,1.0372,1.0181,0.9992,0.9805,0.962,0.9437,0.9256,0.9077,0.89,0.8725,0.8552,0.8381,0.8212,0.8045,0.788,0.7717,0.7556,0.7397,0.724,0.7085,0.6932,0.6781,0.6632,0.6485,0.634,0.6197,0.6056,0.5917,0.578,0.5645,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3589,2.329,2.2993,2.2698,2.2405,2.2114,2.1825,2.1538,2.1253,2.097,2.0689,2.041,2.0133,1.9858,1.9585,1.9314,1.9045,1.8778,1.8513,1.825,1.7989,1.773,1.7473,1.7218,1.6965,1.6714,1.6465,1.6218,1.5973,1.573,1.5489,1.525,1.5013,1.4778,1.4545,1.4314,1.4085,1.3858,1.3633,1.341,1.3189,1.297,1.2753,1.2538,1.2325,1.2114,1.1905,1.1698,1.1493,1.129,1.1089,1.089,1.0693,1.0498,1.0305,1.0114,0.9925,0.9738,0.9553,0.937,0.9189,0.901,0.8833,0.8658,0.8485,0.8314,0.8145,0.7978,0.7813,0.765,0.7489,0.733,0.7173,0.7018,0.6865,0.6714,0.6565,0.6418,0.6273,0.613,0.5989,0.585,0.5713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3524,2.3225,2.2928,2.2633,2.234,2.2049,2.176,2.1473,2.1188,2.0905,2.0624,2.0345,2.0068,1.9793,1.952,1.9249,1.898,1.8713,1.8448,1.8185,1.7924,1.7665,1.7408,1.7153,1.69,1.6649,1.64,1.6153,1.5908,1.5665,1.5424,1.5185,1.4948,1.4713,1.448,1.4249,1.402,1.3793,1.3568,1.3345,1.3124,1.2905,1.2688,1.2473,1.226,1.2049,1.184,1.1633,1.1428,1.1225,1.1024,1.0825,1.0628,1.0433,1.024,1.0049,0.986,0.9673,0.9488,0.9305,0.9124,0.8945,0.8768,0.8593,0.842,0.8249,0.808,0.7913,0.7748,0.7585,0.7424,0.7265,0.7108,0.6953,0.68,0.6649,0.65,0.6353,0.6208,0.6065,0.5924,0.5785,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3461,2.3162,2.2865,2.257,2.2277,2.1986,2.1697,2.141,2.1125,2.0842,2.0561,2.0282,2.0005,1.973,1.9457,1.9186,1.8917,1.865,1.8385,1.8122,1.7861,1.7602,1.7345,1.709,1.6837,1.6586,1.6337,1.609,1.5845,1.5602,1.5361,1.5122,1.4885,1.465,1.4417,1.4186,1.3957,1.373,1.3505,1.3282,1.3061,1.2842,1.2625,1.241,1.2197,1.1986,1.1777,1.157,1.1365,1.1162,1.0961,1.0762,1.0565,1.037,1.0177,0.9986,0.9797,0.961,0.9425,0.9242,0.9061,0.8882,0.8705,0.853,0.8357,0.8186,0.8017,0.785,0.7685,0.7522,0.7361,0.7202,0.7045,0.689,0.6737,0.6586,0.6437,0.629,0.6145,0.6002,0.5861,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.34,2.3101,2.2804,2.2509,2.2216,2.1925,2.1636,2.1349,2.1064,2.0781,2.05,2.0221,1.9944,1.9669,1.9396,1.9125,1.8856,1.8589,1.8324,1.8061,1.78,1.7541,1.7284,1.7029,1.6776,1.6525,1.6276,1.6029,1.5784,1.5541,1.53,1.5061,1.4824,1.4589,1.4356,1.4125,1.3896,1.3669,1.3444,1.3221,1.3,1.2781,1.2564,1.2349,1.2136,1.1925,1.1716,1.1509,1.1304,1.1101,1.09,1.0701,1.0504,1.0309,1.0116,0.9925,0.9736,0.9549,0.9364,0.9181,0.9,0.8821,0.8644,0.8469,0.8296,0.8125,0.7956,0.7789,0.7624,0.7461,0.73,0.7141,0.6984,0.6829,0.6676,0.6525,0.6376,0.6229,0.6084,0.5941,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3341,2.3042,2.2745,2.245,2.2157,2.1866,2.1577,2.129,2.1005,2.0722,2.0441,2.0162,1.9885,1.961,1.9337,1.9066,1.8797,1.853,1.8265,1.8002,1.7741,1.7482,1.7225,1.697,1.6717,1.6466,1.6217,1.597,1.5725,1.5482,1.5241,1.5002,1.4765,1.453,1.4297,1.4066,1.3837,1.361,1.3385,1.3162,1.2941,1.2722,1.2505,1.229,1.2077,1.1866,1.1657,1.145,1.1245,1.1042,1.0841,1.0642,1.0445,1.025,1.0057,0.9866,0.9677,0.949,0.9305,0.9122,0.8941,0.8762,0.8585,0.841,0.8237,0.8066,0.7897,0.773,0.7565,0.7402,0.7241,0.7082,0.6925,0.677,0.6617,0.6466,0.6317,0.617,0.6025,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3284,2.2985,2.2688,2.2393,2.21,2.1809,2.152,2.1233,2.0948,2.0665,2.0384,2.0105,1.9828,1.9553,1.928,1.9009,1.874,1.8473,1.8208,1.7945,1.7684,1.7425,1.7168,1.6913,1.666,1.6409,1.616,1.5913,1.5668,1.5425,1.5184,1.4945,1.4708,1.4473,1.424,1.4009,1.378,1.3553,1.3328,1.3105,1.2884,1.2665,1.2448,1.2233,1.202,1.1809,1.16,1.1393,1.1188,1.0985,1.0784,1.0585,1.0388,1.0193,1,0.9809,0.962,0.9433,0.9248,0.9065,0.8884,0.8705,0.8528,0.8353,0.818,0.8009,0.784,0.7673,0.7508,0.7345,0.7184,0.7025,0.6868,0.6713,0.656,0.6409,0.626,0.6113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3229,2.293,2.2633,2.2338,2.2045,2.1754,2.1465,2.1178,2.0893,2.061,2.0329,2.005,1.9773,1.9498,1.9225,1.8954,1.8685,1.8418,1.8153,1.789,1.7629,1.737,1.7113,1.6858,1.6605,1.6354,1.6105,1.5858,1.5613,1.537,1.5129,1.489,1.4653,1.4418,1.4185,1.3954,1.3725,1.3498,1.3273,1.305,1.2829,1.261,1.2393,1.2178,1.1965,1.1754,1.1545,1.1338,1.1133,1.093,1.0729,1.053,1.0333,1.0138,0.9945,0.9754,0.9565,0.9378,0.9193,0.901,0.8829,0.865,0.8473,0.8298,0.8125,0.7954,0.7785,0.7618,0.7453,0.729,0.7129,0.697,0.6813,0.6658,0.6505,0.6354,0.6205,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3176,2.2877,2.258,2.2285,2.1992,2.1701,2.1412,2.1125,2.084,2.0557,2.0276,1.9997,1.972,1.9445,1.9172,1.8901,1.8632,1.8365,1.81,1.7837,1.7576,1.7317,1.706,1.6805,1.6552,1.6301,1.6052,1.5805,1.556,1.5317,1.5076,1.4837,1.46,1.4365,1.4132,1.3901,1.3672,1.3445,1.322,1.2997,1.2776,1.2557,1.234,1.2125,1.1912,1.1701,1.1492,1.1285,1.108,1.0877,1.0676,1.0477,1.028,1.0085,0.9892,0.9701,0.9512,0.9325,0.914,0.8957,0.8776,0.8597,0.842,0.8245,0.8072,0.7901,0.7732,0.7565,0.74,0.7237,0.7076,0.6917,0.676,0.6605,0.6452,0.6301,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3125,2.2826,2.2529,2.2234,2.1941,2.165,2.1361,2.1074,2.0789,2.0506,2.0225,1.9946,1.9669,1.9394,1.9121,1.885,1.8581,1.8314,1.8049,1.7786,1.7525,1.7266,1.7009,1.6754,1.6501,1.625,1.6001,1.5754,1.5509,1.5266,1.5025,1.4786,1.4549,1.4314,1.4081,1.385,1.3621,1.3394,1.3169,1.2946,1.2725,1.2506,1.2289,1.2074,1.1861,1.165,1.1441,1.1234,1.1029,1.0826,1.0625,1.0426,1.0229,1.0034,0.9841,0.965,0.9461,0.9274,0.9089,0.8906,0.8725,0.8546,0.8369,0.8194,0.8021,0.785,0.7681,0.7514,0.7349,0.7186,0.7025,0.6866,0.6709,0.6554,0.6401,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3076,2.2777,2.248,2.2185,2.1892,2.1601,2.1312,2.1025,2.074,2.0457,2.0176,1.9897,1.962,1.9345,1.9072,1.8801,1.8532,1.8265,1.8,1.7737,1.7476,1.7217,1.696,1.6705,1.6452,1.6201,1.5952,1.5705,1.546,1.5217,1.4976,1.4737,1.45,1.4265,1.4032,1.3801,1.3572,1.3345,1.312,1.2897,1.2676,1.2457,1.224,1.2025,1.1812,1.1601,1.1392,1.1185,1.098,1.0777,1.0576,1.0377,1.018,0.9985,0.9792,0.9601,0.9412,0.9225,0.904,0.8857,0.8676,0.8497,0.832,0.8145,0.7972,0.7801,0.7632,0.7465,0.73,0.7137,0.6976,0.6817,0.666,0.6505,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3029,2.273,2.2433,2.2138,2.1845,2.1554,2.1265,2.0978,2.0693,2.041,2.0129,1.985,1.9573,1.9298,1.9025,1.8754,1.8485,1.8218,1.7953,1.769,1.7429,1.717,1.6913,1.6658,1.6405,1.6154,1.5905,1.5658,1.5413,1.517,1.4929,1.469,1.4453,1.4218,1.3985,1.3754,1.3525,1.3298,1.3073,1.285,1.2629,1.241,1.2193,1.1978,1.1765,1.1554,1.1345,1.1138,1.0933,1.073,1.0529,1.033,1.0133,0.9938,0.9745,0.9554,0.9365,0.9178,0.8993,0.881,0.8629,0.845,0.8273,0.8098,0.7925,0.7754,0.7585,0.7418,0.7253,0.709,0.6929,0.677,0.6613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2984,2.2685,2.2388,2.2093,2.18,2.1509,2.122,2.0933,2.0648,2.0365,2.0084,1.9805,1.9528,1.9253,1.898,1.8709,1.844,1.8173,1.7908,1.7645,1.7384,1.7125,1.6868,1.6613,1.636,1.6109,1.586,1.5613,1.5368,1.5125,1.4884,1.4645,1.4408,1.4173,1.394,1.3709,1.348,1.3253,1.3028,1.2805,1.2584,1.2365,1.2148,1.1933,1.172,1.1509,1.13,1.1093,1.0888,1.0685,1.0484,1.0285,1.0088,0.9893,0.97,0.9509,0.932,0.9133,0.8948,0.8765,0.8584,0.8405,0.8228,0.8053,0.788,0.7709,0.754,0.7373,0.7208,0.7045,0.6884,0.6725,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2941,2.2642,2.2345,2.205,2.1757,2.1466,2.1177,2.089,2.0605,2.0322,2.0041,1.9762,1.9485,1.921,1.8937,1.8666,1.8397,1.813,1.7865,1.7602,1.7341,1.7082,1.6825,1.657,1.6317,1.6066,1.5817,1.557,1.5325,1.5082,1.4841,1.4602,1.4365,1.413,1.3897,1.3666,1.3437,1.321,1.2985,1.2762,1.2541,1.2322,1.2105,1.189,1.1677,1.1466,1.1257,1.105,1.0845,1.0642,1.0441,1.0242,1.0045,0.985,0.9657,0.9466,0.9277,0.909,0.8905,0.8722,0.8541,0.8362,0.8185,0.801,0.7837,0.7666,0.7497,0.733,0.7165,0.7002,0.6841,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.29,2.2601,2.2304,2.2009,2.1716,2.1425,2.1136,2.0849,2.0564,2.0281,2,1.9721,1.9444,1.9169,1.8896,1.8625,1.8356,1.8089,1.7824,1.7561,1.73,1.7041,1.6784,1.6529,1.6276,1.6025,1.5776,1.5529,1.5284,1.5041,1.48,1.4561,1.4324,1.4089,1.3856,1.3625,1.3396,1.3169,1.2944,1.2721,1.25,1.2281,1.2064,1.1849,1.1636,1.1425,1.1216,1.1009,1.0804,1.0601,1.04,1.0201,1.0004,0.9809,0.9616,0.9425,0.9236,0.9049,0.8864,0.8681,0.85,0.8321,0.8144,0.7969,0.7796,0.7625,0.7456,0.7289,0.7124,0.6961,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2861,2.2562,2.2265,2.197,2.1677,2.1386,2.1097,2.081,2.0525,2.0242,1.9961,1.9682,1.9405,1.913,1.8857,1.8586,1.8317,1.805,1.7785,1.7522,1.7261,1.7002,1.6745,1.649,1.6237,1.5986,1.5737,1.549,1.5245,1.5002,1.4761,1.4522,1.4285,1.405,1.3817,1.3586,1.3357,1.313,1.2905,1.2682,1.2461,1.2242,1.2025,1.181,1.1597,1.1386,1.1177,1.097,1.0765,1.0562,1.0361,1.0162,0.9965,0.977,0.9577,0.9386,0.9197,0.901,0.8825,0.8642,0.8461,0.8282,0.8105,0.793,0.7757,0.7586,0.7417,0.725,0.7085,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2824,2.2525,2.2228,2.1933,2.164,2.1349,2.106,2.0773,2.0488,2.0205,1.9924,1.9645,1.9368,1.9093,1.882,1.8549,1.828,1.8013,1.7748,1.7485,1.7224,1.6965,1.6708,1.6453,1.62,1.5949,1.57,1.5453,1.5208,1.4965,1.4724,1.4485,1.4248,1.4013,1.378,1.3549,1.332,1.3093,1.2868,1.2645,1.2424,1.2205,1.1988,1.1773,1.156,1.1349,1.114,1.0933,1.0728,1.0525,1.0324,1.0125,0.9928,0.9733,0.954,0.9349,0.916,0.8973,0.8788,0.8605,0.8424,0.8245,0.8068,0.7893,0.772,0.7549,0.738,0.7213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2789,2.249,2.2193,2.1898,2.1605,2.1314,2.1025,2.0738,2.0453,2.017,1.9889,1.961,1.9333,1.9058,1.8785,1.8514,1.8245,1.7978,1.7713,1.745,1.7189,1.693,1.6673,1.6418,1.6165,1.5914,1.5665,1.5418,1.5173,1.493,1.4689,1.445,1.4213,1.3978,1.3745,1.3514,1.3285,1.3058,1.2833,1.261,1.2389,1.217,1.1953,1.1738,1.1525,1.1314,1.1105,1.0898,1.0693,1.049,1.0289,1.009,0.9893,0.9698,0.9505,0.9314,0.9125,0.8938,0.8753,0.857,0.8389,0.821,0.8033,0.7858,0.7685,0.7514,0.7345,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2756,2.2457,2.216,2.1865,2.1572,2.1281,2.0992,2.0705,2.042,2.0137,1.9856,1.9577,1.93,1.9025,1.8752,1.8481,1.8212,1.7945,1.768,1.7417,1.7156,1.6897,1.664,1.6385,1.6132,1.5881,1.5632,1.5385,1.514,1.4897,1.4656,1.4417,1.418,1.3945,1.3712,1.3481,1.3252,1.3025,1.28,1.2577,1.2356,1.2137,1.192,1.1705,1.1492,1.1281,1.1072,1.0865,1.066,1.0457,1.0256,1.0057,0.986,0.9665,0.9472,0.9281,0.9092,0.8905,0.872,0.8537,0.8356,0.8177,0.8,0.7825,0.7652,0.7481,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2725,2.2426,2.2129,2.1834,2.1541,2.125,2.0961,2.0674,2.0389,2.0106,1.9825,1.9546,1.9269,1.8994,1.8721,1.845,1.8181,1.7914,1.7649,1.7386,1.7125,1.6866,1.6609,1.6354,1.6101,1.585,1.5601,1.5354,1.5109,1.4866,1.4625,1.4386,1.4149,1.3914,1.3681,1.345,1.3221,1.2994,1.2769,1.2546,1.2325,1.2106,1.1889,1.1674,1.1461,1.125,1.1041,1.0834,1.0629,1.0426,1.0225,1.0026,0.9829,0.9634,0.9441,0.925,0.9061,0.8874,0.8689,0.8506,0.8325,0.8146,0.7969,0.7794,0.7621,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2696,2.2397,2.21,2.1805,2.1512,2.1221,2.0932,2.0645,2.036,2.0077,1.9796,1.9517,1.924,1.8965,1.8692,1.8421,1.8152,1.7885,1.762,1.7357,1.7096,1.6837,1.658,1.6325,1.6072,1.5821,1.5572,1.5325,1.508,1.4837,1.4596,1.4357,1.412,1.3885,1.3652,1.3421,1.3192,1.2965,1.274,1.2517,1.2296,1.2077,1.186,1.1645,1.1432,1.1221,1.1012,1.0805,1.06,1.0397,1.0196,0.9997,0.98,0.9605,0.9412,0.9221,0.9032,0.8845,0.866,0.8477,0.8296,0.8117,0.794,0.7765,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2669,2.237,2.2073,2.1778,2.1485,2.1194,2.0905,2.0618,2.0333,2.005,1.9769,1.949,1.9213,1.8938,1.8665,1.8394,1.8125,1.7858,1.7593,1.733,1.7069,1.681,1.6553,1.6298,1.6045,1.5794,1.5545,1.5298,1.5053,1.481,1.4569,1.433,1.4093,1.3858,1.3625,1.3394,1.3165,1.2938,1.2713,1.249,1.2269,1.205,1.1833,1.1618,1.1405,1.1194,1.0985,1.0778,1.0573,1.037,1.0169,0.997,0.9773,0.9578,0.9385,0.9194,0.9005,0.8818,0.8633,0.845,0.8269,0.809,0.7913,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2644,2.2345,2.2048,2.1753,2.146,2.1169,2.088,2.0593,2.0308,2.0025,1.9744,1.9465,1.9188,1.8913,1.864,1.8369,1.81,1.7833,1.7568,1.7305,1.7044,1.6785,1.6528,1.6273,1.602,1.5769,1.552,1.5273,1.5028,1.4785,1.4544,1.4305,1.4068,1.3833,1.36,1.3369,1.314,1.2913,1.2688,1.2465,1.2244,1.2025,1.1808,1.1593,1.138,1.1169,1.096,1.0753,1.0548,1.0345,1.0144,0.9945,0.9748,0.9553,0.936,0.9169,0.898,0.8793,0.8608,0.8425,0.8244,0.8065,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2621,2.2322,2.2025,2.173,2.1437,2.1146,2.0857,2.057,2.0285,2.0002,1.9721,1.9442,1.9165,1.889,1.8617,1.8346,1.8077,1.781,1.7545,1.7282,1.7021,1.6762,1.6505,1.625,1.5997,1.5746,1.5497,1.525,1.5005,1.4762,1.4521,1.4282,1.4045,1.381,1.3577,1.3346,1.3117,1.289,1.2665,1.2442,1.2221,1.2002,1.1785,1.157,1.1357,1.1146,1.0937,1.073,1.0525,1.0322,1.0121,0.9922,0.9725,0.953,0.9337,0.9146,0.8957,0.877,0.8585,0.8402,0.8221,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.26,2.2301,2.2004,2.1709,2.1416,2.1125,2.0836,2.0549,2.0264,1.9981,1.97,1.9421,1.9144,1.8869,1.8596,1.8325,1.8056,1.7789,1.7524,1.7261,1.7,1.6741,1.6484,1.6229,1.5976,1.5725,1.5476,1.5229,1.4984,1.4741,1.45,1.4261,1.4024,1.3789,1.3556,1.3325,1.3096,1.2869,1.2644,1.2421,1.22,1.1981,1.1764,1.1549,1.1336,1.1125,1.0916,1.0709,1.0504,1.0301,1.01,0.9901,0.9704,0.9509,0.9316,0.9125,0.8936,0.8749,0.8564,0.8381,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2581,2.2282,2.1985,2.169,2.1397,2.1106,2.0817,2.053,2.0245,1.9962,1.9681,1.9402,1.9125,1.885,1.8577,1.8306,1.8037,1.777,1.7505,1.7242,1.6981,1.6722,1.6465,1.621,1.5957,1.5706,1.5457,1.521,1.4965,1.4722,1.4481,1.4242,1.4005,1.377,1.3537,1.3306,1.3077,1.285,1.2625,1.2402,1.2181,1.1962,1.1745,1.153,1.1317,1.1106,1.0897,1.069,1.0485,1.0282,1.0081,0.9882,0.9685,0.949,0.9297,0.9106,0.8917,0.873,0.8545,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2564,2.2265,2.1968,2.1673,2.138,2.1089,2.08,2.0513,2.0228,1.9945,1.9664,1.9385,1.9108,1.8833,1.856,1.8289,1.802,1.7753,1.7488,1.7225,1.6964,1.6705,1.6448,1.6193,1.594,1.5689,1.544,1.5193,1.4948,1.4705,1.4464,1.4225,1.3988,1.3753,1.352,1.3289,1.306,1.2833,1.2608,1.2385,1.2164,1.1945,1.1728,1.1513,1.13,1.1089,1.088,1.0673,1.0468,1.0265,1.0064,0.9865,0.9668,0.9473,0.928,0.9089,0.89,0.8713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2549,2.225,2.1953,2.1658,2.1365,2.1074,2.0785,2.0498,2.0213,1.993,1.9649,1.937,1.9093,1.8818,1.8545,1.8274,1.8005,1.7738,1.7473,1.721,1.6949,1.669,1.6433,1.6178,1.5925,1.5674,1.5425,1.5178,1.4933,1.469,1.4449,1.421,1.3973,1.3738,1.3505,1.3274,1.3045,1.2818,1.2593,1.237,1.2149,1.193,1.1713,1.1498,1.1285,1.1074,1.0865,1.0658,1.0453,1.025,1.0049,0.985,0.9653,0.9458,0.9265,0.9074,0.8885,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2536,2.2237,2.194,2.1645,2.1352,2.1061,2.0772,2.0485,2.02,1.9917,1.9636,1.9357,1.908,1.8805,1.8532,1.8261,1.7992,1.7725,1.746,1.7197,1.6936,1.6677,1.642,1.6165,1.5912,1.5661,1.5412,1.5165,1.492,1.4677,1.4436,1.4197,1.396,1.3725,1.3492,1.3261,1.3032,1.2805,1.258,1.2357,1.2136,1.1917,1.17,1.1485,1.1272,1.1061,1.0852,1.0645,1.044,1.0237,1.0036,0.9837,0.964,0.9445,0.9252,0.9061,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2525,2.2226,2.1929,2.1634,2.1341,2.105,2.0761,2.0474,2.0189,1.9906,1.9625,1.9346,1.9069,1.8794,1.8521,1.825,1.7981,1.7714,1.7449,1.7186,1.6925,1.6666,1.6409,1.6154,1.5901,1.565,1.5401,1.5154,1.4909,1.4666,1.4425,1.4186,1.3949,1.3714,1.3481,1.325,1.3021,1.2794,1.2569,1.2346,1.2125,1.1906,1.1689,1.1474,1.1261,1.105,1.0841,1.0634,1.0429,1.0226,1.0025,0.9826,0.9629,0.9434,0.9241,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2516,2.2217,2.192,2.1625,2.1332,2.1041,2.0752,2.0465,2.018,1.9897,1.9616,1.9337,1.906,1.8785,1.8512,1.8241,1.7972,1.7705,1.744,1.7177,1.6916,1.6657,1.64,1.6145,1.5892,1.5641,1.5392,1.5145,1.49,1.4657,1.4416,1.4177,1.394,1.3705,1.3472,1.3241,1.3012,1.2785,1.256,1.2337,1.2116,1.1897,1.168,1.1465,1.1252,1.1041,1.0832,1.0625,1.042,1.0217,1.0016,0.9817,0.962,0.9425,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2509,2.221,2.1913,2.1618,2.1325,2.1034,2.0745,2.0458,2.0173,1.989,1.9609,1.933,1.9053,1.8778,1.8505,1.8234,1.7965,1.7698,1.7433,1.717,1.6909,1.665,1.6393,1.6138,1.5885,1.5634,1.5385,1.5138,1.4893,1.465,1.4409,1.417,1.3933,1.3698,1.3465,1.3234,1.3005,1.2778,1.2553,1.233,1.2109,1.189,1.1673,1.1458,1.1245,1.1034,1.0825,1.0618,1.0413,1.021,1.0009,0.981,0.9613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2504,2.2205,2.1908,2.1613,2.132,2.1029,2.074,2.0453,2.0168,1.9885,1.9604,1.9325,1.9048,1.8773,1.85,1.8229,1.796,1.7693,1.7428,1.7165,1.6904,1.6645,1.6388,1.6133,1.588,1.5629,1.538,1.5133,1.4888,1.4645,1.4404,1.4165,1.3928,1.3693,1.346,1.3229,1.3,1.2773,1.2548,1.2325,1.2104,1.1885,1.1668,1.1453,1.124,1.1029,1.082,1.0613,1.0408,1.0205,1.0004,0.9805,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2501,2.2202,2.1905,2.161,2.1317,2.1026,2.0737,2.045,2.0165,1.9882,1.9601,1.9322,1.9045,1.877,1.8497,1.8226,1.7957,1.769,1.7425,1.7162,1.6901,1.6642,1.6385,1.613,1.5877,1.5626,1.5377,1.513,1.4885,1.4642,1.4401,1.4162,1.3925,1.369,1.3457,1.3226,1.2997,1.277,1.2545,1.2322,1.2101,1.1882,1.1665,1.145,1.1237,1.1026,1.0817,1.061,1.0405,1.0202,1.0001,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.25,2.2201,2.1904,2.1609,2.1316,2.1025,2.0736,2.0449,2.0164,1.9881,1.96,1.9321,1.9044,1.8769,1.8496,1.8225,1.7956,1.7689,1.7424,1.7161,1.69,1.6641,1.6384,1.6129,1.5876,1.5625,1.5376,1.5129,1.4884,1.4641,1.44,1.4161,1.3924,1.3689,1.3456,1.3225,1.2996,1.2769,1.2544,1.2321,1.21,1.1881,1.1664,1.1449,1.1236,1.1025,1.0816,1.0609,1.0404,1.0201,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2501,2.2202,2.1905,2.161,2.1317,2.1026,2.0737,2.045,2.0165,1.9882,1.9601,1.9322,1.9045,1.877,1.8497,1.8226,1.7957,1.769,1.7425,1.7162,1.6901,1.6642,1.6385,1.613,1.5877,1.5626,1.5377,1.513,1.4885,1.4642,1.4401,1.4162,1.3925,1.369,1.3457,1.3226,1.2997,1.277,1.2545,1.2322,1.2101,1.1882,1.1665,1.145,1.1237,1.1026,1.0817,1.061,1.0405,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2504,2.2205,2.1908,2.1613,2.132,2.1029,2.074,2.0453,2.0168,1.9885,1.9604,1.9325,1.9048,1.8773,1.85,1.8229,1.796,1.7693,1.7428,1.7165,1.6904,1.6645,1.6388,1.6133,1.588,1.5629,1.538,1.5133,1.4888,1.4645,1.4404,1.4165,1.3928,1.3693,1.346,1.3229,1.3,1.2773,1.2548,1.2325,1.2104,1.1885,1.1668,1.1453,1.124,1.1029,1.082,1.0613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2509,2.221,2.1913,2.1618,2.1325,2.1034,2.0745,2.0458,2.0173,1.989,1.9609,1.933,1.9053,1.8778,1.8505,1.8234,1.7965,1.7698,1.7433,1.717,1.6909,1.665,1.6393,1.6138,1.5885,1.5634,1.5385,1.5138,1.4893,1.465,1.4409,1.417,1.3933,1.3698,1.3465,1.3234,1.3005,1.2778,1.2553,1.233,1.2109,1.189,1.1673,1.1458,1.1245,1.1034,1.0825,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2516,2.2217,2.192,2.1625,2.1332,2.1041,2.0752,2.0465,2.018,1.9897,1.9616,1.9337,1.906,1.8785,1.8512,1.8241,1.7972,1.7705,1.744,1.7177,1.6916,1.6657,1.64,1.6145,1.5892,1.5641,1.5392,1.5145,1.49,1.4657,1.4416,1.4177,1.394,1.3705,1.3472,1.3241,1.3012,1.2785,1.256,1.2337,1.2116,1.1897,1.168,1.1465,1.1252,1.1041,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2525,2.2226,2.1929,2.1634,2.1341,2.105,2.0761,2.0474,2.0189,1.9906,1.9625,1.9346,1.9069,1.8794,1.8521,1.825,1.7981,1.7714,1.7449,1.7186,1.6925,1.6666,1.6409,1.6154,1.5901,1.565,1.5401,1.5154,1.4909,1.4666,1.4425,1.4186,1.3949,1.3714,1.3481,1.325,1.3021,1.2794,1.2569,1.2346,1.2125,1.1906,1.1689,1.1474,1.1261,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2536,2.2237,2.194,2.1645,2.1352,2.1061,2.0772,2.0485,2.02,1.9917,1.9636,1.9357,1.908,1.8805,1.8532,1.8261,1.7992,1.7725,1.746,1.7197,1.6936,1.6677,1.642,1.6165,1.5912,1.5661,1.5412,1.5165,1.492,1.4677,1.4436,1.4197,1.396,1.3725,1.3492,1.3261,1.3032,1.2805,1.258,1.2357,1.2136,1.1917,1.17,1.1485,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2549,2.225,2.1953,2.1658,2.1365,2.1074,2.0785,2.0498,2.0213,1.993,1.9649,1.937,1.9093,1.8818,1.8545,1.8274,1.8005,1.7738,1.7473,1.721,1.6949,1.669,1.6433,1.6178,1.5925,1.5674,1.5425,1.5178,1.4933,1.469,1.4449,1.421,1.3973,1.3738,1.3505,1.3274,1.3045,1.2818,1.2593,1.237,1.2149,1.193,1.1713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2564,2.2265,2.1968,2.1673,2.138,2.1089,2.08,2.0513,2.0228,1.9945,1.9664,1.9385,1.9108,1.8833,1.856,1.8289,1.802,1.7753,1.7488,1.7225,1.6964,1.6705,1.6448,1.6193,1.594,1.5689,1.544,1.5193,1.4948,1.4705,1.4464,1.4225,1.3988,1.3753,1.352,1.3289,1.306,1.2833,1.2608,1.2385,1.2164,1.1945,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2581,2.2282,2.1985,2.169,2.1397,2.1106,2.0817,2.053,2.0245,1.9962,1.9681,1.9402,1.9125,1.885,1.8577,1.8306,1.8037,1.777,1.7505,1.7242,1.6981,1.6722,1.6465,1.621,1.5957,1.5706,1.5457,1.521,1.4965,1.4722,1.4481,1.4242,1.4005,1.377,1.3537,1.3306,1.3077,1.285,1.2625,1.2402,1.2181,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.26,2.2301,2.2004,2.1709,2.1416,2.1125,2.0836,2.0549,2.0264,1.9981,1.97,1.9421,1.9144,1.8869,1.8596,1.8325,1.8056,1.7789,1.7524,1.7261,1.7,1.6741,1.6484,1.6229,1.5976,1.5725,1.5476,1.5229,1.4984,1.4741,1.45,1.4261,1.4024,1.3789,1.3556,1.3325,1.3096,1.2869,1.2644,1.2421,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2621,2.2322,2.2025,2.173,2.1437,2.1146,2.0857,2.057,2.0285,2.0002,1.9721,1.9442,1.9165,1.889,1.8617,1.8346,1.8077,1.781,1.7545,1.7282,1.7021,1.6762,1.6505,1.625,1.5997,1.5746,1.5497,1.525,1.5005,1.4762,1.4521,1.4282,1.4045,1.381,1.3577,1.3346,1.3117,1.289,1.2665,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2644,2.2345,2.2048,2.1753,2.146,2.1169,2.088,2.0593,2.0308,2.0025,1.9744,1.9465,1.9188,1.8913,1.864,1.8369,1.81,1.7833,1.7568,1.7305,1.7044,1.6785,1.6528,1.6273,1.602,1.5769,1.552,1.5273,1.5028,1.4785,1.4544,1.4305,1.4068,1.3833,1.36,1.3369,1.314,1.2913,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2669,2.237,2.2073,2.1778,2.1485,2.1194,2.0905,2.0618,2.0333,2.005,1.9769,1.949,1.9213,1.8938,1.8665,1.8394,1.8125,1.7858,1.7593,1.733,1.7069,1.681,1.6553,1.6298,1.6045,1.5794,1.5545,1.5298,1.5053,1.481,1.4569,1.433,1.4093,1.3858,1.3625,1.3394,1.3165,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2696,2.2397,2.21,2.1805,2.1512,2.1221,2.0932,2.0645,2.036,2.0077,1.9796,1.9517,1.924,1.8965,1.8692,1.8421,1.8152,1.7885,1.762,1.7357,1.7096,1.6837,1.658,1.6325,1.6072,1.5821,1.5572,1.5325,1.508,1.4837,1.4596,1.4357,1.412,1.3885,1.3652,1.3421,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2725,2.2426,2.2129,2.1834,2.1541,2.125,2.0961,2.0674,2.0389,2.0106,1.9825,1.9546,1.9269,1.8994,1.8721,1.845,1.8181,1.7914,1.7649,1.7386,1.7125,1.6866,1.6609,1.6354,1.6101,1.585,1.5601,1.5354,1.5109,1.4866,1.4625,1.4386,1.4149,1.3914,1.3681,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2756,2.2457,2.216,2.1865,2.1572,2.1281,2.0992,2.0705,2.042,2.0137,1.9856,1.9577,1.93,1.9025,1.8752,1.8481,1.8212,1.7945,1.768,1.7417,1.7156,1.6897,1.664,1.6385,1.6132,1.5881,1.5632,1.5385,1.514,1.4897,1.4656,1.4417,1.418,1.3945,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2789,2.249,2.2193,2.1898,2.1605,2.1314,2.1025,2.0738,2.0453,2.017,1.9889,1.961,1.9333,1.9058,1.8785,1.8514,1.8245,1.7978,1.7713,1.745,1.7189,1.693,1.6673,1.6418,1.6165,1.5914,1.5665,1.5418,1.5173,1.493,1.4689,1.445,1.4213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2824,2.2525,2.2228,2.1933,2.164,2.1349,2.106,2.0773,2.0488,2.0205,1.9924,1.9645,1.9368,1.9093,1.882,1.8549,1.828,1.8013,1.7748,1.7485,1.7224,1.6965,1.6708,1.6453,1.62,1.5949,1.57,1.5453,1.5208,1.4965,1.4724,1.4485,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2861,2.2562,2.2265,2.197,2.1677,2.1386,2.1097,2.081,2.0525,2.0242,1.9961,1.9682,1.9405,1.913,1.8857,1.8586,1.8317,1.805,1.7785,1.7522,1.7261,1.7002,1.6745,1.649,1.6237,1.5986,1.5737,1.549,1.5245,1.5002,1.4761,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.29,2.2601,2.2304,2.2009,2.1716,2.1425,2.1136,2.0849,2.0564,2.0281,2,1.9721,1.9444,1.9169,1.8896,1.8625,1.8356,1.8089,1.7824,1.7561,1.73,1.7041,1.6784,1.6529,1.6276,1.6025,1.5776,1.5529,1.5284,1.5041,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2941,2.2642,2.2345,2.205,2.1757,2.1466,2.1177,2.089,2.0605,2.0322,2.0041,1.9762,1.9485,1.921,1.8937,1.8666,1.8397,1.813,1.7865,1.7602,1.7341,1.7082,1.6825,1.657,1.6317,1.6066,1.5817,1.557,1.5325,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2984,2.2685,2.2388,2.2093,2.18,2.1509,2.122,2.0933,2.0648,2.0365,2.0084,1.9805,1.9528,1.9253,1.898,1.8709,1.844,1.8173,1.7908,1.7645,1.7384,1.7125,1.6868,1.6613,1.636,1.6109,1.586,1.5613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3029,2.273,2.2433,2.2138,2.1845,2.1554,2.1265,2.0978,2.0693,2.041,2.0129,1.985,1.9573,1.9298,1.9025,1.8754,1.8485,1.8218,1.7953,1.769,1.7429,1.717,1.6913,1.6658,1.6405,1.6154,1.5905,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3076,2.2777,2.248,2.2185,2.1892,2.1601,2.1312,2.1025,2.074,2.0457,2.0176,1.9897,1.962,1.9345,1.9072,1.8801,1.8532,1.8265,1.8,1.7737,1.7476,1.7217,1.696,1.6705,1.6452,1.6201,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3125,2.2826,2.2529,2.2234,2.1941,2.165,2.1361,2.1074,2.0789,2.0506,2.0225,1.9946,1.9669,1.9394,1.9121,1.885,1.8581,1.8314,1.8049,1.7786,1.7525,1.7266,1.7009,1.6754,1.6501,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3176,2.2877,2.258,2.2285,2.1992,2.1701,2.1412,2.1125,2.084,2.0557,2.0276,1.9997,1.972,1.9445,1.9172,1.8901,1.8632,1.8365,1.81,1.7837,1.7576,1.7317,1.706,1.6805,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3229,2.293,2.2633,2.2338,2.2045,2.1754,2.1465,2.1178,2.0893,2.061,2.0329,2.005,1.9773,1.9498,1.9225,1.8954,1.8685,1.8418,1.8153,1.789,1.7629,1.737,1.7113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3284,2.2985,2.2688,2.2393,2.21,2.1809,2.152,2.1233,2.0948,2.0665,2.0384,2.0105,1.9828,1.9553,1.928,1.9009,1.874,1.8473,1.8208,1.7945,1.7684,1.7425,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3341,2.3042,2.2745,2.245,2.2157,2.1866,2.1577,2.129,2.1005,2.0722,2.0441,2.0162,1.9885,1.961,1.9337,1.9066,1.8797,1.853,1.8265,1.8002,1.7741,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.34,2.3101,2.2804,2.2509,2.2216,2.1925,2.1636,2.1349,2.1064,2.0781,2.05,2.0221,1.9944,1.9669,1.9396,1.9125,1.8856,1.8589,1.8324,1.8061,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3461,2.3162,2.2865,2.257,2.2277,2.1986,2.1697,2.141,2.1125,2.0842,2.0561,2.0282,2.0005,1.973,1.9457,1.9186,1.8917,1.865,1.8385,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3524,2.3225,2.2928,2.2633,2.234,2.2049,2.176,2.1473,2.1188,2.0905,2.0624,2.0345,2.0068,1.9793,1.952,1.9249,1.898,1.8713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3589,2.329,2.2993,2.2698,2.2405,2.2114,2.1825,2.1538,2.1253,2.097,2.0689,2.041,2.0133,1.9858,1.9585,1.9314,1.9045,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3656,2.3357,2.306,2.2765,2.2472,2.2181,2.1892,2.1605,2.132,2.1037,2.0756,2.0477,2.02,1.9925,1.9652,1.9381,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3725,2.3426,2.3129,2.2834,2.2541,2.225,2.1961,2.1674,2.1389,2.1106,2.0825,2.0546,2.0269,1.9994,1.9721,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3796,2.3497,2.32,2.2905,2.2612,2.2321,2.2032,2.1745,2.146,2.1177,2.0896,2.0617,2.034,2.0065,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3869,2.357,2.3273,2.2978,2.2685,2.2394,2.2105,2.1818,2.1533,2.125,2.0969,2.069,2.0413,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3944,2.3645,2.3348,2.3053,2.276,2.2469,2.218,2.1893,2.1608,2.1325,2.1044,2.0765,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4021,2.3722,2.3425,2.313,2.2837,2.2546,2.2257,2.197,2.1685,2.1402,2.1121,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.41,2.3801,2.3504,2.3209,2.2916,2.2625,2.2336,2.2049,2.1764,2.1481,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4181,2.3882,2.3585,2.329,2.2997,2.2706,2.2417,2.213,2.1845,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4264,2.3965,2.3668,2.3373,2.308,2.2789,2.25,2.2213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4349,2.405,2.3753,2.3458,2.3165,2.2874,2.2585,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4436,2.4137,2.384,2.3545,2.3252,2.2961,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4525,2.4226,2.3929,2.3634,2.3341,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4616,2.4317,2.402,2.3725,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4709,2.441,2.4113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4804,2.4505,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4901,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null]],"type":"surface","colorscale":"Viridis","inherit":true},"46f470c65cd6":{"x":{},"y":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"scatter3d","mode":"markers","marker":{"size":6,"color":"red","symbol":104},"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"plot_bgcolor":"rgb(254, 247, 234)","paper_bgcolor":"transparent","scene":{"xaxis":{"title":"X1"},"yaxis":{"title":"X2"},"zaxis":{"title":[{}]}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"z","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":"Viridis","showscale":true,"x":[-1.5,-1.49,-1.48,-1.47,-1.46,-1.45,-1.44,-1.43,-1.42,-1.41,-1.4,-1.39,-1.38,-1.37,-1.36,-1.35,-1.34,-1.33,-1.32,-1.31,-1.3,-1.29,-1.28,-1.27,-1.26,-1.25,-1.24,-1.23,-1.22,-1.21,-1.2,-1.19,-1.18,-1.17,-1.16,-1.15,-1.14,-1.13,-1.12,-1.11,-1.1,-1.09,-1.08,-1.07,-1.06,-1.05,-1.04,-1.03,-1.02,-1.01,-1,-0.99,-0.98,-0.97,-0.96,-0.95,-0.94,-0.93,-0.92,-0.91,-0.9,-0.89,-0.88,-0.87,-0.86,-0.85,-0.84,-0.83,-0.82,-0.81,-0.8,-0.79,-0.78,-0.77,-0.76,-0.75,-0.74,-0.73,-0.72,-0.71,-0.7,-0.69,-0.68,-0.67,-0.66,-0.65,-0.64,-0.63,-0.62,-0.61,-0.6,-0.59,-0.58,-0.57,-0.56,-0.55,-0.54,-0.53,-0.52,-0.51,-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.0999999999999999,-0.0900000000000001,-0.0800000000000001,-0.0700000000000001,-0.0600000000000001,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.0800000000000001,0.0900000000000001,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1],"y":[-1.5,-1.49,-1.48,-1.47,-1.46,-1.45,-1.44,-1.43,-1.42,-1.41,-1.4,-1.39,-1.38,-1.37,-1.36,-1.35,-1.34,-1.33,-1.32,-1.31,-1.3,-1.29,-1.28,-1.27,-1.26,-1.25,-1.24,-1.23,-1.22,-1.21,-1.2,-1.19,-1.18,-1.17,-1.16,-1.15,-1.14,-1.13,-1.12,-1.11,-1.1,-1.09,-1.08,-1.07,-1.06,-1.05,-1.04,-1.03,-1.02,-1.01,-1,-0.99,-0.98,-0.97,-0.96,-0.95,-0.94,-0.93,-0.92,-0.91,-0.9,-0.89,-0.88,-0.87,-0.86,-0.85,-0.84,-0.83,-0.82,-0.81,-0.8,-0.79,-0.78,-0.77,-0.76,-0.75,-0.74,-0.73,-0.72,-0.71,-0.7,-0.69,-0.68,-0.67,-0.66,-0.65,-0.64,-0.63,-0.62,-0.61,-0.6,-0.59,-0.58,-0.57,-0.56,-0.55,-0.54,-0.53,-0.52,-0.51,-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.0999999999999999,-0.0900000000000001,-0.0800000000000001,-0.0700000000000001,-0.0600000000000001,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.0800000000000001,0.0900000000000001,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1],"z":[[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.4901,2.4804,2.4709,2.4616,2.4525,2.4436,2.4349,2.4264,2.4181,2.41,2.4021,2.3944,2.3869,2.3796,2.3725,2.3656,2.3589,2.3524,2.3461,2.34,2.3341,2.3284,2.3229,2.3176,2.3125,2.3076,2.3029,2.2984,2.2941,2.29,2.2861,2.2824,2.2789,2.2756,2.2725,2.2696,2.2669,2.2644,2.2621,2.26,2.2581,2.2564,2.2549,2.2536,2.2525,2.2516,2.2509,2.2504,2.2501,2.25,2.2501,2.2504,2.2509,2.2516,2.2525,2.2536,2.2549,2.2564,2.2581,2.26,2.2621,2.2644,2.2669,2.2696,2.2725,2.2756,2.2789,2.2824,2.2861,2.29,2.2941,2.2984,2.3029,2.3076,2.3125,2.3176,2.3229,2.3284,2.3341,2.34,2.3461,2.3524,2.3589,2.3656,2.3725,2.3796,2.3869,2.3944,2.4021,2.41,2.4181,2.4264,2.4349,2.4436,2.4525,2.4616,2.4709,2.4804,2.4901,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.4701,2.4602,2.4505,2.441,2.4317,2.4226,2.4137,2.405,2.3965,2.3882,2.3801,2.3722,2.3645,2.357,2.3497,2.3426,2.3357,2.329,2.3225,2.3162,2.3101,2.3042,2.2985,2.293,2.2877,2.2826,2.2777,2.273,2.2685,2.2642,2.2601,2.2562,2.2525,2.249,2.2457,2.2426,2.2397,2.237,2.2345,2.2322,2.2301,2.2282,2.2265,2.225,2.2237,2.2226,2.2217,2.221,2.2205,2.2202,2.2201,2.2202,2.2205,2.221,2.2217,2.2226,2.2237,2.225,2.2265,2.2282,2.2301,2.2322,2.2345,2.237,2.2397,2.2426,2.2457,2.249,2.2525,2.2562,2.2601,2.2642,2.2685,2.273,2.2777,2.2826,2.2877,2.293,2.2985,2.3042,2.3101,2.3162,2.3225,2.329,2.3357,2.3426,2.3497,2.357,2.3645,2.3722,2.3801,2.3882,2.3965,2.405,2.4137,2.4226,2.4317,2.441,2.4505,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.4505,2.4404,2.4305,2.4208,2.4113,2.402,2.3929,2.384,2.3753,2.3668,2.3585,2.3504,2.3425,2.3348,2.3273,2.32,2.3129,2.306,2.2993,2.2928,2.2865,2.2804,2.2745,2.2688,2.2633,2.258,2.2529,2.248,2.2433,2.2388,2.2345,2.2304,2.2265,2.2228,2.2193,2.216,2.2129,2.21,2.2073,2.2048,2.2025,2.2004,2.1985,2.1968,2.1953,2.194,2.1929,2.192,2.1913,2.1908,2.1905,2.1904,2.1905,2.1908,2.1913,2.192,2.1929,2.194,2.1953,2.1968,2.1985,2.2004,2.2025,2.2048,2.2073,2.21,2.2129,2.216,2.2193,2.2228,2.2265,2.2304,2.2345,2.2388,2.2433,2.248,2.2529,2.258,2.2633,2.2688,2.2745,2.2804,2.2865,2.2928,2.2993,2.306,2.3129,2.32,2.3273,2.3348,2.3425,2.3504,2.3585,2.3668,2.3753,2.384,2.3929,2.402,2.4113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.4313,2.421,2.4109,2.401,2.3913,2.3818,2.3725,2.3634,2.3545,2.3458,2.3373,2.329,2.3209,2.313,2.3053,2.2978,2.2905,2.2834,2.2765,2.2698,2.2633,2.257,2.2509,2.245,2.2393,2.2338,2.2285,2.2234,2.2185,2.2138,2.2093,2.205,2.2009,2.197,2.1933,2.1898,2.1865,2.1834,2.1805,2.1778,2.1753,2.173,2.1709,2.169,2.1673,2.1658,2.1645,2.1634,2.1625,2.1618,2.1613,2.161,2.1609,2.161,2.1613,2.1618,2.1625,2.1634,2.1645,2.1658,2.1673,2.169,2.1709,2.173,2.1753,2.1778,2.1805,2.1834,2.1865,2.1898,2.1933,2.197,2.2009,2.205,2.2093,2.2138,2.2185,2.2234,2.2285,2.2338,2.2393,2.245,2.2509,2.257,2.2633,2.2698,2.2765,2.2834,2.2905,2.2978,2.3053,2.313,2.3209,2.329,2.3373,2.3458,2.3545,2.3634,2.3725,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.4125,2.402,2.3917,2.3816,2.3717,2.362,2.3525,2.3432,2.3341,2.3252,2.3165,2.308,2.2997,2.2916,2.2837,2.276,2.2685,2.2612,2.2541,2.2472,2.2405,2.234,2.2277,2.2216,2.2157,2.21,2.2045,2.1992,2.1941,2.1892,2.1845,2.18,2.1757,2.1716,2.1677,2.164,2.1605,2.1572,2.1541,2.1512,2.1485,2.146,2.1437,2.1416,2.1397,2.138,2.1365,2.1352,2.1341,2.1332,2.1325,2.132,2.1317,2.1316,2.1317,2.132,2.1325,2.1332,2.1341,2.1352,2.1365,2.138,2.1397,2.1416,2.1437,2.146,2.1485,2.1512,2.1541,2.1572,2.1605,2.164,2.1677,2.1716,2.1757,2.18,2.1845,2.1892,2.1941,2.1992,2.2045,2.21,2.2157,2.2216,2.2277,2.234,2.2405,2.2472,2.2541,2.2612,2.2685,2.276,2.2837,2.2916,2.2997,2.308,2.3165,2.3252,2.3341,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.3941,2.3834,2.3729,2.3626,2.3525,2.3426,2.3329,2.3234,2.3141,2.305,2.2961,2.2874,2.2789,2.2706,2.2625,2.2546,2.2469,2.2394,2.2321,2.225,2.2181,2.2114,2.2049,2.1986,2.1925,2.1866,2.1809,2.1754,2.1701,2.165,2.1601,2.1554,2.1509,2.1466,2.1425,2.1386,2.1349,2.1314,2.1281,2.125,2.1221,2.1194,2.1169,2.1146,2.1125,2.1106,2.1089,2.1074,2.1061,2.105,2.1041,2.1034,2.1029,2.1026,2.1025,2.1026,2.1029,2.1034,2.1041,2.105,2.1061,2.1074,2.1089,2.1106,2.1125,2.1146,2.1169,2.1194,2.1221,2.125,2.1281,2.1314,2.1349,2.1386,2.1425,2.1466,2.1509,2.1554,2.1601,2.165,2.1701,2.1754,2.1809,2.1866,2.1925,2.1986,2.2049,2.2114,2.2181,2.225,2.2321,2.2394,2.2469,2.2546,2.2625,2.2706,2.2789,2.2874,2.2961,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.3761,2.3652,2.3545,2.344,2.3337,2.3236,2.3137,2.304,2.2945,2.2852,2.2761,2.2672,2.2585,2.25,2.2417,2.2336,2.2257,2.218,2.2105,2.2032,2.1961,2.1892,2.1825,2.176,2.1697,2.1636,2.1577,2.152,2.1465,2.1412,2.1361,2.1312,2.1265,2.122,2.1177,2.1136,2.1097,2.106,2.1025,2.0992,2.0961,2.0932,2.0905,2.088,2.0857,2.0836,2.0817,2.08,2.0785,2.0772,2.0761,2.0752,2.0745,2.074,2.0737,2.0736,2.0737,2.074,2.0745,2.0752,2.0761,2.0772,2.0785,2.08,2.0817,2.0836,2.0857,2.088,2.0905,2.0932,2.0961,2.0992,2.1025,2.106,2.1097,2.1136,2.1177,2.122,2.1265,2.1312,2.1361,2.1412,2.1465,2.152,2.1577,2.1636,2.1697,2.176,2.1825,2.1892,2.1961,2.2032,2.2105,2.218,2.2257,2.2336,2.2417,2.25,2.2585,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.3585,2.3474,2.3365,2.3258,2.3153,2.305,2.2949,2.285,2.2753,2.2658,2.2565,2.2474,2.2385,2.2298,2.2213,2.213,2.2049,2.197,2.1893,2.1818,2.1745,2.1674,2.1605,2.1538,2.1473,2.141,2.1349,2.129,2.1233,2.1178,2.1125,2.1074,2.1025,2.0978,2.0933,2.089,2.0849,2.081,2.0773,2.0738,2.0705,2.0674,2.0645,2.0618,2.0593,2.057,2.0549,2.053,2.0513,2.0498,2.0485,2.0474,2.0465,2.0458,2.0453,2.045,2.0449,2.045,2.0453,2.0458,2.0465,2.0474,2.0485,2.0498,2.0513,2.053,2.0549,2.057,2.0593,2.0618,2.0645,2.0674,2.0705,2.0738,2.0773,2.081,2.0849,2.089,2.0933,2.0978,2.1025,2.1074,2.1125,2.1178,2.1233,2.129,2.1349,2.141,2.1473,2.1538,2.1605,2.1674,2.1745,2.1818,2.1893,2.197,2.2049,2.213,2.2213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.3413,2.33,2.3189,2.308,2.2973,2.2868,2.2765,2.2664,2.2565,2.2468,2.2373,2.228,2.2189,2.21,2.2013,2.1928,2.1845,2.1764,2.1685,2.1608,2.1533,2.146,2.1389,2.132,2.1253,2.1188,2.1125,2.1064,2.1005,2.0948,2.0893,2.084,2.0789,2.074,2.0693,2.0648,2.0605,2.0564,2.0525,2.0488,2.0453,2.042,2.0389,2.036,2.0333,2.0308,2.0285,2.0264,2.0245,2.0228,2.0213,2.02,2.0189,2.018,2.0173,2.0168,2.0165,2.0164,2.0165,2.0168,2.0173,2.018,2.0189,2.02,2.0213,2.0228,2.0245,2.0264,2.0285,2.0308,2.0333,2.036,2.0389,2.042,2.0453,2.0488,2.0525,2.0564,2.0605,2.0648,2.0693,2.074,2.0789,2.084,2.0893,2.0948,2.1005,2.1064,2.1125,2.1188,2.1253,2.132,2.1389,2.146,2.1533,2.1608,2.1685,2.1764,2.1845,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.3245,2.313,2.3017,2.2906,2.2797,2.269,2.2585,2.2482,2.2381,2.2282,2.2185,2.209,2.1997,2.1906,2.1817,2.173,2.1645,2.1562,2.1481,2.1402,2.1325,2.125,2.1177,2.1106,2.1037,2.097,2.0905,2.0842,2.0781,2.0722,2.0665,2.061,2.0557,2.0506,2.0457,2.041,2.0365,2.0322,2.0281,2.0242,2.0205,2.017,2.0137,2.0106,2.0077,2.005,2.0025,2.0002,1.9981,1.9962,1.9945,1.993,1.9917,1.9906,1.9897,1.989,1.9885,1.9882,1.9881,1.9882,1.9885,1.989,1.9897,1.9906,1.9917,1.993,1.9945,1.9962,1.9981,2.0002,2.0025,2.005,2.0077,2.0106,2.0137,2.017,2.0205,2.0242,2.0281,2.0322,2.0365,2.041,2.0457,2.0506,2.0557,2.061,2.0665,2.0722,2.0781,2.0842,2.0905,2.097,2.1037,2.1106,2.1177,2.125,2.1325,2.1402,2.1481,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.3081,2.2964,2.2849,2.2736,2.2625,2.2516,2.2409,2.2304,2.2201,2.21,2.2001,2.1904,2.1809,2.1716,2.1625,2.1536,2.1449,2.1364,2.1281,2.12,2.1121,2.1044,2.0969,2.0896,2.0825,2.0756,2.0689,2.0624,2.0561,2.05,2.0441,2.0384,2.0329,2.0276,2.0225,2.0176,2.0129,2.0084,2.0041,2,1.9961,1.9924,1.9889,1.9856,1.9825,1.9796,1.9769,1.9744,1.9721,1.97,1.9681,1.9664,1.9649,1.9636,1.9625,1.9616,1.9609,1.9604,1.9601,1.96,1.9601,1.9604,1.9609,1.9616,1.9625,1.9636,1.9649,1.9664,1.9681,1.97,1.9721,1.9744,1.9769,1.9796,1.9825,1.9856,1.9889,1.9924,1.9961,2,2.0041,2.0084,2.0129,2.0176,2.0225,2.0276,2.0329,2.0384,2.0441,2.05,2.0561,2.0624,2.0689,2.0756,2.0825,2.0896,2.0969,2.1044,2.1121,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2921,2.2802,2.2685,2.257,2.2457,2.2346,2.2237,2.213,2.2025,2.1922,2.1821,2.1722,2.1625,2.153,2.1437,2.1346,2.1257,2.117,2.1085,2.1002,2.0921,2.0842,2.0765,2.069,2.0617,2.0546,2.0477,2.041,2.0345,2.0282,2.0221,2.0162,2.0105,2.005,1.9997,1.9946,1.9897,1.985,1.9805,1.9762,1.9721,1.9682,1.9645,1.961,1.9577,1.9546,1.9517,1.949,1.9465,1.9442,1.9421,1.9402,1.9385,1.937,1.9357,1.9346,1.9337,1.933,1.9325,1.9322,1.9321,1.9322,1.9325,1.933,1.9337,1.9346,1.9357,1.937,1.9385,1.9402,1.9421,1.9442,1.9465,1.949,1.9517,1.9546,1.9577,1.961,1.9645,1.9682,1.9721,1.9762,1.9805,1.985,1.9897,1.9946,1.9997,2.005,2.0105,2.0162,2.0221,2.0282,2.0345,2.041,2.0477,2.0546,2.0617,2.069,2.0765,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2765,2.2644,2.2525,2.2408,2.2293,2.218,2.2069,2.196,2.1853,2.1748,2.1645,2.1544,2.1445,2.1348,2.1253,2.116,2.1069,2.098,2.0893,2.0808,2.0725,2.0644,2.0565,2.0488,2.0413,2.034,2.0269,2.02,2.0133,2.0068,2.0005,1.9944,1.9885,1.9828,1.9773,1.972,1.9669,1.962,1.9573,1.9528,1.9485,1.9444,1.9405,1.9368,1.9333,1.93,1.9269,1.924,1.9213,1.9188,1.9165,1.9144,1.9125,1.9108,1.9093,1.908,1.9069,1.906,1.9053,1.9048,1.9045,1.9044,1.9045,1.9048,1.9053,1.906,1.9069,1.908,1.9093,1.9108,1.9125,1.9144,1.9165,1.9188,1.9213,1.924,1.9269,1.93,1.9333,1.9368,1.9405,1.9444,1.9485,1.9528,1.9573,1.962,1.9669,1.972,1.9773,1.9828,1.9885,1.9944,2.0005,2.0068,2.0133,2.02,2.0269,2.034,2.0413,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2613,2.249,2.2369,2.225,2.2133,2.2018,2.1905,2.1794,2.1685,2.1578,2.1473,2.137,2.1269,2.117,2.1073,2.0978,2.0885,2.0794,2.0705,2.0618,2.0533,2.045,2.0369,2.029,2.0213,2.0138,2.0065,1.9994,1.9925,1.9858,1.9793,1.973,1.9669,1.961,1.9553,1.9498,1.9445,1.9394,1.9345,1.9298,1.9253,1.921,1.9169,1.913,1.9093,1.9058,1.9025,1.8994,1.8965,1.8938,1.8913,1.889,1.8869,1.885,1.8833,1.8818,1.8805,1.8794,1.8785,1.8778,1.8773,1.877,1.8769,1.877,1.8773,1.8778,1.8785,1.8794,1.8805,1.8818,1.8833,1.885,1.8869,1.889,1.8913,1.8938,1.8965,1.8994,1.9025,1.9058,1.9093,1.913,1.9169,1.921,1.9253,1.9298,1.9345,1.9394,1.9445,1.9498,1.9553,1.961,1.9669,1.973,1.9793,1.9858,1.9925,1.9994,2.0065,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2465,2.234,2.2217,2.2096,2.1977,2.186,2.1745,2.1632,2.1521,2.1412,2.1305,2.12,2.1097,2.0996,2.0897,2.08,2.0705,2.0612,2.0521,2.0432,2.0345,2.026,2.0177,2.0096,2.0017,1.994,1.9865,1.9792,1.9721,1.9652,1.9585,1.952,1.9457,1.9396,1.9337,1.928,1.9225,1.9172,1.9121,1.9072,1.9025,1.898,1.8937,1.8896,1.8857,1.882,1.8785,1.8752,1.8721,1.8692,1.8665,1.864,1.8617,1.8596,1.8577,1.856,1.8545,1.8532,1.8521,1.8512,1.8505,1.85,1.8497,1.8496,1.8497,1.85,1.8505,1.8512,1.8521,1.8532,1.8545,1.856,1.8577,1.8596,1.8617,1.864,1.8665,1.8692,1.8721,1.8752,1.8785,1.882,1.8857,1.8896,1.8937,1.898,1.9025,1.9072,1.9121,1.9172,1.9225,1.928,1.9337,1.9396,1.9457,1.952,1.9585,1.9652,1.9721,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2321,2.2194,2.2069,2.1946,2.1825,2.1706,2.1589,2.1474,2.1361,2.125,2.1141,2.1034,2.0929,2.0826,2.0725,2.0626,2.0529,2.0434,2.0341,2.025,2.0161,2.0074,1.9989,1.9906,1.9825,1.9746,1.9669,1.9594,1.9521,1.945,1.9381,1.9314,1.9249,1.9186,1.9125,1.9066,1.9009,1.8954,1.8901,1.885,1.8801,1.8754,1.8709,1.8666,1.8625,1.8586,1.8549,1.8514,1.8481,1.845,1.8421,1.8394,1.8369,1.8346,1.8325,1.8306,1.8289,1.8274,1.8261,1.825,1.8241,1.8234,1.8229,1.8226,1.8225,1.8226,1.8229,1.8234,1.8241,1.825,1.8261,1.8274,1.8289,1.8306,1.8325,1.8346,1.8369,1.8394,1.8421,1.845,1.8481,1.8514,1.8549,1.8586,1.8625,1.8666,1.8709,1.8754,1.8801,1.885,1.8901,1.8954,1.9009,1.9066,1.9125,1.9186,1.9249,1.9314,1.9381,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2181,2.2052,2.1925,2.18,2.1677,2.1556,2.1437,2.132,2.1205,2.1092,2.0981,2.0872,2.0765,2.066,2.0557,2.0456,2.0357,2.026,2.0165,2.0072,1.9981,1.9892,1.9805,1.972,1.9637,1.9556,1.9477,1.94,1.9325,1.9252,1.9181,1.9112,1.9045,1.898,1.8917,1.8856,1.8797,1.874,1.8685,1.8632,1.8581,1.8532,1.8485,1.844,1.8397,1.8356,1.8317,1.828,1.8245,1.8212,1.8181,1.8152,1.8125,1.81,1.8077,1.8056,1.8037,1.802,1.8005,1.7992,1.7981,1.7972,1.7965,1.796,1.7957,1.7956,1.7957,1.796,1.7965,1.7972,1.7981,1.7992,1.8005,1.802,1.8037,1.8056,1.8077,1.81,1.8125,1.8152,1.8181,1.8212,1.8245,1.828,1.8317,1.8356,1.8397,1.844,1.8485,1.8532,1.8581,1.8632,1.8685,1.874,1.8797,1.8856,1.8917,1.898,1.9045,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2045,2.1914,2.1785,2.1658,2.1533,2.141,2.1289,2.117,2.1053,2.0938,2.0825,2.0714,2.0605,2.0498,2.0393,2.029,2.0189,2.009,1.9993,1.9898,1.9805,1.9714,1.9625,1.9538,1.9453,1.937,1.9289,1.921,1.9133,1.9058,1.8985,1.8914,1.8845,1.8778,1.8713,1.865,1.8589,1.853,1.8473,1.8418,1.8365,1.8314,1.8265,1.8218,1.8173,1.813,1.8089,1.805,1.8013,1.7978,1.7945,1.7914,1.7885,1.7858,1.7833,1.781,1.7789,1.777,1.7753,1.7738,1.7725,1.7714,1.7705,1.7698,1.7693,1.769,1.7689,1.769,1.7693,1.7698,1.7705,1.7714,1.7725,1.7738,1.7753,1.777,1.7789,1.781,1.7833,1.7858,1.7885,1.7914,1.7945,1.7978,1.8013,1.805,1.8089,1.813,1.8173,1.8218,1.8265,1.8314,1.8365,1.8418,1.8473,1.853,1.8589,1.865,1.8713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1913,2.178,2.1649,2.152,2.1393,2.1268,2.1145,2.1024,2.0905,2.0788,2.0673,2.056,2.0449,2.034,2.0233,2.0128,2.0025,1.9924,1.9825,1.9728,1.9633,1.954,1.9449,1.936,1.9273,1.9188,1.9105,1.9024,1.8945,1.8868,1.8793,1.872,1.8649,1.858,1.8513,1.8448,1.8385,1.8324,1.8265,1.8208,1.8153,1.81,1.8049,1.8,1.7953,1.7908,1.7865,1.7824,1.7785,1.7748,1.7713,1.768,1.7649,1.762,1.7593,1.7568,1.7545,1.7524,1.7505,1.7488,1.7473,1.746,1.7449,1.744,1.7433,1.7428,1.7425,1.7424,1.7425,1.7428,1.7433,1.744,1.7449,1.746,1.7473,1.7488,1.7505,1.7524,1.7545,1.7568,1.7593,1.762,1.7649,1.768,1.7713,1.7748,1.7785,1.7824,1.7865,1.7908,1.7953,1.8,1.8049,1.81,1.8153,1.8208,1.8265,1.8324,1.8385,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1785,2.165,2.1517,2.1386,2.1257,2.113,2.1005,2.0882,2.0761,2.0642,2.0525,2.041,2.0297,2.0186,2.0077,1.997,1.9865,1.9762,1.9661,1.9562,1.9465,1.937,1.9277,1.9186,1.9097,1.901,1.8925,1.8842,1.8761,1.8682,1.8605,1.853,1.8457,1.8386,1.8317,1.825,1.8185,1.8122,1.8061,1.8002,1.7945,1.789,1.7837,1.7786,1.7737,1.769,1.7645,1.7602,1.7561,1.7522,1.7485,1.745,1.7417,1.7386,1.7357,1.733,1.7305,1.7282,1.7261,1.7242,1.7225,1.721,1.7197,1.7186,1.7177,1.717,1.7165,1.7162,1.7161,1.7162,1.7165,1.717,1.7177,1.7186,1.7197,1.721,1.7225,1.7242,1.7261,1.7282,1.7305,1.733,1.7357,1.7386,1.7417,1.745,1.7485,1.7522,1.7561,1.7602,1.7645,1.769,1.7737,1.7786,1.7837,1.789,1.7945,1.8002,1.8061,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1661,2.1524,2.1389,2.1256,2.1125,2.0996,2.0869,2.0744,2.0621,2.05,2.0381,2.0264,2.0149,2.0036,1.9925,1.9816,1.9709,1.9604,1.9501,1.94,1.9301,1.9204,1.9109,1.9016,1.8925,1.8836,1.8749,1.8664,1.8581,1.85,1.8421,1.8344,1.8269,1.8196,1.8125,1.8056,1.7989,1.7924,1.7861,1.78,1.7741,1.7684,1.7629,1.7576,1.7525,1.7476,1.7429,1.7384,1.7341,1.73,1.7261,1.7224,1.7189,1.7156,1.7125,1.7096,1.7069,1.7044,1.7021,1.7,1.6981,1.6964,1.6949,1.6936,1.6925,1.6916,1.6909,1.6904,1.6901,1.69,1.6901,1.6904,1.6909,1.6916,1.6925,1.6936,1.6949,1.6964,1.6981,1.7,1.7021,1.7044,1.7069,1.7096,1.7125,1.7156,1.7189,1.7224,1.7261,1.73,1.7341,1.7384,1.7429,1.7476,1.7525,1.7576,1.7629,1.7684,1.7741,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1541,2.1402,2.1265,2.113,2.0997,2.0866,2.0737,2.061,2.0485,2.0362,2.0241,2.0122,2.0005,1.989,1.9777,1.9666,1.9557,1.945,1.9345,1.9242,1.9141,1.9042,1.8945,1.885,1.8757,1.8666,1.8577,1.849,1.8405,1.8322,1.8241,1.8162,1.8085,1.801,1.7937,1.7866,1.7797,1.773,1.7665,1.7602,1.7541,1.7482,1.7425,1.737,1.7317,1.7266,1.7217,1.717,1.7125,1.7082,1.7041,1.7002,1.6965,1.693,1.6897,1.6866,1.6837,1.681,1.6785,1.6762,1.6741,1.6722,1.6705,1.669,1.6677,1.6666,1.6657,1.665,1.6645,1.6642,1.6641,1.6642,1.6645,1.665,1.6657,1.6666,1.6677,1.669,1.6705,1.6722,1.6741,1.6762,1.6785,1.681,1.6837,1.6866,1.6897,1.693,1.6965,1.7002,1.7041,1.7082,1.7125,1.717,1.7217,1.7266,1.7317,1.737,1.7425,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1425,2.1284,2.1145,2.1008,2.0873,2.074,2.0609,2.048,2.0353,2.0228,2.0105,1.9984,1.9865,1.9748,1.9633,1.952,1.9409,1.93,1.9193,1.9088,1.8985,1.8884,1.8785,1.8688,1.8593,1.85,1.8409,1.832,1.8233,1.8148,1.8065,1.7984,1.7905,1.7828,1.7753,1.768,1.7609,1.754,1.7473,1.7408,1.7345,1.7284,1.7225,1.7168,1.7113,1.706,1.7009,1.696,1.6913,1.6868,1.6825,1.6784,1.6745,1.6708,1.6673,1.664,1.6609,1.658,1.6553,1.6528,1.6505,1.6484,1.6465,1.6448,1.6433,1.642,1.6409,1.64,1.6393,1.6388,1.6385,1.6384,1.6385,1.6388,1.6393,1.64,1.6409,1.642,1.6433,1.6448,1.6465,1.6484,1.6505,1.6528,1.6553,1.658,1.6609,1.664,1.6673,1.6708,1.6745,1.6784,1.6825,1.6868,1.6913,1.696,1.7009,1.706,1.7113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1313,2.117,2.1029,2.089,2.0753,2.0618,2.0485,2.0354,2.0225,2.0098,1.9973,1.985,1.9729,1.961,1.9493,1.9378,1.9265,1.9154,1.9045,1.8938,1.8833,1.873,1.8629,1.853,1.8433,1.8338,1.8245,1.8154,1.8065,1.7978,1.7893,1.781,1.7729,1.765,1.7573,1.7498,1.7425,1.7354,1.7285,1.7218,1.7153,1.709,1.7029,1.697,1.6913,1.6858,1.6805,1.6754,1.6705,1.6658,1.6613,1.657,1.6529,1.649,1.6453,1.6418,1.6385,1.6354,1.6325,1.6298,1.6273,1.625,1.6229,1.621,1.6193,1.6178,1.6165,1.6154,1.6145,1.6138,1.6133,1.613,1.6129,1.613,1.6133,1.6138,1.6145,1.6154,1.6165,1.6178,1.6193,1.621,1.6229,1.625,1.6273,1.6298,1.6325,1.6354,1.6385,1.6418,1.6453,1.649,1.6529,1.657,1.6613,1.6658,1.6705,1.6754,1.6805,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1205,2.106,2.0917,2.0776,2.0637,2.05,2.0365,2.0232,2.0101,1.9972,1.9845,1.972,1.9597,1.9476,1.9357,1.924,1.9125,1.9012,1.8901,1.8792,1.8685,1.858,1.8477,1.8376,1.8277,1.818,1.8085,1.7992,1.7901,1.7812,1.7725,1.764,1.7557,1.7476,1.7397,1.732,1.7245,1.7172,1.7101,1.7032,1.6965,1.69,1.6837,1.6776,1.6717,1.666,1.6605,1.6552,1.6501,1.6452,1.6405,1.636,1.6317,1.6276,1.6237,1.62,1.6165,1.6132,1.6101,1.6072,1.6045,1.602,1.5997,1.5976,1.5957,1.594,1.5925,1.5912,1.5901,1.5892,1.5885,1.588,1.5877,1.5876,1.5877,1.588,1.5885,1.5892,1.5901,1.5912,1.5925,1.594,1.5957,1.5976,1.5997,1.602,1.6045,1.6072,1.6101,1.6132,1.6165,1.62,1.6237,1.6276,1.6317,1.636,1.6405,1.6452,1.6501,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1101,2.0954,2.0809,2.0666,2.0525,2.0386,2.0249,2.0114,1.9981,1.985,1.9721,1.9594,1.9469,1.9346,1.9225,1.9106,1.8989,1.8874,1.8761,1.865,1.8541,1.8434,1.8329,1.8226,1.8125,1.8026,1.7929,1.7834,1.7741,1.765,1.7561,1.7474,1.7389,1.7306,1.7225,1.7146,1.7069,1.6994,1.6921,1.685,1.6781,1.6714,1.6649,1.6586,1.6525,1.6466,1.6409,1.6354,1.6301,1.625,1.6201,1.6154,1.6109,1.6066,1.6025,1.5986,1.5949,1.5914,1.5881,1.585,1.5821,1.5794,1.5769,1.5746,1.5725,1.5706,1.5689,1.5674,1.5661,1.565,1.5641,1.5634,1.5629,1.5626,1.5625,1.5626,1.5629,1.5634,1.5641,1.565,1.5661,1.5674,1.5689,1.5706,1.5725,1.5746,1.5769,1.5794,1.5821,1.585,1.5881,1.5914,1.5949,1.5986,1.6025,1.6066,1.6109,1.6154,1.6201,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1001,2.0852,2.0705,2.056,2.0417,2.0276,2.0137,2,1.9865,1.9732,1.9601,1.9472,1.9345,1.922,1.9097,1.8976,1.8857,1.874,1.8625,1.8512,1.8401,1.8292,1.8185,1.808,1.7977,1.7876,1.7777,1.768,1.7585,1.7492,1.7401,1.7312,1.7225,1.714,1.7057,1.6976,1.6897,1.682,1.6745,1.6672,1.6601,1.6532,1.6465,1.64,1.6337,1.6276,1.6217,1.616,1.6105,1.6052,1.6001,1.5952,1.5905,1.586,1.5817,1.5776,1.5737,1.57,1.5665,1.5632,1.5601,1.5572,1.5545,1.552,1.5497,1.5476,1.5457,1.544,1.5425,1.5412,1.5401,1.5392,1.5385,1.538,1.5377,1.5376,1.5377,1.538,1.5385,1.5392,1.5401,1.5412,1.5425,1.544,1.5457,1.5476,1.5497,1.552,1.5545,1.5572,1.5601,1.5632,1.5665,1.57,1.5737,1.5776,1.5817,1.586,1.5905,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0905,2.0754,2.0605,2.0458,2.0313,2.017,2.0029,1.989,1.9753,1.9618,1.9485,1.9354,1.9225,1.9098,1.8973,1.885,1.8729,1.861,1.8493,1.8378,1.8265,1.8154,1.8045,1.7938,1.7833,1.773,1.7629,1.753,1.7433,1.7338,1.7245,1.7154,1.7065,1.6978,1.6893,1.681,1.6729,1.665,1.6573,1.6498,1.6425,1.6354,1.6285,1.6218,1.6153,1.609,1.6029,1.597,1.5913,1.5858,1.5805,1.5754,1.5705,1.5658,1.5613,1.557,1.5529,1.549,1.5453,1.5418,1.5385,1.5354,1.5325,1.5298,1.5273,1.525,1.5229,1.521,1.5193,1.5178,1.5165,1.5154,1.5145,1.5138,1.5133,1.513,1.5129,1.513,1.5133,1.5138,1.5145,1.5154,1.5165,1.5178,1.5193,1.521,1.5229,1.525,1.5273,1.5298,1.5325,1.5354,1.5385,1.5418,1.5453,1.549,1.5529,1.557,1.5613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0813,2.066,2.0509,2.036,2.0213,2.0068,1.9925,1.9784,1.9645,1.9508,1.9373,1.924,1.9109,1.898,1.8853,1.8728,1.8605,1.8484,1.8365,1.8248,1.8133,1.802,1.7909,1.78,1.7693,1.7588,1.7485,1.7384,1.7285,1.7188,1.7093,1.7,1.6909,1.682,1.6733,1.6648,1.6565,1.6484,1.6405,1.6328,1.6253,1.618,1.6109,1.604,1.5973,1.5908,1.5845,1.5784,1.5725,1.5668,1.5613,1.556,1.5509,1.546,1.5413,1.5368,1.5325,1.5284,1.5245,1.5208,1.5173,1.514,1.5109,1.508,1.5053,1.5028,1.5005,1.4984,1.4965,1.4948,1.4933,1.492,1.4909,1.49,1.4893,1.4888,1.4885,1.4884,1.4885,1.4888,1.4893,1.49,1.4909,1.492,1.4933,1.4948,1.4965,1.4984,1.5005,1.5028,1.5053,1.508,1.5109,1.514,1.5173,1.5208,1.5245,1.5284,1.5325,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0725,2.057,2.0417,2.0266,2.0117,1.997,1.9825,1.9682,1.9541,1.9402,1.9265,1.913,1.8997,1.8866,1.8737,1.861,1.8485,1.8362,1.8241,1.8122,1.8005,1.789,1.7777,1.7666,1.7557,1.745,1.7345,1.7242,1.7141,1.7042,1.6945,1.685,1.6757,1.6666,1.6577,1.649,1.6405,1.6322,1.6241,1.6162,1.6085,1.601,1.5937,1.5866,1.5797,1.573,1.5665,1.5602,1.5541,1.5482,1.5425,1.537,1.5317,1.5266,1.5217,1.517,1.5125,1.5082,1.5041,1.5002,1.4965,1.493,1.4897,1.4866,1.4837,1.481,1.4785,1.4762,1.4741,1.4722,1.4705,1.469,1.4677,1.4666,1.4657,1.465,1.4645,1.4642,1.4641,1.4642,1.4645,1.465,1.4657,1.4666,1.4677,1.469,1.4705,1.4722,1.4741,1.4762,1.4785,1.481,1.4837,1.4866,1.4897,1.493,1.4965,1.5002,1.5041,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0641,2.0484,2.0329,2.0176,2.0025,1.9876,1.9729,1.9584,1.9441,1.93,1.9161,1.9024,1.8889,1.8756,1.8625,1.8496,1.8369,1.8244,1.8121,1.8,1.7881,1.7764,1.7649,1.7536,1.7425,1.7316,1.7209,1.7104,1.7001,1.69,1.6801,1.6704,1.6609,1.6516,1.6425,1.6336,1.6249,1.6164,1.6081,1.6,1.5921,1.5844,1.5769,1.5696,1.5625,1.5556,1.5489,1.5424,1.5361,1.53,1.5241,1.5184,1.5129,1.5076,1.5025,1.4976,1.4929,1.4884,1.4841,1.48,1.4761,1.4724,1.4689,1.4656,1.4625,1.4596,1.4569,1.4544,1.4521,1.45,1.4481,1.4464,1.4449,1.4436,1.4425,1.4416,1.4409,1.4404,1.4401,1.44,1.4401,1.4404,1.4409,1.4416,1.4425,1.4436,1.4449,1.4464,1.4481,1.45,1.4521,1.4544,1.4569,1.4596,1.4625,1.4656,1.4689,1.4724,1.4761,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0561,2.0402,2.0245,2.009,1.9937,1.9786,1.9637,1.949,1.9345,1.9202,1.9061,1.8922,1.8785,1.865,1.8517,1.8386,1.8257,1.813,1.8005,1.7882,1.7761,1.7642,1.7525,1.741,1.7297,1.7186,1.7077,1.697,1.6865,1.6762,1.6661,1.6562,1.6465,1.637,1.6277,1.6186,1.6097,1.601,1.5925,1.5842,1.5761,1.5682,1.5605,1.553,1.5457,1.5386,1.5317,1.525,1.5185,1.5122,1.5061,1.5002,1.4945,1.489,1.4837,1.4786,1.4737,1.469,1.4645,1.4602,1.4561,1.4522,1.4485,1.445,1.4417,1.4386,1.4357,1.433,1.4305,1.4282,1.4261,1.4242,1.4225,1.421,1.4197,1.4186,1.4177,1.417,1.4165,1.4162,1.4161,1.4162,1.4165,1.417,1.4177,1.4186,1.4197,1.421,1.4225,1.4242,1.4261,1.4282,1.4305,1.433,1.4357,1.4386,1.4417,1.445,1.4485,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0485,2.0324,2.0165,2.0008,1.9853,1.97,1.9549,1.94,1.9253,1.9108,1.8965,1.8824,1.8685,1.8548,1.8413,1.828,1.8149,1.802,1.7893,1.7768,1.7645,1.7524,1.7405,1.7288,1.7173,1.706,1.6949,1.684,1.6733,1.6628,1.6525,1.6424,1.6325,1.6228,1.6133,1.604,1.5949,1.586,1.5773,1.5688,1.5605,1.5524,1.5445,1.5368,1.5293,1.522,1.5149,1.508,1.5013,1.4948,1.4885,1.4824,1.4765,1.4708,1.4653,1.46,1.4549,1.45,1.4453,1.4408,1.4365,1.4324,1.4285,1.4248,1.4213,1.418,1.4149,1.412,1.4093,1.4068,1.4045,1.4024,1.4005,1.3988,1.3973,1.396,1.3949,1.394,1.3933,1.3928,1.3925,1.3924,1.3925,1.3928,1.3933,1.394,1.3949,1.396,1.3973,1.3988,1.4005,1.4024,1.4045,1.4068,1.4093,1.412,1.4149,1.418,1.4213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0413,2.025,2.0089,1.993,1.9773,1.9618,1.9465,1.9314,1.9165,1.9018,1.8873,1.873,1.8589,1.845,1.8313,1.8178,1.8045,1.7914,1.7785,1.7658,1.7533,1.741,1.7289,1.717,1.7053,1.6938,1.6825,1.6714,1.6605,1.6498,1.6393,1.629,1.6189,1.609,1.5993,1.5898,1.5805,1.5714,1.5625,1.5538,1.5453,1.537,1.5289,1.521,1.5133,1.5058,1.4985,1.4914,1.4845,1.4778,1.4713,1.465,1.4589,1.453,1.4473,1.4418,1.4365,1.4314,1.4265,1.4218,1.4173,1.413,1.4089,1.405,1.4013,1.3978,1.3945,1.3914,1.3885,1.3858,1.3833,1.381,1.3789,1.377,1.3753,1.3738,1.3725,1.3714,1.3705,1.3698,1.3693,1.369,1.3689,1.369,1.3693,1.3698,1.3705,1.3714,1.3725,1.3738,1.3753,1.377,1.3789,1.381,1.3833,1.3858,1.3885,1.3914,1.3945,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0345,2.018,2.0017,1.9856,1.9697,1.954,1.9385,1.9232,1.9081,1.8932,1.8785,1.864,1.8497,1.8356,1.8217,1.808,1.7945,1.7812,1.7681,1.7552,1.7425,1.73,1.7177,1.7056,1.6937,1.682,1.6705,1.6592,1.6481,1.6372,1.6265,1.616,1.6057,1.5956,1.5857,1.576,1.5665,1.5572,1.5481,1.5392,1.5305,1.522,1.5137,1.5056,1.4977,1.49,1.4825,1.4752,1.4681,1.4612,1.4545,1.448,1.4417,1.4356,1.4297,1.424,1.4185,1.4132,1.4081,1.4032,1.3985,1.394,1.3897,1.3856,1.3817,1.378,1.3745,1.3712,1.3681,1.3652,1.3625,1.36,1.3577,1.3556,1.3537,1.352,1.3505,1.3492,1.3481,1.3472,1.3465,1.346,1.3457,1.3456,1.3457,1.346,1.3465,1.3472,1.3481,1.3492,1.3505,1.352,1.3537,1.3556,1.3577,1.36,1.3625,1.3652,1.3681,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0281,2.0114,1.9949,1.9786,1.9625,1.9466,1.9309,1.9154,1.9001,1.885,1.8701,1.8554,1.8409,1.8266,1.8125,1.7986,1.7849,1.7714,1.7581,1.745,1.7321,1.7194,1.7069,1.6946,1.6825,1.6706,1.6589,1.6474,1.6361,1.625,1.6141,1.6034,1.5929,1.5826,1.5725,1.5626,1.5529,1.5434,1.5341,1.525,1.5161,1.5074,1.4989,1.4906,1.4825,1.4746,1.4669,1.4594,1.4521,1.445,1.4381,1.4314,1.4249,1.4186,1.4125,1.4066,1.4009,1.3954,1.3901,1.385,1.3801,1.3754,1.3709,1.3666,1.3625,1.3586,1.3549,1.3514,1.3481,1.345,1.3421,1.3394,1.3369,1.3346,1.3325,1.3306,1.3289,1.3274,1.3261,1.325,1.3241,1.3234,1.3229,1.3226,1.3225,1.3226,1.3229,1.3234,1.3241,1.325,1.3261,1.3274,1.3289,1.3306,1.3325,1.3346,1.3369,1.3394,1.3421,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0221,2.0052,1.9885,1.972,1.9557,1.9396,1.9237,1.908,1.8925,1.8772,1.8621,1.8472,1.8325,1.818,1.8037,1.7896,1.7757,1.762,1.7485,1.7352,1.7221,1.7092,1.6965,1.684,1.6717,1.6596,1.6477,1.636,1.6245,1.6132,1.6021,1.5912,1.5805,1.57,1.5597,1.5496,1.5397,1.53,1.5205,1.5112,1.5021,1.4932,1.4845,1.476,1.4677,1.4596,1.4517,1.444,1.4365,1.4292,1.4221,1.4152,1.4085,1.402,1.3957,1.3896,1.3837,1.378,1.3725,1.3672,1.3621,1.3572,1.3525,1.348,1.3437,1.3396,1.3357,1.332,1.3285,1.3252,1.3221,1.3192,1.3165,1.314,1.3117,1.3096,1.3077,1.306,1.3045,1.3032,1.3021,1.3012,1.3005,1.3,1.2997,1.2996,1.2997,1.3,1.3005,1.3012,1.3021,1.3032,1.3045,1.306,1.3077,1.3096,1.3117,1.314,1.3165,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0165,1.9994,1.9825,1.9658,1.9493,1.933,1.9169,1.901,1.8853,1.8698,1.8545,1.8394,1.8245,1.8098,1.7953,1.781,1.7669,1.753,1.7393,1.7258,1.7125,1.6994,1.6865,1.6738,1.6613,1.649,1.6369,1.625,1.6133,1.6018,1.5905,1.5794,1.5685,1.5578,1.5473,1.537,1.5269,1.517,1.5073,1.4978,1.4885,1.4794,1.4705,1.4618,1.4533,1.445,1.4369,1.429,1.4213,1.4138,1.4065,1.3994,1.3925,1.3858,1.3793,1.373,1.3669,1.361,1.3553,1.3498,1.3445,1.3394,1.3345,1.3298,1.3253,1.321,1.3169,1.313,1.3093,1.3058,1.3025,1.2994,1.2965,1.2938,1.2913,1.289,1.2869,1.285,1.2833,1.2818,1.2805,1.2794,1.2785,1.2778,1.2773,1.277,1.2769,1.277,1.2773,1.2778,1.2785,1.2794,1.2805,1.2818,1.2833,1.285,1.2869,1.289,1.2913,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0113,1.994,1.9769,1.96,1.9433,1.9268,1.9105,1.8944,1.8785,1.8628,1.8473,1.832,1.8169,1.802,1.7873,1.7728,1.7585,1.7444,1.7305,1.7168,1.7033,1.69,1.6769,1.664,1.6513,1.6388,1.6265,1.6144,1.6025,1.5908,1.5793,1.568,1.5569,1.546,1.5353,1.5248,1.5145,1.5044,1.4945,1.4848,1.4753,1.466,1.4569,1.448,1.4393,1.4308,1.4225,1.4144,1.4065,1.3988,1.3913,1.384,1.3769,1.37,1.3633,1.3568,1.3505,1.3444,1.3385,1.3328,1.3273,1.322,1.3169,1.312,1.3073,1.3028,1.2985,1.2944,1.2905,1.2868,1.2833,1.28,1.2769,1.274,1.2713,1.2688,1.2665,1.2644,1.2625,1.2608,1.2593,1.258,1.2569,1.256,1.2553,1.2548,1.2545,1.2544,1.2545,1.2548,1.2553,1.256,1.2569,1.258,1.2593,1.2608,1.2625,1.2644,1.2665,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0065,1.989,1.9717,1.9546,1.9377,1.921,1.9045,1.8882,1.8721,1.8562,1.8405,1.825,1.8097,1.7946,1.7797,1.765,1.7505,1.7362,1.7221,1.7082,1.6945,1.681,1.6677,1.6546,1.6417,1.629,1.6165,1.6042,1.5921,1.5802,1.5685,1.557,1.5457,1.5346,1.5237,1.513,1.5025,1.4922,1.4821,1.4722,1.4625,1.453,1.4437,1.4346,1.4257,1.417,1.4085,1.4002,1.3921,1.3842,1.3765,1.369,1.3617,1.3546,1.3477,1.341,1.3345,1.3282,1.3221,1.3162,1.3105,1.305,1.2997,1.2946,1.2897,1.285,1.2805,1.2762,1.2721,1.2682,1.2645,1.261,1.2577,1.2546,1.2517,1.249,1.2465,1.2442,1.2421,1.2402,1.2385,1.237,1.2357,1.2346,1.2337,1.233,1.2325,1.2322,1.2321,1.2322,1.2325,1.233,1.2337,1.2346,1.2357,1.237,1.2385,1.2402,1.2421,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0021,1.9844,1.9669,1.9496,1.9325,1.9156,1.8989,1.8824,1.8661,1.85,1.8341,1.8184,1.8029,1.7876,1.7725,1.7576,1.7429,1.7284,1.7141,1.7,1.6861,1.6724,1.6589,1.6456,1.6325,1.6196,1.6069,1.5944,1.5821,1.57,1.5581,1.5464,1.5349,1.5236,1.5125,1.5016,1.4909,1.4804,1.4701,1.46,1.4501,1.4404,1.4309,1.4216,1.4125,1.4036,1.3949,1.3864,1.3781,1.37,1.3621,1.3544,1.3469,1.3396,1.3325,1.3256,1.3189,1.3124,1.3061,1.3,1.2941,1.2884,1.2829,1.2776,1.2725,1.2676,1.2629,1.2584,1.2541,1.25,1.2461,1.2424,1.2389,1.2356,1.2325,1.2296,1.2269,1.2244,1.2221,1.22,1.2181,1.2164,1.2149,1.2136,1.2125,1.2116,1.2109,1.2104,1.2101,1.21,1.2101,1.2104,1.2109,1.2116,1.2125,1.2136,1.2149,1.2164,1.2181,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9981,1.9802,1.9625,1.945,1.9277,1.9106,1.8937,1.877,1.8605,1.8442,1.8281,1.8122,1.7965,1.781,1.7657,1.7506,1.7357,1.721,1.7065,1.6922,1.6781,1.6642,1.6505,1.637,1.6237,1.6106,1.5977,1.585,1.5725,1.5602,1.5481,1.5362,1.5245,1.513,1.5017,1.4906,1.4797,1.469,1.4585,1.4482,1.4381,1.4282,1.4185,1.409,1.3997,1.3906,1.3817,1.373,1.3645,1.3562,1.3481,1.3402,1.3325,1.325,1.3177,1.3106,1.3037,1.297,1.2905,1.2842,1.2781,1.2722,1.2665,1.261,1.2557,1.2506,1.2457,1.241,1.2365,1.2322,1.2281,1.2242,1.2205,1.217,1.2137,1.2106,1.2077,1.205,1.2025,1.2002,1.1981,1.1962,1.1945,1.193,1.1917,1.1906,1.1897,1.189,1.1885,1.1882,1.1881,1.1882,1.1885,1.189,1.1897,1.1906,1.1917,1.193,1.1945,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9945,1.9764,1.9585,1.9408,1.9233,1.906,1.8889,1.872,1.8553,1.8388,1.8225,1.8064,1.7905,1.7748,1.7593,1.744,1.7289,1.714,1.6993,1.6848,1.6705,1.6564,1.6425,1.6288,1.6153,1.602,1.5889,1.576,1.5633,1.5508,1.5385,1.5264,1.5145,1.5028,1.4913,1.48,1.4689,1.458,1.4473,1.4368,1.4265,1.4164,1.4065,1.3968,1.3873,1.378,1.3689,1.36,1.3513,1.3428,1.3345,1.3264,1.3185,1.3108,1.3033,1.296,1.2889,1.282,1.2753,1.2688,1.2625,1.2564,1.2505,1.2448,1.2393,1.234,1.2289,1.224,1.2193,1.2148,1.2105,1.2064,1.2025,1.1988,1.1953,1.192,1.1889,1.186,1.1833,1.1808,1.1785,1.1764,1.1745,1.1728,1.1713,1.17,1.1689,1.168,1.1673,1.1668,1.1665,1.1664,1.1665,1.1668,1.1673,1.168,1.1689,1.17,1.1713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9913,1.973,1.9549,1.937,1.9193,1.9018,1.8845,1.8674,1.8505,1.8338,1.8173,1.801,1.7849,1.769,1.7533,1.7378,1.7225,1.7074,1.6925,1.6778,1.6633,1.649,1.6349,1.621,1.6073,1.5938,1.5805,1.5674,1.5545,1.5418,1.5293,1.517,1.5049,1.493,1.4813,1.4698,1.4585,1.4474,1.4365,1.4258,1.4153,1.405,1.3949,1.385,1.3753,1.3658,1.3565,1.3474,1.3385,1.3298,1.3213,1.313,1.3049,1.297,1.2893,1.2818,1.2745,1.2674,1.2605,1.2538,1.2473,1.241,1.2349,1.229,1.2233,1.2178,1.2125,1.2074,1.2025,1.1978,1.1933,1.189,1.1849,1.181,1.1773,1.1738,1.1705,1.1674,1.1645,1.1618,1.1593,1.157,1.1549,1.153,1.1513,1.1498,1.1485,1.1474,1.1465,1.1458,1.1453,1.145,1.1449,1.145,1.1453,1.1458,1.1465,1.1474,1.1485,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9885,1.97,1.9517,1.9336,1.9157,1.898,1.8805,1.8632,1.8461,1.8292,1.8125,1.796,1.7797,1.7636,1.7477,1.732,1.7165,1.7012,1.6861,1.6712,1.6565,1.642,1.6277,1.6136,1.5997,1.586,1.5725,1.5592,1.5461,1.5332,1.5205,1.508,1.4957,1.4836,1.4717,1.46,1.4485,1.4372,1.4261,1.4152,1.4045,1.394,1.3837,1.3736,1.3637,1.354,1.3445,1.3352,1.3261,1.3172,1.3085,1.3,1.2917,1.2836,1.2757,1.268,1.2605,1.2532,1.2461,1.2392,1.2325,1.226,1.2197,1.2136,1.2077,1.202,1.1965,1.1912,1.1861,1.1812,1.1765,1.172,1.1677,1.1636,1.1597,1.156,1.1525,1.1492,1.1461,1.1432,1.1405,1.138,1.1357,1.1336,1.1317,1.13,1.1285,1.1272,1.1261,1.1252,1.1245,1.124,1.1237,1.1236,1.1237,1.124,1.1245,1.1252,1.1261,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9861,1.9674,1.9489,1.9306,1.9125,1.8946,1.8769,1.8594,1.8421,1.825,1.8081,1.7914,1.7749,1.7586,1.7425,1.7266,1.7109,1.6954,1.6801,1.665,1.6501,1.6354,1.6209,1.6066,1.5925,1.5786,1.5649,1.5514,1.5381,1.525,1.5121,1.4994,1.4869,1.4746,1.4625,1.4506,1.4389,1.4274,1.4161,1.405,1.3941,1.3834,1.3729,1.3626,1.3525,1.3426,1.3329,1.3234,1.3141,1.305,1.2961,1.2874,1.2789,1.2706,1.2625,1.2546,1.2469,1.2394,1.2321,1.225,1.2181,1.2114,1.2049,1.1986,1.1925,1.1866,1.1809,1.1754,1.1701,1.165,1.1601,1.1554,1.1509,1.1466,1.1425,1.1386,1.1349,1.1314,1.1281,1.125,1.1221,1.1194,1.1169,1.1146,1.1125,1.1106,1.1089,1.1074,1.1061,1.105,1.1041,1.1034,1.1029,1.1026,1.1025,1.1026,1.1029,1.1034,1.1041,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9841,1.9652,1.9465,1.928,1.9097,1.8916,1.8737,1.856,1.8385,1.8212,1.8041,1.7872,1.7705,1.754,1.7377,1.7216,1.7057,1.69,1.6745,1.6592,1.6441,1.6292,1.6145,1.6,1.5857,1.5716,1.5577,1.544,1.5305,1.5172,1.5041,1.4912,1.4785,1.466,1.4537,1.4416,1.4297,1.418,1.4065,1.3952,1.3841,1.3732,1.3625,1.352,1.3417,1.3316,1.3217,1.312,1.3025,1.2932,1.2841,1.2752,1.2665,1.258,1.2497,1.2416,1.2337,1.226,1.2185,1.2112,1.2041,1.1972,1.1905,1.184,1.1777,1.1716,1.1657,1.16,1.1545,1.1492,1.1441,1.1392,1.1345,1.13,1.1257,1.1216,1.1177,1.114,1.1105,1.1072,1.1041,1.1012,1.0985,1.096,1.0937,1.0916,1.0897,1.088,1.0865,1.0852,1.0841,1.0832,1.0825,1.082,1.0817,1.0816,1.0817,1.082,1.0825,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9825,1.9634,1.9445,1.9258,1.9073,1.889,1.8709,1.853,1.8353,1.8178,1.8005,1.7834,1.7665,1.7498,1.7333,1.717,1.7009,1.685,1.6693,1.6538,1.6385,1.6234,1.6085,1.5938,1.5793,1.565,1.5509,1.537,1.5233,1.5098,1.4965,1.4834,1.4705,1.4578,1.4453,1.433,1.4209,1.409,1.3973,1.3858,1.3745,1.3634,1.3525,1.3418,1.3313,1.321,1.3109,1.301,1.2913,1.2818,1.2725,1.2634,1.2545,1.2458,1.2373,1.229,1.2209,1.213,1.2053,1.1978,1.1905,1.1834,1.1765,1.1698,1.1633,1.157,1.1509,1.145,1.1393,1.1338,1.1285,1.1234,1.1185,1.1138,1.1093,1.105,1.1009,1.097,1.0933,1.0898,1.0865,1.0834,1.0805,1.0778,1.0753,1.073,1.0709,1.069,1.0673,1.0658,1.0645,1.0634,1.0625,1.0618,1.0613,1.061,1.0609,1.061,1.0613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9813,1.962,1.9429,1.924,1.9053,1.8868,1.8685,1.8504,1.8325,1.8148,1.7973,1.78,1.7629,1.746,1.7293,1.7128,1.6965,1.6804,1.6645,1.6488,1.6333,1.618,1.6029,1.588,1.5733,1.5588,1.5445,1.5304,1.5165,1.5028,1.4893,1.476,1.4629,1.45,1.4373,1.4248,1.4125,1.4004,1.3885,1.3768,1.3653,1.354,1.3429,1.332,1.3213,1.3108,1.3005,1.2904,1.2805,1.2708,1.2613,1.252,1.2429,1.234,1.2253,1.2168,1.2085,1.2004,1.1925,1.1848,1.1773,1.17,1.1629,1.156,1.1493,1.1428,1.1365,1.1304,1.1245,1.1188,1.1133,1.108,1.1029,1.098,1.0933,1.0888,1.0845,1.0804,1.0765,1.0728,1.0693,1.066,1.0629,1.06,1.0573,1.0548,1.0525,1.0504,1.0485,1.0468,1.0453,1.044,1.0429,1.042,1.0413,1.0408,1.0405,1.0404,1.0405,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9805,1.961,1.9417,1.9226,1.9037,1.885,1.8665,1.8482,1.8301,1.8122,1.7945,1.777,1.7597,1.7426,1.7257,1.709,1.6925,1.6762,1.6601,1.6442,1.6285,1.613,1.5977,1.5826,1.5677,1.553,1.5385,1.5242,1.5101,1.4962,1.4825,1.469,1.4557,1.4426,1.4297,1.417,1.4045,1.3922,1.3801,1.3682,1.3565,1.345,1.3337,1.3226,1.3117,1.301,1.2905,1.2802,1.2701,1.2602,1.2505,1.241,1.2317,1.2226,1.2137,1.205,1.1965,1.1882,1.1801,1.1722,1.1645,1.157,1.1497,1.1426,1.1357,1.129,1.1225,1.1162,1.1101,1.1042,1.0985,1.093,1.0877,1.0826,1.0777,1.073,1.0685,1.0642,1.0601,1.0562,1.0525,1.049,1.0457,1.0426,1.0397,1.037,1.0345,1.0322,1.0301,1.0282,1.0265,1.025,1.0237,1.0226,1.0217,1.021,1.0205,1.0202,1.0201,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9801,1.9604,1.9409,1.9216,1.9025,1.8836,1.8649,1.8464,1.8281,1.81,1.7921,1.7744,1.7569,1.7396,1.7225,1.7056,1.6889,1.6724,1.6561,1.64,1.6241,1.6084,1.5929,1.5776,1.5625,1.5476,1.5329,1.5184,1.5041,1.49,1.4761,1.4624,1.4489,1.4356,1.4225,1.4096,1.3969,1.3844,1.3721,1.36,1.3481,1.3364,1.3249,1.3136,1.3025,1.2916,1.2809,1.2704,1.2601,1.25,1.2401,1.2304,1.2209,1.2116,1.2025,1.1936,1.1849,1.1764,1.1681,1.16,1.1521,1.1444,1.1369,1.1296,1.1225,1.1156,1.1089,1.1024,1.0961,1.09,1.0841,1.0784,1.0729,1.0676,1.0625,1.0576,1.0529,1.0484,1.0441,1.04,1.0361,1.0324,1.0289,1.0256,1.0225,1.0196,1.0169,1.0144,1.0121,1.01,1.0081,1.0064,1.0049,1.0036,1.0025,1.0016,1.0009,1.0004,1.0001,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9801,1.9602,1.9405,1.921,1.9017,1.8826,1.8637,1.845,1.8265,1.8082,1.7901,1.7722,1.7545,1.737,1.7197,1.7026,1.6857,1.669,1.6525,1.6362,1.6201,1.6042,1.5885,1.573,1.5577,1.5426,1.5277,1.513,1.4985,1.4842,1.4701,1.4562,1.4425,1.429,1.4157,1.4026,1.3897,1.377,1.3645,1.3522,1.3401,1.3282,1.3165,1.305,1.2937,1.2826,1.2717,1.261,1.2505,1.2402,1.2301,1.2202,1.2105,1.201,1.1917,1.1826,1.1737,1.165,1.1565,1.1482,1.1401,1.1322,1.1245,1.117,1.1097,1.1026,1.0957,1.089,1.0825,1.0762,1.0701,1.0642,1.0585,1.053,1.0477,1.0426,1.0377,1.033,1.0285,1.0242,1.0201,1.0162,1.0125,1.009,1.0057,1.0026,0.9997,0.997,0.9945,0.9922,0.9901,0.9882,0.9865,0.985,0.9837,0.9826,0.9817,0.981,0.9805,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9805,1.9604,1.9405,1.9208,1.9013,1.882,1.8629,1.844,1.8253,1.8068,1.7885,1.7704,1.7525,1.7348,1.7173,1.7,1.6829,1.666,1.6493,1.6328,1.6165,1.6004,1.5845,1.5688,1.5533,1.538,1.5229,1.508,1.4933,1.4788,1.4645,1.4504,1.4365,1.4228,1.4093,1.396,1.3829,1.37,1.3573,1.3448,1.3325,1.3204,1.3085,1.2968,1.2853,1.274,1.2629,1.252,1.2413,1.2308,1.2205,1.2104,1.2005,1.1908,1.1813,1.172,1.1629,1.154,1.1453,1.1368,1.1285,1.1204,1.1125,1.1048,1.0973,1.09,1.0829,1.076,1.0693,1.0628,1.0565,1.0504,1.0445,1.0388,1.0333,1.028,1.0229,1.018,1.0133,1.0088,1.0045,1.0004,0.9965,0.9928,0.9893,0.986,0.9829,0.98,0.9773,0.9748,0.9725,0.9704,0.9685,0.9668,0.9653,0.964,0.9629,0.962,0.9613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9813,1.961,1.9409,1.921,1.9013,1.8818,1.8625,1.8434,1.8245,1.8058,1.7873,1.769,1.7509,1.733,1.7153,1.6978,1.6805,1.6634,1.6465,1.6298,1.6133,1.597,1.5809,1.565,1.5493,1.5338,1.5185,1.5034,1.4885,1.4738,1.4593,1.445,1.4309,1.417,1.4033,1.3898,1.3765,1.3634,1.3505,1.3378,1.3253,1.313,1.3009,1.289,1.2773,1.2658,1.2545,1.2434,1.2325,1.2218,1.2113,1.201,1.1909,1.181,1.1713,1.1618,1.1525,1.1434,1.1345,1.1258,1.1173,1.109,1.1009,1.093,1.0853,1.0778,1.0705,1.0634,1.0565,1.0498,1.0433,1.037,1.0309,1.025,1.0193,1.0138,1.0085,1.0034,0.9985,0.9938,0.9893,0.985,0.9809,0.977,0.9733,0.9698,0.9665,0.9634,0.9605,0.9578,0.9553,0.953,0.9509,0.949,0.9473,0.9458,0.9445,0.9434,0.9425,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9825,1.962,1.9417,1.9216,1.9017,1.882,1.8625,1.8432,1.8241,1.8052,1.7865,1.768,1.7497,1.7316,1.7137,1.696,1.6785,1.6612,1.6441,1.6272,1.6105,1.594,1.5777,1.5616,1.5457,1.53,1.5145,1.4992,1.4841,1.4692,1.4545,1.44,1.4257,1.4116,1.3977,1.384,1.3705,1.3572,1.3441,1.3312,1.3185,1.306,1.2937,1.2816,1.2697,1.258,1.2465,1.2352,1.2241,1.2132,1.2025,1.192,1.1817,1.1716,1.1617,1.152,1.1425,1.1332,1.1241,1.1152,1.1065,1.098,1.0897,1.0816,1.0737,1.066,1.0585,1.0512,1.0441,1.0372,1.0305,1.024,1.0177,1.0116,1.0057,1,0.9945,0.9892,0.9841,0.9792,0.9745,0.97,0.9657,0.9616,0.9577,0.954,0.9505,0.9472,0.9441,0.9412,0.9385,0.936,0.9337,0.9316,0.9297,0.928,0.9265,0.9252,0.9241,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9841,1.9634,1.9429,1.9226,1.9025,1.8826,1.8629,1.8434,1.8241,1.805,1.7861,1.7674,1.7489,1.7306,1.7125,1.6946,1.6769,1.6594,1.6421,1.625,1.6081,1.5914,1.5749,1.5586,1.5425,1.5266,1.5109,1.4954,1.4801,1.465,1.4501,1.4354,1.4209,1.4066,1.3925,1.3786,1.3649,1.3514,1.3381,1.325,1.3121,1.2994,1.2869,1.2746,1.2625,1.2506,1.2389,1.2274,1.2161,1.205,1.1941,1.1834,1.1729,1.1626,1.1525,1.1426,1.1329,1.1234,1.1141,1.105,1.0961,1.0874,1.0789,1.0706,1.0625,1.0546,1.0469,1.0394,1.0321,1.025,1.0181,1.0114,1.0049,0.9986,0.9925,0.9866,0.9809,0.9754,0.9701,0.965,0.9601,0.9554,0.9509,0.9466,0.9425,0.9386,0.9349,0.9314,0.9281,0.925,0.9221,0.9194,0.9169,0.9146,0.9125,0.9106,0.9089,0.9074,0.9061,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9861,1.9652,1.9445,1.924,1.9037,1.8836,1.8637,1.844,1.8245,1.8052,1.7861,1.7672,1.7485,1.73,1.7117,1.6936,1.6757,1.658,1.6405,1.6232,1.6061,1.5892,1.5725,1.556,1.5397,1.5236,1.5077,1.492,1.4765,1.4612,1.4461,1.4312,1.4165,1.402,1.3877,1.3736,1.3597,1.346,1.3325,1.3192,1.3061,1.2932,1.2805,1.268,1.2557,1.2436,1.2317,1.22,1.2085,1.1972,1.1861,1.1752,1.1645,1.154,1.1437,1.1336,1.1237,1.114,1.1045,1.0952,1.0861,1.0772,1.0685,1.06,1.0517,1.0436,1.0357,1.028,1.0205,1.0132,1.0061,0.9992,0.9925,0.986,0.9797,0.9736,0.9677,0.962,0.9565,0.9512,0.9461,0.9412,0.9365,0.932,0.9277,0.9236,0.9197,0.916,0.9125,0.9092,0.9061,0.9032,0.9005,0.898,0.8957,0.8936,0.8917,0.89,0.8885,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9885,1.9674,1.9465,1.9258,1.9053,1.885,1.8649,1.845,1.8253,1.8058,1.7865,1.7674,1.7485,1.7298,1.7113,1.693,1.6749,1.657,1.6393,1.6218,1.6045,1.5874,1.5705,1.5538,1.5373,1.521,1.5049,1.489,1.4733,1.4578,1.4425,1.4274,1.4125,1.3978,1.3833,1.369,1.3549,1.341,1.3273,1.3138,1.3005,1.2874,1.2745,1.2618,1.2493,1.237,1.2249,1.213,1.2013,1.1898,1.1785,1.1674,1.1565,1.1458,1.1353,1.125,1.1149,1.105,1.0953,1.0858,1.0765,1.0674,1.0585,1.0498,1.0413,1.033,1.0249,1.017,1.0093,1.0018,0.9945,0.9874,0.9805,0.9738,0.9673,0.961,0.9549,0.949,0.9433,0.9378,0.9325,0.9274,0.9225,0.9178,0.9133,0.909,0.9049,0.901,0.8973,0.8938,0.8905,0.8874,0.8845,0.8818,0.8793,0.877,0.8749,0.873,0.8713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9913,1.97,1.9489,1.928,1.9073,1.8868,1.8665,1.8464,1.8265,1.8068,1.7873,1.768,1.7489,1.73,1.7113,1.6928,1.6745,1.6564,1.6385,1.6208,1.6033,1.586,1.5689,1.552,1.5353,1.5188,1.5025,1.4864,1.4705,1.4548,1.4393,1.424,1.4089,1.394,1.3793,1.3648,1.3505,1.3364,1.3225,1.3088,1.2953,1.282,1.2689,1.256,1.2433,1.2308,1.2185,1.2064,1.1945,1.1828,1.1713,1.16,1.1489,1.138,1.1273,1.1168,1.1065,1.0964,1.0865,1.0768,1.0673,1.058,1.0489,1.04,1.0313,1.0228,1.0145,1.0064,0.9985,0.9908,0.9833,0.976,0.9689,0.962,0.9553,0.9488,0.9425,0.9364,0.9305,0.9248,0.9193,0.914,0.9089,0.904,0.8993,0.8948,0.8905,0.8864,0.8825,0.8788,0.8753,0.872,0.8689,0.866,0.8633,0.8608,0.8585,0.8564,0.8545,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9945,1.973,1.9517,1.9306,1.9097,1.889,1.8685,1.8482,1.8281,1.8082,1.7885,1.769,1.7497,1.7306,1.7117,1.693,1.6745,1.6562,1.6381,1.6202,1.6025,1.585,1.5677,1.5506,1.5337,1.517,1.5005,1.4842,1.4681,1.4522,1.4365,1.421,1.4057,1.3906,1.3757,1.361,1.3465,1.3322,1.3181,1.3042,1.2905,1.277,1.2637,1.2506,1.2377,1.225,1.2125,1.2002,1.1881,1.1762,1.1645,1.153,1.1417,1.1306,1.1197,1.109,1.0985,1.0882,1.0781,1.0682,1.0585,1.049,1.0397,1.0306,1.0217,1.013,1.0045,0.9962,0.9881,0.9802,0.9725,0.965,0.9577,0.9506,0.9437,0.937,0.9305,0.9242,0.9181,0.9122,0.9065,0.901,0.8957,0.8906,0.8857,0.881,0.8765,0.8722,0.8681,0.8642,0.8605,0.857,0.8537,0.8506,0.8477,0.845,0.8425,0.8402,0.8381,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1.9981,1.9764,1.9549,1.9336,1.9125,1.8916,1.8709,1.8504,1.8301,1.81,1.7901,1.7704,1.7509,1.7316,1.7125,1.6936,1.6749,1.6564,1.6381,1.62,1.6021,1.5844,1.5669,1.5496,1.5325,1.5156,1.4989,1.4824,1.4661,1.45,1.4341,1.4184,1.4029,1.3876,1.3725,1.3576,1.3429,1.3284,1.3141,1.3,1.2861,1.2724,1.2589,1.2456,1.2325,1.2196,1.2069,1.1944,1.1821,1.17,1.1581,1.1464,1.1349,1.1236,1.1125,1.1016,1.0909,1.0804,1.0701,1.06,1.0501,1.0404,1.0309,1.0216,1.0125,1.0036,0.9949,0.9864,0.9781,0.97,0.9621,0.9544,0.9469,0.9396,0.9325,0.9256,0.9189,0.9124,0.9061,0.9,0.8941,0.8884,0.8829,0.8776,0.8725,0.8676,0.8629,0.8584,0.8541,0.85,0.8461,0.8424,0.8389,0.8356,0.8325,0.8296,0.8269,0.8244,0.8221,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0021,1.9802,1.9585,1.937,1.9157,1.8946,1.8737,1.853,1.8325,1.8122,1.7921,1.7722,1.7525,1.733,1.7137,1.6946,1.6757,1.657,1.6385,1.6202,1.6021,1.5842,1.5665,1.549,1.5317,1.5146,1.4977,1.481,1.4645,1.4482,1.4321,1.4162,1.4005,1.385,1.3697,1.3546,1.3397,1.325,1.3105,1.2962,1.2821,1.2682,1.2545,1.241,1.2277,1.2146,1.2017,1.189,1.1765,1.1642,1.1521,1.1402,1.1285,1.117,1.1057,1.0946,1.0837,1.073,1.0625,1.0522,1.0421,1.0322,1.0225,1.013,1.0037,0.9946,0.9857,0.977,0.9685,0.9602,0.9521,0.9442,0.9365,0.929,0.9217,0.9146,0.9077,0.901,0.8945,0.8882,0.8821,0.8762,0.8705,0.865,0.8597,0.8546,0.8497,0.845,0.8405,0.8362,0.8321,0.8282,0.8245,0.821,0.8177,0.8146,0.8117,0.809,0.8065,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0065,1.9844,1.9625,1.9408,1.9193,1.898,1.8769,1.856,1.8353,1.8148,1.7945,1.7744,1.7545,1.7348,1.7153,1.696,1.6769,1.658,1.6393,1.6208,1.6025,1.5844,1.5665,1.5488,1.5313,1.514,1.4969,1.48,1.4633,1.4468,1.4305,1.4144,1.3985,1.3828,1.3673,1.352,1.3369,1.322,1.3073,1.2928,1.2785,1.2644,1.2505,1.2368,1.2233,1.21,1.1969,1.184,1.1713,1.1588,1.1465,1.1344,1.1225,1.1108,1.0993,1.088,1.0769,1.066,1.0553,1.0448,1.0345,1.0244,1.0145,1.0048,0.9953,0.986,0.9769,0.968,0.9593,0.9508,0.9425,0.9344,0.9265,0.9188,0.9113,0.904,0.8969,0.89,0.8833,0.8768,0.8705,0.8644,0.8585,0.8528,0.8473,0.842,0.8369,0.832,0.8273,0.8228,0.8185,0.8144,0.8105,0.8068,0.8033,0.8,0.7969,0.794,0.7913,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0113,1.989,1.9669,1.945,1.9233,1.9018,1.8805,1.8594,1.8385,1.8178,1.7973,1.777,1.7569,1.737,1.7173,1.6978,1.6785,1.6594,1.6405,1.6218,1.6033,1.585,1.5669,1.549,1.5313,1.5138,1.4965,1.4794,1.4625,1.4458,1.4293,1.413,1.3969,1.381,1.3653,1.3498,1.3345,1.3194,1.3045,1.2898,1.2753,1.261,1.2469,1.233,1.2193,1.2058,1.1925,1.1794,1.1665,1.1538,1.1413,1.129,1.1169,1.105,1.0933,1.0818,1.0705,1.0594,1.0485,1.0378,1.0273,1.017,1.0069,0.997,0.9873,0.9778,0.9685,0.9594,0.9505,0.9418,0.9333,0.925,0.9169,0.909,0.9013,0.8938,0.8865,0.8794,0.8725,0.8658,0.8593,0.853,0.8469,0.841,0.8353,0.8298,0.8245,0.8194,0.8145,0.8098,0.8053,0.801,0.7969,0.793,0.7893,0.7858,0.7825,0.7794,0.7765,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0165,1.994,1.9717,1.9496,1.9277,1.906,1.8845,1.8632,1.8421,1.8212,1.8005,1.78,1.7597,1.7396,1.7197,1.7,1.6805,1.6612,1.6421,1.6232,1.6045,1.586,1.5677,1.5496,1.5317,1.514,1.4965,1.4792,1.4621,1.4452,1.4285,1.412,1.3957,1.3796,1.3637,1.348,1.3325,1.3172,1.3021,1.2872,1.2725,1.258,1.2437,1.2296,1.2157,1.202,1.1885,1.1752,1.1621,1.1492,1.1365,1.124,1.1117,1.0996,1.0877,1.076,1.0645,1.0532,1.0421,1.0312,1.0205,1.01,0.9997,0.9896,0.9797,0.97,0.9605,0.9512,0.9421,0.9332,0.9245,0.916,0.9077,0.8996,0.8917,0.884,0.8765,0.8692,0.8621,0.8552,0.8485,0.842,0.8357,0.8296,0.8237,0.818,0.8125,0.8072,0.8021,0.7972,0.7925,0.788,0.7837,0.7796,0.7757,0.772,0.7685,0.7652,0.7621,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0221,1.9994,1.9769,1.9546,1.9325,1.9106,1.8889,1.8674,1.8461,1.825,1.8041,1.7834,1.7629,1.7426,1.7225,1.7026,1.6829,1.6634,1.6441,1.625,1.6061,1.5874,1.5689,1.5506,1.5325,1.5146,1.4969,1.4794,1.4621,1.445,1.4281,1.4114,1.3949,1.3786,1.3625,1.3466,1.3309,1.3154,1.3001,1.285,1.2701,1.2554,1.2409,1.2266,1.2125,1.1986,1.1849,1.1714,1.1581,1.145,1.1321,1.1194,1.1069,1.0946,1.0825,1.0706,1.0589,1.0474,1.0361,1.025,1.0141,1.0034,0.9929,0.9826,0.9725,0.9626,0.9529,0.9434,0.9341,0.925,0.9161,0.9074,0.8989,0.8906,0.8825,0.8746,0.8669,0.8594,0.8521,0.845,0.8381,0.8314,0.8249,0.8186,0.8125,0.8066,0.8009,0.7954,0.7901,0.785,0.7801,0.7754,0.7709,0.7666,0.7625,0.7586,0.7549,0.7514,0.7481,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0281,2.0052,1.9825,1.96,1.9377,1.9156,1.8937,1.872,1.8505,1.8292,1.8081,1.7872,1.7665,1.746,1.7257,1.7056,1.6857,1.666,1.6465,1.6272,1.6081,1.5892,1.5705,1.552,1.5337,1.5156,1.4977,1.48,1.4625,1.4452,1.4281,1.4112,1.3945,1.378,1.3617,1.3456,1.3297,1.314,1.2985,1.2832,1.2681,1.2532,1.2385,1.224,1.2097,1.1956,1.1817,1.168,1.1545,1.1412,1.1281,1.1152,1.1025,1.09,1.0777,1.0656,1.0537,1.042,1.0305,1.0192,1.0081,0.9972,0.9865,0.976,0.9657,0.9556,0.9457,0.936,0.9265,0.9172,0.9081,0.8992,0.8905,0.882,0.8737,0.8656,0.8577,0.85,0.8425,0.8352,0.8281,0.8212,0.8145,0.808,0.8017,0.7956,0.7897,0.784,0.7785,0.7732,0.7681,0.7632,0.7585,0.754,0.7497,0.7456,0.7417,0.738,0.7345,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0345,2.0114,1.9885,1.9658,1.9433,1.921,1.8989,1.877,1.8553,1.8338,1.8125,1.7914,1.7705,1.7498,1.7293,1.709,1.6889,1.669,1.6493,1.6298,1.6105,1.5914,1.5725,1.5538,1.5353,1.517,1.4989,1.481,1.4633,1.4458,1.4285,1.4114,1.3945,1.3778,1.3613,1.345,1.3289,1.313,1.2973,1.2818,1.2665,1.2514,1.2365,1.2218,1.2073,1.193,1.1789,1.165,1.1513,1.1378,1.1245,1.1114,1.0985,1.0858,1.0733,1.061,1.0489,1.037,1.0253,1.0138,1.0025,0.9914,0.9805,0.9698,0.9593,0.949,0.9389,0.929,0.9193,0.9098,0.9005,0.8914,0.8825,0.8738,0.8653,0.857,0.8489,0.841,0.8333,0.8258,0.8185,0.8114,0.8045,0.7978,0.7913,0.785,0.7789,0.773,0.7673,0.7618,0.7565,0.7514,0.7465,0.7418,0.7373,0.733,0.7289,0.725,0.7213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0413,2.018,1.9949,1.972,1.9493,1.9268,1.9045,1.8824,1.8605,1.8388,1.8173,1.796,1.7749,1.754,1.7333,1.7128,1.6925,1.6724,1.6525,1.6328,1.6133,1.594,1.5749,1.556,1.5373,1.5188,1.5005,1.4824,1.4645,1.4468,1.4293,1.412,1.3949,1.378,1.3613,1.3448,1.3285,1.3124,1.2965,1.2808,1.2653,1.25,1.2349,1.22,1.2053,1.1908,1.1765,1.1624,1.1485,1.1348,1.1213,1.108,1.0949,1.082,1.0693,1.0568,1.0445,1.0324,1.0205,1.0088,0.9973,0.986,0.9749,0.964,0.9533,0.9428,0.9325,0.9224,0.9125,0.9028,0.8933,0.884,0.8749,0.866,0.8573,0.8488,0.8405,0.8324,0.8245,0.8168,0.8093,0.802,0.7949,0.788,0.7813,0.7748,0.7685,0.7624,0.7565,0.7508,0.7453,0.74,0.7349,0.73,0.7253,0.7208,0.7165,0.7124,0.7085,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0485,2.025,2.0017,1.9786,1.9557,1.933,1.9105,1.8882,1.8661,1.8442,1.8225,1.801,1.7797,1.7586,1.7377,1.717,1.6965,1.6762,1.6561,1.6362,1.6165,1.597,1.5777,1.5586,1.5397,1.521,1.5025,1.4842,1.4661,1.4482,1.4305,1.413,1.3957,1.3786,1.3617,1.345,1.3285,1.3122,1.2961,1.2802,1.2645,1.249,1.2337,1.2186,1.2037,1.189,1.1745,1.1602,1.1461,1.1322,1.1185,1.105,1.0917,1.0786,1.0657,1.053,1.0405,1.0282,1.0161,1.0042,0.9925,0.981,0.9697,0.9586,0.9477,0.937,0.9265,0.9162,0.9061,0.8962,0.8865,0.877,0.8677,0.8586,0.8497,0.841,0.8325,0.8242,0.8161,0.8082,0.8005,0.793,0.7857,0.7786,0.7717,0.765,0.7585,0.7522,0.7461,0.7402,0.7345,0.729,0.7237,0.7186,0.7137,0.709,0.7045,0.7002,0.6961,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0561,2.0324,2.0089,1.9856,1.9625,1.9396,1.9169,1.8944,1.8721,1.85,1.8281,1.8064,1.7849,1.7636,1.7425,1.7216,1.7009,1.6804,1.6601,1.64,1.6201,1.6004,1.5809,1.5616,1.5425,1.5236,1.5049,1.4864,1.4681,1.45,1.4321,1.4144,1.3969,1.3796,1.3625,1.3456,1.3289,1.3124,1.2961,1.28,1.2641,1.2484,1.2329,1.2176,1.2025,1.1876,1.1729,1.1584,1.1441,1.13,1.1161,1.1024,1.0889,1.0756,1.0625,1.0496,1.0369,1.0244,1.0121,1,0.9881,0.9764,0.9649,0.9536,0.9425,0.9316,0.9209,0.9104,0.9001,0.89,0.8801,0.8704,0.8609,0.8516,0.8425,0.8336,0.8249,0.8164,0.8081,0.8,0.7921,0.7844,0.7769,0.7696,0.7625,0.7556,0.7489,0.7424,0.7361,0.73,0.7241,0.7184,0.7129,0.7076,0.7025,0.6976,0.6929,0.6884,0.6841,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0641,2.0402,2.0165,1.993,1.9697,1.9466,1.9237,1.901,1.8785,1.8562,1.8341,1.8122,1.7905,1.769,1.7477,1.7266,1.7057,1.685,1.6645,1.6442,1.6241,1.6042,1.5845,1.565,1.5457,1.5266,1.5077,1.489,1.4705,1.4522,1.4341,1.4162,1.3985,1.381,1.3637,1.3466,1.3297,1.313,1.2965,1.2802,1.2641,1.2482,1.2325,1.217,1.2017,1.1866,1.1717,1.157,1.1425,1.1282,1.1141,1.1002,1.0865,1.073,1.0597,1.0466,1.0337,1.021,1.0085,0.9962,0.9841,0.9722,0.9605,0.949,0.9377,0.9266,0.9157,0.905,0.8945,0.8842,0.8741,0.8642,0.8545,0.845,0.8357,0.8266,0.8177,0.809,0.8005,0.7922,0.7841,0.7762,0.7685,0.761,0.7537,0.7466,0.7397,0.733,0.7265,0.7202,0.7141,0.7082,0.7025,0.697,0.6917,0.6866,0.6817,0.677,0.6725,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0725,2.0484,2.0245,2.0008,1.9773,1.954,1.9309,1.908,1.8853,1.8628,1.8405,1.8184,1.7965,1.7748,1.7533,1.732,1.7109,1.69,1.6693,1.6488,1.6285,1.6084,1.5885,1.5688,1.5493,1.53,1.5109,1.492,1.4733,1.4548,1.4365,1.4184,1.4005,1.3828,1.3653,1.348,1.3309,1.314,1.2973,1.2808,1.2645,1.2484,1.2325,1.2168,1.2013,1.186,1.1709,1.156,1.1413,1.1268,1.1125,1.0984,1.0845,1.0708,1.0573,1.044,1.0309,1.018,1.0053,0.9928,0.9805,0.9684,0.9565,0.9448,0.9333,0.922,0.9109,0.9,0.8893,0.8788,0.8685,0.8584,0.8485,0.8388,0.8293,0.82,0.8109,0.802,0.7933,0.7848,0.7765,0.7684,0.7605,0.7528,0.7453,0.738,0.7309,0.724,0.7173,0.7108,0.7045,0.6984,0.6925,0.6868,0.6813,0.676,0.6709,0.666,0.6613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0813,2.057,2.0329,2.009,1.9853,1.9618,1.9385,1.9154,1.8925,1.8698,1.8473,1.825,1.8029,1.781,1.7593,1.7378,1.7165,1.6954,1.6745,1.6538,1.6333,1.613,1.5929,1.573,1.5533,1.5338,1.5145,1.4954,1.4765,1.4578,1.4393,1.421,1.4029,1.385,1.3673,1.3498,1.3325,1.3154,1.2985,1.2818,1.2653,1.249,1.2329,1.217,1.2013,1.1858,1.1705,1.1554,1.1405,1.1258,1.1113,1.097,1.0829,1.069,1.0553,1.0418,1.0285,1.0154,1.0025,0.9898,0.9773,0.965,0.9529,0.941,0.9293,0.9178,0.9065,0.8954,0.8845,0.8738,0.8633,0.853,0.8429,0.833,0.8233,0.8138,0.8045,0.7954,0.7865,0.7778,0.7693,0.761,0.7529,0.745,0.7373,0.7298,0.7225,0.7154,0.7085,0.7018,0.6953,0.689,0.6829,0.677,0.6713,0.6658,0.6605,0.6554,0.6505,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.0905,2.066,2.0417,2.0176,1.9937,1.97,1.9465,1.9232,1.9001,1.8772,1.8545,1.832,1.8097,1.7876,1.7657,1.744,1.7225,1.7012,1.6801,1.6592,1.6385,1.618,1.5977,1.5776,1.5577,1.538,1.5185,1.4992,1.4801,1.4612,1.4425,1.424,1.4057,1.3876,1.3697,1.352,1.3345,1.3172,1.3001,1.2832,1.2665,1.25,1.2337,1.2176,1.2017,1.186,1.1705,1.1552,1.1401,1.1252,1.1105,1.096,1.0817,1.0676,1.0537,1.04,1.0265,1.0132,1.0001,0.9872,0.9745,0.962,0.9497,0.9376,0.9257,0.914,0.9025,0.8912,0.8801,0.8692,0.8585,0.848,0.8377,0.8276,0.8177,0.808,0.7985,0.7892,0.7801,0.7712,0.7625,0.754,0.7457,0.7376,0.7297,0.722,0.7145,0.7072,0.7001,0.6932,0.6865,0.68,0.6737,0.6676,0.6617,0.656,0.6505,0.6452,0.6401,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1001,2.0754,2.0509,2.0266,2.0025,1.9786,1.9549,1.9314,1.9081,1.885,1.8621,1.8394,1.8169,1.7946,1.7725,1.7506,1.7289,1.7074,1.6861,1.665,1.6441,1.6234,1.6029,1.5826,1.5625,1.5426,1.5229,1.5034,1.4841,1.465,1.4461,1.4274,1.4089,1.3906,1.3725,1.3546,1.3369,1.3194,1.3021,1.285,1.2681,1.2514,1.2349,1.2186,1.2025,1.1866,1.1709,1.1554,1.1401,1.125,1.1101,1.0954,1.0809,1.0666,1.0525,1.0386,1.0249,1.0114,0.9981,0.985,0.9721,0.9594,0.9469,0.9346,0.9225,0.9106,0.8989,0.8874,0.8761,0.865,0.8541,0.8434,0.8329,0.8226,0.8125,0.8026,0.7929,0.7834,0.7741,0.765,0.7561,0.7474,0.7389,0.7306,0.7225,0.7146,0.7069,0.6994,0.6921,0.685,0.6781,0.6714,0.6649,0.6586,0.6525,0.6466,0.6409,0.6354,0.6301,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1101,2.0852,2.0605,2.036,2.0117,1.9876,1.9637,1.94,1.9165,1.8932,1.8701,1.8472,1.8245,1.802,1.7797,1.7576,1.7357,1.714,1.6925,1.6712,1.6501,1.6292,1.6085,1.588,1.5677,1.5476,1.5277,1.508,1.4885,1.4692,1.4501,1.4312,1.4125,1.394,1.3757,1.3576,1.3397,1.322,1.3045,1.2872,1.2701,1.2532,1.2365,1.22,1.2037,1.1876,1.1717,1.156,1.1405,1.1252,1.1101,1.0952,1.0805,1.066,1.0517,1.0376,1.0237,1.01,0.9965,0.9832,0.9701,0.9572,0.9445,0.932,0.9197,0.9076,0.8957,0.884,0.8725,0.8612,0.8501,0.8392,0.8285,0.818,0.8077,0.7976,0.7877,0.778,0.7685,0.7592,0.7501,0.7412,0.7325,0.724,0.7157,0.7076,0.6997,0.692,0.6845,0.6772,0.6701,0.6632,0.6565,0.65,0.6437,0.6376,0.6317,0.626,0.6205,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1205,2.0954,2.0705,2.0458,2.0213,1.997,1.9729,1.949,1.9253,1.9018,1.8785,1.8554,1.8325,1.8098,1.7873,1.765,1.7429,1.721,1.6993,1.6778,1.6565,1.6354,1.6145,1.5938,1.5733,1.553,1.5329,1.513,1.4933,1.4738,1.4545,1.4354,1.4165,1.3978,1.3793,1.361,1.3429,1.325,1.3073,1.2898,1.2725,1.2554,1.2385,1.2218,1.2053,1.189,1.1729,1.157,1.1413,1.1258,1.1105,1.0954,1.0805,1.0658,1.0513,1.037,1.0229,1.009,0.9953,0.9818,0.9685,0.9554,0.9425,0.9298,0.9173,0.905,0.8929,0.881,0.8693,0.8578,0.8465,0.8354,0.8245,0.8138,0.8033,0.793,0.7829,0.773,0.7633,0.7538,0.7445,0.7354,0.7265,0.7178,0.7093,0.701,0.6929,0.685,0.6773,0.6698,0.6625,0.6554,0.6485,0.6418,0.6353,0.629,0.6229,0.617,0.6113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1313,2.106,2.0809,2.056,2.0313,2.0068,1.9825,1.9584,1.9345,1.9108,1.8873,1.864,1.8409,1.818,1.7953,1.7728,1.7505,1.7284,1.7065,1.6848,1.6633,1.642,1.6209,1.6,1.5793,1.5588,1.5385,1.5184,1.4985,1.4788,1.4593,1.44,1.4209,1.402,1.3833,1.3648,1.3465,1.3284,1.3105,1.2928,1.2753,1.258,1.2409,1.224,1.2073,1.1908,1.1745,1.1584,1.1425,1.1268,1.1113,1.096,1.0809,1.066,1.0513,1.0368,1.0225,1.0084,0.9945,0.9808,0.9673,0.954,0.9409,0.928,0.9153,0.9028,0.8905,0.8784,0.8665,0.8548,0.8433,0.832,0.8209,0.81,0.7993,0.7888,0.7785,0.7684,0.7585,0.7488,0.7393,0.73,0.7209,0.712,0.7033,0.6948,0.6865,0.6784,0.6705,0.6628,0.6553,0.648,0.6409,0.634,0.6273,0.6208,0.6145,0.6084,0.6025,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1425,2.117,2.0917,2.0666,2.0417,2.017,1.9925,1.9682,1.9441,1.9202,1.8965,1.873,1.8497,1.8266,1.8037,1.781,1.7585,1.7362,1.7141,1.6922,1.6705,1.649,1.6277,1.6066,1.5857,1.565,1.5445,1.5242,1.5041,1.4842,1.4645,1.445,1.4257,1.4066,1.3877,1.369,1.3505,1.3322,1.3141,1.2962,1.2785,1.261,1.2437,1.2266,1.2097,1.193,1.1765,1.1602,1.1441,1.1282,1.1125,1.097,1.0817,1.0666,1.0517,1.037,1.0225,1.0082,0.9941,0.9802,0.9665,0.953,0.9397,0.9266,0.9137,0.901,0.8885,0.8762,0.8641,0.8522,0.8405,0.829,0.8177,0.8066,0.7957,0.785,0.7745,0.7642,0.7541,0.7442,0.7345,0.725,0.7157,0.7066,0.6977,0.689,0.6805,0.6722,0.6641,0.6562,0.6485,0.641,0.6337,0.6266,0.6197,0.613,0.6065,0.6002,0.5941,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1541,2.1284,2.1029,2.0776,2.0525,2.0276,2.0029,1.9784,1.9541,1.93,1.9061,1.8824,1.8589,1.8356,1.8125,1.7896,1.7669,1.7444,1.7221,1.7,1.6781,1.6564,1.6349,1.6136,1.5925,1.5716,1.5509,1.5304,1.5101,1.49,1.4701,1.4504,1.4309,1.4116,1.3925,1.3736,1.3549,1.3364,1.3181,1.3,1.2821,1.2644,1.2469,1.2296,1.2125,1.1956,1.1789,1.1624,1.1461,1.13,1.1141,1.0984,1.0829,1.0676,1.0525,1.0376,1.0229,1.0084,0.9941,0.98,0.9661,0.9524,0.9389,0.9256,0.9125,0.8996,0.8869,0.8744,0.8621,0.85,0.8381,0.8264,0.8149,0.8036,0.7925,0.7816,0.7709,0.7604,0.7501,0.74,0.7301,0.7204,0.7109,0.7016,0.6925,0.6836,0.6749,0.6664,0.6581,0.65,0.6421,0.6344,0.6269,0.6196,0.6125,0.6056,0.5989,0.5924,0.5861,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1661,2.1402,2.1145,2.089,2.0637,2.0386,2.0137,1.989,1.9645,1.9402,1.9161,1.8922,1.8685,1.845,1.8217,1.7986,1.7757,1.753,1.7305,1.7082,1.6861,1.6642,1.6425,1.621,1.5997,1.5786,1.5577,1.537,1.5165,1.4962,1.4761,1.4562,1.4365,1.417,1.3977,1.3786,1.3597,1.341,1.3225,1.3042,1.2861,1.2682,1.2505,1.233,1.2157,1.1986,1.1817,1.165,1.1485,1.1322,1.1161,1.1002,1.0845,1.069,1.0537,1.0386,1.0237,1.009,0.9945,0.9802,0.9661,0.9522,0.9385,0.925,0.9117,0.8986,0.8857,0.873,0.8605,0.8482,0.8361,0.8242,0.8125,0.801,0.7897,0.7786,0.7677,0.757,0.7465,0.7362,0.7261,0.7162,0.7065,0.697,0.6877,0.6786,0.6697,0.661,0.6525,0.6442,0.6361,0.6282,0.6205,0.613,0.6057,0.5986,0.5917,0.585,0.5785,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1785,2.1524,2.1265,2.1008,2.0753,2.05,2.0249,2,1.9753,1.9508,1.9265,1.9024,1.8785,1.8548,1.8313,1.808,1.7849,1.762,1.7393,1.7168,1.6945,1.6724,1.6505,1.6288,1.6073,1.586,1.5649,1.544,1.5233,1.5028,1.4825,1.4624,1.4425,1.4228,1.4033,1.384,1.3649,1.346,1.3273,1.3088,1.2905,1.2724,1.2545,1.2368,1.2193,1.202,1.1849,1.168,1.1513,1.1348,1.1185,1.1024,1.0865,1.0708,1.0553,1.04,1.0249,1.01,0.9953,0.9808,0.9665,0.9524,0.9385,0.9248,0.9113,0.898,0.8849,0.872,0.8593,0.8468,0.8345,0.8224,0.8105,0.7988,0.7873,0.776,0.7649,0.754,0.7433,0.7328,0.7225,0.7124,0.7025,0.6928,0.6833,0.674,0.6649,0.656,0.6473,0.6388,0.6305,0.6224,0.6145,0.6068,0.5993,0.592,0.5849,0.578,0.5713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.1913,2.165,2.1389,2.113,2.0873,2.0618,2.0365,2.0114,1.9865,1.9618,1.9373,1.913,1.8889,1.865,1.8413,1.8178,1.7945,1.7714,1.7485,1.7258,1.7033,1.681,1.6589,1.637,1.6153,1.5938,1.5725,1.5514,1.5305,1.5098,1.4893,1.469,1.4489,1.429,1.4093,1.3898,1.3705,1.3514,1.3325,1.3138,1.2953,1.277,1.2589,1.241,1.2233,1.2058,1.1885,1.1714,1.1545,1.1378,1.1213,1.105,1.0889,1.073,1.0573,1.0418,1.0265,1.0114,0.9965,0.9818,0.9673,0.953,0.9389,0.925,0.9113,0.8978,0.8845,0.8714,0.8585,0.8458,0.8333,0.821,0.8089,0.797,0.7853,0.7738,0.7625,0.7514,0.7405,0.7298,0.7193,0.709,0.6989,0.689,0.6793,0.6698,0.6605,0.6514,0.6425,0.6338,0.6253,0.617,0.6089,0.601,0.5933,0.5858,0.5785,0.5714,0.5645,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2045,2.178,2.1517,2.1256,2.0997,2.074,2.0485,2.0232,1.9981,1.9732,1.9485,1.924,1.8997,1.8756,1.8517,1.828,1.8045,1.7812,1.7581,1.7352,1.7125,1.69,1.6677,1.6456,1.6237,1.602,1.5805,1.5592,1.5381,1.5172,1.4965,1.476,1.4557,1.4356,1.4157,1.396,1.3765,1.3572,1.3381,1.3192,1.3005,1.282,1.2637,1.2456,1.2277,1.21,1.1925,1.1752,1.1581,1.1412,1.1245,1.108,1.0917,1.0756,1.0597,1.044,1.0285,1.0132,0.9981,0.9832,0.9685,0.954,0.9397,0.9256,0.9117,0.898,0.8845,0.8712,0.8581,0.8452,0.8325,0.82,0.8077,0.7956,0.7837,0.772,0.7605,0.7492,0.7381,0.7272,0.7165,0.706,0.6957,0.6856,0.6757,0.666,0.6565,0.6472,0.6381,0.6292,0.6205,0.612,0.6037,0.5956,0.5877,0.58,0.5725,0.5652,0.5581,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2181,2.1914,2.1649,2.1386,2.1125,2.0866,2.0609,2.0354,2.0101,1.985,1.9601,1.9354,1.9109,1.8866,1.8625,1.8386,1.8149,1.7914,1.7681,1.745,1.7221,1.6994,1.6769,1.6546,1.6325,1.6106,1.5889,1.5674,1.5461,1.525,1.5041,1.4834,1.4629,1.4426,1.4225,1.4026,1.3829,1.3634,1.3441,1.325,1.3061,1.2874,1.2689,1.2506,1.2325,1.2146,1.1969,1.1794,1.1621,1.145,1.1281,1.1114,1.0949,1.0786,1.0625,1.0466,1.0309,1.0154,1.0001,0.985,0.9701,0.9554,0.9409,0.9266,0.9125,0.8986,0.8849,0.8714,0.8581,0.845,0.8321,0.8194,0.8069,0.7946,0.7825,0.7706,0.7589,0.7474,0.7361,0.725,0.7141,0.7034,0.6929,0.6826,0.6725,0.6626,0.6529,0.6434,0.6341,0.625,0.6161,0.6074,0.5989,0.5906,0.5825,0.5746,0.5669,0.5594,0.5521,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2321,2.2052,2.1785,2.152,2.1257,2.0996,2.0737,2.048,2.0225,1.9972,1.9721,1.9472,1.9225,1.898,1.8737,1.8496,1.8257,1.802,1.7785,1.7552,1.7321,1.7092,1.6865,1.664,1.6417,1.6196,1.5977,1.576,1.5545,1.5332,1.5121,1.4912,1.4705,1.45,1.4297,1.4096,1.3897,1.37,1.3505,1.3312,1.3121,1.2932,1.2745,1.256,1.2377,1.2196,1.2017,1.184,1.1665,1.1492,1.1321,1.1152,1.0985,1.082,1.0657,1.0496,1.0337,1.018,1.0025,0.9872,0.9721,0.9572,0.9425,0.928,0.9137,0.8996,0.8857,0.872,0.8585,0.8452,0.8321,0.8192,0.8065,0.794,0.7817,0.7696,0.7577,0.746,0.7345,0.7232,0.7121,0.7012,0.6905,0.68,0.6697,0.6596,0.6497,0.64,0.6305,0.6212,0.6121,0.6032,0.5945,0.586,0.5777,0.5696,0.5617,0.554,0.5465,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,2.2465,2.2194,2.1925,2.1658,2.1393,2.113,2.0869,2.061,2.0353,2.0098,1.9845,1.9594,1.9345,1.9098,1.8853,1.861,1.8369,1.813,1.7893,1.7658,1.7425,1.7194,1.6965,1.6738,1.6513,1.629,1.6069,1.585,1.5633,1.5418,1.5205,1.4994,1.4785,1.4578,1.4373,1.417,1.3969,1.377,1.3573,1.3378,1.3185,1.2994,1.2805,1.2618,1.2433,1.225,1.2069,1.189,1.1713,1.1538,1.1365,1.1194,1.1025,1.0858,1.0693,1.053,1.0369,1.021,1.0053,0.9898,0.9745,0.9594,0.9445,0.9298,0.9153,0.901,0.8869,0.873,0.8593,0.8458,0.8325,0.8194,0.8065,0.7938,0.7813,0.769,0.7569,0.745,0.7333,0.7218,0.7105,0.6994,0.6885,0.6778,0.6673,0.657,0.6469,0.637,0.6273,0.6178,0.6085,0.5994,0.5905,0.5818,0.5733,0.565,0.5569,0.549,0.5413,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,2.2613,2.234,2.2069,2.18,2.1533,2.1268,2.1005,2.0744,2.0485,2.0228,1.9973,1.972,1.9469,1.922,1.8973,1.8728,1.8485,1.8244,1.8005,1.7768,1.7533,1.73,1.7069,1.684,1.6613,1.6388,1.6165,1.5944,1.5725,1.5508,1.5293,1.508,1.4869,1.466,1.4453,1.4248,1.4045,1.3844,1.3645,1.3448,1.3253,1.306,1.2869,1.268,1.2493,1.2308,1.2125,1.1944,1.1765,1.1588,1.1413,1.124,1.1069,1.09,1.0733,1.0568,1.0405,1.0244,1.0085,0.9928,0.9773,0.962,0.9469,0.932,0.9173,0.9028,0.8885,0.8744,0.8605,0.8468,0.8333,0.82,0.8069,0.794,0.7813,0.7688,0.7565,0.7444,0.7325,0.7208,0.7093,0.698,0.6869,0.676,0.6653,0.6548,0.6445,0.6344,0.6245,0.6148,0.6053,0.596,0.5869,0.578,0.5693,0.5608,0.5525,0.5444,0.5365,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,2.2765,2.249,2.2217,2.1946,2.1677,2.141,2.1145,2.0882,2.0621,2.0362,2.0105,1.985,1.9597,1.9346,1.9097,1.885,1.8605,1.8362,1.8121,1.7882,1.7645,1.741,1.7177,1.6946,1.6717,1.649,1.6265,1.6042,1.5821,1.5602,1.5385,1.517,1.4957,1.4746,1.4537,1.433,1.4125,1.3922,1.3721,1.3522,1.3325,1.313,1.2937,1.2746,1.2557,1.237,1.2185,1.2002,1.1821,1.1642,1.1465,1.129,1.1117,1.0946,1.0777,1.061,1.0445,1.0282,1.0121,0.9962,0.9805,0.965,0.9497,0.9346,0.9197,0.905,0.8905,0.8762,0.8621,0.8482,0.8345,0.821,0.8077,0.7946,0.7817,0.769,0.7565,0.7442,0.7321,0.7202,0.7085,0.697,0.6857,0.6746,0.6637,0.653,0.6425,0.6322,0.6221,0.6122,0.6025,0.593,0.5837,0.5746,0.5657,0.557,0.5485,0.5402,0.5321,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,2.2921,2.2644,2.2369,2.2096,2.1825,2.1556,2.1289,2.1024,2.0761,2.05,2.0241,1.9984,1.9729,1.9476,1.9225,1.8976,1.8729,1.8484,1.8241,1.8,1.7761,1.7524,1.7289,1.7056,1.6825,1.6596,1.6369,1.6144,1.5921,1.57,1.5481,1.5264,1.5049,1.4836,1.4625,1.4416,1.4209,1.4004,1.3801,1.36,1.3401,1.3204,1.3009,1.2816,1.2625,1.2436,1.2249,1.2064,1.1881,1.17,1.1521,1.1344,1.1169,1.0996,1.0825,1.0656,1.0489,1.0324,1.0161,1,0.9841,0.9684,0.9529,0.9376,0.9225,0.9076,0.8929,0.8784,0.8641,0.85,0.8361,0.8224,0.8089,0.7956,0.7825,0.7696,0.7569,0.7444,0.7321,0.72,0.7081,0.6964,0.6849,0.6736,0.6625,0.6516,0.6409,0.6304,0.6201,0.61,0.6001,0.5904,0.5809,0.5716,0.5625,0.5536,0.5449,0.5364,0.5281,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,2.3081,2.2802,2.2525,2.225,2.1977,2.1706,2.1437,2.117,2.0905,2.0642,2.0381,2.0122,1.9865,1.961,1.9357,1.9106,1.8857,1.861,1.8365,1.8122,1.7881,1.7642,1.7405,1.717,1.6937,1.6706,1.6477,1.625,1.6025,1.5802,1.5581,1.5362,1.5145,1.493,1.4717,1.4506,1.4297,1.409,1.3885,1.3682,1.3481,1.3282,1.3085,1.289,1.2697,1.2506,1.2317,1.213,1.1945,1.1762,1.1581,1.1402,1.1225,1.105,1.0877,1.0706,1.0537,1.037,1.0205,1.0042,0.9881,0.9722,0.9565,0.941,0.9257,0.9106,0.8957,0.881,0.8665,0.8522,0.8381,0.8242,0.8105,0.797,0.7837,0.7706,0.7577,0.745,0.7325,0.7202,0.7081,0.6962,0.6845,0.673,0.6617,0.6506,0.6397,0.629,0.6185,0.6082,0.5981,0.5882,0.5785,0.569,0.5597,0.5506,0.5417,0.533,0.5245,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,2.3245,2.2964,2.2685,2.2408,2.2133,2.186,2.1589,2.132,2.1053,2.0788,2.0525,2.0264,2.0005,1.9748,1.9493,1.924,1.8989,1.874,1.8493,1.8248,1.8005,1.7764,1.7525,1.7288,1.7053,1.682,1.6589,1.636,1.6133,1.5908,1.5685,1.5464,1.5245,1.5028,1.4813,1.46,1.4389,1.418,1.3973,1.3768,1.3565,1.3364,1.3165,1.2968,1.2773,1.258,1.2389,1.22,1.2013,1.1828,1.1645,1.1464,1.1285,1.1108,1.0933,1.076,1.0589,1.042,1.0253,1.0088,0.9925,0.9764,0.9605,0.9448,0.9293,0.914,0.8989,0.884,0.8693,0.8548,0.8405,0.8264,0.8125,0.7988,0.7853,0.772,0.7589,0.746,0.7333,0.7208,0.7085,0.6964,0.6845,0.6728,0.6613,0.65,0.6389,0.628,0.6173,0.6068,0.5965,0.5864,0.5765,0.5668,0.5573,0.548,0.5389,0.53,0.5213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,2.3413,2.313,2.2849,2.257,2.2293,2.2018,2.1745,2.1474,2.1205,2.0938,2.0673,2.041,2.0149,1.989,1.9633,1.9378,1.9125,1.8874,1.8625,1.8378,1.8133,1.789,1.7649,1.741,1.7173,1.6938,1.6705,1.6474,1.6245,1.6018,1.5793,1.557,1.5349,1.513,1.4913,1.4698,1.4485,1.4274,1.4065,1.3858,1.3653,1.345,1.3249,1.305,1.2853,1.2658,1.2465,1.2274,1.2085,1.1898,1.1713,1.153,1.1349,1.117,1.0993,1.0818,1.0645,1.0474,1.0305,1.0138,0.9973,0.981,0.9649,0.949,0.9333,0.9178,0.9025,0.8874,0.8725,0.8578,0.8433,0.829,0.8149,0.801,0.7873,0.7738,0.7605,0.7474,0.7345,0.7218,0.7093,0.697,0.6849,0.673,0.6613,0.6498,0.6385,0.6274,0.6165,0.6058,0.5953,0.585,0.5749,0.565,0.5553,0.5458,0.5365,0.5274,0.5185,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,2.3585,2.33,2.3017,2.2736,2.2457,2.218,2.1905,2.1632,2.1361,2.1092,2.0825,2.056,2.0297,2.0036,1.9777,1.952,1.9265,1.9012,1.8761,1.8512,1.8265,1.802,1.7777,1.7536,1.7297,1.706,1.6825,1.6592,1.6361,1.6132,1.5905,1.568,1.5457,1.5236,1.5017,1.48,1.4585,1.4372,1.4161,1.3952,1.3745,1.354,1.3337,1.3136,1.2937,1.274,1.2545,1.2352,1.2161,1.1972,1.1785,1.16,1.1417,1.1236,1.1057,1.088,1.0705,1.0532,1.0361,1.0192,1.0025,0.986,0.9697,0.9536,0.9377,0.922,0.9065,0.8912,0.8761,0.8612,0.8465,0.832,0.8177,0.8036,0.7897,0.776,0.7625,0.7492,0.7361,0.7232,0.7105,0.698,0.6857,0.6736,0.6617,0.65,0.6385,0.6272,0.6161,0.6052,0.5945,0.584,0.5737,0.5636,0.5537,0.544,0.5345,0.5252,0.5161,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,2.3761,2.3474,2.3189,2.2906,2.2625,2.2346,2.2069,2.1794,2.1521,2.125,2.0981,2.0714,2.0449,2.0186,1.9925,1.9666,1.9409,1.9154,1.8901,1.865,1.8401,1.8154,1.7909,1.7666,1.7425,1.7186,1.6949,1.6714,1.6481,1.625,1.6021,1.5794,1.5569,1.5346,1.5125,1.4906,1.4689,1.4474,1.4261,1.405,1.3841,1.3634,1.3429,1.3226,1.3025,1.2826,1.2629,1.2434,1.2241,1.205,1.1861,1.1674,1.1489,1.1306,1.1125,1.0946,1.0769,1.0594,1.0421,1.025,1.0081,0.9914,0.9749,0.9586,0.9425,0.9266,0.9109,0.8954,0.8801,0.865,0.8501,0.8354,0.8209,0.8066,0.7925,0.7786,0.7649,0.7514,0.7381,0.725,0.7121,0.6994,0.6869,0.6746,0.6625,0.6506,0.6389,0.6274,0.6161,0.605,0.5941,0.5834,0.5729,0.5626,0.5525,0.5426,0.5329,0.5234,0.5141,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,2.3941,2.3652,2.3365,2.308,2.2797,2.2516,2.2237,2.196,2.1685,2.1412,2.1141,2.0872,2.0605,2.034,2.0077,1.9816,1.9557,1.93,1.9045,1.8792,1.8541,1.8292,1.8045,1.78,1.7557,1.7316,1.7077,1.684,1.6605,1.6372,1.6141,1.5912,1.5685,1.546,1.5237,1.5016,1.4797,1.458,1.4365,1.4152,1.3941,1.3732,1.3525,1.332,1.3117,1.2916,1.2717,1.252,1.2325,1.2132,1.1941,1.1752,1.1565,1.138,1.1197,1.1016,1.0837,1.066,1.0485,1.0312,1.0141,0.9972,0.9805,0.964,0.9477,0.9316,0.9157,0.9,0.8845,0.8692,0.8541,0.8392,0.8245,0.81,0.7957,0.7816,0.7677,0.754,0.7405,0.7272,0.7141,0.7012,0.6885,0.676,0.6637,0.6516,0.6397,0.628,0.6165,0.6052,0.5941,0.5832,0.5725,0.562,0.5517,0.5416,0.5317,0.522,0.5125,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,2.4125,2.3834,2.3545,2.3258,2.2973,2.269,2.2409,2.213,2.1853,2.1578,2.1305,2.1034,2.0765,2.0498,2.0233,1.997,1.9709,1.945,1.9193,1.8938,1.8685,1.8434,1.8185,1.7938,1.7693,1.745,1.7209,1.697,1.6733,1.6498,1.6265,1.6034,1.5805,1.5578,1.5353,1.513,1.4909,1.469,1.4473,1.4258,1.4045,1.3834,1.3625,1.3418,1.3213,1.301,1.2809,1.261,1.2413,1.2218,1.2025,1.1834,1.1645,1.1458,1.1273,1.109,1.0909,1.073,1.0553,1.0378,1.0205,1.0034,0.9865,0.9698,0.9533,0.937,0.9209,0.905,0.8893,0.8738,0.8585,0.8434,0.8285,0.8138,0.7993,0.785,0.7709,0.757,0.7433,0.7298,0.7165,0.7034,0.6905,0.6778,0.6653,0.653,0.6409,0.629,0.6173,0.6058,0.5945,0.5834,0.5725,0.5618,0.5513,0.541,0.5309,0.521,0.5113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,2.4313,2.402,2.3729,2.344,2.3153,2.2868,2.2585,2.2304,2.2025,2.1748,2.1473,2.12,2.0929,2.066,2.0393,2.0128,1.9865,1.9604,1.9345,1.9088,1.8833,1.858,1.8329,1.808,1.7833,1.7588,1.7345,1.7104,1.6865,1.6628,1.6393,1.616,1.5929,1.57,1.5473,1.5248,1.5025,1.4804,1.4585,1.4368,1.4153,1.394,1.3729,1.352,1.3313,1.3108,1.2905,1.2704,1.2505,1.2308,1.2113,1.192,1.1729,1.154,1.1353,1.1168,1.0985,1.0804,1.0625,1.0448,1.0273,1.01,0.9929,0.976,0.9593,0.9428,0.9265,0.9104,0.8945,0.8788,0.8633,0.848,0.8329,0.818,0.8033,0.7888,0.7745,0.7604,0.7465,0.7328,0.7193,0.706,0.6929,0.68,0.6673,0.6548,0.6425,0.6304,0.6185,0.6068,0.5953,0.584,0.5729,0.562,0.5513,0.5408,0.5305,0.5204,0.5105,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,2.4505,2.421,2.3917,2.3626,2.3337,2.305,2.2765,2.2482,2.2201,2.1922,2.1645,2.137,2.1097,2.0826,2.0557,2.029,2.0025,1.9762,1.9501,1.9242,1.8985,1.873,1.8477,1.8226,1.7977,1.773,1.7485,1.7242,1.7001,1.6762,1.6525,1.629,1.6057,1.5826,1.5597,1.537,1.5145,1.4922,1.4701,1.4482,1.4265,1.405,1.3837,1.3626,1.3417,1.321,1.3005,1.2802,1.2601,1.2402,1.2205,1.201,1.1817,1.1626,1.1437,1.125,1.1065,1.0882,1.0701,1.0522,1.0345,1.017,0.9997,0.9826,0.9657,0.949,0.9325,0.9162,0.9001,0.8842,0.8685,0.853,0.8377,0.8226,0.8077,0.793,0.7785,0.7642,0.7501,0.7362,0.7225,0.709,0.6957,0.6826,0.6697,0.657,0.6445,0.6322,0.6201,0.6082,0.5965,0.585,0.5737,0.5626,0.5517,0.541,0.5305,0.5202,0.5101,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,2.4701,2.4404,2.4109,2.3816,2.3525,2.3236,2.2949,2.2664,2.2381,2.21,2.1821,2.1544,2.1269,2.0996,2.0725,2.0456,2.0189,1.9924,1.9661,1.94,1.9141,1.8884,1.8629,1.8376,1.8125,1.7876,1.7629,1.7384,1.7141,1.69,1.6661,1.6424,1.6189,1.5956,1.5725,1.5496,1.5269,1.5044,1.4821,1.46,1.4381,1.4164,1.3949,1.3736,1.3525,1.3316,1.3109,1.2904,1.2701,1.25,1.2301,1.2104,1.1909,1.1716,1.1525,1.1336,1.1149,1.0964,1.0781,1.06,1.0421,1.0244,1.0069,0.9896,0.9725,0.9556,0.9389,0.9224,0.9061,0.89,0.8741,0.8584,0.8429,0.8276,0.8125,0.7976,0.7829,0.7684,0.7541,0.74,0.7261,0.7124,0.6989,0.6856,0.6725,0.6596,0.6469,0.6344,0.6221,0.61,0.5981,0.5864,0.5749,0.5636,0.5525,0.5416,0.5309,0.5204,0.5101,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4901,2.4602,2.4305,2.401,2.3717,2.3426,2.3137,2.285,2.2565,2.2282,2.2001,2.1722,2.1445,2.117,2.0897,2.0626,2.0357,2.009,1.9825,1.9562,1.9301,1.9042,1.8785,1.853,1.8277,1.8026,1.7777,1.753,1.7285,1.7042,1.6801,1.6562,1.6325,1.609,1.5857,1.5626,1.5397,1.517,1.4945,1.4722,1.4501,1.4282,1.4065,1.385,1.3637,1.3426,1.3217,1.301,1.2805,1.2602,1.2401,1.2202,1.2005,1.181,1.1617,1.1426,1.1237,1.105,1.0865,1.0682,1.0501,1.0322,1.0145,0.997,0.9797,0.9626,0.9457,0.929,0.9125,0.8962,0.8801,0.8642,0.8485,0.833,0.8177,0.8026,0.7877,0.773,0.7585,0.7442,0.7301,0.7162,0.7025,0.689,0.6757,0.6626,0.6497,0.637,0.6245,0.6122,0.6001,0.5882,0.5765,0.565,0.5537,0.5426,0.5317,0.521,0.5105,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4804,2.4505,2.4208,2.3913,2.362,2.3329,2.304,2.2753,2.2468,2.2185,2.1904,2.1625,2.1348,2.1073,2.08,2.0529,2.026,1.9993,1.9728,1.9465,1.9204,1.8945,1.8688,1.8433,1.818,1.7929,1.768,1.7433,1.7188,1.6945,1.6704,1.6465,1.6228,1.5993,1.576,1.5529,1.53,1.5073,1.4848,1.4625,1.4404,1.4185,1.3968,1.3753,1.354,1.3329,1.312,1.2913,1.2708,1.2505,1.2304,1.2105,1.1908,1.1713,1.152,1.1329,1.114,1.0953,1.0768,1.0585,1.0404,1.0225,1.0048,0.9873,0.97,0.9529,0.936,0.9193,0.9028,0.8865,0.8704,0.8545,0.8388,0.8233,0.808,0.7929,0.778,0.7633,0.7488,0.7345,0.7204,0.7065,0.6928,0.6793,0.666,0.6529,0.64,0.6273,0.6148,0.6025,0.5904,0.5785,0.5668,0.5553,0.544,0.5329,0.522,0.5113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4709,2.441,2.4113,2.3818,2.3525,2.3234,2.2945,2.2658,2.2373,2.209,2.1809,2.153,2.1253,2.0978,2.0705,2.0434,2.0165,1.9898,1.9633,1.937,1.9109,1.885,1.8593,1.8338,1.8085,1.7834,1.7585,1.7338,1.7093,1.685,1.6609,1.637,1.6133,1.5898,1.5665,1.5434,1.5205,1.4978,1.4753,1.453,1.4309,1.409,1.3873,1.3658,1.3445,1.3234,1.3025,1.2818,1.2613,1.241,1.2209,1.201,1.1813,1.1618,1.1425,1.1234,1.1045,1.0858,1.0673,1.049,1.0309,1.013,0.9953,0.9778,0.9605,0.9434,0.9265,0.9098,0.8933,0.877,0.8609,0.845,0.8293,0.8138,0.7985,0.7834,0.7685,0.7538,0.7393,0.725,0.7109,0.697,0.6833,0.6698,0.6565,0.6434,0.6305,0.6178,0.6053,0.593,0.5809,0.569,0.5573,0.5458,0.5345,0.5234,0.5125,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4616,2.4317,2.402,2.3725,2.3432,2.3141,2.2852,2.2565,2.228,2.1997,2.1716,2.1437,2.116,2.0885,2.0612,2.0341,2.0072,1.9805,1.954,1.9277,1.9016,1.8757,1.85,1.8245,1.7992,1.7741,1.7492,1.7245,1.7,1.6757,1.6516,1.6277,1.604,1.5805,1.5572,1.5341,1.5112,1.4885,1.466,1.4437,1.4216,1.3997,1.378,1.3565,1.3352,1.3141,1.2932,1.2725,1.252,1.2317,1.2116,1.1917,1.172,1.1525,1.1332,1.1141,1.0952,1.0765,1.058,1.0397,1.0216,1.0037,0.986,0.9685,0.9512,0.9341,0.9172,0.9005,0.884,0.8677,0.8516,0.8357,0.82,0.8045,0.7892,0.7741,0.7592,0.7445,0.73,0.7157,0.7016,0.6877,0.674,0.6605,0.6472,0.6341,0.6212,0.6085,0.596,0.5837,0.5716,0.5597,0.548,0.5365,0.5252,0.5141,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4525,2.4226,2.3929,2.3634,2.3341,2.305,2.2761,2.2474,2.2189,2.1906,2.1625,2.1346,2.1069,2.0794,2.0521,2.025,1.9981,1.9714,1.9449,1.9186,1.8925,1.8666,1.8409,1.8154,1.7901,1.765,1.7401,1.7154,1.6909,1.6666,1.6425,1.6186,1.5949,1.5714,1.5481,1.525,1.5021,1.4794,1.4569,1.4346,1.4125,1.3906,1.3689,1.3474,1.3261,1.305,1.2841,1.2634,1.2429,1.2226,1.2025,1.1826,1.1629,1.1434,1.1241,1.105,1.0861,1.0674,1.0489,1.0306,1.0125,0.9946,0.9769,0.9594,0.9421,0.925,0.9081,0.8914,0.8749,0.8586,0.8425,0.8266,0.8109,0.7954,0.7801,0.765,0.7501,0.7354,0.7209,0.7066,0.6925,0.6786,0.6649,0.6514,0.6381,0.625,0.6121,0.5994,0.5869,0.5746,0.5625,0.5506,0.5389,0.5274,0.5161,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4436,2.4137,2.384,2.3545,2.3252,2.2961,2.2672,2.2385,2.21,2.1817,2.1536,2.1257,2.098,2.0705,2.0432,2.0161,1.9892,1.9625,1.936,1.9097,1.8836,1.8577,1.832,1.8065,1.7812,1.7561,1.7312,1.7065,1.682,1.6577,1.6336,1.6097,1.586,1.5625,1.5392,1.5161,1.4932,1.4705,1.448,1.4257,1.4036,1.3817,1.36,1.3385,1.3172,1.2961,1.2752,1.2545,1.234,1.2137,1.1936,1.1737,1.154,1.1345,1.1152,1.0961,1.0772,1.0585,1.04,1.0217,1.0036,0.9857,0.968,0.9505,0.9332,0.9161,0.8992,0.8825,0.866,0.8497,0.8336,0.8177,0.802,0.7865,0.7712,0.7561,0.7412,0.7265,0.712,0.6977,0.6836,0.6697,0.656,0.6425,0.6292,0.6161,0.6032,0.5905,0.578,0.5657,0.5536,0.5417,0.53,0.5185,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4349,2.405,2.3753,2.3458,2.3165,2.2874,2.2585,2.2298,2.2013,2.173,2.1449,2.117,2.0893,2.0618,2.0345,2.0074,1.9805,1.9538,1.9273,1.901,1.8749,1.849,1.8233,1.7978,1.7725,1.7474,1.7225,1.6978,1.6733,1.649,1.6249,1.601,1.5773,1.5538,1.5305,1.5074,1.4845,1.4618,1.4393,1.417,1.3949,1.373,1.3513,1.3298,1.3085,1.2874,1.2665,1.2458,1.2253,1.205,1.1849,1.165,1.1453,1.1258,1.1065,1.0874,1.0685,1.0498,1.0313,1.013,0.9949,0.977,0.9593,0.9418,0.9245,0.9074,0.8905,0.8738,0.8573,0.841,0.8249,0.809,0.7933,0.7778,0.7625,0.7474,0.7325,0.7178,0.7033,0.689,0.6749,0.661,0.6473,0.6338,0.6205,0.6074,0.5945,0.5818,0.5693,0.557,0.5449,0.533,0.5213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4264,2.3965,2.3668,2.3373,2.308,2.2789,2.25,2.2213,2.1928,2.1645,2.1364,2.1085,2.0808,2.0533,2.026,1.9989,1.972,1.9453,1.9188,1.8925,1.8664,1.8405,1.8148,1.7893,1.764,1.7389,1.714,1.6893,1.6648,1.6405,1.6164,1.5925,1.5688,1.5453,1.522,1.4989,1.476,1.4533,1.4308,1.4085,1.3864,1.3645,1.3428,1.3213,1.3,1.2789,1.258,1.2373,1.2168,1.1965,1.1764,1.1565,1.1368,1.1173,1.098,1.0789,1.06,1.0413,1.0228,1.0045,0.9864,0.9685,0.9508,0.9333,0.916,0.8989,0.882,0.8653,0.8488,0.8325,0.8164,0.8005,0.7848,0.7693,0.754,0.7389,0.724,0.7093,0.6948,0.6805,0.6664,0.6525,0.6388,0.6253,0.612,0.5989,0.586,0.5733,0.5608,0.5485,0.5364,0.5245,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4181,2.3882,2.3585,2.329,2.2997,2.2706,2.2417,2.213,2.1845,2.1562,2.1281,2.1002,2.0725,2.045,2.0177,1.9906,1.9637,1.937,1.9105,1.8842,1.8581,1.8322,1.8065,1.781,1.7557,1.7306,1.7057,1.681,1.6565,1.6322,1.6081,1.5842,1.5605,1.537,1.5137,1.4906,1.4677,1.445,1.4225,1.4002,1.3781,1.3562,1.3345,1.313,1.2917,1.2706,1.2497,1.229,1.2085,1.1882,1.1681,1.1482,1.1285,1.109,1.0897,1.0706,1.0517,1.033,1.0145,0.9962,0.9781,0.9602,0.9425,0.925,0.9077,0.8906,0.8737,0.857,0.8405,0.8242,0.8081,0.7922,0.7765,0.761,0.7457,0.7306,0.7157,0.701,0.6865,0.6722,0.6581,0.6442,0.6305,0.617,0.6037,0.5906,0.5777,0.565,0.5525,0.5402,0.5281,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.41,2.3801,2.3504,2.3209,2.2916,2.2625,2.2336,2.2049,2.1764,2.1481,2.12,2.0921,2.0644,2.0369,2.0096,1.9825,1.9556,1.9289,1.9024,1.8761,1.85,1.8241,1.7984,1.7729,1.7476,1.7225,1.6976,1.6729,1.6484,1.6241,1.6,1.5761,1.5524,1.5289,1.5056,1.4825,1.4596,1.4369,1.4144,1.3921,1.37,1.3481,1.3264,1.3049,1.2836,1.2625,1.2416,1.2209,1.2004,1.1801,1.16,1.1401,1.1204,1.1009,1.0816,1.0625,1.0436,1.0249,1.0064,0.9881,0.97,0.9521,0.9344,0.9169,0.8996,0.8825,0.8656,0.8489,0.8324,0.8161,0.8,0.7841,0.7684,0.7529,0.7376,0.7225,0.7076,0.6929,0.6784,0.6641,0.65,0.6361,0.6224,0.6089,0.5956,0.5825,0.5696,0.5569,0.5444,0.5321,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4021,2.3722,2.3425,2.313,2.2837,2.2546,2.2257,2.197,2.1685,2.1402,2.1121,2.0842,2.0565,2.029,2.0017,1.9746,1.9477,1.921,1.8945,1.8682,1.8421,1.8162,1.7905,1.765,1.7397,1.7146,1.6897,1.665,1.6405,1.6162,1.5921,1.5682,1.5445,1.521,1.4977,1.4746,1.4517,1.429,1.4065,1.3842,1.3621,1.3402,1.3185,1.297,1.2757,1.2546,1.2337,1.213,1.1925,1.1722,1.1521,1.1322,1.1125,1.093,1.0737,1.0546,1.0357,1.017,0.9985,0.9802,0.9621,0.9442,0.9265,0.909,0.8917,0.8746,0.8577,0.841,0.8245,0.8082,0.7921,0.7762,0.7605,0.745,0.7297,0.7146,0.6997,0.685,0.6705,0.6562,0.6421,0.6282,0.6145,0.601,0.5877,0.5746,0.5617,0.549,0.5365,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3944,2.3645,2.3348,2.3053,2.276,2.2469,2.218,2.1893,2.1608,2.1325,2.1044,2.0765,2.0488,2.0213,1.994,1.9669,1.94,1.9133,1.8868,1.8605,1.8344,1.8085,1.7828,1.7573,1.732,1.7069,1.682,1.6573,1.6328,1.6085,1.5844,1.5605,1.5368,1.5133,1.49,1.4669,1.444,1.4213,1.3988,1.3765,1.3544,1.3325,1.3108,1.2893,1.268,1.2469,1.226,1.2053,1.1848,1.1645,1.1444,1.1245,1.1048,1.0853,1.066,1.0469,1.028,1.0093,0.9908,0.9725,0.9544,0.9365,0.9188,0.9013,0.884,0.8669,0.85,0.8333,0.8168,0.8005,0.7844,0.7685,0.7528,0.7373,0.722,0.7069,0.692,0.6773,0.6628,0.6485,0.6344,0.6205,0.6068,0.5933,0.58,0.5669,0.554,0.5413,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3869,2.357,2.3273,2.2978,2.2685,2.2394,2.2105,2.1818,2.1533,2.125,2.0969,2.069,2.0413,2.0138,1.9865,1.9594,1.9325,1.9058,1.8793,1.853,1.8269,1.801,1.7753,1.7498,1.7245,1.6994,1.6745,1.6498,1.6253,1.601,1.5769,1.553,1.5293,1.5058,1.4825,1.4594,1.4365,1.4138,1.3913,1.369,1.3469,1.325,1.3033,1.2818,1.2605,1.2394,1.2185,1.1978,1.1773,1.157,1.1369,1.117,1.0973,1.0778,1.0585,1.0394,1.0205,1.0018,0.9833,0.965,0.9469,0.929,0.9113,0.8938,0.8765,0.8594,0.8425,0.8258,0.8093,0.793,0.7769,0.761,0.7453,0.7298,0.7145,0.6994,0.6845,0.6698,0.6553,0.641,0.6269,0.613,0.5993,0.5858,0.5725,0.5594,0.5465,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3796,2.3497,2.32,2.2905,2.2612,2.2321,2.2032,2.1745,2.146,2.1177,2.0896,2.0617,2.034,2.0065,1.9792,1.9521,1.9252,1.8985,1.872,1.8457,1.8196,1.7937,1.768,1.7425,1.7172,1.6921,1.6672,1.6425,1.618,1.5937,1.5696,1.5457,1.522,1.4985,1.4752,1.4521,1.4292,1.4065,1.384,1.3617,1.3396,1.3177,1.296,1.2745,1.2532,1.2321,1.2112,1.1905,1.17,1.1497,1.1296,1.1097,1.09,1.0705,1.0512,1.0321,1.0132,0.9945,0.976,0.9577,0.9396,0.9217,0.904,0.8865,0.8692,0.8521,0.8352,0.8185,0.802,0.7857,0.7696,0.7537,0.738,0.7225,0.7072,0.6921,0.6772,0.6625,0.648,0.6337,0.6196,0.6057,0.592,0.5785,0.5652,0.5521,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3725,2.3426,2.3129,2.2834,2.2541,2.225,2.1961,2.1674,2.1389,2.1106,2.0825,2.0546,2.0269,1.9994,1.9721,1.945,1.9181,1.8914,1.8649,1.8386,1.8125,1.7866,1.7609,1.7354,1.7101,1.685,1.6601,1.6354,1.6109,1.5866,1.5625,1.5386,1.5149,1.4914,1.4681,1.445,1.4221,1.3994,1.3769,1.3546,1.3325,1.3106,1.2889,1.2674,1.2461,1.225,1.2041,1.1834,1.1629,1.1426,1.1225,1.1026,1.0829,1.0634,1.0441,1.025,1.0061,0.9874,0.9689,0.9506,0.9325,0.9146,0.8969,0.8794,0.8621,0.845,0.8281,0.8114,0.7949,0.7786,0.7625,0.7466,0.7309,0.7154,0.7001,0.685,0.6701,0.6554,0.6409,0.6266,0.6125,0.5986,0.5849,0.5714,0.5581,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3656,2.3357,2.306,2.2765,2.2472,2.2181,2.1892,2.1605,2.132,2.1037,2.0756,2.0477,2.02,1.9925,1.9652,1.9381,1.9112,1.8845,1.858,1.8317,1.8056,1.7797,1.754,1.7285,1.7032,1.6781,1.6532,1.6285,1.604,1.5797,1.5556,1.5317,1.508,1.4845,1.4612,1.4381,1.4152,1.3925,1.37,1.3477,1.3256,1.3037,1.282,1.2605,1.2392,1.2181,1.1972,1.1765,1.156,1.1357,1.1156,1.0957,1.076,1.0565,1.0372,1.0181,0.9992,0.9805,0.962,0.9437,0.9256,0.9077,0.89,0.8725,0.8552,0.8381,0.8212,0.8045,0.788,0.7717,0.7556,0.7397,0.724,0.7085,0.6932,0.6781,0.6632,0.6485,0.634,0.6197,0.6056,0.5917,0.578,0.5645,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3589,2.329,2.2993,2.2698,2.2405,2.2114,2.1825,2.1538,2.1253,2.097,2.0689,2.041,2.0133,1.9858,1.9585,1.9314,1.9045,1.8778,1.8513,1.825,1.7989,1.773,1.7473,1.7218,1.6965,1.6714,1.6465,1.6218,1.5973,1.573,1.5489,1.525,1.5013,1.4778,1.4545,1.4314,1.4085,1.3858,1.3633,1.341,1.3189,1.297,1.2753,1.2538,1.2325,1.2114,1.1905,1.1698,1.1493,1.129,1.1089,1.089,1.0693,1.0498,1.0305,1.0114,0.9925,0.9738,0.9553,0.937,0.9189,0.901,0.8833,0.8658,0.8485,0.8314,0.8145,0.7978,0.7813,0.765,0.7489,0.733,0.7173,0.7018,0.6865,0.6714,0.6565,0.6418,0.6273,0.613,0.5989,0.585,0.5713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3524,2.3225,2.2928,2.2633,2.234,2.2049,2.176,2.1473,2.1188,2.0905,2.0624,2.0345,2.0068,1.9793,1.952,1.9249,1.898,1.8713,1.8448,1.8185,1.7924,1.7665,1.7408,1.7153,1.69,1.6649,1.64,1.6153,1.5908,1.5665,1.5424,1.5185,1.4948,1.4713,1.448,1.4249,1.402,1.3793,1.3568,1.3345,1.3124,1.2905,1.2688,1.2473,1.226,1.2049,1.184,1.1633,1.1428,1.1225,1.1024,1.0825,1.0628,1.0433,1.024,1.0049,0.986,0.9673,0.9488,0.9305,0.9124,0.8945,0.8768,0.8593,0.842,0.8249,0.808,0.7913,0.7748,0.7585,0.7424,0.7265,0.7108,0.6953,0.68,0.6649,0.65,0.6353,0.6208,0.6065,0.5924,0.5785,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3461,2.3162,2.2865,2.257,2.2277,2.1986,2.1697,2.141,2.1125,2.0842,2.0561,2.0282,2.0005,1.973,1.9457,1.9186,1.8917,1.865,1.8385,1.8122,1.7861,1.7602,1.7345,1.709,1.6837,1.6586,1.6337,1.609,1.5845,1.5602,1.5361,1.5122,1.4885,1.465,1.4417,1.4186,1.3957,1.373,1.3505,1.3282,1.3061,1.2842,1.2625,1.241,1.2197,1.1986,1.1777,1.157,1.1365,1.1162,1.0961,1.0762,1.0565,1.037,1.0177,0.9986,0.9797,0.961,0.9425,0.9242,0.9061,0.8882,0.8705,0.853,0.8357,0.8186,0.8017,0.785,0.7685,0.7522,0.7361,0.7202,0.7045,0.689,0.6737,0.6586,0.6437,0.629,0.6145,0.6002,0.5861,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.34,2.3101,2.2804,2.2509,2.2216,2.1925,2.1636,2.1349,2.1064,2.0781,2.05,2.0221,1.9944,1.9669,1.9396,1.9125,1.8856,1.8589,1.8324,1.8061,1.78,1.7541,1.7284,1.7029,1.6776,1.6525,1.6276,1.6029,1.5784,1.5541,1.53,1.5061,1.4824,1.4589,1.4356,1.4125,1.3896,1.3669,1.3444,1.3221,1.3,1.2781,1.2564,1.2349,1.2136,1.1925,1.1716,1.1509,1.1304,1.1101,1.09,1.0701,1.0504,1.0309,1.0116,0.9925,0.9736,0.9549,0.9364,0.9181,0.9,0.8821,0.8644,0.8469,0.8296,0.8125,0.7956,0.7789,0.7624,0.7461,0.73,0.7141,0.6984,0.6829,0.6676,0.6525,0.6376,0.6229,0.6084,0.5941,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3341,2.3042,2.2745,2.245,2.2157,2.1866,2.1577,2.129,2.1005,2.0722,2.0441,2.0162,1.9885,1.961,1.9337,1.9066,1.8797,1.853,1.8265,1.8002,1.7741,1.7482,1.7225,1.697,1.6717,1.6466,1.6217,1.597,1.5725,1.5482,1.5241,1.5002,1.4765,1.453,1.4297,1.4066,1.3837,1.361,1.3385,1.3162,1.2941,1.2722,1.2505,1.229,1.2077,1.1866,1.1657,1.145,1.1245,1.1042,1.0841,1.0642,1.0445,1.025,1.0057,0.9866,0.9677,0.949,0.9305,0.9122,0.8941,0.8762,0.8585,0.841,0.8237,0.8066,0.7897,0.773,0.7565,0.7402,0.7241,0.7082,0.6925,0.677,0.6617,0.6466,0.6317,0.617,0.6025,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3284,2.2985,2.2688,2.2393,2.21,2.1809,2.152,2.1233,2.0948,2.0665,2.0384,2.0105,1.9828,1.9553,1.928,1.9009,1.874,1.8473,1.8208,1.7945,1.7684,1.7425,1.7168,1.6913,1.666,1.6409,1.616,1.5913,1.5668,1.5425,1.5184,1.4945,1.4708,1.4473,1.424,1.4009,1.378,1.3553,1.3328,1.3105,1.2884,1.2665,1.2448,1.2233,1.202,1.1809,1.16,1.1393,1.1188,1.0985,1.0784,1.0585,1.0388,1.0193,1,0.9809,0.962,0.9433,0.9248,0.9065,0.8884,0.8705,0.8528,0.8353,0.818,0.8009,0.784,0.7673,0.7508,0.7345,0.7184,0.7025,0.6868,0.6713,0.656,0.6409,0.626,0.6113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3229,2.293,2.2633,2.2338,2.2045,2.1754,2.1465,2.1178,2.0893,2.061,2.0329,2.005,1.9773,1.9498,1.9225,1.8954,1.8685,1.8418,1.8153,1.789,1.7629,1.737,1.7113,1.6858,1.6605,1.6354,1.6105,1.5858,1.5613,1.537,1.5129,1.489,1.4653,1.4418,1.4185,1.3954,1.3725,1.3498,1.3273,1.305,1.2829,1.261,1.2393,1.2178,1.1965,1.1754,1.1545,1.1338,1.1133,1.093,1.0729,1.053,1.0333,1.0138,0.9945,0.9754,0.9565,0.9378,0.9193,0.901,0.8829,0.865,0.8473,0.8298,0.8125,0.7954,0.7785,0.7618,0.7453,0.729,0.7129,0.697,0.6813,0.6658,0.6505,0.6354,0.6205,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3176,2.2877,2.258,2.2285,2.1992,2.1701,2.1412,2.1125,2.084,2.0557,2.0276,1.9997,1.972,1.9445,1.9172,1.8901,1.8632,1.8365,1.81,1.7837,1.7576,1.7317,1.706,1.6805,1.6552,1.6301,1.6052,1.5805,1.556,1.5317,1.5076,1.4837,1.46,1.4365,1.4132,1.3901,1.3672,1.3445,1.322,1.2997,1.2776,1.2557,1.234,1.2125,1.1912,1.1701,1.1492,1.1285,1.108,1.0877,1.0676,1.0477,1.028,1.0085,0.9892,0.9701,0.9512,0.9325,0.914,0.8957,0.8776,0.8597,0.842,0.8245,0.8072,0.7901,0.7732,0.7565,0.74,0.7237,0.7076,0.6917,0.676,0.6605,0.6452,0.6301,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3125,2.2826,2.2529,2.2234,2.1941,2.165,2.1361,2.1074,2.0789,2.0506,2.0225,1.9946,1.9669,1.9394,1.9121,1.885,1.8581,1.8314,1.8049,1.7786,1.7525,1.7266,1.7009,1.6754,1.6501,1.625,1.6001,1.5754,1.5509,1.5266,1.5025,1.4786,1.4549,1.4314,1.4081,1.385,1.3621,1.3394,1.3169,1.2946,1.2725,1.2506,1.2289,1.2074,1.1861,1.165,1.1441,1.1234,1.1029,1.0826,1.0625,1.0426,1.0229,1.0034,0.9841,0.965,0.9461,0.9274,0.9089,0.8906,0.8725,0.8546,0.8369,0.8194,0.8021,0.785,0.7681,0.7514,0.7349,0.7186,0.7025,0.6866,0.6709,0.6554,0.6401,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3076,2.2777,2.248,2.2185,2.1892,2.1601,2.1312,2.1025,2.074,2.0457,2.0176,1.9897,1.962,1.9345,1.9072,1.8801,1.8532,1.8265,1.8,1.7737,1.7476,1.7217,1.696,1.6705,1.6452,1.6201,1.5952,1.5705,1.546,1.5217,1.4976,1.4737,1.45,1.4265,1.4032,1.3801,1.3572,1.3345,1.312,1.2897,1.2676,1.2457,1.224,1.2025,1.1812,1.1601,1.1392,1.1185,1.098,1.0777,1.0576,1.0377,1.018,0.9985,0.9792,0.9601,0.9412,0.9225,0.904,0.8857,0.8676,0.8497,0.832,0.8145,0.7972,0.7801,0.7632,0.7465,0.73,0.7137,0.6976,0.6817,0.666,0.6505,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3029,2.273,2.2433,2.2138,2.1845,2.1554,2.1265,2.0978,2.0693,2.041,2.0129,1.985,1.9573,1.9298,1.9025,1.8754,1.8485,1.8218,1.7953,1.769,1.7429,1.717,1.6913,1.6658,1.6405,1.6154,1.5905,1.5658,1.5413,1.517,1.4929,1.469,1.4453,1.4218,1.3985,1.3754,1.3525,1.3298,1.3073,1.285,1.2629,1.241,1.2193,1.1978,1.1765,1.1554,1.1345,1.1138,1.0933,1.073,1.0529,1.033,1.0133,0.9938,0.9745,0.9554,0.9365,0.9178,0.8993,0.881,0.8629,0.845,0.8273,0.8098,0.7925,0.7754,0.7585,0.7418,0.7253,0.709,0.6929,0.677,0.6613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2984,2.2685,2.2388,2.2093,2.18,2.1509,2.122,2.0933,2.0648,2.0365,2.0084,1.9805,1.9528,1.9253,1.898,1.8709,1.844,1.8173,1.7908,1.7645,1.7384,1.7125,1.6868,1.6613,1.636,1.6109,1.586,1.5613,1.5368,1.5125,1.4884,1.4645,1.4408,1.4173,1.394,1.3709,1.348,1.3253,1.3028,1.2805,1.2584,1.2365,1.2148,1.1933,1.172,1.1509,1.13,1.1093,1.0888,1.0685,1.0484,1.0285,1.0088,0.9893,0.97,0.9509,0.932,0.9133,0.8948,0.8765,0.8584,0.8405,0.8228,0.8053,0.788,0.7709,0.754,0.7373,0.7208,0.7045,0.6884,0.6725,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2941,2.2642,2.2345,2.205,2.1757,2.1466,2.1177,2.089,2.0605,2.0322,2.0041,1.9762,1.9485,1.921,1.8937,1.8666,1.8397,1.813,1.7865,1.7602,1.7341,1.7082,1.6825,1.657,1.6317,1.6066,1.5817,1.557,1.5325,1.5082,1.4841,1.4602,1.4365,1.413,1.3897,1.3666,1.3437,1.321,1.2985,1.2762,1.2541,1.2322,1.2105,1.189,1.1677,1.1466,1.1257,1.105,1.0845,1.0642,1.0441,1.0242,1.0045,0.985,0.9657,0.9466,0.9277,0.909,0.8905,0.8722,0.8541,0.8362,0.8185,0.801,0.7837,0.7666,0.7497,0.733,0.7165,0.7002,0.6841,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.29,2.2601,2.2304,2.2009,2.1716,2.1425,2.1136,2.0849,2.0564,2.0281,2,1.9721,1.9444,1.9169,1.8896,1.8625,1.8356,1.8089,1.7824,1.7561,1.73,1.7041,1.6784,1.6529,1.6276,1.6025,1.5776,1.5529,1.5284,1.5041,1.48,1.4561,1.4324,1.4089,1.3856,1.3625,1.3396,1.3169,1.2944,1.2721,1.25,1.2281,1.2064,1.1849,1.1636,1.1425,1.1216,1.1009,1.0804,1.0601,1.04,1.0201,1.0004,0.9809,0.9616,0.9425,0.9236,0.9049,0.8864,0.8681,0.85,0.8321,0.8144,0.7969,0.7796,0.7625,0.7456,0.7289,0.7124,0.6961,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2861,2.2562,2.2265,2.197,2.1677,2.1386,2.1097,2.081,2.0525,2.0242,1.9961,1.9682,1.9405,1.913,1.8857,1.8586,1.8317,1.805,1.7785,1.7522,1.7261,1.7002,1.6745,1.649,1.6237,1.5986,1.5737,1.549,1.5245,1.5002,1.4761,1.4522,1.4285,1.405,1.3817,1.3586,1.3357,1.313,1.2905,1.2682,1.2461,1.2242,1.2025,1.181,1.1597,1.1386,1.1177,1.097,1.0765,1.0562,1.0361,1.0162,0.9965,0.977,0.9577,0.9386,0.9197,0.901,0.8825,0.8642,0.8461,0.8282,0.8105,0.793,0.7757,0.7586,0.7417,0.725,0.7085,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2824,2.2525,2.2228,2.1933,2.164,2.1349,2.106,2.0773,2.0488,2.0205,1.9924,1.9645,1.9368,1.9093,1.882,1.8549,1.828,1.8013,1.7748,1.7485,1.7224,1.6965,1.6708,1.6453,1.62,1.5949,1.57,1.5453,1.5208,1.4965,1.4724,1.4485,1.4248,1.4013,1.378,1.3549,1.332,1.3093,1.2868,1.2645,1.2424,1.2205,1.1988,1.1773,1.156,1.1349,1.114,1.0933,1.0728,1.0525,1.0324,1.0125,0.9928,0.9733,0.954,0.9349,0.916,0.8973,0.8788,0.8605,0.8424,0.8245,0.8068,0.7893,0.772,0.7549,0.738,0.7213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2789,2.249,2.2193,2.1898,2.1605,2.1314,2.1025,2.0738,2.0453,2.017,1.9889,1.961,1.9333,1.9058,1.8785,1.8514,1.8245,1.7978,1.7713,1.745,1.7189,1.693,1.6673,1.6418,1.6165,1.5914,1.5665,1.5418,1.5173,1.493,1.4689,1.445,1.4213,1.3978,1.3745,1.3514,1.3285,1.3058,1.2833,1.261,1.2389,1.217,1.1953,1.1738,1.1525,1.1314,1.1105,1.0898,1.0693,1.049,1.0289,1.009,0.9893,0.9698,0.9505,0.9314,0.9125,0.8938,0.8753,0.857,0.8389,0.821,0.8033,0.7858,0.7685,0.7514,0.7345,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2756,2.2457,2.216,2.1865,2.1572,2.1281,2.0992,2.0705,2.042,2.0137,1.9856,1.9577,1.93,1.9025,1.8752,1.8481,1.8212,1.7945,1.768,1.7417,1.7156,1.6897,1.664,1.6385,1.6132,1.5881,1.5632,1.5385,1.514,1.4897,1.4656,1.4417,1.418,1.3945,1.3712,1.3481,1.3252,1.3025,1.28,1.2577,1.2356,1.2137,1.192,1.1705,1.1492,1.1281,1.1072,1.0865,1.066,1.0457,1.0256,1.0057,0.986,0.9665,0.9472,0.9281,0.9092,0.8905,0.872,0.8537,0.8356,0.8177,0.8,0.7825,0.7652,0.7481,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2725,2.2426,2.2129,2.1834,2.1541,2.125,2.0961,2.0674,2.0389,2.0106,1.9825,1.9546,1.9269,1.8994,1.8721,1.845,1.8181,1.7914,1.7649,1.7386,1.7125,1.6866,1.6609,1.6354,1.6101,1.585,1.5601,1.5354,1.5109,1.4866,1.4625,1.4386,1.4149,1.3914,1.3681,1.345,1.3221,1.2994,1.2769,1.2546,1.2325,1.2106,1.1889,1.1674,1.1461,1.125,1.1041,1.0834,1.0629,1.0426,1.0225,1.0026,0.9829,0.9634,0.9441,0.925,0.9061,0.8874,0.8689,0.8506,0.8325,0.8146,0.7969,0.7794,0.7621,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2696,2.2397,2.21,2.1805,2.1512,2.1221,2.0932,2.0645,2.036,2.0077,1.9796,1.9517,1.924,1.8965,1.8692,1.8421,1.8152,1.7885,1.762,1.7357,1.7096,1.6837,1.658,1.6325,1.6072,1.5821,1.5572,1.5325,1.508,1.4837,1.4596,1.4357,1.412,1.3885,1.3652,1.3421,1.3192,1.2965,1.274,1.2517,1.2296,1.2077,1.186,1.1645,1.1432,1.1221,1.1012,1.0805,1.06,1.0397,1.0196,0.9997,0.98,0.9605,0.9412,0.9221,0.9032,0.8845,0.866,0.8477,0.8296,0.8117,0.794,0.7765,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2669,2.237,2.2073,2.1778,2.1485,2.1194,2.0905,2.0618,2.0333,2.005,1.9769,1.949,1.9213,1.8938,1.8665,1.8394,1.8125,1.7858,1.7593,1.733,1.7069,1.681,1.6553,1.6298,1.6045,1.5794,1.5545,1.5298,1.5053,1.481,1.4569,1.433,1.4093,1.3858,1.3625,1.3394,1.3165,1.2938,1.2713,1.249,1.2269,1.205,1.1833,1.1618,1.1405,1.1194,1.0985,1.0778,1.0573,1.037,1.0169,0.997,0.9773,0.9578,0.9385,0.9194,0.9005,0.8818,0.8633,0.845,0.8269,0.809,0.7913,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2644,2.2345,2.2048,2.1753,2.146,2.1169,2.088,2.0593,2.0308,2.0025,1.9744,1.9465,1.9188,1.8913,1.864,1.8369,1.81,1.7833,1.7568,1.7305,1.7044,1.6785,1.6528,1.6273,1.602,1.5769,1.552,1.5273,1.5028,1.4785,1.4544,1.4305,1.4068,1.3833,1.36,1.3369,1.314,1.2913,1.2688,1.2465,1.2244,1.2025,1.1808,1.1593,1.138,1.1169,1.096,1.0753,1.0548,1.0345,1.0144,0.9945,0.9748,0.9553,0.936,0.9169,0.898,0.8793,0.8608,0.8425,0.8244,0.8065,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2621,2.2322,2.2025,2.173,2.1437,2.1146,2.0857,2.057,2.0285,2.0002,1.9721,1.9442,1.9165,1.889,1.8617,1.8346,1.8077,1.781,1.7545,1.7282,1.7021,1.6762,1.6505,1.625,1.5997,1.5746,1.5497,1.525,1.5005,1.4762,1.4521,1.4282,1.4045,1.381,1.3577,1.3346,1.3117,1.289,1.2665,1.2442,1.2221,1.2002,1.1785,1.157,1.1357,1.1146,1.0937,1.073,1.0525,1.0322,1.0121,0.9922,0.9725,0.953,0.9337,0.9146,0.8957,0.877,0.8585,0.8402,0.8221,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.26,2.2301,2.2004,2.1709,2.1416,2.1125,2.0836,2.0549,2.0264,1.9981,1.97,1.9421,1.9144,1.8869,1.8596,1.8325,1.8056,1.7789,1.7524,1.7261,1.7,1.6741,1.6484,1.6229,1.5976,1.5725,1.5476,1.5229,1.4984,1.4741,1.45,1.4261,1.4024,1.3789,1.3556,1.3325,1.3096,1.2869,1.2644,1.2421,1.22,1.1981,1.1764,1.1549,1.1336,1.1125,1.0916,1.0709,1.0504,1.0301,1.01,0.9901,0.9704,0.9509,0.9316,0.9125,0.8936,0.8749,0.8564,0.8381,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2581,2.2282,2.1985,2.169,2.1397,2.1106,2.0817,2.053,2.0245,1.9962,1.9681,1.9402,1.9125,1.885,1.8577,1.8306,1.8037,1.777,1.7505,1.7242,1.6981,1.6722,1.6465,1.621,1.5957,1.5706,1.5457,1.521,1.4965,1.4722,1.4481,1.4242,1.4005,1.377,1.3537,1.3306,1.3077,1.285,1.2625,1.2402,1.2181,1.1962,1.1745,1.153,1.1317,1.1106,1.0897,1.069,1.0485,1.0282,1.0081,0.9882,0.9685,0.949,0.9297,0.9106,0.8917,0.873,0.8545,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2564,2.2265,2.1968,2.1673,2.138,2.1089,2.08,2.0513,2.0228,1.9945,1.9664,1.9385,1.9108,1.8833,1.856,1.8289,1.802,1.7753,1.7488,1.7225,1.6964,1.6705,1.6448,1.6193,1.594,1.5689,1.544,1.5193,1.4948,1.4705,1.4464,1.4225,1.3988,1.3753,1.352,1.3289,1.306,1.2833,1.2608,1.2385,1.2164,1.1945,1.1728,1.1513,1.13,1.1089,1.088,1.0673,1.0468,1.0265,1.0064,0.9865,0.9668,0.9473,0.928,0.9089,0.89,0.8713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2549,2.225,2.1953,2.1658,2.1365,2.1074,2.0785,2.0498,2.0213,1.993,1.9649,1.937,1.9093,1.8818,1.8545,1.8274,1.8005,1.7738,1.7473,1.721,1.6949,1.669,1.6433,1.6178,1.5925,1.5674,1.5425,1.5178,1.4933,1.469,1.4449,1.421,1.3973,1.3738,1.3505,1.3274,1.3045,1.2818,1.2593,1.237,1.2149,1.193,1.1713,1.1498,1.1285,1.1074,1.0865,1.0658,1.0453,1.025,1.0049,0.985,0.9653,0.9458,0.9265,0.9074,0.8885,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2536,2.2237,2.194,2.1645,2.1352,2.1061,2.0772,2.0485,2.02,1.9917,1.9636,1.9357,1.908,1.8805,1.8532,1.8261,1.7992,1.7725,1.746,1.7197,1.6936,1.6677,1.642,1.6165,1.5912,1.5661,1.5412,1.5165,1.492,1.4677,1.4436,1.4197,1.396,1.3725,1.3492,1.3261,1.3032,1.2805,1.258,1.2357,1.2136,1.1917,1.17,1.1485,1.1272,1.1061,1.0852,1.0645,1.044,1.0237,1.0036,0.9837,0.964,0.9445,0.9252,0.9061,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2525,2.2226,2.1929,2.1634,2.1341,2.105,2.0761,2.0474,2.0189,1.9906,1.9625,1.9346,1.9069,1.8794,1.8521,1.825,1.7981,1.7714,1.7449,1.7186,1.6925,1.6666,1.6409,1.6154,1.5901,1.565,1.5401,1.5154,1.4909,1.4666,1.4425,1.4186,1.3949,1.3714,1.3481,1.325,1.3021,1.2794,1.2569,1.2346,1.2125,1.1906,1.1689,1.1474,1.1261,1.105,1.0841,1.0634,1.0429,1.0226,1.0025,0.9826,0.9629,0.9434,0.9241,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2516,2.2217,2.192,2.1625,2.1332,2.1041,2.0752,2.0465,2.018,1.9897,1.9616,1.9337,1.906,1.8785,1.8512,1.8241,1.7972,1.7705,1.744,1.7177,1.6916,1.6657,1.64,1.6145,1.5892,1.5641,1.5392,1.5145,1.49,1.4657,1.4416,1.4177,1.394,1.3705,1.3472,1.3241,1.3012,1.2785,1.256,1.2337,1.2116,1.1897,1.168,1.1465,1.1252,1.1041,1.0832,1.0625,1.042,1.0217,1.0016,0.9817,0.962,0.9425,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2509,2.221,2.1913,2.1618,2.1325,2.1034,2.0745,2.0458,2.0173,1.989,1.9609,1.933,1.9053,1.8778,1.8505,1.8234,1.7965,1.7698,1.7433,1.717,1.6909,1.665,1.6393,1.6138,1.5885,1.5634,1.5385,1.5138,1.4893,1.465,1.4409,1.417,1.3933,1.3698,1.3465,1.3234,1.3005,1.2778,1.2553,1.233,1.2109,1.189,1.1673,1.1458,1.1245,1.1034,1.0825,1.0618,1.0413,1.021,1.0009,0.981,0.9613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2504,2.2205,2.1908,2.1613,2.132,2.1029,2.074,2.0453,2.0168,1.9885,1.9604,1.9325,1.9048,1.8773,1.85,1.8229,1.796,1.7693,1.7428,1.7165,1.6904,1.6645,1.6388,1.6133,1.588,1.5629,1.538,1.5133,1.4888,1.4645,1.4404,1.4165,1.3928,1.3693,1.346,1.3229,1.3,1.2773,1.2548,1.2325,1.2104,1.1885,1.1668,1.1453,1.124,1.1029,1.082,1.0613,1.0408,1.0205,1.0004,0.9805,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2501,2.2202,2.1905,2.161,2.1317,2.1026,2.0737,2.045,2.0165,1.9882,1.9601,1.9322,1.9045,1.877,1.8497,1.8226,1.7957,1.769,1.7425,1.7162,1.6901,1.6642,1.6385,1.613,1.5877,1.5626,1.5377,1.513,1.4885,1.4642,1.4401,1.4162,1.3925,1.369,1.3457,1.3226,1.2997,1.277,1.2545,1.2322,1.2101,1.1882,1.1665,1.145,1.1237,1.1026,1.0817,1.061,1.0405,1.0202,1.0001,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.25,2.2201,2.1904,2.1609,2.1316,2.1025,2.0736,2.0449,2.0164,1.9881,1.96,1.9321,1.9044,1.8769,1.8496,1.8225,1.7956,1.7689,1.7424,1.7161,1.69,1.6641,1.6384,1.6129,1.5876,1.5625,1.5376,1.5129,1.4884,1.4641,1.44,1.4161,1.3924,1.3689,1.3456,1.3225,1.2996,1.2769,1.2544,1.2321,1.21,1.1881,1.1664,1.1449,1.1236,1.1025,1.0816,1.0609,1.0404,1.0201,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2501,2.2202,2.1905,2.161,2.1317,2.1026,2.0737,2.045,2.0165,1.9882,1.9601,1.9322,1.9045,1.877,1.8497,1.8226,1.7957,1.769,1.7425,1.7162,1.6901,1.6642,1.6385,1.613,1.5877,1.5626,1.5377,1.513,1.4885,1.4642,1.4401,1.4162,1.3925,1.369,1.3457,1.3226,1.2997,1.277,1.2545,1.2322,1.2101,1.1882,1.1665,1.145,1.1237,1.1026,1.0817,1.061,1.0405,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2504,2.2205,2.1908,2.1613,2.132,2.1029,2.074,2.0453,2.0168,1.9885,1.9604,1.9325,1.9048,1.8773,1.85,1.8229,1.796,1.7693,1.7428,1.7165,1.6904,1.6645,1.6388,1.6133,1.588,1.5629,1.538,1.5133,1.4888,1.4645,1.4404,1.4165,1.3928,1.3693,1.346,1.3229,1.3,1.2773,1.2548,1.2325,1.2104,1.1885,1.1668,1.1453,1.124,1.1029,1.082,1.0613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2509,2.221,2.1913,2.1618,2.1325,2.1034,2.0745,2.0458,2.0173,1.989,1.9609,1.933,1.9053,1.8778,1.8505,1.8234,1.7965,1.7698,1.7433,1.717,1.6909,1.665,1.6393,1.6138,1.5885,1.5634,1.5385,1.5138,1.4893,1.465,1.4409,1.417,1.3933,1.3698,1.3465,1.3234,1.3005,1.2778,1.2553,1.233,1.2109,1.189,1.1673,1.1458,1.1245,1.1034,1.0825,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2516,2.2217,2.192,2.1625,2.1332,2.1041,2.0752,2.0465,2.018,1.9897,1.9616,1.9337,1.906,1.8785,1.8512,1.8241,1.7972,1.7705,1.744,1.7177,1.6916,1.6657,1.64,1.6145,1.5892,1.5641,1.5392,1.5145,1.49,1.4657,1.4416,1.4177,1.394,1.3705,1.3472,1.3241,1.3012,1.2785,1.256,1.2337,1.2116,1.1897,1.168,1.1465,1.1252,1.1041,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2525,2.2226,2.1929,2.1634,2.1341,2.105,2.0761,2.0474,2.0189,1.9906,1.9625,1.9346,1.9069,1.8794,1.8521,1.825,1.7981,1.7714,1.7449,1.7186,1.6925,1.6666,1.6409,1.6154,1.5901,1.565,1.5401,1.5154,1.4909,1.4666,1.4425,1.4186,1.3949,1.3714,1.3481,1.325,1.3021,1.2794,1.2569,1.2346,1.2125,1.1906,1.1689,1.1474,1.1261,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2536,2.2237,2.194,2.1645,2.1352,2.1061,2.0772,2.0485,2.02,1.9917,1.9636,1.9357,1.908,1.8805,1.8532,1.8261,1.7992,1.7725,1.746,1.7197,1.6936,1.6677,1.642,1.6165,1.5912,1.5661,1.5412,1.5165,1.492,1.4677,1.4436,1.4197,1.396,1.3725,1.3492,1.3261,1.3032,1.2805,1.258,1.2357,1.2136,1.1917,1.17,1.1485,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2549,2.225,2.1953,2.1658,2.1365,2.1074,2.0785,2.0498,2.0213,1.993,1.9649,1.937,1.9093,1.8818,1.8545,1.8274,1.8005,1.7738,1.7473,1.721,1.6949,1.669,1.6433,1.6178,1.5925,1.5674,1.5425,1.5178,1.4933,1.469,1.4449,1.421,1.3973,1.3738,1.3505,1.3274,1.3045,1.2818,1.2593,1.237,1.2149,1.193,1.1713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2564,2.2265,2.1968,2.1673,2.138,2.1089,2.08,2.0513,2.0228,1.9945,1.9664,1.9385,1.9108,1.8833,1.856,1.8289,1.802,1.7753,1.7488,1.7225,1.6964,1.6705,1.6448,1.6193,1.594,1.5689,1.544,1.5193,1.4948,1.4705,1.4464,1.4225,1.3988,1.3753,1.352,1.3289,1.306,1.2833,1.2608,1.2385,1.2164,1.1945,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2581,2.2282,2.1985,2.169,2.1397,2.1106,2.0817,2.053,2.0245,1.9962,1.9681,1.9402,1.9125,1.885,1.8577,1.8306,1.8037,1.777,1.7505,1.7242,1.6981,1.6722,1.6465,1.621,1.5957,1.5706,1.5457,1.521,1.4965,1.4722,1.4481,1.4242,1.4005,1.377,1.3537,1.3306,1.3077,1.285,1.2625,1.2402,1.2181,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.26,2.2301,2.2004,2.1709,2.1416,2.1125,2.0836,2.0549,2.0264,1.9981,1.97,1.9421,1.9144,1.8869,1.8596,1.8325,1.8056,1.7789,1.7524,1.7261,1.7,1.6741,1.6484,1.6229,1.5976,1.5725,1.5476,1.5229,1.4984,1.4741,1.45,1.4261,1.4024,1.3789,1.3556,1.3325,1.3096,1.2869,1.2644,1.2421,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2621,2.2322,2.2025,2.173,2.1437,2.1146,2.0857,2.057,2.0285,2.0002,1.9721,1.9442,1.9165,1.889,1.8617,1.8346,1.8077,1.781,1.7545,1.7282,1.7021,1.6762,1.6505,1.625,1.5997,1.5746,1.5497,1.525,1.5005,1.4762,1.4521,1.4282,1.4045,1.381,1.3577,1.3346,1.3117,1.289,1.2665,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2644,2.2345,2.2048,2.1753,2.146,2.1169,2.088,2.0593,2.0308,2.0025,1.9744,1.9465,1.9188,1.8913,1.864,1.8369,1.81,1.7833,1.7568,1.7305,1.7044,1.6785,1.6528,1.6273,1.602,1.5769,1.552,1.5273,1.5028,1.4785,1.4544,1.4305,1.4068,1.3833,1.36,1.3369,1.314,1.2913,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2669,2.237,2.2073,2.1778,2.1485,2.1194,2.0905,2.0618,2.0333,2.005,1.9769,1.949,1.9213,1.8938,1.8665,1.8394,1.8125,1.7858,1.7593,1.733,1.7069,1.681,1.6553,1.6298,1.6045,1.5794,1.5545,1.5298,1.5053,1.481,1.4569,1.433,1.4093,1.3858,1.3625,1.3394,1.3165,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2696,2.2397,2.21,2.1805,2.1512,2.1221,2.0932,2.0645,2.036,2.0077,1.9796,1.9517,1.924,1.8965,1.8692,1.8421,1.8152,1.7885,1.762,1.7357,1.7096,1.6837,1.658,1.6325,1.6072,1.5821,1.5572,1.5325,1.508,1.4837,1.4596,1.4357,1.412,1.3885,1.3652,1.3421,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2725,2.2426,2.2129,2.1834,2.1541,2.125,2.0961,2.0674,2.0389,2.0106,1.9825,1.9546,1.9269,1.8994,1.8721,1.845,1.8181,1.7914,1.7649,1.7386,1.7125,1.6866,1.6609,1.6354,1.6101,1.585,1.5601,1.5354,1.5109,1.4866,1.4625,1.4386,1.4149,1.3914,1.3681,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2756,2.2457,2.216,2.1865,2.1572,2.1281,2.0992,2.0705,2.042,2.0137,1.9856,1.9577,1.93,1.9025,1.8752,1.8481,1.8212,1.7945,1.768,1.7417,1.7156,1.6897,1.664,1.6385,1.6132,1.5881,1.5632,1.5385,1.514,1.4897,1.4656,1.4417,1.418,1.3945,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2789,2.249,2.2193,2.1898,2.1605,2.1314,2.1025,2.0738,2.0453,2.017,1.9889,1.961,1.9333,1.9058,1.8785,1.8514,1.8245,1.7978,1.7713,1.745,1.7189,1.693,1.6673,1.6418,1.6165,1.5914,1.5665,1.5418,1.5173,1.493,1.4689,1.445,1.4213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2824,2.2525,2.2228,2.1933,2.164,2.1349,2.106,2.0773,2.0488,2.0205,1.9924,1.9645,1.9368,1.9093,1.882,1.8549,1.828,1.8013,1.7748,1.7485,1.7224,1.6965,1.6708,1.6453,1.62,1.5949,1.57,1.5453,1.5208,1.4965,1.4724,1.4485,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2861,2.2562,2.2265,2.197,2.1677,2.1386,2.1097,2.081,2.0525,2.0242,1.9961,1.9682,1.9405,1.913,1.8857,1.8586,1.8317,1.805,1.7785,1.7522,1.7261,1.7002,1.6745,1.649,1.6237,1.5986,1.5737,1.549,1.5245,1.5002,1.4761,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.29,2.2601,2.2304,2.2009,2.1716,2.1425,2.1136,2.0849,2.0564,2.0281,2,1.9721,1.9444,1.9169,1.8896,1.8625,1.8356,1.8089,1.7824,1.7561,1.73,1.7041,1.6784,1.6529,1.6276,1.6025,1.5776,1.5529,1.5284,1.5041,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2941,2.2642,2.2345,2.205,2.1757,2.1466,2.1177,2.089,2.0605,2.0322,2.0041,1.9762,1.9485,1.921,1.8937,1.8666,1.8397,1.813,1.7865,1.7602,1.7341,1.7082,1.6825,1.657,1.6317,1.6066,1.5817,1.557,1.5325,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.2984,2.2685,2.2388,2.2093,2.18,2.1509,2.122,2.0933,2.0648,2.0365,2.0084,1.9805,1.9528,1.9253,1.898,1.8709,1.844,1.8173,1.7908,1.7645,1.7384,1.7125,1.6868,1.6613,1.636,1.6109,1.586,1.5613,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3029,2.273,2.2433,2.2138,2.1845,2.1554,2.1265,2.0978,2.0693,2.041,2.0129,1.985,1.9573,1.9298,1.9025,1.8754,1.8485,1.8218,1.7953,1.769,1.7429,1.717,1.6913,1.6658,1.6405,1.6154,1.5905,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3076,2.2777,2.248,2.2185,2.1892,2.1601,2.1312,2.1025,2.074,2.0457,2.0176,1.9897,1.962,1.9345,1.9072,1.8801,1.8532,1.8265,1.8,1.7737,1.7476,1.7217,1.696,1.6705,1.6452,1.6201,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3125,2.2826,2.2529,2.2234,2.1941,2.165,2.1361,2.1074,2.0789,2.0506,2.0225,1.9946,1.9669,1.9394,1.9121,1.885,1.8581,1.8314,1.8049,1.7786,1.7525,1.7266,1.7009,1.6754,1.6501,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3176,2.2877,2.258,2.2285,2.1992,2.1701,2.1412,2.1125,2.084,2.0557,2.0276,1.9997,1.972,1.9445,1.9172,1.8901,1.8632,1.8365,1.81,1.7837,1.7576,1.7317,1.706,1.6805,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3229,2.293,2.2633,2.2338,2.2045,2.1754,2.1465,2.1178,2.0893,2.061,2.0329,2.005,1.9773,1.9498,1.9225,1.8954,1.8685,1.8418,1.8153,1.789,1.7629,1.737,1.7113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3284,2.2985,2.2688,2.2393,2.21,2.1809,2.152,2.1233,2.0948,2.0665,2.0384,2.0105,1.9828,1.9553,1.928,1.9009,1.874,1.8473,1.8208,1.7945,1.7684,1.7425,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3341,2.3042,2.2745,2.245,2.2157,2.1866,2.1577,2.129,2.1005,2.0722,2.0441,2.0162,1.9885,1.961,1.9337,1.9066,1.8797,1.853,1.8265,1.8002,1.7741,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.34,2.3101,2.2804,2.2509,2.2216,2.1925,2.1636,2.1349,2.1064,2.0781,2.05,2.0221,1.9944,1.9669,1.9396,1.9125,1.8856,1.8589,1.8324,1.8061,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3461,2.3162,2.2865,2.257,2.2277,2.1986,2.1697,2.141,2.1125,2.0842,2.0561,2.0282,2.0005,1.973,1.9457,1.9186,1.8917,1.865,1.8385,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3524,2.3225,2.2928,2.2633,2.234,2.2049,2.176,2.1473,2.1188,2.0905,2.0624,2.0345,2.0068,1.9793,1.952,1.9249,1.898,1.8713,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3589,2.329,2.2993,2.2698,2.2405,2.2114,2.1825,2.1538,2.1253,2.097,2.0689,2.041,2.0133,1.9858,1.9585,1.9314,1.9045,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3656,2.3357,2.306,2.2765,2.2472,2.2181,2.1892,2.1605,2.132,2.1037,2.0756,2.0477,2.02,1.9925,1.9652,1.9381,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3725,2.3426,2.3129,2.2834,2.2541,2.225,2.1961,2.1674,2.1389,2.1106,2.0825,2.0546,2.0269,1.9994,1.9721,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3796,2.3497,2.32,2.2905,2.2612,2.2321,2.2032,2.1745,2.146,2.1177,2.0896,2.0617,2.034,2.0065,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3869,2.357,2.3273,2.2978,2.2685,2.2394,2.2105,2.1818,2.1533,2.125,2.0969,2.069,2.0413,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.3944,2.3645,2.3348,2.3053,2.276,2.2469,2.218,2.1893,2.1608,2.1325,2.1044,2.0765,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4021,2.3722,2.3425,2.313,2.2837,2.2546,2.2257,2.197,2.1685,2.1402,2.1121,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.41,2.3801,2.3504,2.3209,2.2916,2.2625,2.2336,2.2049,2.1764,2.1481,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4181,2.3882,2.3585,2.329,2.2997,2.2706,2.2417,2.213,2.1845,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4264,2.3965,2.3668,2.3373,2.308,2.2789,2.25,2.2213,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4349,2.405,2.3753,2.3458,2.3165,2.2874,2.2585,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4436,2.4137,2.384,2.3545,2.3252,2.2961,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4525,2.4226,2.3929,2.3634,2.3341,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4616,2.4317,2.402,2.3725,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4709,2.441,2.4113,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4804,2.4505,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[2.4901,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null]],"type":"surface","frame":null},{"x":[-0.5],"y":[-0.5],"z":[0.5],"type":"scatter3d","mode":"markers","marker":{"color":"red","size":6,"symbol":104,"line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>In general, we will be dealing with a problem in the form of</p>
<p><span class="math display">\[\begin{align}
&amp;\underset{\mathbf{x}}{\text{minimize}} &amp; \quad f(\mathbf{x}) \\
&amp;\text{subject to} &amp; g_i(\mathbf{x}) &amp; \leq 0, \, i = 1,\ldots, m \\
&amp; &amp; h_j(\mathbf{x}) &amp;= 0, \, j = 1,\ldots, k
\end{align}\]</span></p>
<p>where <span class="math inline">\(g_i(\mathbf{x})\)</span>s are a set of inequality constrains, and <span class="math inline">\(h_j(\mathbf{x})\)</span>s are equality constrains. There are established result showing what type of constrains would lead to a convex set, but let’s assuming for now that we will be dealing a well behaved problem. We shall see in later chapters that many models such as, Lasso, Ridge and support vector machines can all be formulated into this form.</p>
</div>
<div id="global_local" class="section level2 hasAnchor" number="4.2">
<h2 class="hasAnchor"><span class="header-section-number">4.2</span> Global vs. Local Optima<a href="#global_local" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Although we would like to deal with convex optimization problems, non-convex problems appears more and more frequently. For example, deep learning models are almost always non-convex except overly simplified ones. However, <strong>for convex optimization problems, a local minimum is also a global minimum</strong>, i.e., a <span class="math inline">\(x^\ast\)</span> such that for any <span class="math inline">\(x\)</span> in the feasible set, <span class="math inline">\(f(x^\ast) \leq f(x)\)</span>. This can be achieved by a variety of descent algorithms, to be introduced. However, for non-convex problems, we may still be interested in a local minimum, which satisfies that for any <span class="math inline">\(x\)</span> in a <strong>neighboring set of <span class="math inline">\(x^\ast\)</span></strong>, <span class="math inline">\(f(x^\ast) \leq f(x)\)</span>. The comparison of these two cases can be demonstrated in the following plots. Again, a descent algorithm can help us find a local minimum, except for some very special cases, such as a saddle point. However, we will not discuss these issues in this book.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-47-1.png" width="75%" style="display: block; margin: auto;" /></p>
</div>
<div id="example-linear-regression-using-optim" class="section level2 hasAnchor" number="4.3">
<h2 class="hasAnchor"><span class="header-section-number">4.3</span> Example: Linear Regression using <code>optim()</code><a href="#example-linear-regression-using-optim" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Although completely not necessary, we may also view linear regression as an optimization problem. This is of course an unconstrained problem, meaning that <span class="math inline">\(C \in \mathbb{R}^p\)</span>. Such problems can be solved using the <code>optim()</code> function. Also, let’s temporarily switch back to the <span class="math inline">\(\boldsymbol \beta\)</span> notation of parameters. Hence, if we observe a set of observations <span class="math inline">\(\{\mathbf{x}_i, y_i\}_{i = 1}^n\)</span>, our optimization problem is to minimize the objection function, i.e., residual sum of squares (RSS):</p>
<p><span class="math display">\[\begin{align}
\underset{\boldsymbol \beta}{\text{minimize}} \quad f(\boldsymbol \beta) = \frac{1}{n} \sum_i (y_i - \mathbf{x}_i^\text{T}\boldsymbol \beta)^2 \\
\end{align}\]</span></p>
<p>We generate 200 random observations, and also write a function to calculate the RSS for any given <span class="math inline">\(\boldsymbol \beta\)</span> values. The objective function looks like the following:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate data from a simple linear model </span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">20</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    n <span class="ot">=</span> <span class="dv">200</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="fu">rnorm</span>(n))</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> x <span class="sc">%*%</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>) <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate the residual sum of squares for a grid of beta values</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    rss <span class="ot">&lt;-</span> <span class="cf">function</span>(b, trainx, trainy) <span class="fu">sum</span>((trainy <span class="sc">-</span> trainx <span class="sc">%*%</span> b)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<div id="htmlwidget-24ffdbfee61bf33d931c" style="width:80%;height:576px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-24ffdbfee61bf33d931c">{"x":{"visdat":{"46f4f4b4803":["function () ","plotlyVisDat"],"46f43fb357e":["function () ","data"]},"cur_data":"46f43fb357e","attrs":{"46f4f4b4803":{"x":[0,0.105263157894737,0.210526315789474,0.315789473684211,0.421052631578947,0.526315789473684,0.631578947368421,0.736842105263158,0.842105263157895,0.947368421052632,1.05263157894737,1.15789473684211,1.26315789473684,1.36842105263158,1.47368421052632,1.57894736842105,1.68421052631579,1.78947368421053,1.89473684210526,2],"y":[0,0.105263157894737,0.210526315789474,0.315789473684211,0.421052631578947,0.526315789473684,0.631578947368421,0.736842105263158,0.842105263157895,0.947368421052632,1.05263157894737,1.15789473684211,1.26315789473684,1.36842105263158,1.47368421052632,1.57894736842105,1.68421052631579,1.78947368421053,1.89473684210526,2],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[420.876036570751,402.175705540557,387.907507474353,378.071442372136,372.667510233909,371.695711059671,375.156044849422,383.048511603162,395.37311132089,412.129844002608,433.318709648314,458.939708258009,488.992839831694,523.478104369367,562.395501871029,605.74503233668,653.52669576632,705.740492159949,762.386421517567,823.464483839173],[383.791513565719,365.300937234838,351.242493867946,341.616183465042,336.422006026128,335.659961551202,339.330050040266,347.432271493318,359.966625910359,376.933113291389,398.331733636408,424.162486945416,454.425373218413,489.120392455399,528.247544656374,571.806829821337,619.79824795029,672.221799043231,729.077483100162,790.365300121081],[351.079360653702,332.798539022133,318.949850354554,309.533294650963,304.548871911361,303.996582135748,307.876425324124,316.188401476489,328.932510592843,346.108752673186,367.717127717518,393.757635725838,424.230276698148,459.135050634446,498.471957534734,542.24099739901,590.442170227275,643.075476019529,700.140914775773,761.638486496005],[322.7395778347,304.668510902444,291.029576934177,281.822775929899,277.04810788961,276.70557281331,280.795170700998,289.316901552676,302.270765368342,319.656762147998,341.474891891642,367.725154599276,398.407550270898,433.522078906509,473.068740506109,517.047535069698,565.458462597276,618.301523088842,675.576716544398,737.284042963943],[298.772165108713,280.91085287577,267.481673606816,258.48462730185,253.919713960874,253.786933583886,258.086286170887,266.817771721878,279.981390236857,297.577141715825,319.605026158782,346.065043565728,376.957193936663,412.281477271587,452.037893570499,496.226442833401,544.847125060291,597.899940251171,655.384888406039,717.301969524897],[279.177122475742,261.525564942111,248.306140372469,239.518848766817,235.163690125153,235.240664447478,239.749771733792,248.691011984095,262.064385198387,279.869891376667,302.107530518937,328.777302625195,359.879207695443,395.413245729679,435.379416727905,479.777720690119,528.608157616322,581.870727506514,639.565430360695,701.692266178865],[263.954449935785,246.512647101467,233.502977231138,224.925440324798,220.780036382447,221.066765404085,225.785627389711,234.936622339327,248.519750252931,266.535011130525,288.982404972107,315.861931777678,347.173591547238,382.917384280787,423.093309978325,467.701368639852,516.741560265368,570.213884854873,628.118342408367,690.454932925849],[253.104147488844,235.872099353839,223.072184182822,214.704401975795,210.768752732756,211.265236453706,216.193853138646,225.554602787574,239.347485400491,257.572500977397,280.229649518292,307.318931023176,338.840345492049,374.79389292491,415.179573321761,459.997386682601,509.247333007429,562.929412296246,621.043624549053,683.589969765848],[246.626215134918,229.603921699225,217.013761227521,208.855733719806,205.129839176081,205.836077596344,210.974448980595,220.544953328836,234.547590641066,252.982360917285,275.849264157492,303.148300361689,334.879469529874,371.042771662049,411.638206758212,456.665774818364,506.125475842505,560.017309830635,618.341276782754,681.097376698862],[244.520652874007,227.708114137627,215.327708365236,207.379435556833,203.86329571242,204.779288831996,210.12741491556,219.907673963114,234.120065974656,252.764590950188,275.841248889708,303.350039793217,335.290963660715,371.664020492202,412.469210287678,457.706533047143,507.375988770597,561.477577458039,620.011299109471,682.977153724892],[246.787460706111,230.184676669043,218.014025595965,210.275507486875,206.969122341775,208.094870160663,213.65275094354,223.642764690407,238.064911401262,256.919191076106,280.205603714938,307.92414931776,340.074827884571,376.657639415371,417.672583910159,463.119661368937,512.998871791703,567.310215178459,626.053691529203,689.229300843936],[253.42663863123,237.033609293475,225.07271291971,217.543949509933,214.447319064145,215.782821582346,221.550457064536,231.750225510714,246.382126920882,265.446161295039,288.942328633184,316.870628935319,349.231062201442,386.023628431554,427.248327625656,472.905159783746,522.994124905825,577.515222991893,636.46845404195,699.853818055996],[264.438186649365,248.254912010922,236.503770336469,229.184761626005,226.29788587953,227.843143097043,233.820533278546,244.230056424037,259.071712533518,278.345501606987,302.051423644445,330.189478645892,362.759666611328,399.761987540753,441.196441434167,487.06302829157,537.361748112962,592.092600898342,651.255586647712,714.85070536107],[279.822104760514,263.848584821385,252.307197846244,245.197943835093,242.52082278793,244.275834704756,250.462979585571,261.082257430375,276.133668239168,295.61721201195,319.532888748721,347.880698449481,380.66064111423,417.872716742967,459.516925335694,505.593266892409,556.101741413114,611.042348897807,670.415089346489,734.21996275916],[299.578392964679,283.814627724862,272.482995449034,265.583496137195,263.116129789345,265.080896405484,271.477795985612,282.306828529729,297.567994037834,317.261292509929,341.386723946013,369.944288346085,402.933985710146,440.355816038197,482.209779330236,528.495875586264,579.214104806281,634.364466990287,693.946962138282,757.961590250265],[323.707051261859,308.153040721355,297.031163144839,290.341418532313,288.083806883776,290.258328199227,296.864982478668,307.903769722097,323.374689929516,343.277743100923,367.612929236319,396.380248335704,429.579700399078,467.211285426441,509.275003417793,555.770854373133,606.698838292463,662.058955175782,721.851205023089,786.075587834386],[352.208079652054,336.863823810862,325.95170093366,319.471711020446,317.423854071221,319.808130085986,326.624539064739,337.873081007481,353.553755914212,373.666563784932,398.21150461964,427.188578418338,460.597785181025,498.4391249077,540.712597598365,587.418203253018,638.55594187166,694.125813454292,754.127818000912,818.561955511521],[385.081478135264,369.946976993385,359.244608815495,352.974373601594,351.136271351682,353.730302065759,360.756465743825,372.21476238588,388.105191991923,408.427754561956,433.182450095977,462.369278593987,495.988240055987,534.039334481975,576.522561871952,623.437922225918,674.785415543873,730.565041825817,790.77680107175,855.420693281672],[422.327246711489,407.402500268923,396.909886790346,390.849406275758,389.221058725158,392.024844138548,399.260762515926,410.928813857293,427.02899816265,447.561315431995,472.525765665329,501.922348862652,535.751065023964,574.011914149265,616.704896238555,663.830011291833,715.387259309101,771.376640290357,831.798154235603,896.651801144837],[463.94538538073,449.230393637476,438.947534858212,433.096809042936,431.678216191649,434.691756304351,442.137429381043,454.015235421723,470.325174426391,491.067246395049,516.241451327696,545.847789224332,579.886260084956,618.35686390957,661.259600698172,708.594470450764,760.361473167344,816.560608847913,877.191877492471,942.255279101018]],"type":"surface","colorscale":"Viridis","inherit":true},"46f43fb357e":{"x":{},"y":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"scatter3d","mode":"markers","marker":{"size":6,"color":"red","symbol":104},"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"plot_bgcolor":"rgb(254, 247, 234)","paper_bgcolor":"transparent","scene":{"xaxis":{"title":"beta0"},"yaxis":{"title":"beta1"},"zaxis":{"title":"RSS"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"z","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":"Viridis","showscale":true,"x":[0,0.105263157894737,0.210526315789474,0.315789473684211,0.421052631578947,0.526315789473684,0.631578947368421,0.736842105263158,0.842105263157895,0.947368421052632,1.05263157894737,1.15789473684211,1.26315789473684,1.36842105263158,1.47368421052632,1.57894736842105,1.68421052631579,1.78947368421053,1.89473684210526,2],"y":[0,0.105263157894737,0.210526315789474,0.315789473684211,0.421052631578947,0.526315789473684,0.631578947368421,0.736842105263158,0.842105263157895,0.947368421052632,1.05263157894737,1.15789473684211,1.26315789473684,1.36842105263158,1.47368421052632,1.57894736842105,1.68421052631579,1.78947368421053,1.89473684210526,2],"z":[[420.876036570751,402.175705540557,387.907507474353,378.071442372136,372.667510233909,371.695711059671,375.156044849422,383.048511603162,395.37311132089,412.129844002608,433.318709648314,458.939708258009,488.992839831694,523.478104369367,562.395501871029,605.74503233668,653.52669576632,705.740492159949,762.386421517567,823.464483839173],[383.791513565719,365.300937234838,351.242493867946,341.616183465042,336.422006026128,335.659961551202,339.330050040266,347.432271493318,359.966625910359,376.933113291389,398.331733636408,424.162486945416,454.425373218413,489.120392455399,528.247544656374,571.806829821337,619.79824795029,672.221799043231,729.077483100162,790.365300121081],[351.079360653702,332.798539022133,318.949850354554,309.533294650963,304.548871911361,303.996582135748,307.876425324124,316.188401476489,328.932510592843,346.108752673186,367.717127717518,393.757635725838,424.230276698148,459.135050634446,498.471957534734,542.24099739901,590.442170227275,643.075476019529,700.140914775773,761.638486496005],[322.7395778347,304.668510902444,291.029576934177,281.822775929899,277.04810788961,276.70557281331,280.795170700998,289.316901552676,302.270765368342,319.656762147998,341.474891891642,367.725154599276,398.407550270898,433.522078906509,473.068740506109,517.047535069698,565.458462597276,618.301523088842,675.576716544398,737.284042963943],[298.772165108713,280.91085287577,267.481673606816,258.48462730185,253.919713960874,253.786933583886,258.086286170887,266.817771721878,279.981390236857,297.577141715825,319.605026158782,346.065043565728,376.957193936663,412.281477271587,452.037893570499,496.226442833401,544.847125060291,597.899940251171,655.384888406039,717.301969524897],[279.177122475742,261.525564942111,248.306140372469,239.518848766817,235.163690125153,235.240664447478,239.749771733792,248.691011984095,262.064385198387,279.869891376667,302.107530518937,328.777302625195,359.879207695443,395.413245729679,435.379416727905,479.777720690119,528.608157616322,581.870727506514,639.565430360695,701.692266178865],[263.954449935785,246.512647101467,233.502977231138,224.925440324798,220.780036382447,221.066765404085,225.785627389711,234.936622339327,248.519750252931,266.535011130525,288.982404972107,315.861931777678,347.173591547238,382.917384280787,423.093309978325,467.701368639852,516.741560265368,570.213884854873,628.118342408367,690.454932925849],[253.104147488844,235.872099353839,223.072184182822,214.704401975795,210.768752732756,211.265236453706,216.193853138646,225.554602787574,239.347485400491,257.572500977397,280.229649518292,307.318931023176,338.840345492049,374.79389292491,415.179573321761,459.997386682601,509.247333007429,562.929412296246,621.043624549053,683.589969765848],[246.626215134918,229.603921699225,217.013761227521,208.855733719806,205.129839176081,205.836077596344,210.974448980595,220.544953328836,234.547590641066,252.982360917285,275.849264157492,303.148300361689,334.879469529874,371.042771662049,411.638206758212,456.665774818364,506.125475842505,560.017309830635,618.341276782754,681.097376698862],[244.520652874007,227.708114137627,215.327708365236,207.379435556833,203.86329571242,204.779288831996,210.12741491556,219.907673963114,234.120065974656,252.764590950188,275.841248889708,303.350039793217,335.290963660715,371.664020492202,412.469210287678,457.706533047143,507.375988770597,561.477577458039,620.011299109471,682.977153724892],[246.787460706111,230.184676669043,218.014025595965,210.275507486875,206.969122341775,208.094870160663,213.65275094354,223.642764690407,238.064911401262,256.919191076106,280.205603714938,307.92414931776,340.074827884571,376.657639415371,417.672583910159,463.119661368937,512.998871791703,567.310215178459,626.053691529203,689.229300843936],[253.42663863123,237.033609293475,225.07271291971,217.543949509933,214.447319064145,215.782821582346,221.550457064536,231.750225510714,246.382126920882,265.446161295039,288.942328633184,316.870628935319,349.231062201442,386.023628431554,427.248327625656,472.905159783746,522.994124905825,577.515222991893,636.46845404195,699.853818055996],[264.438186649365,248.254912010922,236.503770336469,229.184761626005,226.29788587953,227.843143097043,233.820533278546,244.230056424037,259.071712533518,278.345501606987,302.051423644445,330.189478645892,362.759666611328,399.761987540753,441.196441434167,487.06302829157,537.361748112962,592.092600898342,651.255586647712,714.85070536107],[279.822104760514,263.848584821385,252.307197846244,245.197943835093,242.52082278793,244.275834704756,250.462979585571,261.082257430375,276.133668239168,295.61721201195,319.532888748721,347.880698449481,380.66064111423,417.872716742967,459.516925335694,505.593266892409,556.101741413114,611.042348897807,670.415089346489,734.21996275916],[299.578392964679,283.814627724862,272.482995449034,265.583496137195,263.116129789345,265.080896405484,271.477795985612,282.306828529729,297.567994037834,317.261292509929,341.386723946013,369.944288346085,402.933985710146,440.355816038197,482.209779330236,528.495875586264,579.214104806281,634.364466990287,693.946962138282,757.961590250265],[323.707051261859,308.153040721355,297.031163144839,290.341418532313,288.083806883776,290.258328199227,296.864982478668,307.903769722097,323.374689929516,343.277743100923,367.612929236319,396.380248335704,429.579700399078,467.211285426441,509.275003417793,555.770854373133,606.698838292463,662.058955175782,721.851205023089,786.075587834386],[352.208079652054,336.863823810862,325.95170093366,319.471711020446,317.423854071221,319.808130085986,326.624539064739,337.873081007481,353.553755914212,373.666563784932,398.21150461964,427.188578418338,460.597785181025,498.4391249077,540.712597598365,587.418203253018,638.55594187166,694.125813454292,754.127818000912,818.561955511521],[385.081478135264,369.946976993385,359.244608815495,352.974373601594,351.136271351682,353.730302065759,360.756465743825,372.21476238588,388.105191991923,408.427754561956,433.182450095977,462.369278593987,495.988240055987,534.039334481975,576.522561871952,623.437922225918,674.785415543873,730.565041825817,790.77680107175,855.420693281672],[422.327246711489,407.402500268923,396.909886790346,390.849406275758,389.221058725158,392.024844138548,399.260762515926,410.928813857293,427.02899816265,447.561315431995,472.525765665329,501.922348862652,535.751065023964,574.011914149265,616.704896238555,663.830011291833,715.387259309101,771.376640290357,831.798154235603,896.651801144837],[463.94538538073,449.230393637476,438.947534858212,433.096809042936,431.678216191649,434.691756304351,442.137429381043,454.015235421723,470.325174426391,491.067246395049,516.241451327696,545.847789224332,579.886260084956,618.35686390957,661.259600698172,708.594470450764,760.361473167344,816.560608847913,877.191877492471,942.255279101018]],"type":"surface","frame":null},{"x":[0.5],"y":[1],"z":[205.219803152021],"type":"scatter3d","mode":"markers","marker":{"color":"red","size":6,"symbol":104,"line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Now the question is how to solve this problem. The <code>optim()</code> function uses the following syntax:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The solution can be solved by any optimization algorithm </span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    lm.optim <span class="ot">&lt;-</span> <span class="fu">optim</span>(<span class="at">par =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="at">fn =</span> rss, <span class="at">trainx =</span> x, <span class="at">trainy =</span> y)</span></code></pre></div>
<ul>
<li>The <code>par</code> argument specifies an initial value, in this case, <span class="math inline">\(\beta_0 = \beta_1 = 2\)</span></li>
<li>The <code>fn</code> argument specifies the name of an <code>R</code> function that can calculate the objective function. Please note that the first argument in this function has be the parameter being optimized, i.e, <span class="math inline">\(\boldsymbol \beta\)</span>. Also, it must be a vector, not a matrix or other types.</li>
<li>The arguments <code>trainx = x</code>, <code>trainy = y</code> specifies any additional arguments that the objective function <code>fn</code>, i.e., <code>rss</code> needs. It behaves the same as if you are supplying this to <code>rss</code>.</li>
</ul>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>    lm.optim</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="do">## $par</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.4532072 0.9236502</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="do">## $value</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 203.5623</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="do">## $counts</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="do">## function gradient </span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="do">##       63       NA </span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="do">## $convergence</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a><span class="do">## $message</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="do">## NULL</span></span></code></pre></div>
<p>The result shows that the estimated parameters (<code>$par</code>) are 0.453 and 0.924, with a functional value 203.562. The convergence code is 0, meaning that the algorithm converged. The parameter estimates are almost the same as <code>lm()</code>, with small numerical errors.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The solution form lm()</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summary</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">-</span> <span class="dv">1</span>))<span class="sc">$</span>coefficients</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="do">##     Estimate Std. Error   t value     Pr(&gt;|t|)</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="do">## x1 0.4530498 0.07177854  6.311772 1.773276e-09</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="do">## x2 0.9236934 0.07226742 12.781602 1.136397e-27</span></span></code></pre></div>
<p>What we will be introducing in the following are some basic approaches to solve such a numerical problem. We will start with unconstrained problems, then introduce constrained problems.</p>
</div>
<div id="first-and-second-order-properties" class="section level2 hasAnchor" number="4.4">
<h2 class="hasAnchor"><span class="header-section-number">4.4</span> First and Second Order Properties<a href="#first-and-second-order-properties" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>These properties are usually applied to unconstrained optimization problems. They are essentially just describing the landscape around a point <span class="math inline">\(\mathbf{x}^\ast\)</span> such that it becomes the local optimizer. Since we generally concerns a convex problem, a local solution is also the global solution. However, these properties are still generally applied when solving a non-convex problem. Note that these statements are multi-dimensional.</p>
<p><strong>First-Order Necessary Conditions</strong>: If <span class="math inline">\(f\)</span> is continuously differentiable in an open neighborhood of local minimum <span class="math inline">\(\mathbf{x}^\ast\)</span>, then <span class="math inline">\(\nabla f(\mathbf{x}^\ast) = \mathbf{0}\)</span>.</p>
<p>When we have a point <span class="math inline">\(\mathbf{x}^\ast\)</span> with <span class="math inline">\(\nabla f(\mathbf{x}^\ast) = \mathbf{0}\)</span>, we call <span class="math inline">\(\mathbf{x}^\ast\)</span> a <strong>stationary point</strong>. This is only a necessary condition, but not sufficient. Since example, <span class="math inline">\(f(x) = x^3\)</span> has zero derivative at <span class="math inline">\(x = 0\)</span>, but this is not an optimizer. The figure in @ref(global_local) also contains such a point. TO further strengthen this, we have</p>
<p><strong>Second-order Necessary Conditions</strong>: If <span class="math inline">\(f\)</span> is twice continuously differentiable in an open neighborhood of local minimum <span class="math inline">\(\mathbf{x}^\ast\)</span>, then <span class="math inline">\(\nabla f(\mathbf{x}^\ast) = \mathbf{0}\)</span> and <span class="math inline">\(\nabla^2 f(\mathbf{x}^\ast)\)</span> is positive semi-definite.</p>
<p>This does rule out some cases, with a higher cost (<span class="math inline">\(f\)</span> needs to be twice continuously differentiable). But requiring positive semi-definite would not ensure everything. The same example <span class="math inline">\(f(x) = x^3\)</span> still satisfies this, but its not a local minimum. A positive definite <span class="math inline">\(\nabla^2 f(\mathbf{x}^\ast)\)</span> would be sufficient:</p>
<p><strong>Second-order Sufficient Conditions</strong>: <span class="math inline">\(f\)</span> is twice continuously differentiable in an open neighborhood of <span class="math inline">\(\mathbf{x}^\ast\)</span>. If <span class="math inline">\(\nabla f(\mathbf{x}^\ast) = \mathbf{0}\)</span> and <span class="math inline">\(\nabla^2 f(\mathbf{x}^\ast)\)</span> is positive definite, i.e.,
<span class="math display">\[
\nabla^2 f(\mathbf{x}) = \left(\frac{\partial^2 f(\mathbf{x})}{\partial x_i \partial x_j}\right) = \mathbf{H}(\mathbf{x}) \succeq 0
\]</span></p>
<p>then <span class="math inline">\(\mathbf{x}^\ast\)</span> is a strict local minimizer of <span class="math inline">\(f\)</span>. Here <span class="math inline">\(\mathbf{H}(\mathbf{x})\)</span> is called the <strong>Hessian matrix</strong>, which will be frequently used in second-order methods.</p>
</div>
<div id="algorithm" class="section level2 hasAnchor" number="4.5">
<h2 class="hasAnchor"><span class="header-section-number">4.5</span> Algorithm<a href="#algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Most optimization algorithms follow the same idea: starting from a point <span class="math inline">\(\mathbf{x}^{(0)}\)</span> (which is usually specified by the user) and move to a new point <span class="math inline">\(\mathbf{x}^{(1)}\)</span> that improves the objective function value. Repeatedly performing this to get a sequence of points <span class="math inline">\(\mathbf{x}^{(0)}, \mathbf{x}^{(1)}, \ldots\)</span> until the certain stopping criterion is reached.</p>
<p>A <strong>stopping criterion</strong> could be</p>
<ul>
<li>Using the gradient of the objective function: <span class="math inline">\(\lVert \nabla f(\mathbf{x}^{(k)}) \rVert &lt; \epsilon\)</span></li>
<li>Using the (relative) change of distance: <span class="math inline">\(\lVert \mathbf{x}^{(k)} - \mathbf{x}^{(k-1)} \rVert / \lVert \mathbf{x}^{(k-1)}\rVert&lt; \epsilon\)</span> or <span class="math inline">\(\lVert \mathbf{x}^{(k)} - \mathbf{x}^{(k-1)} \rVert &lt; \epsilon\)</span></li>
<li>Using the (relative) change of functional value: <span class="math inline">\(| f(\mathbf{x}^{(k)}) - f(\mathbf{x}^{(k-1)})| &lt; \epsilon\)</span> or <span class="math inline">\(| f(\mathbf{x}^{(k)}) - f(\mathbf{x}^{(k-1)})| / |f(\mathbf{x}^{(k)})| &lt; \epsilon\)</span></li>
<li>Stop at a pre-specified number of iterations.</li>
</ul>
<p>Most algorithms differ in terms of how to move from the current point <span class="math inline">\(\mathbf{x}^{(k)}\)</span> to the next, better target point <span class="math inline">\(\mathbf{x}^{(k+1)}\)</span>. This may depend on the smoothness or structure of <span class="math inline">\(f\)</span>, constrains on the domain, computational complexity, memory limitation, and many others.</p>
</div>
<div id="second-order-methods" class="section level2 hasAnchor" number="4.6">
<h2 class="hasAnchor"><span class="header-section-number">4.6</span> Second-order Methods<a href="#second-order-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="newtons-method" class="section level3 hasAnchor" number="4.6.1">
<h3 class="hasAnchor"><span class="header-section-number">4.6.1</span> Newton’s Method<a href="#newtons-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, let’s discuss several specific methods. One of the oldest one is <strong>Newton’s method</strong>. This is motivated form a quadratic approximation (essentially Taylor expansion) at a current point <span class="math inline">\(\mathbf{x}\)</span>,</p>
<p><span class="math display">\[f(\mathbf{x}^\ast) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^\text{T}(\mathbf{x}^\ast - \mathbf{x}) + \frac{1}{2} (\mathbf{x}^\ast - \mathbf{x})^\text{T}\mathbf{H}(\mathbf{x}) (\mathbf{x}^\ast - \mathbf{x})\]</span>
Our goal is to find a new stationary point <span class="math inline">\(\mathbf{x}^\ast\)</span> such that <span class="math inline">\(\nabla f(\mathbf{x}^\ast) = 0\)</span>. By taking derivative of the above equation on both sides, with respect to <span class="math inline">\(\mathbf{x}^\ast\)</span>, we need</p>
<p><span class="math display">\[0 = \nabla f(\mathbf{x}^\ast) = 0 + \nabla f(\mathbf{x}) + (\mathbf{x}^\ast - \mathbf{x})^\text{T}\mathbf{H}(\mathbf{x})\]</span>
which leads to</p>
<p><span class="math display">\[\mathbf{x}^\ast = \mathbf{x}-  \mathbf{H}(\mathbf{x})^{-1} \nabla f(\mathbf{x}).\]</span></p>
<p>Hence, if we are currently at a point <span class="math inline">\(\mathbf{x}^{(k)}\)</span>, we need to calculate the gradient <span class="math inline">\(\nabla f(\mathbf{x}^{(k)})\)</span> and Hessian <span class="math inline">\(\mathbf{H}(\mathbf{x})\)</span> at this point, then move to the new point using <span class="math inline">\(\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \mathbf{H}(\mathbf{x}^{(k)})^{-1} \nabla f(\mathbf{x}^{(k)})\)</span>. Some properties and things to concern regarding Newton’s method:</p>
<ul>
<li>Newton’s method is scale invariant, meaning that you do not need to worry about the step size. It is automatically taken care of by the Hessian matrix. However, in practice, the local approximation may not be accurate, which makes the new point <span class="math inline">\(\mathbf{x}^{(k+1)}\)</span> behaves differently than what we expect. Hence, it might still be safe to introduce a smaller step size <span class="math inline">\(\delta \in (0, 1)\)</span> and move with<br />
<span class="math display">\[\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} -  \delta \, \mathbf{H}(\mathbf{x}^{(k)})^{-1} \nabla f(\mathbf{x}^{(k)})\]</span></li>
<li>There are also alternatives to take care of the step size. For example, <strong>line search</strong> is frequently used, which will try to find the optimal <span class="math inline">\(\delta\)</span> that minimizes the function
<span class="math display">\[f(\mathbf{x}^{(k)} + \delta \mathbf{v})\]</span>
where the direction <span class="math inline">\(\mathbf{v}\)</span> in this case is <span class="math inline">\(\mathbf{v}= \mathbf{H}(\mathbf{x}^{(k)})^{-1} \nabla f(\mathbf{x}^{(k)})\)</span>. It is also popular to use <strong>backtracking line search</strong>, which reduces the computational cost. The idea is to start with a large <span class="math inline">\(\delta\)</span> and gradually reduces it by a certain proportion if the new point doesn’t significantly improves, i.e.,
<span class="math display">\[f(\mathbf{x}^{(k)} + \delta \mathbf{v}) &gt; f(\mathbf{x}^{(k)}) - \frac{1}{2} \delta \nabla f(\mathbf{x}^{(k)})^\text{T}\mathbf{v}\]</span>
Note that when the direction <span class="math inline">\(\mathbf{v}\)</span> is <span class="math inline">\(\mathbf{H}(\mathbf{x}^{(k)})^{-1} \nabla f(\mathbf{x}^{(k)})\)</span>, <span class="math inline">\(\nabla f(\mathbf{x}^{(k)})^\text{T}\mathbf{v}\)</span> is essentially the norm defined by the Hessian matrix.</li>
<li>When you do not have the explicit formula of Hessian and even the gradient, you may <strong>numerically approximate the derivative</strong> using the definition. For example, we could use
<span class="math display">\[ \frac{f(\mathbf{x}^{(k)} + \epsilon) - f(\mathbf{x}^{(k)})}{\epsilon} \]</span>
with <span class="math inline">\(\epsilon\)</span> small enough, e.g., <span class="math inline">\(10^{-5}\)</span>. However, this is very costly for the Hessian matrix if the number of variables is large.</li>
</ul>
</div>
<div id="quasi-newton-methods" class="section level3 hasAnchor" number="4.6.2">
<h3 class="hasAnchor"><span class="header-section-number">4.6.2</span> Quasi-Newton Methods<a href="#quasi-newton-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since the idea of Newton’s method is to solve a vector <span class="math inline">\(\mathbf{v}\)</span> such that</p>
<p><span class="math display">\[\mathbf{H}(\mathbf{x}^{(k)}) \mathbf{v}= - \nabla f(\mathbf{x}^{(k)}), \]</span>
If <span class="math inline">\(\mathbf{H}\)</span> is difficult to compute, we may use some matrix to substitute it. For example, if we simplify use the identity matrix <span class="math inline">\(\mathbf{I}\)</span>, then this reduces to a first-order method to be introduced later. However, if we can get a good approximation, we can still solve this linear system and get to a better point. Then the question is how to obtain such matrix in a computationally inexpensive way. The Broyden–Fletcher–Goldfarb–Shanno (BFSG) algorithm is such an approach by iteratively updating its (inverse) estimation. The algorithm proceed as follows:</p>
<ol style="list-style-type: decimal">
<li>Start with <span class="math inline">\(x^{(0)}\)</span> and a positive definite matrix, e.g., <span class="math inline">\(\mathbf{B}^{(0)} = \mathbf{I}\)</span></li>
<li>For <span class="math inline">\(k = 0, 1, 2, \ldots\)</span>,
<ul>
<li>Search a updating direction by solving the linear system <span class="math inline">\(\mathbf{B}^{(k)} \mathbf{p}_k = - \nabla f(\mathbf{x}^{(k)})\)</span></li>
<li>Perform line search in the direction of <span class="math inline">\(\mathbf{v}_k\)</span> and obtain the next point <span class="math inline">\(\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \delta \mathbf{p}_k\)</span></li>
<li>Update the approximation by
<span class="math display">\[ \mathbf{B}^{(k+1)} = \mathbf{B}^{(k)} + \frac{\mathbf{y}_k^\text{T}\mathbf{y}_{k}}{ \mathbf{y}_{k}^\text{T}\mathbf{s}_{k} } -  \frac{\mathbf{B}^{(k)}\mathbf{s}_{k}\mathbf{s}_{k}^\text{T}{\mathbf{B}^{(k)}}^\text{T}}{\mathbf{s}_{k}^\text{T}\mathbf{B}^{(k)} \mathbf{s}_{k} }, \]</span>
where <span class="math inline">\(\mathbf{y}_k = \nabla f(\mathbf{x}^{(k+1)}) - \nabla f(\mathbf{x}^{(k)})\)</span> and <span class="math inline">\(\mathbf{s}_{k} = \mathbf{x}^{(k+1)} - \mathbf{x}^{(k)}\)</span>.</li>
</ul></li>
</ol>
<p>The BFGS is performing a rank-two update by assuming that
<span class="math display">\[ \mathbf{B}^{(k+1)} = \mathbf{B}^{(k)} + a \mathbf{u}\mathbf{u}^\text{T}+ b \mathbf{v}\mathbf{v}^\text{T},\]</span>
Alternatives of such type of methods include the symmetric rank-one and Davidon-Fletcher-Powell (DFP) updates.</p>
</div>
</div>
<div id="first-order-methods" class="section level2 hasAnchor" number="4.7">
<h2 class="hasAnchor"><span class="header-section-number">4.7</span> First-order Methods<a href="#first-order-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="gradient-descent" class="section level3 hasAnchor" number="4.7.1">
<h3 class="hasAnchor"><span class="header-section-number">4.7.1</span> Gradient Descent<a href="#gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When simply using <span class="math inline">\(\mathbf{H}= \mathbf{I}\)</span>, we update
<span class="math display">\[\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \delta \nabla f(\mathbf{x}^{(k)}).\]</span>
However, it is then crucial to figure out the step size <span class="math inline">\(\delta\)</span>. A step size too large may not even converge at all, however, a step size too small will take too many iterations to converge. Alternatively, line search could be used.</p>
</div>
<div id="gradient-descent-example-linear-regression" class="section level3 hasAnchor" number="4.7.2">
<h3 class="hasAnchor"><span class="header-section-number">4.7.2</span> Gradient Descent Example: Linear Regression<a href="#gradient-descent-example-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use linear regression as an example. The objective function for linear regression is:</p>
<p><span class="math display">\[ \ell(\boldsymbol \beta) = \frac{1}{2n}||\mathbf{y} - \mathbf{X} \boldsymbol \beta ||^2 \]</span>
with solution is</p>
<p><span class="math display">\[\widehat{\boldsymbol \beta} = \left(\mathbf{X}^\text{T}\mathbf{X}\right)^{-1} \mathbf{X}^\text{T} \mathbf{y} \]</span></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(MASS)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">3</span>)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">200</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># create some data with linear model</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.7</span>, <span class="fl">0.7</span>, <span class="dv">1</span>), <span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">2</span><span class="sc">*</span>X[,<span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>])</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>  beta1 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">4</span>, <span class="fl">0.005</span>)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>  beta2 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">4</span>, <span class="fl">0.005</span>)</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>  allbeta <span class="ot">&lt;-</span> <span class="fu">data.matrix</span>(<span class="fu">expand.grid</span>(beta1, beta2))</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>  rss <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">apply</span>(allbeta, <span class="dv">1</span>, <span class="cf">function</span>(b, X, y) <span class="fu">sum</span>((y <span class="sc">-</span> X <span class="sc">%*%</span> b)<span class="sc">^</span><span class="dv">2</span>), X, y), </span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>                <span class="fu">length</span>(beta1), <span class="fu">length</span>(beta2))</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># quantile levels for drawing contour</span></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>  quanlvl <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.025</span>, <span class="fl">0.05</span>, <span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the contour</span></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the truth</span></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>  b <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(b[<span class="dv">1</span>], b[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-53-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We use an optimization approach to solve this problem. By taking the derivative with respect to <span class="math inline">\(\boldsymbol \beta\)</span>, we have the gradient</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta} = -\frac{1}{n} \sum_{i=1}^n (y_i - x_i^\text{T} \boldsymbol \beta) x_i.
\end{align}
\]</span>
To perform the optimization, we will first set an initial beta value, say <span class="math inline">\(\boldsymbol \beta = \mathbf{0}\)</span> for all entries, then proceed with the update</p>
<p><span class="math display">\[ \boldsymbol \beta^\text{new} = \boldsymbol \beta^\text{old} - \frac{\partial \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta} \times \delta.\]</span></p>
<p>Let’s set <span class="math inline">\(\delta = 0.2\)</span> for now. The following function performs gradient descent.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient descent function, which also record the path</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>  mylm_g <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y, </span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">b0 =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">ncol</span>(x)), <span class="co"># initial value</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">delta =</span> <span class="fl">0.2</span>, <span class="co"># step size</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">epsilon =</span> <span class="fl">1e-6</span>, <span class="co">#stopping rule</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>                     <span class="at">maxitr =</span> <span class="dv">5000</span>) <span class="co"># maximum iterations</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.matrix</span>(x)) <span class="fu">stop</span>(<span class="st">&quot;x must be a matrix&quot;</span>)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.vector</span>(y)) <span class="fu">stop</span>(<span class="st">&quot;y must be a vector&quot;</span>)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">nrow</span>(x) <span class="sc">!=</span> <span class="fu">length</span>(y)) <span class="fu">stop</span>(<span class="st">&quot;number of observations different&quot;</span>)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize beta values</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    allb <span class="ot">=</span> <span class="fu">matrix</span>(b0, <span class="dv">1</span>, <span class="fu">length</span>(b0))</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># iterative update</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxitr)</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>      <span class="co"># the new beta value</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>      b1 <span class="ot">=</span> b0 <span class="sc">+</span> <span class="fu">t</span>(x) <span class="sc">%*%</span> (y <span class="sc">-</span> x <span class="sc">%*%</span> b0) <span class="sc">*</span> delta <span class="sc">/</span> <span class="fu">length</span>(y)      </span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>      <span class="co"># record the new beta</span></span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>      allb <span class="ot">=</span> <span class="fu">rbind</span>(allb, <span class="fu">as.vector</span>(b1))</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>      <span class="co"># stopping rule</span></span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">max</span>(<span class="fu">abs</span>(b0 <span class="sc">-</span> b1)) <span class="sc">&lt;</span> epsilon)</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span>;</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>      <span class="co"># reset beta0</span></span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>      b0 <span class="ot">=</span> b1</span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (k <span class="sc">==</span> maxitr) <span class="fu">cat</span>(<span class="st">&quot;maximum iteration reached</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(<span class="st">&quot;allb&quot;</span> <span class="ot">=</span> allb, <span class="st">&quot;beta&quot;</span> <span class="ot">=</span> b1))</span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit the model </span></span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>  mybeta <span class="ot">=</span> <span class="fu">mylm_g</span>(X, y, <span class="at">b0 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">bg=</span><span class="st">&quot;transparent&quot;</span>)</span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(mybeta<span class="sc">$</span>allb[,<span class="dv">1</span>], mybeta<span class="sc">$</span>allb[,<span class="dv">2</span>], <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(b[<span class="dv">1</span>], b[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-54-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>The descent path is very smooth because we choose a very small step size. However, if the step size is too large, we may observe unstable results or even unable to converge. For example, if We set <span class="math inline">\(\delta = 1\)</span> or <span class="math inline">\(\delta = 1.5\)</span>.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit the model with a larger step size</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>  mybeta <span class="ot">=</span> <span class="fu">mylm_g</span>(X, y, <span class="at">b0 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">delta =</span> <span class="dv">1</span>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(mybeta<span class="sc">$</span>allb[,<span class="dv">1</span>], mybeta<span class="sc">$</span>allb[,<span class="dv">2</span>], <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(b[<span class="dv">1</span>], b[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># and even larger</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>  mybeta <span class="ot">=</span> <span class="fu">mylm_g</span>(X, y, <span class="at">b0 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">delta =</span> <span class="fl">1.5</span>, <span class="at">maxitr =</span> <span class="dv">6</span>)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a><span class="do">## maximum iteration reached</span></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(mybeta<span class="sc">$</span>allb[,<span class="dv">1</span>], mybeta<span class="sc">$</span>allb[,<span class="dv">2</span>], <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(b[<span class="dv">1</span>], b[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-55-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="coordinate" class="section level2 hasAnchor" number="4.8">
<h2 class="hasAnchor"><span class="header-section-number">4.8</span> Coordinate Descent<a href="#coordinate" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Instead of updating all parameters at a time, we can also update one parameter each time. The <strong>Gauss-Seidel style</strong> coordinate descent algorithm at the <span class="math inline">\(k\)</span>th iteration will sequentially update all <span class="math inline">\(p\)</span> parameters:</p>
<p><span class="math display">\[\begin{align}
    x_1^{(k+1)} &amp;= \underset{\color{OrangeRed}{x_1}}{\arg\min} \quad f(\color{OrangeRed}{x_1}, x_2^{(k)}, \ldots, x_p^{(k)}) \nonumber \\
    x_2^{(k+1)} &amp;= \underset{\color{OrangeRed}{x_2}}{\arg\min} \quad f(x_1^{\color{DodgerBlue}{(k+1)}}, \color{OrangeRed}{\mathbf{x}_2}, \ldots, x_p^{(k)}) \nonumber \\
    \cdots &amp;\nonumber \\
    x_p^{(k+1)} &amp;= \underset{\color{OrangeRed}{x_p}}{\arg\min} \quad f(x_1^{\color{DodgerBlue}{(k+1)}}, x_2^{\color{DodgerBlue}{(k+1)}}, \ldots, \color{OrangeRed}{x_p}) \nonumber \\
\end{align}\]</span></p>
<p>Note that after updating one coordinate, the new parameter value is used for updating the next coordinate. After we complete this loop, all <span class="math inline">\(j\)</span> are updated to their new values, and we proceed to the next step.</p>
<p>Another type of update is the <strong>Jacobi style</strong>, which can be performed in parallel at the <span class="math inline">\(k\)</span>th iteration:</p>
<p><span class="math display">\[\begin{align}
    x_1^{(k+1)} &amp;= \underset{\color{OrangeRed}{x_1}}{\arg\min} \quad f(\color{OrangeRed}{x_1}, x_2^{(k)}, \ldots, x_p^{(k)}) \nonumber \\
    x_2^{(k+1)} &amp;= \underset{\color{OrangeRed}{x_2}}{\arg\min} \quad f(x_1^{(k+1)}, \color{OrangeRed}{\mathbf{x}_2}, \ldots, x_p^{(k)}) \nonumber \\
    \cdots &amp;\nonumber \\
    x_p^{(k+1)} &amp;= \underset{\color{OrangeRed}{x_p}}{\arg\min} \quad f(x_1^{(k+1)}, x_2^{(k+1)}, \ldots, \color{OrangeRed}{x_p}) \nonumber \\
\end{align}\]</span></p>
<p>For differentiable convex functions <span class="math inline">\(f\)</span>, we can ensure that if all parameters are optimized then the entire problem is also optimized. If <span class="math inline">\(f\)</span> is not differentiable, we may have trouble (see the example on <a href="https://en.wikipedia.org/wiki/Coordinate_descent">wiki</a>). However, there are also cases where coordinate descent would still guarantee a convergence, e.g., a sperable case:</p>
<p><span class="math display">\[f(\mathbf{x}) = g(\mathbf{x}) + \sum_{j=1}^p h_j(x_j)\]</span>
This is the Lasso formulation which will be discussed in later section.</p>
<div id="coordinate-descent-example-linear-regression" class="section level3 hasAnchor" number="4.8.1">
<h3 class="hasAnchor"><span class="header-section-number">4.8.1</span> Coordinate Descent Example: Linear Regression<a href="#coordinate-descent-example-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Coordinate descent for linear regression is not really necessary. However, we will still use this as an example. Note that the update for a single parameter is</p>
<p><span class="math display">\[
\underset{\boldsymbol \beta_j}{\text{argmin}} \frac{1}{2n} ||\mathbf{y}- X_j \beta_j - \mathbf{X}_{(-j)} \boldsymbol \beta_{(-j)} ||^2
\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}_{(-j)}\)</span> is the data matrix without the <span class="math inline">\(j\)</span>th column. Note that when updating <span class="math inline">\(\beta_j\)</span> coordinate-wise, we can first calculate the residual defined as <span class="math inline">\(\mathbf{r} = \mathbf{y} - \mathbf{X}_{(-j)} \boldsymbol \beta_{(-j)}\)</span> which does not depend on <span class="math inline">\(\beta_j\)</span>, and optimize the rest of the formula for <span class="math inline">\(\beta_j\)</span>. This is essentially the same as performing a one-dimensional regression by regressing <span class="math inline">\(\mathbf{r}\)</span> on <span class="math inline">\(X_j\)</span> and obtain the update.
<span class="math display">\[
\beta_j = \frac{X_j^T \mathbf{r}}{X_j^T X_j}
\]</span>
The coordinate descent usually does not involve choosing a step size. Note that the following function is <strong>NOT</strong> efficient because there are a lot of wasted calculations. It is only for demonstration purpose. Here we use the Gauss-Seidel style update.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient descent function, which also record the path</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>  mylm_c <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y, <span class="at">b0 =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">ncol</span>(x)), <span class="at">epsilon =</span> <span class="fl">1e-6</span>, <span class="at">maxitr =</span> <span class="dv">5000</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.matrix</span>(x)) <span class="fu">stop</span>(<span class="st">&quot;x must be a matrix&quot;</span>)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.vector</span>(y)) <span class="fu">stop</span>(<span class="st">&quot;y must be a vector&quot;</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">nrow</span>(x) <span class="sc">!=</span> <span class="fu">length</span>(y)) <span class="fu">stop</span>(<span class="st">&quot;number of observations different&quot;</span>)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize beta values</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    allb <span class="ot">=</span> <span class="fu">matrix</span>(b0, <span class="dv">1</span>, <span class="fu">length</span>(b0))</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># iterative update</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxitr)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>      <span class="co"># initiate a vector for new beta</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>      b1 <span class="ot">=</span> b0</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(x))</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>      {</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate the residual</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>        r <span class="ot">=</span> y <span class="sc">-</span> x[, <span class="sc">-</span>j, drop <span class="ot">=</span> <span class="cn">FALSE</span>] <span class="sc">%*%</span> b1[<span class="sc">-</span>j]</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update jth coordinate</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>        b1[j] <span class="ot">=</span> <span class="fu">t</span>(r) <span class="sc">%*%</span> x[,j] <span class="sc">/</span> (<span class="fu">t</span>(x[,j, <span class="at">drop =</span> <span class="cn">FALSE</span>]) <span class="sc">%*%</span> x[,j])</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># record the update</span></span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>        allb <span class="ot">=</span> <span class="fu">rbind</span>(allb, <span class="fu">as.vector</span>(b1))</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">max</span>(<span class="fu">abs</span>(b0 <span class="sc">-</span> b1)) <span class="sc">&lt;</span> epsilon)</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span>;</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>      <span class="co"># reset beta0</span></span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>      b0 <span class="ot">=</span> b1</span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (k <span class="sc">==</span> maxitr) <span class="fu">cat</span>(<span class="st">&quot;maximum iteration reached</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(<span class="st">&quot;allb&quot;</span> <span class="ot">=</span> allb, <span class="st">&quot;beta&quot;</span> <span class="ot">=</span> b1))</span>
<span id="cb43-38"><a href="#cb43-38" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb43-39"><a href="#cb43-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-40"><a href="#cb43-40" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit the model </span></span>
<span id="cb43-41"><a href="#cb43-41" aria-hidden="true" tabindex="-1"></a>  mybeta <span class="ot">=</span> <span class="fu">mylm_c</span>(X, y, <span class="at">b0 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">3</span>))</span>
<span id="cb43-42"><a href="#cb43-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-43"><a href="#cb43-43" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb43-44"><a href="#cb43-44" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb43-45"><a href="#cb43-45" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(mybeta<span class="sc">$</span>allb[,<span class="dv">1</span>], mybeta<span class="sc">$</span>allb[,<span class="dv">2</span>], <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb43-46"><a href="#cb43-46" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(b[<span class="dv">1</span>], b[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb43-47"><a href="#cb43-47" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-56-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="stocastic-gradient-descent" class="section level2 hasAnchor" number="4.9">
<h2 class="hasAnchor"><span class="header-section-number">4.9</span> Stocastic Gradient Descent<a href="#stocastic-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The main advantage of using Stochastic Gradient Descent (SGD) is its computational speed. Calculating the gradient using all observations can be costly. Instead, we consider update the parameter <strong>based on a single observation</strong>. Hence, the gradient is defined as</p>
<p><span class="math display">\[
\frac{\partial \ell_i(\boldsymbol \beta)}{\partial \boldsymbol \beta} = - (y_i - x_i^\text{T} \boldsymbol \beta) x_i.
\]</span>
Compared with using all observations, this is <span class="math inline">\(1/n\)</span> of the cost. However, because this is rater not accurate for each iteration, but can still converge in the long run. There is a decay rate involved in SGD step size. If the step size does not decreases to 0, the algorithm cannot converge. However, it also has to sum up to infinite to allow us to go as far as we can. For example, a choice could be <span class="math inline">\(\delta_k = 1/k\)</span>, hence <span class="math inline">\(\sum \delta_k = \infty\)</span> and <span class="math inline">\(\sum \delta_k^2 &lt; \infty\)</span>.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient descent function, which also record the path</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>  mylm_sgd <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y, <span class="at">b0 =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">ncol</span>(x)), <span class="at">delta =</span> <span class="fl">0.05</span>, <span class="at">maxitr =</span> <span class="dv">10</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.matrix</span>(x)) <span class="fu">stop</span>(<span class="st">&quot;x must be a matrix&quot;</span>)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.vector</span>(y)) <span class="fu">stop</span>(<span class="st">&quot;y must be a vector&quot;</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">nrow</span>(x) <span class="sc">!=</span> <span class="fu">length</span>(y)) <span class="fu">stop</span>(<span class="st">&quot;number of observations different&quot;</span>)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize beta values</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>    allb <span class="ot">=</span> <span class="fu">matrix</span>(b0, <span class="dv">1</span>, <span class="fu">length</span>(b0))</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># iterative update</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxitr)</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>      <span class="co"># going through all samples</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(x)))</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>      {</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update based on the gradient of a single subject</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>        b0 <span class="ot">=</span> b0 <span class="sc">+</span> (y[i] <span class="sc">-</span> <span class="fu">sum</span>(x[i, ] <span class="sc">*</span> b0)) <span class="sc">*</span> x[i, ] <span class="sc">*</span> delta</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># record the update</span></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>        allb <span class="ot">=</span> <span class="fu">rbind</span>(allb, <span class="fu">as.vector</span>(b0))</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># learning rate decay</span></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>        delta <span class="ot">=</span> delta <span class="sc">*</span> <span class="dv">1</span><span class="sc">/</span>k</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(<span class="st">&quot;allb&quot;</span> <span class="ot">=</span> allb, <span class="st">&quot;beta&quot;</span> <span class="ot">=</span> b0))</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit the model </span></span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>  mybeta <span class="ot">=</span> <span class="fu">mylm_sgd</span>(X, y, <span class="at">b0 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">maxitr =</span> <span class="dv">3</span>)</span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(mybeta<span class="sc">$</span>allb[,<span class="dv">1</span>], mybeta<span class="sc">$</span>allb[,<span class="dv">2</span>], <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(b[<span class="dv">1</span>], b[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-57-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div id="mini-batch-stocastic-gradient-descent" class="section level3 hasAnchor" number="4.9.1">
<h3 class="hasAnchor"><span class="header-section-number">4.9.1</span> Mini-batch Stocastic Gradient Descent<a href="#mini-batch-stocastic-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Instead of using just one observation, we could also consider splitting the data into several small “batches” and use one batch of sample to calculate the gradient at each iteration.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gradient descent function, which also record the path</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>  mylm_sgd_mb <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y, <span class="at">b0 =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">ncol</span>(x)), <span class="at">delta =</span> <span class="fl">0.3</span>, <span class="at">maxitr =</span> <span class="dv">20</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.matrix</span>(x)) <span class="fu">stop</span>(<span class="st">&quot;x must be a matrix&quot;</span>)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.vector</span>(y)) <span class="fu">stop</span>(<span class="st">&quot;y must be a vector&quot;</span>)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">nrow</span>(x) <span class="sc">!=</span> <span class="fu">length</span>(y)) <span class="fu">stop</span>(<span class="st">&quot;number of observations different&quot;</span>)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initiate batches with 10 observations each</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>    batch <span class="ot">=</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">floor</span>(<span class="fu">nrow</span>(x)<span class="sc">/</span><span class="dv">10</span>), <span class="at">length.out =</span> <span class="fu">nrow</span>(x)))</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize beta values</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>    allb <span class="ot">=</span> <span class="fu">matrix</span>(b0, <span class="dv">1</span>, <span class="fu">length</span>(b0))</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># iterative update</span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxitr)</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">max</span>(batch)) <span class="co"># loop through batches</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>      {</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update based on the gradient of a single subject</span></span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>        b0 <span class="ot">=</span> b0 <span class="sc">+</span> <span class="fu">t</span>(x[batch<span class="sc">==</span>i, ]) <span class="sc">%*%</span> (y[batch<span class="sc">==</span>i] <span class="sc">-</span> x[batch<span class="sc">==</span>i, ] <span class="sc">%*%</span> b0) <span class="sc">*</span> </span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>          delta <span class="sc">/</span> <span class="fu">sum</span>(batch<span class="sc">==</span>i)</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># record the update</span></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>        allb <span class="ot">=</span> <span class="fu">rbind</span>(allb, <span class="fu">as.vector</span>(b0))</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># learning rate decay</span></span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>        delta <span class="ot">=</span> delta <span class="sc">*</span> <span class="dv">1</span><span class="sc">/</span>k</span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(<span class="st">&quot;allb&quot;</span> <span class="ot">=</span> allb, <span class="st">&quot;beta&quot;</span> <span class="ot">=</span> b0))</span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit the model </span></span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>  mybeta <span class="ot">=</span> <span class="fu">mylm_sgd_mb</span>(X, y, <span class="at">b0 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">maxitr =</span> <span class="dv">3</span>)</span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(mybeta<span class="sc">$</span>allb[,<span class="dv">1</span>], mybeta<span class="sc">$</span>allb[,<span class="dv">2</span>], <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(b[<span class="dv">1</span>], b[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb45-40"><a href="#cb45-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-58-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>You may further play around with these tuning parameters to see how sensitive the optimization is to them. A stopping rule can be difficult to determine, hence in practice, early stop is also used.</p>
</div>
</div>
<div id="lagrangian-multiplier-for-constrained-problems" class="section level2 hasAnchor" number="4.10">
<h2 class="hasAnchor"><span class="header-section-number">4.10</span> Lagrangian Multiplier for Constrained Problems<a href="#lagrangian-multiplier-for-constrained-problems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Constrained optimization problems appear very frequently. Both Lasso and Ridge regressions can be viewed as constrained problems, while support vector machines (SVM) is another example, which will be introduced later on. Let’s investigate this using a toy example. Suppose we have an optimization problem</p>
<p><span class="math display">\[\text{minimize} \quad f(x, y) = x^2 + y^2\]</span>
<span class="math display">\[\text{subj. to} \quad g(x, y) = xy - 4 = 0\]</span></p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="fl">0.05</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="fl">0.05</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>  mygrid <span class="ot">&lt;-</span> <span class="fu">data.matrix</span>(<span class="fu">expand.grid</span>(x, y))</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>  f <span class="ot">&lt;-</span> <span class="fu">matrix</span>(mygrid[,<span class="dv">1</span>]<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> mygrid[,<span class="dv">2</span>]<span class="sc">^</span><span class="dv">2</span>, <span class="fu">length</span>(x), <span class="fu">length</span>(y))</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>  f2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(mygrid[,<span class="dv">1</span>]<span class="sc">*</span>mygrid[,<span class="dv">2</span>], <span class="fu">length</span>(x), <span class="fu">length</span>(y))</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the contour</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(x, y, f, <span class="at">levels =</span> <span class="fu">c</span>(<span class="fl">0.2</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>))</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(x, y, f2, <span class="at">levels =</span> <span class="dv">4</span>, <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="fl">0.01</span>), <span class="dv">4</span><span class="sc">-</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="fl">0.01</span>), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)    </span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-59-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>The problem itself is very simple. We know that the optimizer is the red dot. But an interesting point of view is to look at the level curves of the objective function. As it is growing (expanding), there is one point (the red dot) at which level curve barely touches the constrain curve (blue line). This should be the optimizer. But this also implies that the tangent line (orange line) of this leveling curve must coincide with the tangent line of the constraint. Noticing that the tangent line can be obtained by taking the derivative of the function, this observation implies that gradients of the two functions (the objective function and the constraint function) must be a multiple of the other. Hence,</p>
<p><span class="math display">\[
\begin{align}
&amp; \bigtriangledown f = \lambda \bigtriangledown g \\
\\
\Longrightarrow \qquad &amp;  \begin{cases}
    2x = \lambda y &amp; \text{by taking derivative w.r.t.} \,\, x\\
    2y = \lambda x &amp; \text{by taking derivative w.r.t.} \,\, y\\
    xy - 4 = 0 &amp; \text{the constraint itself}
  \end{cases}
\end{align}
\]</span></p>
<p>The three equations put together is very easy to solve. We have <span class="math inline">\(x = y = 0\)</span> or <span class="math inline">\(\lambda = \pm 2\)</span> based on the first two equations. The first one is not feasible based on the constraint. The second solution leads to two feasible solutions: <span class="math inline">\(x = y = 2\)</span> or <span class="math inline">\(x = y = -2\)</span>. Hence, we now know that there are two solutions.</p>
<p>Now, looking back at the equation <span class="math inline">\(\bigtriangledown f = \lambda \bigtriangledown g\)</span>, this is simply the derivative of the <strong>Lagrangian function</strong> defined as</p>
<p><span class="math display">\[{\cal L}(x, y, \lambda) = f(x, y) - \lambda g(x, y),\]</span>
while solving for the solution of the constrained problem becomes finding the stationary point of the Lagrangian. Be aware that in some cases, the solution you found can be maximizers instead of minimizers. Hence, its necessary to compare all of them and see which one is smaller.</p>
<!--chapter:end:01.4-optimization.Rmd-->
</div>
</div>
<div id="part-linear-and-penalized-linear-models" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART) Linear and Penalized Linear Models<a href="#part-linear-and-penalized-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="linear-regression-and-model-selection" class="section level1 hasAnchor" number="5">
<h1 class="hasAnchor"><span class="header-section-number">5</span> Linear Regression and Model Selection<a href="#linear-regression-and-model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter severs several purposes. First, we will review some basic knowledge of linear regression. This includes the concept of vector space, projection, which leads to estimating parameters of a linear regression. Most of these knowledge are covered in the prerequisite so you shouldn’t find these concepts too difficult to understand. Secondly, we will mainly use the <code>lm()</code> function as an example to demonstrate some features of <code>R</code>. This includes extracting results, visualizations, handling categorical variables, prediction and model selection. These concepts will be useful for other models. Finally, we will introduce several model selection criteria and algorithms to perform model selection.</p>
<div id="example-real-estate-data" class="section level2 hasAnchor" number="5.1">
<h2 class="hasAnchor"><span class="header-section-number">5.1</span> Example: real estate data<a href="#example-real-estate-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This <a href="https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set">Real Estate data</a> <span class="citation">(<a href="#ref-yeh2018building" role="doc-biblioref">Yeh and Hsu 2018</a>)</span> is provided on the <a href="https://archive.ics.uci.edu/ml/index.php">UCI machine learning repository</a>. The goal of this dataset is to predict the unit house price based on six different covariates:</p>
<ul>
<li><code>date</code>: The transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)</li>
<li><code>age</code>: The house age (unit: year)</li>
<li><code>distance</code>: The distance to the nearest MRT station (unit: meter)</li>
<li><code>stores</code>: The number of convenience stores in the living circle on foot (integer)</li>
<li><code>latitude</code>: Latitude (unit: degree)</li>
<li><code>longitude</code>: Longitude (unit: degree)</li>
<li><code>price</code>: House price of unit area</li>
</ul>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>    realestate <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/realestate.csv&quot;</span>, <span class="at">row.names =</span> <span class="dv">1</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(DT)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">datatable</span>(realestate, <span class="at">filter =</span> <span class="st">&quot;top&quot;</span>, <span class="at">rownames =</span> <span class="cn">FALSE</span>,</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">8</span>))</span></code></pre></div>
<div id="htmlwidget-7e293be7c01b4d1e0b5c" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-7e293be7c01b4d1e0b5c">{"x":{"filter":"top","vertical":false,"filterHTML":"<tr>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"2012.667\" data-max=\"2013.583\" data-scale=\"3\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"0\" data-max=\"43.8\" data-scale=\"1\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"23.38284\" data-max=\"6488.021\" data-scale=\"5\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"integer\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"0\" data-max=\"10\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"24.93207\" data-max=\"25.01459\" data-scale=\"5\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"121.47353\" data-max=\"121.56627\" data-scale=\"5\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"7.6\" data-max=\"117.5\" data-scale=\"1\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n<\/tr>","data":[[2012.917,2012.917,2013.583,2013.5,2012.833,2012.667,2012.667,2013.417,2013.5,2013.417,2013.083,2013.333,2012.917,2012.667,2013.5,2013.583,2013.25,2012.75,2013.417,2012.667,2013.417,2013.417,2012.917,2013.083,2013,2013.083,2012.667,2013.25,2013.5,2013.083,2013.5,2012.75,2012.75,2013.25,2012.75,2013.5,2012.917,2013.167,2012.667,2013.167,2013,2013.5,2013.417,2012.75,2013.583,2013.083,2013.417,2013.583,2013.417,2012.667,2013.417,2013.083,2013.583,2013.083,2013.083,2012.833,2013.417,2012.917,2013.5,2013.083,2013.417,2013.5,2012.917,2013.583,2013.333,2013.417,2013,2013.5,2013.417,2012.833,2013.583,2013.083,2013.583,2013.167,2012.917,2013.5,2013.583,2012.833,2012.917,2013,2013.5,2013,2013.083,2012.917,2013.083,2012.75,2012.833,2013.583,2012.917,2013.5,2012.833,2013.25,2012.917,2012.917,2012.917,2012.917,2013.417,2013.083,2013.417,2013.417,2013.5,2012.833,2013.083,2012.75,2012.667,2012.833,2013.083,2013.333,2013.417,2013.583,2013.083,2013.583,2013.417,2013.333,2012.667,2013.083,2013,2013,2013.5,2013.5,2013.167,2013.5,2013.25,2013.417,2012.917,2013.167,2013.083,2013.25,2013.083,2013.417,2013.25,2013.5,2013.167,2012.833,2012.667,2012.917,2012.75,2013.5,2013.167,2012.667,2013.25,2013.333,2013.417,2013.5,2013.083,2012.917,2012.75,2012.75,2013.5,2012.667,2013.25,2013.5,2013.333,2013.25,2013.5,2013.167,2013.583,2013.25,2013,2012.667,2012.917,2013.417,2012.75,2013.5,2012.833,2012.917,2013.417,2013.417,2013.083,2013.417,2013.333,2013.083,2013.583,2013.083,2013.417,2013.083,2012.833,2013.083,2013.5,2013.083,2012.667,2013.167,2013.5,2013.5,2012.75,2012.75,2013.167,2013,2012.917,2012.917,2013.5,2013.167,2013.167,2013.417,2013.5,2013.333,2013,2013.25,2013.083,2013.417,2013.417,2013.417,2012.917,2012.667,2013,2013.083,2013.25,2013.083,2012.75,2012.833,2013.5,2013.083,2013.333,2013.083,2013.583,2013.333,2013.25,2012.917,2013.417,2012.75,2013.333,2013.333,2013.583,2013.25,2013.333,2013.25,2013,2012.917,2013.417,2013.583,2013.5,2012.833,2012.917,2013.333,2013.25,2012.75,2013.167,2013.167,2013.083,2013.5,2013.083,2013.5,2012.833,2013.417,2013.083,2013.417,2013.417,2013.333,2013,2012.833,2013.167,2012.917,2012.833,2012.667,2012.667,2013.417,2012.667,2013.25,2013.417,2013.083,2013.25,2013.167,2012.917,2013.417,2013.167,2012.833,2013.25,2012.833,2013.417,2013,2013.333,2012.917,2012.75,2013.417,2013.167,2012.667,2013,2013.417,2012.75,2013.417,2013.25,2013.333,2012.917,2013.417,2012.917,2013.167,2012.917,2013,2013.583,2013.333,2013.083,2012.833,2013.083,2012.667,2013.5,2013.167,2012.75,2012.833,2013.333,2013.167,2013.083,2012.75,2013.5,2013.5,2013.417,2013.083,2013.5,2012.833,2013.417,2013.25,2013.583,2013.167,2013.583,2013.333,2013.25,2013.083,2013.25,2012.75,2013.333,2013.25,2012.75,2012.917,2013,2013.417,2012.667,2013.083,2013.5,2013.417,2012.833,2013,2013.083,2013.333,2013.167,2012.75,2012.917,2013.583,2012.833,2012.833,2012.917,2013.333,2013.333,2013,2012.667,2013,2013.5,2012.667,2013.417,2013.583,2012.833,2012.75,2013,2012.833,2012.833,2013.5,2013.417,2013.25,2012.833,2013.417,2013.167,2013.5,2012.667,2013.083,2013.417,2013.5,2013.417,2012.917,2012.75,2012.833,2013.417,2012.667,2012.75,2013.5,2013,2013.083,2013.25,2013.25,2013.417,2013.333,2013.333,2013.333,2013.333,2013.417,2013,2012.667,2012.75,2013,2012.833,2013.25,2013.5,2013.25,2013.5,2013.583,2013.083,2013,2013.5,2012.917,2012.667,2013.417,2013.417,2012.917,2013.25,2013.083,2012.833,2012.667,2013.333,2012.667,2013.167,2013,2013.417,2013,2012.667,2013.25,2013,2013.5],[32,19.5,13.3,13.3,5,7.1,34.5,20.3,31.7,17.9,34.8,6.3,13,20.4,13.2,35.7,0,17.7,16.9,1.5,4.5,10.5,14.7,10.1,39.6,29.3,3.1,10.4,19.2,7.1,25.9,29.6,37.9,16.5,15.4,13.9,14.7,12,3.1,16.2,13.6,16.8,36.1,34.4,2.7,36.6,21.7,35.9,24.2,29.4,21.7,31.3,32.1,13.3,16.1,31.7,33.6,3.5,30.3,13.3,11,5.3,17.2,2.6,17.5,40.1,1,8.5,30.4,12.5,6.6,35.5,32.5,13.8,6.8,12.3,35.9,20.5,38.2,18,11.8,30.8,13.2,25.3,15.1,0,1.8,16.9,8.9,23,0,9.1,20.6,31.9,40.9,8,6.4,28.4,16.4,6.4,17.5,12.7,1.1,0,32.7,0,17.2,12.2,31.4,4,8.1,33.3,9.9,14.8,30.6,20.6,30.9,13.6,25.3,16.6,13.3,13.6,31.5,0,9.9,1.1,38.6,3.8,41.3,38.5,29.6,4,26.6,18,33.4,18.9,11.4,13.6,10,12.9,16.2,5.1,19.8,13.6,11.9,2.1,0,3.2,16.4,34.9,35.8,4.9,12,6.5,16.9,13.8,30.7,16.1,11.6,15.5,3.5,19.2,16,8.5,0,13.7,0,28.2,27.6,8.4,24,3.6,6.6,41.3,4.3,30.2,13.9,33,13.1,14,26.9,11.6,13.5,17,14.1,31.4,20.9,8.9,34.8,16.3,35.3,13.2,43.8,9.7,15.2,15.2,22.8,34.4,34,18.2,17.4,13.1,38.3,15.6,18,12.8,22.2,38.5,11.5,34.8,5.2,0,17.6,6.2,18.1,19.2,37.8,28,13.6,29.3,37.2,9,30.6,9.1,34.5,1.1,16.5,32.4,11.9,31,4,16.2,27.1,39.7,8,12.9,3.6,13,12.8,18.1,11,13.7,2,32.8,4.8,7.5,16.4,21.7,19,18,39.2,31.7,5.9,30.4,1.1,31.5,14.6,17.3,0,17.7,17,16.2,15.9,3.9,32.6,15.7,17.8,34.7,17.2,17.6,10.8,17.7,13,13.2,27.5,1.5,19.1,21.2,0,2.6,2.3,4.7,2,33.5,15,30.1,5.9,19.2,16.6,13.9,37.7,3.4,17.5,12.6,26.4,18.2,12.5,34.9,16.7,33.2,2.5,38,16.5,38.3,20,16.2,14.4,10.3,16.4,30.3,16.4,21.3,35.4,8.3,3.7,15.6,13.3,15.6,7.1,34.6,13.5,16.9,12.9,28.6,12.4,36.6,4.1,3.5,15.9,13.6,32,25.6,39.8,7.8,30,27.3,5.1,31.3,31.5,1.7,33.6,13,5.7,33.5,34.6,0,13.2,17.4,4.6,7.8,13.2,4,18.4,4.1,12.2,3.8,10.3,0,1.1,5.6,32.9,41.4,17.1,32.3,35.3,17.3,14.2,15,18.2,20.2,15.9,4.1,33.9,0,5.4,21.7,14.7,3.9,37.3,0,14.1,8,16.3,29.1,16.1,18.3,0,16.2,10.4,40.9,32.8,6.2,42.7,16.9,32.6,21.2,37.1,13.1,14.7,12.7,26.8,7.6,12.7,30.9,16.4,23,1.9,5.2,18.5,13.7,5.6,18.8,8.1,6.5],[84.87882,306.5947,561.9845,561.9845,390.5684,2175.03,623.4731,287.6025,5512.038,1783.18,405.2134,90.45606,492.2313,2469.645,1164.838,579.2083,292.9978,350.8515,368.1363,23.38284,2275.877,279.1726,1360.139,279.1726,480.6977,1487.868,383.8624,276.449,557.478,451.2438,4519.69,769.4034,488.5727,323.655,205.367,4079.418,1935.009,1360.139,577.9615,289.3248,4082.015,4066.587,519.4617,512.7871,533.4762,488.8193,463.9623,640.7391,4605.749,4510.359,512.5487,1758.406,1438.579,492.2313,289.3248,1160.632,371.2495,56.47425,4510.359,336.0532,1931.207,259.6607,2175.877,533.4762,995.7554,123.7429,193.5845,104.8101,464.223,561.9845,90.45606,640.7391,424.5442,4082.015,379.5575,1360.139,616.4004,2185.128,552.4371,1414.837,533.4762,377.7956,150.9347,2707.392,383.2805,338.9679,1455.798,4066.587,1406.43,3947.945,274.0144,1402.016,2469.645,1146.329,167.5989,104.8101,90.45606,617.4424,289.3248,90.45606,964.7496,170.1289,193.5845,208.3905,392.4459,292.9978,189.5181,1360.139,592.5006,2147.376,104.8101,196.6172,2102.427,393.2606,143.8383,737.9161,6396.283,4197.349,1583.722,289.3248,492.2313,492.2313,414.9476,185.4296,279.1726,193.5845,804.6897,383.8624,124.9912,216.8329,535.527,2147.376,482.7581,373.3937,186.9686,1009.235,390.5684,319.0708,942.4664,492.2313,289.3248,1559.827,640.6071,492.2313,1360.139,451.2438,185.4296,489.8821,3780.59,179.4538,170.7311,387.7721,1360.139,376.1709,4066.587,4082.015,1264.73,815.9314,390.5684,815.9314,49.66105,616.4004,4066.587,104.8101,185.4296,1236.564,292.9978,330.0854,515.1122,1962.628,4527.687,383.8624,90.45606,401.8807,432.0385,472.1745,4573.779,181.0766,1144.436,438.8513,4449.27,201.8939,2147.376,4082.015,2615.465,1447.286,2185.128,3078.176,190.0392,4066.587,616.5735,750.0704,57.58945,421.479,3771.895,461.1016,707.9067,126.7286,157.6052,451.6419,995.7554,561.9845,642.6985,289.3248,1414.837,1449.722,379.5575,665.0636,1360.139,175.6294,390.5684,274.0144,1805.665,90.45606,1783.18,383.7129,590.9292,372.6242,492.2313,529.7771,186.5101,1402.016,431.1114,1402.016,324.9419,193.5845,4082.015,265.0609,3171.329,1156.412,2147.376,4074.736,4412.765,333.3679,2216.612,250.631,373.8389,732.8528,732.8528,837.7233,1712.632,250.631,2077.39,204.1705,1559.827,639.6198,389.8219,1055.067,1009.235,6306.153,424.7132,1159.454,90.45606,1735.595,329.9747,5512.038,339.2289,444.1334,292.9978,837.7233,1485.097,2288.011,289.3248,2147.376,493.657,815.9314,1783.18,482.7581,390.5684,837.7233,252.5822,451.6419,492.2313,170.1289,394.0173,23.38284,461.1016,2185.128,208.3905,1554.25,184.3302,387.7721,1455.798,1978.671,383.2805,718.2937,90.45606,461.1016,323.6912,289.3248,490.3446,56.47425,395.6747,383.2805,335.5273,2179.59,1144.436,567.0349,4082.015,121.7262,156.2442,461.7848,2288.011,439.7105,1626.083,289.3248,169.9803,3079.89,289.3248,1264.73,1643.499,537.7971,318.5292,104.8101,577.9615,1756.411,250.631,752.7669,379.5575,272.6783,4197.349,964.7496,187.4823,197.1338,1712.632,488.8193,56.47425,757.3377,1497.713,4197.349,1156.777,4519.69,617.7134,104.8101,1013.341,337.6016,1867.233,600.8604,258.186,329.9747,270.8895,750.0704,90.45606,563.2854,3085.17,185.4296,1712.632,6488.021,259.6607,104.8101,492.2313,2180.245,2674.961,2147.376,1360.139,383.8624,211.4473,338.9679,193.5845,2408.993,87.30222,281.205,967.4,109.9455,614.1394,2261.432,1801.544,1828.319,350.8515,2185.128,289.3248,312.8963,157.6052,274.0144,390.5684,1157.988,1717.193,49.66105,587.8877,292.9978,289.3248,132.5469,3529.564,506.1144,4066.587,82.88643,185.4296,2103.555,2251.938,122.3619,377.8302,1939.749,443.802,967.4,4136.271,512.5487,918.6357,1164.838,1717.193,170.1289,482.7581,2175.03,187.4823,161.942,289.3248,130.9945,372.1386,2408.993,2175.744,4082.015,90.45606,390.9696,104.8101,90.45606],[10,9,5,5,5,3,7,6,1,3,1,9,5,4,4,2,6,1,8,7,3,7,1,7,4,2,5,5,4,5,0,7,1,6,7,0,2,1,6,5,0,0,5,6,4,8,9,3,0,1,4,1,3,5,5,0,8,7,1,5,2,6,3,4,0,8,6,5,6,5,9,3,8,0,10,1,3,3,2,1,4,6,7,3,7,9,1,0,0,0,1,0,4,0,5,5,9,3,5,9,4,1,6,6,6,6,8,1,2,3,5,7,3,6,8,2,1,0,3,5,5,5,4,0,7,6,4,5,6,7,8,3,5,8,6,0,5,6,0,5,5,3,5,5,1,5,0,8,0,8,7,9,1,6,0,0,0,4,5,4,8,3,0,5,0,1,6,8,5,1,0,5,9,4,7,3,0,9,4,1,0,8,3,0,0,3,3,0,8,0,8,2,7,5,0,5,2,8,7,8,0,5,3,5,1,3,10,3,1,8,5,1,2,9,3,8,1,6,5,8,9,0,10,0,6,6,0,8,0,0,3,0,1,9,4,7,10,0,0,0,2,7,3,8,3,5,6,0,0,1,7,0,9,2,5,1,1,1,6,0,4,3,5,3,7,4,3,5,5,0,1,8,5,1,7,7,5,3,6,3,6,9,1,2,7,3,9,5,6,5,0,7,5,7,6,3,4,4,0,10,4,0,3,0,3,5,1,0,5,0,2,4,9,5,6,2,7,2,10,5,0,4,1,6,2,8,7,3,3,0,0,0,2,5,5,6,2,5,9,5,0,2,9,8,0,0,2,1,6,5,5,3,3,3,1,5,1,9,6,0,10,8,4,10,7,4,1,2,1,3,5,5,7,1,5,0,2,8,8,6,5,9,0,4,0,10,0,3,4,8,9,1,6,4,1,4,1,4,2,1,5,3,1,9,5,6,7,0,3,0,9,7,5,9],[24.98298,24.98034,24.98746,24.98746,24.97937,24.96305,24.97933,24.98042,24.95095,24.96731,24.97349,24.97433,24.96515,24.96108,24.99156,24.9824,24.97744,24.97544,24.9675,24.96772,24.96314,24.97528,24.95204,24.97528,24.97353,24.97542,24.98085,24.95593,24.97419,24.97563,24.94826,24.98281,24.97349,24.97841,24.98419,25.01459,24.96386,24.95204,24.97201,24.98203,24.94155,24.94297,24.96305,24.98748,24.97445,24.97015,24.9703,24.97563,24.94684,24.94925,24.974,24.95402,24.97419,24.96515,24.98203,24.94968,24.97254,24.95744,24.94925,24.95776,24.96365,24.97585,24.96303,24.97445,24.96305,24.97635,24.96571,24.96674,24.97964,24.98746,24.97433,24.97563,24.97587,24.94155,24.98343,24.95204,24.97723,24.96322,24.97598,24.95182,24.97445,24.96427,24.96725,24.96056,24.96735,24.96853,24.9512,24.94297,24.98573,24.94783,24.9748,24.98569,24.96108,24.9492,24.9663,24.96674,24.97433,24.97746,24.98203,24.97433,24.98872,24.97371,24.96571,24.95618,24.96398,24.97744,24.97707,24.95204,24.9726,24.96299,24.96674,24.97701,24.96044,24.96172,24.98155,24.98092,24.94375,24.93885,24.96622,24.98203,24.96515,24.96515,24.98199,24.9711,24.97528,24.96571,24.97838,24.98085,24.96674,24.98086,24.98092,24.96299,24.97433,24.9866,24.96604,24.96357,24.97937,24.96495,24.97843,24.96515,24.98203,24.97213,24.97017,24.96515,24.95204,24.97563,24.9711,24.97017,24.93293,24.97349,24.96719,24.98118,24.95204,24.95418,24.94297,24.94155,24.94883,24.97886,24.97937,24.97886,24.95836,24.97723,24.94297,24.96674,24.9711,24.97694,24.97744,24.97408,24.96299,24.95468,24.94741,24.98085,24.97433,24.98326,24.9805,24.97005,24.94867,24.97697,24.99176,24.97493,24.94898,24.98489,24.96299,24.94155,24.95495,24.97285,24.96322,24.95464,24.97707,24.94297,24.97945,24.97371,24.9675,24.98246,24.93363,24.95425,24.981,24.96881,24.96628,24.96945,24.96305,24.98746,24.97559,24.98203,24.95182,24.97289,24.98343,24.97503,24.95204,24.97347,24.97937,24.9748,24.98672,24.97433,24.96731,24.972,24.97153,24.97838,24.96515,24.98102,24.97703,24.98569,24.98123,24.98569,24.97814,24.96571,24.94155,24.98059,25.00115,24.9489,24.96299,24.94235,24.95032,24.98016,24.96007,24.96606,24.98322,24.97668,24.97668,24.96334,24.96412,24.96606,24.96357,24.98236,24.97213,24.97258,24.96412,24.96211,24.96357,24.95743,24.97429,24.9496,24.97433,24.96464,24.98254,24.95095,24.97519,24.97501,24.97744,24.96334,24.97073,24.95885,24.98203,24.96299,24.96968,24.97886,24.96731,24.97433,24.97937,24.96334,24.9746,24.96945,24.96515,24.97371,24.97305,24.96772,24.95425,24.96322,24.95618,24.97026,24.96581,24.98118,24.9512,24.98674,24.96735,24.97509,24.97433,24.95425,24.97841,24.98203,24.97217,24.95744,24.95674,24.96735,24.9796,24.96299,24.99176,24.97003,24.94155,24.98178,24.96696,24.97229,24.95885,24.97161,24.96622,24.98203,24.97369,24.9546,24.98203,24.94883,24.95394,24.97425,24.97071,24.96674,24.97201,24.9832,24.96606,24.97795,24.98343,24.95562,24.93885,24.98872,24.97388,24.97631,24.96412,24.97015,24.95744,24.97538,24.97003,24.93885,24.94935,24.94826,24.97577,24.96674,24.99006,24.96431,24.98407,24.96871,24.96867,24.98254,24.97281,24.97371,24.97433,24.98223,24.998,24.9711,24.96412,24.95719,24.97585,24.96674,24.96515,24.96324,24.96143,24.96299,24.95204,24.98085,24.97417,24.96853,24.96571,24.95505,24.983,24.97345,24.98872,24.98182,24.97913,24.96182,24.95153,24.96464,24.97544,24.96322,24.98203,24.95591,24.96628,24.9748,24.97937,24.96165,24.96447,24.95836,24.97077,24.97744,24.98203,24.98298,24.93207,24.97845,24.94297,24.983,24.9711,24.96042,24.95957,24.96756,24.97151,24.95155,24.97927,24.98872,24.95544,24.974,24.97198,24.99156,24.96447,24.97371,24.97433,24.96305,24.97388,24.98353,24.98203,24.95663,24.97293,24.95505,24.9633,24.94155,24.97433,24.97923,24.96674,24.97433],[121.54024,121.53951,121.54391,121.54391,121.54245,121.51254,121.53642,121.54228,121.48458,121.51486,121.53372,121.5431,121.53737,121.51046,121.53406,121.54619,121.54458,121.53119,121.54451,121.54102,121.51151,121.54541,121.54842,121.54541,121.53885,121.51726,121.54391,121.53913,121.53797,121.54694,121.49587,121.53408,121.53451,121.54281,121.54243,121.51816,121.51458,121.54842,121.54722,121.54348,121.50381,121.50342,121.53758,121.54301,121.54765,121.54494,121.54458,121.53715,121.49578,121.49542,121.53842,121.55282,121.5175,121.53737,121.54348,121.53009,121.54059,121.53711,121.49542,121.53438,121.51471,121.54516,121.51254,121.54765,121.54915,121.54329,121.54089,121.54067,121.53805,121.54391,121.5431,121.53715,121.53913,121.50381,121.53762,121.54842,121.53767,121.51237,121.53381,121.54887,121.54765,121.53964,121.54252,121.50831,121.54464,121.54413,121.549,121.50342,121.52758,121.50243,121.53059,121.5276,121.51046,121.53076,121.54026,121.54067,121.5431,121.53299,121.54348,121.5431,121.53411,121.52984,121.54089,121.53844,121.5425,121.54458,121.54308,121.54842,121.53561,121.51284,121.54067,121.54224,121.51462,121.53812,121.54142,121.54739,121.47883,121.50383,121.51709,121.54348,121.53737,121.53737,121.54464,121.5317,121.54541,121.54089,121.53477,121.54391,121.54039,121.54162,121.53653,121.51284,121.53863,121.54082,121.54211,121.54951,121.54245,121.54277,121.52406,121.53737,121.54348,121.51627,121.54647,121.53737,121.54842,121.54694,121.5317,121.54494,121.51203,121.54245,121.54269,121.53788,121.54842,121.53713,121.50342,121.50381,121.52954,121.53464,121.54245,121.53464,121.53756,121.53767,121.50342,121.54067,121.5317,121.55391,121.54458,121.54011,121.5432,121.55481,121.49628,121.54391,121.5431,121.5446,121.53778,121.53758,121.49507,121.54262,121.53456,121.5273,121.49621,121.54121,121.51284,121.50381,121.56174,121.5173,121.51237,121.56627,121.54312,121.50342,121.53642,121.54951,121.54069,121.54477,121.51158,121.5399,121.54713,121.54089,121.54196,121.5449,121.54915,121.54391,121.53713,121.54348,121.54887,121.51728,121.53762,121.53692,121.54842,121.54271,121.54245,121.53059,121.52091,121.5431,121.51486,121.54477,121.53559,121.54119,121.53737,121.53655,121.54265,121.5276,121.53743,121.5276,121.5417,121.54089,121.50381,121.53986,121.51776,121.53095,121.51284,121.50357,121.49587,121.53932,121.51361,121.54297,121.53765,121.52518,121.52518,121.54767,121.5167,121.54297,121.51329,121.53923,121.51627,121.54814,121.54273,121.54928,121.54951,121.47516,121.53917,121.53018,121.5431,121.51623,121.54395,121.48458,121.53151,121.5273,121.54458,121.54767,121.517,121.51359,121.54348,121.51284,121.54522,121.53464,121.51486,121.53863,121.54245,121.54767,121.53046,121.5449,121.53737,121.52984,121.53994,121.54102,121.5399,121.51237,121.53844,121.51642,121.54086,121.53788,121.549,121.51844,121.54464,121.53644,121.5431,121.5399,121.5428,121.54348,121.53471,121.53711,121.534,121.54464,121.5414,121.51252,121.53456,121.5458,121.50381,121.54059,121.53992,121.53445,121.51359,121.53423,121.51668,121.54348,121.52979,121.56627,121.54348,121.52954,121.55174,121.53814,121.54069,121.54067,121.54722,121.51812,121.54297,121.53451,121.53762,121.53872,121.50383,121.53411,121.52981,121.54436,121.5167,121.54494,121.53711,121.54971,121.51696,121.50383,121.53046,121.49587,121.53475,121.54067,121.5346,121.54063,121.51748,121.54651,121.54331,121.54395,121.53265,121.54951,121.5431,121.53597,121.5155,121.5317,121.5167,121.47353,121.54516,121.54067,121.53737,121.51241,121.50827,121.51284,121.54842,121.54391,121.52999,121.54413,121.54089,121.55964,121.54022,121.54093,121.53408,121.54086,121.53666,121.51222,121.55254,121.51531,121.53119,121.51237,121.54348,121.53956,121.54196,121.53059,121.54245,121.55011,121.51649,121.53756,121.54634,121.54458,121.54348,121.53981,121.51597,121.53889,121.50342,121.54026,121.5317,121.51462,121.51353,121.5423,121.5435,121.55387,121.53874,121.53408,121.4963,121.53842,121.55063,121.53406,121.51649,121.52984,121.53863,121.51254,121.52981,121.53966,121.54348,121.53765,121.54026,121.55964,121.51243,121.50381,121.5431,121.53986,121.54067,121.5431],[37.9,42.2,47.3,54.8,43.1,32.1,40.3,46.7,18.8,22.1,41.4,58.1,39.3,23.8,34.3,50.5,70.1,37.4,42.3,47.7,29.3,51.6,24.6,47.9,38.8,27,56.2,33.6,47,57.1,22.1,25,34.2,49.3,55.1,27.3,22.9,25.3,47.7,46.2,15.9,18.2,34.7,34.1,53.9,38.3,42,61.5,13.4,13.2,44.2,20.7,27,38.9,51.7,13.7,41.9,53.5,22.6,42.4,21.3,63.2,27.7,55,25.3,44.3,50.7,56.8,36.2,42,59,40.8,36.3,20,54.4,29.5,36.8,25.6,29.8,26.5,40.3,36.8,48.1,17.7,43.7,50.8,27,18.3,48,25.3,45.4,43.2,21.8,16.1,41,51.8,59.5,34.6,51,62.2,38.2,32.9,54.4,45.7,30.5,71,47.1,26.6,34.1,28.4,51.6,39.4,23.1,7.6,53.3,46.4,12.2,13,30.6,59.6,31.3,48,32.5,45.5,57.4,48.6,62.9,55,60.7,41,37.5,30.7,37.5,39.5,42.2,20.8,46.8,47.4,43.5,42.5,51.4,28.9,37.5,40.1,28.4,45.5,52.2,43.2,45.1,39.7,48.5,44.7,28.9,40.9,20.7,15.6,18.3,35.6,39.4,37.4,57.8,39.6,11.6,55.5,55.2,30.6,73.6,43.4,37.4,23.5,14.4,58.8,58.1,35.1,45.2,36.5,19.2,42,36.7,42.6,15.5,55.9,23.6,18.8,21.8,21.5,25.7,22,44.3,20.5,42.3,37.8,42.7,49.3,29.3,34.6,36.6,48.2,39.1,31.6,25.5,45.9,31.5,46.1,26.6,21.4,44,34.2,26.2,40.9,52.2,43.5,31.1,58,20.9,48.1,39.7,40.8,43.8,40.2,78.3,38.5,48.5,42.3,46,49,12.8,40.2,46.6,19,33.4,14.7,17.4,32.4,23.9,39.3,61.9,39,40.6,29.7,28.8,41.4,33.4,48.2,21.7,40.8,40.6,23.1,22.3,15,30,13.8,52.7,25.9,51.8,17.4,26.5,43.9,63.3,28.8,30.7,24.4,53,31.7,40.6,38.1,23.7,41.1,40.1,23,117.5,26.5,40.5,29.3,41,49.7,34,27.7,44,31.1,45.4,44.8,25.6,23.5,34.4,55.3,56.3,32.9,51,44.5,37,54.4,24.5,42.5,38.1,21.8,34.1,28.5,16.7,46.1,36.9,35.7,23.2,38.4,29.4,55,50.2,24.7,53,19.1,24.7,42.2,78,42.8,41.6,27.3,42,37.5,49.8,26.9,18.6,37.7,33.1,42.5,31.3,38.1,62.1,36.7,23.6,19.2,12.8,15.6,39.6,38.4,22.8,36.5,35.6,30.9,36.3,50.4,42.9,37,53.5,46.6,41.2,37.9,30.8,11.2,53.7,47,42.3,28.6,25.7,31.3,30.1,60.7,45.3,44.9,45.1,24.7,47.1,63.3,40,48,33.1,29.5,24.8,20.9,43.1,22.8,42.1,51.7,41.5,52.2,49.5,23.8,30.5,56.8,37.4,69.7,53.3,47.3,29.3,40.3,12.9,46.6,55.3,25.6,27.3,67.7,38.6,31.3,35.3,40.3,24.7,42.5,31.9,32.2,23,37.3,35.5,27.7,28.5,39.7,41.2,37.2,40.5,22.3,28.1,15.4,50,40.6,52.5,63.9]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>date<\/th>\n      <th>age<\/th>\n      <th>distance<\/th>\n      <th>stores<\/th>\n      <th>latitude<\/th>\n      <th>longitude<\/th>\n      <th>price<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":8,"columnDefs":[{"className":"dt-right","targets":[0,1,2,3,4,5,6]}],"order":[],"autoWidth":false,"orderClasses":false,"orderCellsTop":true,"lengthMenu":[8,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
    
    dim(realestate)
## [1] 414   7
</div>
<div id="notation-and-basic-properties" class="section level2 hasAnchor" number="5.2">
<h2 class="hasAnchor"><span class="header-section-number">5.2</span> Notation and Basic Properties<a href="#notation-and-basic-properties" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We usually denote the observed covariates data as the design matrix <span class="math inline">\(\mathbf{X}\)</span>, with dimension <span class="math inline">\(n \times p\)</span>. Hence in this case, the dimension of <span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\(414 \times 7\)</span>. The <span class="math inline">\(j\)</span>th variable is simply the <span class="math inline">\(j\)</span>th column of this matrix, which is denoted as <span class="math inline">\(\mathbf{x}_j\)</span>. The outcome <span class="math inline">\(\mathbf{y}\)</span> (<code>price</code>) is a vector of length <span class="math inline">\(414\)</span>. Please note that we usually use a “bold” symbol to represent a vector, while for a single element (scalar), such as the <span class="math inline">\(j\)</span>th variable of subject <span class="math inline">\(i\)</span>, we use <span class="math inline">\(x_{ij}\)</span>.</p>
<p>A linear regression concerns modeling the relationship (in matrix form)</p>
<p><span class="math display">\[\mathbf{y}_{n \times 1} = \mathbf{X}_{n \times p} \boldsymbol \beta_{p \times 1} + \boldsymbol \epsilon_{n \times 1}\]</span>
And we know that the solution is obtained by minimizing the residual sum of squares (RSS):</p>
<p><span class="math display">\[
\begin{align}
\widehat{\boldsymbol \beta} &amp;= \underset{\boldsymbol \beta}{\mathop{\mathrm{arg\,min}}} \sum_{i=1}^n \left(y_i - x_i^\text{T}\boldsymbol \beta\right)^2 \\
&amp;= \underset{\boldsymbol \beta}{\mathop{\mathrm{arg\,min}}} \big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)^\text{T}\big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)
\end{align}
\]</span>
Classic solution can be obtained by taking the derivative of RSS w.r.t <span class="math inline">\(\boldsymbol \beta\)</span> and set it to zero. This leads to the well known normal equation:</p>
<p><span class="math display">\[
\begin{align}
    \frac{\partial \text{RSS}}{\partial \boldsymbol \beta} &amp;= -2 \mathbf{X}^\text{T}(\mathbf{y}- \mathbf{X}\boldsymbol \beta) \doteq 0 \\
    \Longrightarrow \quad \mathbf{X}^\text{T}\mathbf{y}&amp;= \mathbf{X}^\text{T}\mathbf{X}\boldsymbol \beta
\end{align}
\]</span>
Assuming that <span class="math inline">\(\mathbf{X}\)</span> is full rank, then <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span> is invertible. Then, we have</p>
<p><span class="math display">\[
\widehat{\boldsymbol \beta} = (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}\mathbf{y}
\]</span>
Some additional concepts are frequently used. The fitted values <span class="math inline">\(\widehat{\mathbf{y}}\)</span> are essentially the prediction of the original <span class="math inline">\(n\)</span> training data points:</p>
<p><span class="math display">\[
\begin{align}
\widehat{\mathbf{y}} =&amp; \mathbf{X}\boldsymbol \beta\\
=&amp; \underbrace{\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}}_{\mathbf{H}} \mathbf{y}\\
\doteq&amp; \mathbf{H}_{n \times n} \mathbf{y}
\end{align}
\]</span>
where <span class="math inline">\(\mathbf{H}\)</span> is called the “hat” matrix. It is a projection matrix that projects any vector (<span class="math inline">\(\mathbf{y}\)</span> in our case) onto the column space of <span class="math inline">\(\mathbf{X}\)</span>. A project matrix enjoys two properties</p>
<ul>
<li>Symmetric: <span class="math inline">\(\mathbf{H}^\text{T}= \mathbf{H}\)</span></li>
<li>Idempotent <span class="math inline">\(\mathbf{H}\mathbf{H}= \mathbf{H}\)</span></li>
</ul>
<p>The residuals <span class="math inline">\(\mathbf{r}\)</span> can also be obtained using the hat matrix:</p>
<p><span class="math display">\[ \mathbf{r}= \mathbf{y}- \widehat{\mathbf{y}} = (\mathbf{I}- \mathbf{H}) \mathbf{y}\]</span>
From the properties of a projection matrix, we also know that <span class="math inline">\(\mathbf{r}\)</span> should be orthogonal to any vector from the column space of <span class="math inline">\(\mathbf{X}\)</span>. Hence,</p>
<p><span class="math display">\[\mathbf{X}^\text{T}\mathbf{r}= \mathbf{0}_{p \times 1}\]</span></p>
<p>The residuals is also used to estimate the error variance:</p>
<p><span class="math display">\[\widehat\sigma^2 = \frac{1}{n-p} \sum_{i=1}^n r_i^2 = \frac{\text{RSS}}{n-p}\]</span>
When the data are indeed generated from a linear model, and with suitable conditions on the design matrix and random errors <span class="math inline">\(\boldsymbol \epsilon\)</span>, we can conclude that <span class="math inline">\(\widehat{\boldsymbol \beta}\)</span> is an <strong>unbiased</strong> estimator of <span class="math inline">\(\boldsymbol \beta\)</span>. Its variance-covariance matrix satisfies</p>
<p><span class="math display">\[
\begin{align}
    \text{Var}(\widehat{\boldsymbol \beta}) &amp;= \text{Var}\big( (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}\mathbf{y}\big) \nonumber \\
    &amp;= \text{Var}\big( (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}(\mathbf{X}\boldsymbol \beta+ \boldsymbol \epsilon) \big) \nonumber \\
    &amp;= \text{Var}\big( (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}\boldsymbol \epsilon) \big) \nonumber \\
    &amp;= (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{I}\sigma^2 \nonumber \\
    &amp;= (\mathbf{X}^\text{T}\mathbf{X})^{-1}\sigma^2
\end{align}
\]</span>
All of the above mentioned results are already implemented in R through the <code>lm()</code> function to fit a linear regression.</p>
</div>
<div id="using-the-lm-function" class="section level2 hasAnchor" number="5.3">
<h2 class="hasAnchor"><span class="header-section-number">5.3</span> Using the <code>lm()</code> Function<a href="#using-the-lm-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s consider a simple regression that uses <code>age</code> and <code>distance</code> to model <code>price</code>. We will save the fitted object as <code>lm.fit</code></p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>    lm.fit <span class="ot">=</span> <span class="fu">lm</span>(price <span class="sc">~</span> age <span class="sc">+</span> distance, <span class="at">data =</span> realestate)</span></code></pre></div>
<p>This syntax contains three components:</p>
<ul>
<li><code>data =</code> specifies the dataset</li>
<li>The outcome variable should be on the left hand side of <code>~</code></li>
<li>The covariates should be on the right hand side of <code>~</code></li>
</ul>
<p>To look at the detailed model fitting results, use the <code>summary()</code> function</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>    lm.summary <span class="ot">=</span> <span class="fu">summary</span>(lm.fit)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    lm.summary</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = price ~ age + distance, data = realestate)</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="do">## -36.032  -4.742  -1.037   4.533  71.930 </span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a><span class="do">##               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 49.8855858  0.9677644  51.547  &lt; 2e-16 ***</span></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a><span class="do">## age         -0.2310266  0.0420383  -5.496 6.84e-08 ***</span></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a><span class="do">## distance    -0.0072086  0.0003795 -18.997  &lt; 2e-16 ***</span></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 9.73 on 411 degrees of freedom</span></span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.4911, Adjusted R-squared:  0.4887 </span></span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 198.3 on 2 and 411 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>This shows that both <code>age</code> and <code>distance</code> are highly significant for predicting the price. In fact, this fitted object (<code>lm.fit</code>) and the summary object (<code>lm.summary</code>) are both saved as a list. This is pretty common to handle an output object with many things involved. We may peek into this object to see what are provided using a <code>$</code> after the object.</p>
<center>
<img src="images/reactive.png" style="width:40.0%" />
</center>
<p>The <code>str()</code> function can also display all the items in a list.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">str</span>(lm.summary)</span></code></pre></div>
<p>Usually, printing out the summary is sufficient. However, further details can be useful for other purposes. For example, if we interested in the residual vs. fits plot, we may use</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(lm.fit<span class="sc">$</span>fitted.values, lm.fit<span class="sc">$</span>residuals, </span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;Fitted Values&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-66-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>It seems that the error variance is not constant (as a function of the fitted values), hence additional techniques may be required to handle this issue. However, that is beyond the scope of this book.</p>
<div id="adding-covariates" class="section level3 hasAnchor" number="5.3.1">
<h3 class="hasAnchor"><span class="header-section-number">5.3.1</span> Adding Covariates<a href="#adding-covariates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It is pretty simple if we want to include additional variables. This is usually done by connecting them with the <code>+</code> sign on the right hand side of <code>~</code>. R also provide convenient ways to include interactions and higher order terms. The following code with the interaction term between <code>age</code> and <code>distance</code>, and a squared term of <code>distance</code> should be self-explanatory.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>    lm.fit2 <span class="ot">=</span> <span class="fu">lm</span>(price <span class="sc">~</span> age <span class="sc">+</span> distance <span class="sc">+</span> age<span class="sc">*</span>distance <span class="sc">+</span> <span class="fu">I</span>(distance<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> realestate)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summary</span>(lm.fit2)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = price ~ age + distance + age * distance + I(distance^2), </span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     data = realestate)</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="do">## -37.117  -4.160  -0.594   3.548  69.683 </span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a><span class="do">##                 Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)    5.454e+01  1.099e+00  49.612  &lt; 2e-16 ***</span></span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a><span class="do">## age           -2.615e-01  4.931e-02  -5.302 1.87e-07 ***</span></span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a><span class="do">## distance      -1.603e-02  1.133e-03 -14.152  &lt; 2e-16 ***</span></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a><span class="do">## I(distance^2)  1.907e-06  2.416e-07   7.892 2.75e-14 ***</span></span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a><span class="do">## age:distance   8.727e-06  4.615e-05   0.189     0.85    </span></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 8.939 on 409 degrees of freedom</span></span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.5726, Adjusted R-squared:  0.5684 </span></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic:   137 on 4 and 409 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>If you choose to include all covariates presented in the data, then simply use <code>.</code> on the right hand side of <code>~</code>. However, you should always be careful when doing this because some dataset would contain meaningless variables such as subject ID.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>    lm.fit3 <span class="ot">=</span> <span class="fu">lm</span>(price <span class="sc">~</span> ., <span class="at">data =</span> realestate)</span></code></pre></div>
</div>
<div id="categorical-variables" class="section level3 hasAnchor" number="5.3.2">
<h3 class="hasAnchor"><span class="header-section-number">5.3.2</span> Categorical Variables<a href="#categorical-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>store</code> variable has several different values. We can see that it has 11 different values. One strategy is to model this as a continuous variable. However, we may also consider to discretize it. For example, we may create a new variable, say <code>store.cat</code>, defined as follows</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">table</span>(realestate<span class="sc">$</span>stores)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="do">##  0  1  2  3  4  5  6  7  8  9 10 </span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="do">## 67 46 24 46 31 67 37 31 30 25 10</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># define a new factor variable</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>  realestate<span class="sc">$</span>store.cat <span class="ot">=</span> <span class="fu">as.factor</span>((realestate<span class="sc">$</span>stores <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="sc">+</span> (realestate<span class="sc">$</span>stores <span class="sc">&gt;</span> <span class="dv">4</span>))</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">table</span>(realestate<span class="sc">$</span>store.cat)</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="do">##   0   1   2 </span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="do">##  67 147 200</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">levels</span>(realestate<span class="sc">$</span>store.cat) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;None&quot;</span>, <span class="st">&quot;Several&quot;</span>, <span class="st">&quot;Many&quot;</span>)</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>(realestate<span class="sc">$</span>store.cat)</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] Many    Many    Many    Many    Many    Several</span></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Levels: None Several Many</span></span></code></pre></div>
<p>This variable is defined as a factor, which is often used for categorical variables. Since this variable has three different categories, if we include it in the linear regression, it will introduce two additional variables (using the third as the reference):</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>    lm.fit3 <span class="ot">=</span> <span class="fu">lm</span>(price <span class="sc">~</span> age <span class="sc">+</span> distance <span class="sc">+</span> store.cat, <span class="at">data =</span> realestate)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summary</span>(lm.fit3)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = price ~ age + distance + store.cat, data = realestate)</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="do">## -38.656  -5.360  -0.868   3.913  76.797 </span></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)      43.712887   1.751608  24.956  &lt; 2e-16 ***</span></span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a><span class="do">## age              -0.229882   0.040095  -5.733 1.91e-08 ***</span></span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a><span class="do">## distance         -0.005404   0.000470 -11.497  &lt; 2e-16 ***</span></span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a><span class="do">## store.catSeveral  0.838228   1.435773   0.584     0.56    </span></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a><span class="do">## store.catMany     8.070551   1.646731   4.901 1.38e-06 ***</span></span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 9.279 on 409 degrees of freedom</span></span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.5395, Adjusted R-squared:  0.535 </span></span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 119.8 on 4 and 409 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>There are usually two types of categorical variables:</p>
<ul>
<li>Ordinal: the numbers representing each category is ordered, e.g., how many stores in the neighborhood. Oftentimes nominal data can be treated as a continuous variable.</li>
<li>Nominal: they are not ordered and can be represented using either numbers or letters, e.g., ethnic group.</li>
</ul>
<p>The above example is treating <code>store.cat</code> as a nominal variable, and the <code>lm()</code> function is using dummy variables for each category. This should be our default approach to handle nominal variables.</p>
</div>
</div>
<div id="model-selection-criteria" class="section level2 hasAnchor" number="5.4">
<h2 class="hasAnchor"><span class="header-section-number">5.4</span> Model Selection Criteria<a href="#model-selection-criteria" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will use the <code>diabetes</code> dataset from the <code>lars</code> package as a demonstration of model selection. Ten baseline variables include age, sex, body mass index, average blood pressure, and six blood serum measurements. These measurements were obtained for each of n = 442 diabetes patients, as well as the outcome of interest, a quantitative measure of disease progression one year after baseline. More details are available in <span class="citation">Efron et al. (<a href="#ref-efron2004least" role="doc-biblioref">2004</a>)</span>. Our goal is to select a linear model, preferably with a small number of variables, that can predict the outcome. To select the best model, commonly used strategies include Marrow’s <span class="math inline">\(C_p\)</span>, AIC (Akaike information criterion) and BIC (Bayesian information criterion). Further derivations will be provide at a later section.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># load the diabetes data</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(lars)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Loaded lars 1.3</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data</span>(diabetes)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    diab <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="fu">cbind</span>(diabetes<span class="sc">$</span>x, <span class="st">&quot;Y&quot;</span> <span class="ot">=</span> diabetes<span class="sc">$</span>y))</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fit linear regression with all covariates</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    lm.fit <span class="ot">=</span> <span class="fu">lm</span>(Y<span class="sc">~</span>., <span class="at">data=</span>diab)</span></code></pre></div>
<p>The idea of model selection is to apply some penalty on the number of parameters used in the model. In general, they are usually in the form of</p>
<p><span class="math display">\[\text{Goodness-of-Fit} + \text{Complexity Penality}\]</span></p>
<div id="using-marrows-c_p" class="section level3 hasAnchor" number="5.4.1">
<h3 class="hasAnchor"><span class="header-section-number">5.4.1</span> Using Marrows’ <span class="math inline">\(C_p\)</span><a href="#using-marrows-c_p" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For example, the Marrows’ <span class="math inline">\(C_p\)</span> criterion minimize the following quantity (a derivation is provided at Section @ref(marrows-cp)):</p>
<p><span class="math display">\[\text{RSS} + 2 p \widehat\sigma_{\text{full}}^2\]</span>
Note that the <span class="math inline">\(\sigma_{\text{full}}^2\)</span> refers to the residual variance estimation based on the full model, i.e., will all variables. Hence, this formula cannot be used when <span class="math inline">\(p &gt; n\)</span> because you would not be able to obtain a valid estimation of <span class="math inline">\(\sigma_{\text{full}}^2\)</span>. Nonetheless, we can calculate this quantity with the diabetes dataset</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># number of variables (including intercept)</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    p <span class="ot">=</span> <span class="dv">11</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    n <span class="ot">=</span> <span class="fu">nrow</span>(diab)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># obtain residual sum of squares</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    RSS <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">residuals</span>(lm.fit)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use the formula directly to calculate the Cp criterion </span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>    Cp <span class="ot">=</span> RSS <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>p<span class="sc">*</span><span class="fu">summary</span>(lm.fit)<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    Cp</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1328502</span></span></code></pre></div>
<p>We can compare this with another sub-model, say, with just <code>age</code> and <code>glu</code>:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>    lm.fit_sub <span class="ot">=</span> <span class="fu">lm</span>(Y<span class="sc">~</span> age <span class="sc">+</span> glu, <span class="at">data=</span>diab)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># obtain residual sum of squares</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    RSS_sub <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">residuals</span>(lm.fit_sub)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use the formula directly to calculate the Cp criterion </span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    Cp_sub <span class="ot">=</span> RSS_sub <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="dv">3</span><span class="sc">*</span><span class="fu">summary</span>(lm.fit)<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    Cp_sub</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2240019</span></span></code></pre></div>
<p>Comparing this with the previous one, the full model is better.</p>
</div>
<div id="using-aic-and-bic" class="section level3 hasAnchor" number="5.4.2">
<h3 class="hasAnchor"><span class="header-section-number">5.4.2</span> Using AIC and BIC<a href="#using-aic-and-bic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Calculating the AIC and BIC criteria in <code>R</code> is a lot simpler, with the existing functions. The AIC score is given by</p>
<p><span class="math display">\[-2 \text{Log-likelihood} + 2 p,\]</span>
while the BIC score is given by</p>
<p><span class="math display">\[-2 \text{Log-likelihood} + \log(n) p,\]</span></p>
<p>Interestingly, when assuming that the error distribution is Gaussian, the log-likelihood part is just a function of the RSS. In general, AIC performs similarly to <span class="math inline">\(C_p\)</span>, while BIC tend to select a much smaller set due to the larger penalty. Theoretically, both AIC and <span class="math inline">\(C_p\)</span> are interested in the prediction error, regardless of whether the model is specified correctly, while BIC is interested in selecting the true set of variables, while assuming that the true model is being considered.</p>
<p>The AIC score can be done using the <code>AIC()</code> function. We can match this result by writing out the normal density function and plug in the estimated parameters. Note that this requires one additional parameter, which is the variance. Hence the total number of parameters is 12. We can calculate this with our own code:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ?AIC</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># a build-in function for calculating AIC using -2log likelihood</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">AIC</span>(lm.fit) </span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 4795.985</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Match the result</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    n<span class="sc">*</span><span class="fu">log</span>(RSS<span class="sc">/</span>n) <span class="sc">+</span> n <span class="sc">+</span> n<span class="sc">*</span><span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>p</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 4795.985</span></span></code></pre></div>
<p>Alternatively, the <code>extractAIC()</code> function can calculate both AIC and BIC. However, note that the <code>n + n*log(2*pi) + 2</code> part in the above code does not change regardless of how many parameters we use. Hence, this quantify does not affect the comparison between different models. Then we can safely remove this part and only focus on the essential ones.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ?extractAIC</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># AIC for the full model</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">extractAIC</span>(lm.fit)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1]   11.000 3539.643</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    n<span class="sc">*</span><span class="fu">log</span>(RSS<span class="sc">/</span>n) <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>p</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 3539.643</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># BIC for the full model</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">extractAIC</span>(lm.fit, <span class="at">k =</span> <span class="fu">log</span>(n))</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="do">## [1]   11.000 3584.648</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>    n<span class="sc">*</span><span class="fu">log</span>(RSS<span class="sc">/</span>n) <span class="sc">+</span> <span class="fu">log</span>(n)<span class="sc">*</span>p</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 3584.648</span></span></code></pre></div>
<p>Now, we can compare AIC or BIC using of two different models and select whichever one that gives a smaller value. For example the AIC of the previous sub-model is</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># AIC for the sub-model</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">extractAIC</span>(lm.fit_sub)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1]    3.000 3773.077</span></span></code></pre></div>
</div>
</div>
<div id="model-selection-algorithms" class="section level2 hasAnchor" number="5.5">
<h2 class="hasAnchor"><span class="header-section-number">5.5</span> Model Selection Algorithms<a href="#model-selection-algorithms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In previous examples, we have to manually fit two models and calculate their respective selection criteria and compare them. This is a rather tedious process if we have many variables and a huge number of combinations to consider. To automatically compare different models and select the best one, there are two common computational approaches: best subset regression and step-wise regression. As their name suggest, the best subset selection will exhaust all possible combination of variables, while the step-wise regression would adjust the model by adding or subtracting one variable at a time to reach the best model.</p>
<div id="best-subset-selection-with-leaps" class="section level3 hasAnchor" number="5.5.1">
<h3 class="hasAnchor"><span class="header-section-number">5.5.1</span> Best Subset Selection with <code>leaps</code><a href="#best-subset-selection-with-leaps" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since the penalty is only affected by the number of variables, we may first choose the best model with the smallest RSS for each model size, and then compare across these models by attaching the penalty terms of their corresponding sizes. The <code>leaps</code> package can be used to calculate the best model of each model size. It essentially performs an exhaustive search, however, still utilizing some tricks to skip some really bad models. Note that the <code>leaps</code> package uses the data matrix directly, instead of specifying a formula.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(leaps)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The package specifies the X matrix and outcome y vector</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    RSSleaps <span class="ot">=</span> <span class="fu">regsubsets</span>(<span class="at">x =</span> <span class="fu">as.matrix</span>(diab[, <span class="sc">-</span><span class="dv">11</span>]), <span class="at">y =</span> diab[, <span class="dv">11</span>])</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summary</span>(RSSleaps, <span class="at">matrix=</span>T)</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Subset selection object</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 10 Variables  (and intercept)</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="do">##     Forced in Forced out</span></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a><span class="do">## age     FALSE      FALSE</span></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="do">## sex     FALSE      FALSE</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a><span class="do">## bmi     FALSE      FALSE</span></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a><span class="do">## map     FALSE      FALSE</span></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a><span class="do">## tc      FALSE      FALSE</span></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a><span class="do">## ldl     FALSE      FALSE</span></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a><span class="do">## hdl     FALSE      FALSE</span></span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a><span class="do">## tch     FALSE      FALSE</span></span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a><span class="do">## ltg     FALSE      FALSE</span></span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a><span class="do">## glu     FALSE      FALSE</span></span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 subsets of each size up to 8</span></span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Selection Algorithm: exhaustive</span></span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a><span class="do">##          age sex bmi map tc  ldl hdl tch ltg glu</span></span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a><span class="do">## 1  ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;</span></span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a><span class="do">## 2  ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a><span class="do">## 3  ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a><span class="do">## 4  ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a><span class="do">## 5  ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb62-27"><a href="#cb62-27" aria-hidden="true" tabindex="-1"></a><span class="do">## 6  ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb62-28"><a href="#cb62-28" aria-hidden="true" tabindex="-1"></a><span class="do">## 7  ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb62-29"><a href="#cb62-29" aria-hidden="true" tabindex="-1"></a><span class="do">## 8  ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;</span></span></code></pre></div>
<p>The results is summarized in a matrix, with each row representing a model size. The <code>"*"</code> sign indicates that the variable is include in the model for the corresponding size. Hence, there should be only one of such in the first row, two in the second row, etc.</p>
<p>By default, the algorithm would only consider models up to size 8. This is controlled by the argument <code>nvmax</code>. If we want to consider larger model sizes, then set this to a larger number. However, be careful that this many drastically increase the computational cost.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Consider maximum of 10 variables</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    RSSleaps <span class="ot">=</span> <span class="fu">regsubsets</span>(<span class="at">x =</span> <span class="fu">as.matrix</span>(diab[, <span class="sc">-</span><span class="dv">11</span>]), <span class="at">y =</span> diab[, <span class="dv">11</span>], <span class="at">nvmax =</span> <span class="dv">10</span>)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summary</span>(RSSleaps,<span class="at">matrix=</span>T)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Subset selection object</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="do">## 10 Variables  (and intercept)</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     Forced in Forced out</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="do">## age     FALSE      FALSE</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a><span class="do">## sex     FALSE      FALSE</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="do">## bmi     FALSE      FALSE</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="do">## map     FALSE      FALSE</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a><span class="do">## tc      FALSE      FALSE</span></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a><span class="do">## ldl     FALSE      FALSE</span></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a><span class="do">## hdl     FALSE      FALSE</span></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a><span class="do">## tch     FALSE      FALSE</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a><span class="do">## ltg     FALSE      FALSE</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a><span class="do">## glu     FALSE      FALSE</span></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 subsets of each size up to 10</span></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Selection Algorithm: exhaustive</span></span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a><span class="do">##           age sex bmi map tc  ldl hdl tch ltg glu</span></span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a><span class="do">## 1  ( 1 )  &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;</span></span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a><span class="do">## 2  ( 1 )  &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a><span class="do">## 3  ( 1 )  &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a><span class="do">## 4  ( 1 )  &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a><span class="do">## 5  ( 1 )  &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a><span class="do">## 6  ( 1 )  &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a><span class="do">## 7  ( 1 )  &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a><span class="do">## 8  ( 1 )  &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;</span></span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a><span class="do">## 9  ( 1 )  &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;</span></span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a><span class="do">## 10  ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;</span></span>
<span id="cb63-30"><a href="#cb63-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-31"><a href="#cb63-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Obtain the matrix that indicates the variables</span></span>
<span id="cb63-32"><a href="#cb63-32" aria-hidden="true" tabindex="-1"></a>    sumleaps <span class="ot">=</span> <span class="fu">summary</span>(RSSleaps, <span class="at">matrix =</span> T)</span>
<span id="cb63-33"><a href="#cb63-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-34"><a href="#cb63-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This object includes the RSS results, which is needed to calculate the scores</span></span>
<span id="cb63-35"><a href="#cb63-35" aria-hidden="true" tabindex="-1"></a>    sumleaps<span class="sc">$</span>rss</span>
<span id="cb63-36"><a href="#cb63-36" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] 1719582 1416694 1362708 1331430 1287879 1271491 1267805 1264712 1264066 1263983</span></span>
<span id="cb63-37"><a href="#cb63-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-38"><a href="#cb63-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This matrix indicates whether a variable is in the best model(s)</span></span>
<span id="cb63-39"><a href="#cb63-39" aria-hidden="true" tabindex="-1"></a>    sumleaps<span class="sc">$</span>which</span>
<span id="cb63-40"><a href="#cb63-40" aria-hidden="true" tabindex="-1"></a><span class="do">##    (Intercept)   age   sex  bmi   map    tc   ldl   hdl   tch   ltg   glu</span></span>
<span id="cb63-41"><a href="#cb63-41" aria-hidden="true" tabindex="-1"></a><span class="do">## 1         TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span id="cb63-42"><a href="#cb63-42" aria-hidden="true" tabindex="-1"></a><span class="do">## 2         TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE</span></span>
<span id="cb63-43"><a href="#cb63-43" aria-hidden="true" tabindex="-1"></a><span class="do">## 3         TRUE FALSE FALSE TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE</span></span>
<span id="cb63-44"><a href="#cb63-44" aria-hidden="true" tabindex="-1"></a><span class="do">## 4         TRUE FALSE FALSE TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE</span></span>
<span id="cb63-45"><a href="#cb63-45" aria-hidden="true" tabindex="-1"></a><span class="do">## 5         TRUE FALSE  TRUE TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE</span></span>
<span id="cb63-46"><a href="#cb63-46" aria-hidden="true" tabindex="-1"></a><span class="do">## 6         TRUE FALSE  TRUE TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE</span></span>
<span id="cb63-47"><a href="#cb63-47" aria-hidden="true" tabindex="-1"></a><span class="do">## 7         TRUE FALSE  TRUE TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE</span></span>
<span id="cb63-48"><a href="#cb63-48" aria-hidden="true" tabindex="-1"></a><span class="do">## 8         TRUE FALSE  TRUE TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE</span></span>
<span id="cb63-49"><a href="#cb63-49" aria-hidden="true" tabindex="-1"></a><span class="do">## 9         TRUE FALSE  TRUE TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE</span></span>
<span id="cb63-50"><a href="#cb63-50" aria-hidden="true" tabindex="-1"></a><span class="do">## 10        TRUE  TRUE  TRUE TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE</span></span>
<span id="cb63-51"><a href="#cb63-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-52"><a href="#cb63-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The package automatically produces the Cp statistic</span></span>
<span id="cb63-53"><a href="#cb63-53" aria-hidden="true" tabindex="-1"></a>    sumleaps<span class="sc">$</span>cp</span>
<span id="cb63-54"><a href="#cb63-54" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] 148.352561  47.072229  30.663634  21.998461   9.148045   5.560162   6.303221   7.248522</span></span>
<span id="cb63-55"><a href="#cb63-55" aria-hidden="true" tabindex="-1"></a><span class="do">##  [9]   9.028080  11.000000</span></span></code></pre></div>
<p>We can calculate different model selection criteria with the best models of each size. The model fitting result already produces the <span class="math inline">\(C_p\)</span> and BIC results. However, please note that both quantities are modified slightly. For the <span class="math inline">\(C_p\)</span> statistics, the quantity is divided by the estimated error variance, and also adjust for the sample size. For the BIC, the difference is a constant regardless of the model size. Hence these difference do will not affect the model selection result because the modification is the same regardless of the number of variables.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>    modelsize<span class="ot">=</span><span class="fu">apply</span>(sumleaps<span class="sc">$</span>which,<span class="dv">1</span>,sum)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>    Cp <span class="ot">=</span> sumleaps<span class="sc">$</span>rss<span class="sc">/</span>(<span class="fu">summary</span>(lm.fit)<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>modelsize <span class="sc">-</span> n;</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>    AIC <span class="ot">=</span> n<span class="sc">*</span><span class="fu">log</span>(sumleaps<span class="sc">$</span>rss<span class="sc">/</span>n) <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>modelsize;</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>    BIC <span class="ot">=</span> n<span class="sc">*</span><span class="fu">log</span>(sumleaps<span class="sc">$</span>rss<span class="sc">/</span>n) <span class="sc">+</span> modelsize<span class="sc">*</span><span class="fu">log</span>(n);</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Comparing the Cp scores </span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cbind</span>(<span class="st">&quot;Our Cp&quot;</span> <span class="ot">=</span> Cp, <span class="st">&quot;leaps Cp&quot;</span> <span class="ot">=</span> sumleaps<span class="sc">$</span>cp) </span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="do">##        Our Cp   leaps Cp</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 1  148.352561 148.352561</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="do">## 2   47.072229  47.072229</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="do">## 3   30.663634  30.663634</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a><span class="do">## 4   21.998461  21.998461</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 5    9.148045   9.148045</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 6    5.560162   5.560162</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="do">## 7    6.303221   6.303221</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a><span class="do">## 8    7.248522   7.248522</span></span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a><span class="do">## 9    9.028080   9.028080</span></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a><span class="do">## 10  11.000000  11.000000</span></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Comparing the BIC results. The difference is a constant, </span></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># which is the score of an intercept model</span></span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cbind</span>(<span class="st">&quot;Our BIC&quot;</span> <span class="ot">=</span> BIC, <span class="st">&quot;leaps BIC&quot;</span> <span class="ot">=</span> sumleaps<span class="sc">$</span>bic, </span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>          <span class="st">&quot;Difference&quot;</span> <span class="ot">=</span> BIC<span class="sc">-</span>sumleaps<span class="sc">$</span>bic, </span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>          <span class="st">&quot;Intercept Score&quot;</span> <span class="ot">=</span> n<span class="sc">*</span><span class="fu">log</span>(<span class="fu">sum</span>((diab[,<span class="dv">11</span>] <span class="sc">-</span> <span class="fu">mean</span>(diab[,<span class="dv">11</span>]))<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>n)))</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a><span class="do">##     Our BIC leaps BIC Difference Intercept Score</span></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a><span class="do">## 1  3665.879 -174.1108    3839.99         3839.99</span></span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a><span class="do">## 2  3586.331 -253.6592    3839.99         3839.99</span></span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a><span class="do">## 3  3575.249 -264.7407    3839.99         3839.99</span></span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a><span class="do">## 4  3571.077 -268.9126    3839.99         3839.99</span></span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a><span class="do">## 5  3562.469 -277.5210    3839.99         3839.99</span></span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a><span class="do">## 6  3562.900 -277.0899    3839.99         3839.99</span></span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a><span class="do">## 7  3567.708 -272.2819    3839.99         3839.99</span></span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a><span class="do">## 8  3572.720 -267.2702    3839.99         3839.99</span></span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a><span class="do">## 9  3578.585 -261.4049    3839.99         3839.99</span></span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a><span class="do">## 10 3584.648 -255.3424    3839.99         3839.99</span></span></code></pre></div>
<p>Finally, we may select the best model, using any of the criteria. The following code would produced a plot to visualize it. We can see that BIC selects 6 variables, while both AIC and <span class="math inline">\(C_p\)</span> selects 7.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Rescale Cp, AIC and BIC to (0,1).</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    inrange <span class="ot">&lt;-</span> <span class="cf">function</span>(x) { (x <span class="sc">-</span> <span class="fu">min</span>(x)) <span class="sc">/</span> (<span class="fu">max</span>(x) <span class="sc">-</span> <span class="fu">min</span>(x)) }</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    Cp <span class="ot">=</span> <span class="fu">inrange</span>(Cp)</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    BIC <span class="ot">=</span> <span class="fu">inrange</span>(BIC)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    AIC <span class="ot">=</span> <span class="fu">inrange</span>(AIC)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(<span class="fu">range</span>(modelsize), <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.4</span>), <span class="at">type=</span><span class="st">&quot;n&quot;</span>, </span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlab=</span><span class="st">&quot;Model Size (with Intercept)&quot;</span>, </span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>         <span class="at">ylab=</span><span class="st">&quot;Model Selection Criteria&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(modelsize, Cp, <span class="at">col =</span> <span class="st">&quot;green4&quot;</span>, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(modelsize, AIC, <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(modelsize, BIC, <span class="at">col =</span> <span class="st">&quot;purple&quot;</span>, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">&quot;Cp&quot;</span>, <span class="st">&quot;AIC&quot;</span>, <span class="st">&quot;BIC&quot;</span>),</span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a>           <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;green4&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;purple&quot;</span>), </span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>           <span class="at">lty =</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">3</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.7</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-80-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="step-wise-regression-using-step" class="section level3 hasAnchor" number="5.5.2">
<h3 class="hasAnchor"><span class="header-section-number">5.5.2</span> Step-wise regression using <code>step()</code><a href="#step-wise-regression-using-step" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea of step-wise regression is very simple: we start with a certain model (e.g. the intercept or the full mode), and add or subtract one variable at a time by making the best decision to improve the model selection score. The <code>step()</code> function implements this procedure. The following example starts with the full model and uses AIC as the selection criteria (default of the function). After removing several variables, the model ends up with six predictors.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># k = 2 (AIC) is default; </span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">step</span>(lm.fit, <span class="at">direction=</span><span class="st">&quot;both&quot;</span>, <span class="at">k =</span> <span class="dv">2</span>)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Start:  AIC=3539.64</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Y ~ age + sex + bmi + map + tc + ldl + hdl + tch + ltg + glu</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a><span class="do">##        Df Sum of Sq     RSS    AIC</span></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a><span class="do">## - age   1        82 1264066 3537.7</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a><span class="do">## - hdl   1       663 1264646 3537.9</span></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a><span class="do">## - glu   1      3080 1267064 3538.7</span></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a><span class="do">## - tch   1      3526 1267509 3538.9</span></span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;none&gt;              1263983 3539.6</span></span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a><span class="do">## - ldl   1      5799 1269782 3539.7</span></span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a><span class="do">## - tc    1     10600 1274583 3541.3</span></span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a><span class="do">## - sex   1     45000 1308983 3553.1</span></span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a><span class="do">## - ltg   1     56015 1319998 3556.8</span></span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a><span class="do">## - map   1     72103 1336086 3562.2</span></span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a><span class="do">## - bmi   1    179028 1443011 3596.2</span></span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Step:  AIC=3537.67</span></span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Y ~ sex + bmi + map + tc + ldl + hdl + tch + ltg + glu</span></span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-22"><a href="#cb66-22" aria-hidden="true" tabindex="-1"></a><span class="do">##        Df Sum of Sq     RSS    AIC</span></span>
<span id="cb66-23"><a href="#cb66-23" aria-hidden="true" tabindex="-1"></a><span class="do">## - hdl   1       646 1264712 3535.9</span></span>
<span id="cb66-24"><a href="#cb66-24" aria-hidden="true" tabindex="-1"></a><span class="do">## - glu   1      3001 1267067 3536.7</span></span>
<span id="cb66-25"><a href="#cb66-25" aria-hidden="true" tabindex="-1"></a><span class="do">## - tch   1      3543 1267608 3536.9</span></span>
<span id="cb66-26"><a href="#cb66-26" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;none&gt;              1264066 3537.7</span></span>
<span id="cb66-27"><a href="#cb66-27" aria-hidden="true" tabindex="-1"></a><span class="do">## - ldl   1      5751 1269817 3537.7</span></span>
<span id="cb66-28"><a href="#cb66-28" aria-hidden="true" tabindex="-1"></a><span class="do">## - tc    1     10569 1274635 3539.4</span></span>
<span id="cb66-29"><a href="#cb66-29" aria-hidden="true" tabindex="-1"></a><span class="do">## + age   1        82 1263983 3539.6</span></span>
<span id="cb66-30"><a href="#cb66-30" aria-hidden="true" tabindex="-1"></a><span class="do">## - sex   1     45831 1309896 3551.4</span></span>
<span id="cb66-31"><a href="#cb66-31" aria-hidden="true" tabindex="-1"></a><span class="do">## - ltg   1     55963 1320029 3554.8</span></span>
<span id="cb66-32"><a href="#cb66-32" aria-hidden="true" tabindex="-1"></a><span class="do">## - map   1     73850 1337915 3560.8</span></span>
<span id="cb66-33"><a href="#cb66-33" aria-hidden="true" tabindex="-1"></a><span class="do">## - bmi   1    179079 1443144 3594.2</span></span>
<span id="cb66-34"><a href="#cb66-34" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-35"><a href="#cb66-35" aria-hidden="true" tabindex="-1"></a><span class="do">## Step:  AIC=3535.9</span></span>
<span id="cb66-36"><a href="#cb66-36" aria-hidden="true" tabindex="-1"></a><span class="do">## Y ~ sex + bmi + map + tc + ldl + tch + ltg + glu</span></span>
<span id="cb66-37"><a href="#cb66-37" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-38"><a href="#cb66-38" aria-hidden="true" tabindex="-1"></a><span class="do">##        Df Sum of Sq     RSS    AIC</span></span>
<span id="cb66-39"><a href="#cb66-39" aria-hidden="true" tabindex="-1"></a><span class="do">## - glu   1      3093 1267805 3535.0</span></span>
<span id="cb66-40"><a href="#cb66-40" aria-hidden="true" tabindex="-1"></a><span class="do">## - tch   1      3247 1267959 3535.0</span></span>
<span id="cb66-41"><a href="#cb66-41" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;none&gt;              1264712 3535.9</span></span>
<span id="cb66-42"><a href="#cb66-42" aria-hidden="true" tabindex="-1"></a><span class="do">## - ldl   1      7505 1272217 3536.5</span></span>
<span id="cb66-43"><a href="#cb66-43" aria-hidden="true" tabindex="-1"></a><span class="do">## + hdl   1       646 1264066 3537.7</span></span>
<span id="cb66-44"><a href="#cb66-44" aria-hidden="true" tabindex="-1"></a><span class="do">## + age   1        66 1264646 3537.9</span></span>
<span id="cb66-45"><a href="#cb66-45" aria-hidden="true" tabindex="-1"></a><span class="do">## - tc    1     26840 1291552 3543.2</span></span>
<span id="cb66-46"><a href="#cb66-46" aria-hidden="true" tabindex="-1"></a><span class="do">## - sex   1     46382 1311094 3549.8</span></span>
<span id="cb66-47"><a href="#cb66-47" aria-hidden="true" tabindex="-1"></a><span class="do">## - map   1     73536 1338248 3558.9</span></span>
<span id="cb66-48"><a href="#cb66-48" aria-hidden="true" tabindex="-1"></a><span class="do">## - ltg   1     97509 1362221 3566.7</span></span>
<span id="cb66-49"><a href="#cb66-49" aria-hidden="true" tabindex="-1"></a><span class="do">## - bmi   1    178537 1443249 3592.3</span></span>
<span id="cb66-50"><a href="#cb66-50" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-51"><a href="#cb66-51" aria-hidden="true" tabindex="-1"></a><span class="do">## Step:  AIC=3534.98</span></span>
<span id="cb66-52"><a href="#cb66-52" aria-hidden="true" tabindex="-1"></a><span class="do">## Y ~ sex + bmi + map + tc + ldl + tch + ltg</span></span>
<span id="cb66-53"><a href="#cb66-53" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-54"><a href="#cb66-54" aria-hidden="true" tabindex="-1"></a><span class="do">##        Df Sum of Sq     RSS    AIC</span></span>
<span id="cb66-55"><a href="#cb66-55" aria-hidden="true" tabindex="-1"></a><span class="do">## - tch   1      3686 1271491 3534.3</span></span>
<span id="cb66-56"><a href="#cb66-56" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;none&gt;              1267805 3535.0</span></span>
<span id="cb66-57"><a href="#cb66-57" aria-hidden="true" tabindex="-1"></a><span class="do">## - ldl   1      7472 1275277 3535.6</span></span>
<span id="cb66-58"><a href="#cb66-58" aria-hidden="true" tabindex="-1"></a><span class="do">## + glu   1      3093 1264712 3535.9</span></span>
<span id="cb66-59"><a href="#cb66-59" aria-hidden="true" tabindex="-1"></a><span class="do">## + hdl   1       738 1267067 3536.7</span></span>
<span id="cb66-60"><a href="#cb66-60" aria-hidden="true" tabindex="-1"></a><span class="do">## + age   1         0 1267805 3537.0</span></span>
<span id="cb66-61"><a href="#cb66-61" aria-hidden="true" tabindex="-1"></a><span class="do">## - tc    1     26378 1294183 3542.1</span></span>
<span id="cb66-62"><a href="#cb66-62" aria-hidden="true" tabindex="-1"></a><span class="do">## - sex   1     44686 1312491 3548.3</span></span>
<span id="cb66-63"><a href="#cb66-63" aria-hidden="true" tabindex="-1"></a><span class="do">## - map   1     82154 1349959 3560.7</span></span>
<span id="cb66-64"><a href="#cb66-64" aria-hidden="true" tabindex="-1"></a><span class="do">## - ltg   1    102520 1370325 3567.3</span></span>
<span id="cb66-65"><a href="#cb66-65" aria-hidden="true" tabindex="-1"></a><span class="do">## - bmi   1    189970 1457775 3594.7</span></span>
<span id="cb66-66"><a href="#cb66-66" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-67"><a href="#cb66-67" aria-hidden="true" tabindex="-1"></a><span class="do">## Step:  AIC=3534.26</span></span>
<span id="cb66-68"><a href="#cb66-68" aria-hidden="true" tabindex="-1"></a><span class="do">## Y ~ sex + bmi + map + tc + ldl + ltg</span></span>
<span id="cb66-69"><a href="#cb66-69" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-70"><a href="#cb66-70" aria-hidden="true" tabindex="-1"></a><span class="do">##        Df Sum of Sq     RSS    AIC</span></span>
<span id="cb66-71"><a href="#cb66-71" aria-hidden="true" tabindex="-1"></a><span class="do">## &lt;none&gt;              1271491 3534.3</span></span>
<span id="cb66-72"><a href="#cb66-72" aria-hidden="true" tabindex="-1"></a><span class="do">## + tch   1      3686 1267805 3535.0</span></span>
<span id="cb66-73"><a href="#cb66-73" aria-hidden="true" tabindex="-1"></a><span class="do">## + glu   1      3532 1267959 3535.0</span></span>
<span id="cb66-74"><a href="#cb66-74" aria-hidden="true" tabindex="-1"></a><span class="do">## + hdl   1       395 1271097 3536.1</span></span>
<span id="cb66-75"><a href="#cb66-75" aria-hidden="true" tabindex="-1"></a><span class="do">## + age   1        11 1271480 3536.3</span></span>
<span id="cb66-76"><a href="#cb66-76" aria-hidden="true" tabindex="-1"></a><span class="do">## - ldl   1     39378 1310869 3545.7</span></span>
<span id="cb66-77"><a href="#cb66-77" aria-hidden="true" tabindex="-1"></a><span class="do">## - sex   1     41858 1313349 3546.6</span></span>
<span id="cb66-78"><a href="#cb66-78" aria-hidden="true" tabindex="-1"></a><span class="do">## - tc    1     65237 1336728 3554.4</span></span>
<span id="cb66-79"><a href="#cb66-79" aria-hidden="true" tabindex="-1"></a><span class="do">## - map   1     79627 1351119 3559.1</span></span>
<span id="cb66-80"><a href="#cb66-80" aria-hidden="true" tabindex="-1"></a><span class="do">## - bmi   1    190586 1462077 3594.0</span></span>
<span id="cb66-81"><a href="#cb66-81" aria-hidden="true" tabindex="-1"></a><span class="do">## - ltg   1    294094 1565585 3624.2</span></span>
<span id="cb66-82"><a href="#cb66-82" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-83"><a href="#cb66-83" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb66-84"><a href="#cb66-84" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab)</span></span>
<span id="cb66-85"><a href="#cb66-85" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-86"><a href="#cb66-86" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb66-87"><a href="#cb66-87" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)          sex          bmi          map           tc          ldl          ltg  </span></span>
<span id="cb66-88"><a href="#cb66-88" aria-hidden="true" tabindex="-1"></a><span class="do">##       152.1       -226.5        529.9        327.2       -757.9        538.6        804.2</span></span></code></pre></div>
<p>We can also use different settings, such as which model to start with, which is the minimum/maximum model, and do we allow to adding/subtracting.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use BIC (k = log(n))instead of AIC</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># trace = 0 will suppress the output of intermediate steps </span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">step</span>(lm.fit, <span class="at">direction=</span><span class="st">&quot;both&quot;</span>, <span class="at">k =</span> <span class="fu">log</span>(n), <span class="at">trace=</span><span class="dv">0</span>)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab)</span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)          sex          bmi          map           tc          ldl          ltg  </span></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a><span class="do">##       152.1       -226.5        529.9        327.2       -757.9        538.6        804.2</span></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Start with an intercept model, and use forward selection (adding only)</span></span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">step</span>(<span class="fu">lm</span>(Y<span class="sc">~</span><span class="dv">1</span>, <span class="at">data=</span>diab), <span class="at">scope=</span><span class="fu">list</span>(<span class="at">upper=</span>lm.fit, <span class="at">lower=</span><span class="sc">~</span><span class="dv">1</span>), </span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>         <span class="at">direction=</span><span class="st">&quot;forward&quot;</span>, <span class="at">trace=</span><span class="dv">0</span>)</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = Y ~ bmi + ltg + map + tc + sex + ldl, data = diab)</span></span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)          bmi          ltg          map           tc          sex          ldl  </span></span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a><span class="do">##       152.1        529.9        804.2        327.2       -757.9       -226.5        538.6</span></span></code></pre></div>
<p>We can see that these results are slightly different from the best subset selection. So which is better? Of course the best subset selection is better because it considers all possible candidates, which step-wise regression may stuck at a sub-optimal model, while adding and subtracting any variable do not benefit further. Hence, the results of step-wise regression may be unstable. On the other hand, best subset selection not really feasible for high-dimensional problems because of the computational cost.</p>
</div>
</div>
<div id="marrows-cp" class="section level2 hasAnchor" number="5.6">
<h2 class="hasAnchor"><span class="header-section-number">5.6</span> Derivation of Marrows’ <span class="math inline">\(C_p\)</span><a href="#marrows-cp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we have a set of training data <span class="math inline">\(\cal{D}_n = \{x_i, \color{DodgerBlue}{y_i}\}_{i=1}^n\)</span> and a set of testing data, with the same covariates <span class="math inline">\(\cal{D}_n^\ast = \{x_i, \color{OrangeRed}{y_i^\ast}\}_{i=1}^n\)</span>. Hence, this is an <strong>in-sample prediction</strong> problem. However, the <span class="math inline">\(\color{OrangeRed}{y_i^\ast}\)</span>s are newly observed. Assuming that the data are generated from a linear model, i.e., in vector form,</p>
<p><span class="math display">\[\color{DodgerBlue}{\mathbf{y}}= \boldsymbol \mu+ \color{DodgerBlue}{\mathbf{e}}= \mathbf{X}\boldsymbol \beta+ \color{DodgerBlue}{\mathbf{e}},\]</span>
and
<span class="math display">\[\color{OrangeRed}{\mathbf{y}^\ast}= \boldsymbol \mu+ \color{OrangeRed}{\mathbf{e}^\ast}= \mathbf{X}\boldsymbol \beta+ \color{OrangeRed}{\mathbf{e}^\ast},\]</span>
where the error terms are i.i.d with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>. We want to know what is the best model that predicts <span class="math inline">\(\color{OrangeRed}{\mathbf{y}^\ast}\)</span>. Let’s look at the testing error first:</p>
<p><span class="math display">\[\begin{align}
\text{E}[\color{OrangeRed}{\text{Testing Error}}] =&amp; ~\text{E}\lVert \color{OrangeRed}{\mathbf{y}^\ast}- \mathbf{X}\color{DodgerBlue}{\widehat{\boldsymbol \beta}}\rVert^2 \\
=&amp; ~\text{E}\lVert (\color{OrangeRed}{\mathbf{y}^\ast}- \mathbf{X}\boldsymbol \beta) + (\mathbf{X}\boldsymbol \beta- \mathbf{X}\color{DodgerBlue}{\widehat{\boldsymbol \beta}}) \rVert^2 \\
=&amp; ~\text{E}\lVert \color{OrangeRed}{\mathbf{e}^\ast}\rVert^2 + \text{E}\lVert \mathbf{X}(\color{DodgerBlue}{\widehat{\boldsymbol \beta}}- \boldsymbol \beta) \rVert^2 \\
=&amp; ~\color{OrangeRed}{n \sigma^2} + \text{E}\big[ \text{Trace}\big( (\color{DodgerBlue}{\widehat{\boldsymbol \beta}}- \boldsymbol \beta)^\text{T}\mathbf{X}^\text{T}\mathbf{X}(\color{DodgerBlue}{\widehat{\boldsymbol \beta}}- \boldsymbol \beta) \big) \big] \\
=&amp; ~\color{OrangeRed}{n \sigma^2} + \text{Trace}\big(\mathbf{X}^\text{T}\mathbf{X}\text{Cov}(\color{DodgerBlue}{\widehat{\boldsymbol \beta}})\big) \\
=&amp; ~\color{OrangeRed}{n \sigma^2} + \color{DodgerBlue}{p \sigma^2}.
\end{align}\]</span></p>
<p>In the above, we used properties</p>
<ul>
<li><span class="math inline">\(\text{Trace}(ABC) = \text{Trace}(CBA)\)</span></li>
<li><span class="math inline">\(\text{E}[\text{Trace}(A)] = \text{Trace}(\text{E}[A])\)</span></li>
</ul>
<p>On the other hand, the training error is</p>
<p><span class="math display">\[\begin{align}
\text{E}[\color{DodgerBlue}{\text{Training Error}}] =&amp; ~\text{E}\lVert \mathbf{y}- \color{DodgerBlue}{\mathbf{y}}\rVert^2 \\
=&amp; ~\text{E}\lVert (\mathbf{I}- \mathbf{H})(\mathbf{X}\boldsymbol \beta+ \color{DodgerBlue}{\mathbf{e}}) \rVert^2 \\
=&amp; ~\text{E}\lVert (\mathbf{I}- \mathbf{H})\color{DodgerBlue}{\mathbf{e}}\rVert^2 \\
=&amp; ~\text{E}[\text{Trace}(\color{DodgerBlue}{\mathbf{e}}^\text{T}(\mathbf{I}- \mathbf{H})^\text{T}(\mathbf{I}- \mathbf{H}) \color{DodgerBlue}{\mathbf{e}})]\\
=&amp; ~\text{Trace}((\mathbf{I}- \mathbf{H})^\text{T}(\mathbf{I}- \mathbf{H}) \text{Cov}(\color{DodgerBlue}{\mathbf{e}})]\\
=&amp; ~\color{DodgerBlue}{(n - p) \sigma^2}.
\end{align}\]</span></p>
<p>In the above, we further used properties</p>
<ul>
<li><span class="math inline">\(\mathbf{H}\)</span> and <span class="math inline">\(\mathbf{I}- \mathbf{H}\)</span> are projection matrices</li>
<li><span class="math inline">\(\mathbf{H}\mathbf{X}= \mathbf{X}\)</span></li>
</ul>
<p>If we contrast the two results above, the difference between the training and testing errors is <span class="math inline">\(2 p \sigma^2\)</span>. Hence, if we can obtain a valid estimation of <span class="math inline">\(\sigma^2\)</span>, then the training error plus <span class="math inline">\(2 p \widehat{\sigma}^2\)</span> is a good approximation of the testing error, which we want to minimize. And that is exactly what Marrows’ <span class="math inline">\(C_p\)</span> does.</p>
<p>We can also generalize this result to the case when the underlying model is not a linear model. Assume that</p>
<p><span class="math display">\[\color{DodgerBlue}{\mathbf{y}}= f(\mathbf{X}) + \color{DodgerBlue}{\mathbf{e}}= \boldsymbol \mu+ \color{DodgerBlue}{\mathbf{e}},\]</span>
and
<span class="math display">\[\color{OrangeRed}{\mathbf{y}^\ast}= f(\mathbf{X}) + \color{OrangeRed}{\mathbf{e}^\ast}= \boldsymbol \mu+ \color{OrangeRed}{\mathbf{e}^\ast}.\]</span>
In this case, a linear model would not estimate <span class="math inline">\(\boldsymbol \mu\)</span>. Instead, it is only capable to produce the best linear approximation of <span class="math inline">\(\boldsymbol \mu\)</span> using the columns in <span class="math inline">\(\mathbf{X}\)</span>, which is <span class="math inline">\(\mathbf{H}\boldsymbol \mu\)</span>, the projection of <span class="math inline">\(\boldsymbol \mu\)</span> on the column space of <span class="math inline">\(\mathbf{X}\)</span>. In general, <span class="math inline">\(\mathbf{H}\boldsymbol \mu\neq \boldsymbol \mu\)</span>, and the remaining part <span class="math inline">\(\boldsymbol \mu- \mathbf{H}\boldsymbol \mu\)</span> is called <strong>bias</strong>. This is a new concept that will appear frequently in this book. Selection variables will essentially trade between bias and variance of a model. The following derivation shows this phenomenon:</p>
<p><span class="math display">\[\begin{align}
\text{E}[\color{OrangeRed}{\text{Testing Error}}] =&amp; ~\text{E}\lVert \color{OrangeRed}{\mathbf{y}^\ast}- \mathbf{X}\color{DodgerBlue}{\widehat{\boldsymbol \beta}}\rVert^2 \\
=&amp; ~\text{E}\lVert \color{OrangeRed}{\mathbf{y}^\ast}- \mathbf{H}\color{DodgerBlue}{\mathbf{y}}\rVert^2 \\
=&amp; ~\text{E}\lVert (\color{OrangeRed}{\mathbf{y}^\ast}- \boldsymbol \mu) + (\boldsymbol \mu- \mathbf{H}\boldsymbol \mu) + (\mathbf{H}\boldsymbol \mu- \mathbf{H}\color{DodgerBlue}{\mathbf{y}}) \rVert^2 \\
=&amp; ~\text{E}\lVert \color{OrangeRed}{\mathbf{y}^\ast}- \boldsymbol \mu\rVert^2 + \text{E}\lVert \boldsymbol \mu- \mathbf{H}\boldsymbol \mu\rVert^2 + \text{E}\lVert \mathbf{H}\boldsymbol \mu- \mathbf{H}\color{DodgerBlue}{\mathbf{y}}\rVert^2 \\
=&amp; ~\text{E}\lVert \color{OrangeRed}{\mathbf{e}^\ast}\rVert^2 + \text{E}\lVert \boldsymbol \mu- \mathbf{H}\boldsymbol \mu\rVert^2 + \text{E}\lVert \mathbf{H}\color{DodgerBlue}{\mathbf{e}}\rVert^2 \\
=&amp; ~\color{OrangeRed}{n \sigma^2} + \text{Bias}^2 + \color{DodgerBlue}{p \sigma^2},
\end{align}\]</span></p>
<p>while the training error is</p>
<p><span class="math display">\[\begin{align}
\text{E}[\color{DodgerBlue}{\text{Training Error}}] =&amp; ~\text{E}\lVert \color{DodgerBlue}{\mathbf{y}}- \mathbf{X}\color{DodgerBlue}{\widehat{\boldsymbol \beta}}\rVert^2 \\
=&amp; ~\text{E}\lVert \color{DodgerBlue}{\mathbf{y}}- \mathbf{H}\color{DodgerBlue}{\mathbf{y}}\rVert^2 \\
=&amp; ~\text{E}\lVert (\mathbf{I}- \mathbf{H})(\boldsymbol \mu+ \color{DodgerBlue}{\mathbf{e}}) \rVert^2 \\
=&amp; ~\text{E}\lVert (\mathbf{I}- \mathbf{H})\boldsymbol \mu\rVert^2 + \text{E}\lVert (\mathbf{I}- \mathbf{H})\color{DodgerBlue}{\mathbf{e}}\rVert^2\\
=&amp; ~\text{Bias}^2 + \color{DodgerBlue}{(n - p) \sigma^2}.
\end{align}\]</span></p>
<p>We can notice again that the difference is <span class="math inline">\(2p\sigma^2\)</span>. Note that this is regardless of whether the linear model is correct or not.</p>
<!--chapter:end:02.1-linear.Rmd-->
</div>
</div>
<div id="ridge-regression" class="section level1 hasAnchor" number="6">
<h1 class="hasAnchor"><span class="header-section-number">6</span> Ridge Regression<a href="#ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Ridge regression was proposed by <span class="citation">Hoerl and Kennard (<a href="#ref-hoerl1970ridge" role="doc-biblioref">1970</a>)</span>, but is also a special case of Tikhonov regularization. The essential idea is very simple: Knowing that the ordinary least squares (OLS) solution is not unique in an ill-posed problem, i.e., <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span> is not invertible, a ridge regression adds a ridge (diagonal matrix) on <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span>:</p>
<p><span class="math display">\[\widehat{\boldsymbol \beta}^\text{ridge} = (\mathbf{X}^\text{T}\mathbf{X}+ n \lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y},\]</span>
It provides a solution of linear regression when multicollinearity happens, especially when the number of variables is larger than the sample size. Alternatively, this is also the solution of a regularized least square estimator. We add an <span class="math inline">\(\ell_2\)</span> penalty to the residual sum of squares, i.e.,</p>
<p><span class="math display">\[
\begin{align}
\widehat{\boldsymbol \beta}^\text{ridge} =&amp; \mathop{\mathrm{arg\,min}}_{\boldsymbol \beta} (\mathbf{y}- \mathbf{X}\boldsymbol \beta)^\text{T}(\mathbf{y}- \mathbf{X}\boldsymbol \beta) + n \lambda \lVert\boldsymbol \beta\rVert_2^2\\
=&amp; \mathop{\mathrm{arg\,min}}_{\boldsymbol \beta} \frac{1}{n} \sum_{i=1}^n (y_i - x_i^\text{T}\boldsymbol \beta)^2 + \lambda \sum_{j=1}^p \beta_j^2,
\end{align}
\]</span></p>
<p>for some penalty <span class="math inline">\(\lambda &gt; 0\)</span>. Another approach that leads to the ridge regression is a constraint on the <span class="math inline">\(\ell_2\)</span> norm of the parameters, which will be introduced in the next Chapter. Ridge regression is used extensively in genetic analyses to address “small-<span class="math inline">\(n\)</span>-large-<span class="math inline">\(p\)</span>” problems. We will start with a motivation example and then discuss the bias-variance trade-off issue.</p>
<div id="motivation-correlated-variables-and-convexity" class="section level2 hasAnchor" number="6.1">
<h2 class="hasAnchor"><span class="header-section-number">6.1</span> Motivation: Correlated Variables and Convexity<a href="#motivation-correlated-variables-and-convexity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ridge regression has many advantages. Most notably, it can address highly correlated variables. From an optimization point of view, having highly correlated variables means that the objective function (<span class="math inline">\(\ell_2\)</span> loss) becomes “flat” along certain directions in the parameter domain. This can be seen from the following example, where the true parameters are both <span class="math inline">\(1\)</span> while the estimated parameters concludes almost all effects to the first variable. You can change different seed to observe the variability of these parameter estimates and notice that they are quite large. Instead, if we fit a ridge regression, the parameter estimates are relatively stable.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(MASS)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">30</span></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># create highly correlated variables and a linear model</span></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.99</span>, <span class="fl">0.99</span>, <span class="dv">1</span>), <span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> X[,<span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>])</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compare parameter estimates</span></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summary</span>(<span class="fu">lm</span>(y<span class="sc">~</span>X<span class="dv">-1</span>))<span class="sc">$</span>coef</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a><span class="do">##     Estimate Std. Error    t value  Pr(&gt;|t|)</span></span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a><span class="do">## X1 1.8461255   1.294541 1.42608527 0.1648987</span></span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a><span class="do">## X2 0.0990278   1.321283 0.07494822 0.9407888</span></span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># note that the true parameters are all 1&#39;s</span></span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Be careful that the `lambda` parameter in lm.ridge is our (n*lambda)</span></span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm.ridge</span>(y<span class="sc">~</span>X<span class="dv">-1</span>, <span class="at">lambda=</span><span class="dv">5</span>)</span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a><span class="do">##        X1        X2 </span></span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.9413221 0.8693253</span></span></code></pre></div>
<p>The variance of both <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are quite large. This is expected because we know from linear regression that the variance of <span class="math inline">\(\widehat{\boldsymbol \beta}\)</span> is <span class="math inline">\(\sigma^2 (\mathbf{X}^\text{T}\mathbf{X})^{-1}\)</span>. However, since the columns of <span class="math inline">\(\mathbf{X}\)</span> are highly correlated, the smallest eigenvalue of <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span> is close to 0, making the largest eigenvalue of <span class="math inline">\((\mathbf{X}^\text{T}\mathbf{X})^{-1}\)</span> very large. This can also be interpreted through an optimization point of view. The objective function for an OLS estimator is demonstrated in the following.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>  beta1 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="fl">0.005</span>)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>  beta2 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="fl">0.005</span>)</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>  allbeta <span class="ot">&lt;-</span> <span class="fu">data.matrix</span>(<span class="fu">expand.grid</span>(beta1, beta2))</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>  rss <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">apply</span>(allbeta, <span class="dv">1</span>, <span class="cf">function</span>(b, X, y) <span class="fu">sum</span>((y <span class="sc">-</span> X <span class="sc">%*%</span> b)<span class="sc">^</span><span class="dv">2</span>), X, y), </span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>                <span class="fu">length</span>(beta1), <span class="fu">length</span>(beta2))</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># quantile levels for drawing contour</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>  quanlvl <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.025</span>, <span class="fl">0.05</span>, <span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>)</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the contour</span></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the truth</span></span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the data </span></span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a>  betahat <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm</span>(y<span class="sc">~</span>X<span class="dv">-1</span>))</span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(betahat[<span class="dv">1</span>], betahat[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-86-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Over many simulation runs, the solution lies around the line of <span class="math inline">\(\beta_1 + \beta_2 = 2\)</span>.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the truth</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate many datasets in a simulation </span></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>)</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.99</span>, <span class="fl">0.99</span>, <span class="dv">1</span>), <span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> X[,<span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>])</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>    betahat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(betahat[<span class="dv">1</span>], betahat[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-87-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="ridge-penalty-and-the-reduced-variation" class="section level2 hasAnchor" number="6.2">
<h2 class="hasAnchor"><span class="header-section-number">6.2</span> Ridge Penalty and the Reduced Variation<a href="#ridge-penalty-and-the-reduced-variation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If we add a ridge regression penalty, the contour is forced to be more convex due to the added eigenvalues on <span class="math inline">\(\mathbf{X}^\mathbf{X}\)</span>, making the eignvalues of <span class="math inline">\((\mathbf{X}^\mathbf{X})^{-1}\)</span> smaller. Here is a plot of the Ridge <span class="math inline">\(\ell_2\)</span> penalty.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-88-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Hence, by adding this to the OLS objective function, the solution is more stable. This may be interpreted in several different ways such as: 1) the objective function is more convex; 2) the variance of the estimator is smaller. However, this causes some bias too. Choosing the tuning parameter is a balance of the bias-variance trade-off, which will be discussed in the following.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># adding a L2 penalty to the objective function</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    rss <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">apply</span>(allbeta, <span class="dv">1</span>, <span class="cf">function</span>(b, X, y) <span class="fu">sum</span>((y <span class="sc">-</span> X <span class="sc">%*%</span> b)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> b <span class="sc">%*%</span> b, X, y),</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">length</span>(beta1), <span class="fu">length</span>(beta2))</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the ridge solution</span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>    bh <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> <span class="fu">diag</span>(<span class="dv">2</span>)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(bh[<span class="dv">1</span>], bh[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">box</span>()</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># adding a larger penalty</span></span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>    rss <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">apply</span>(allbeta, <span class="dv">1</span>, <span class="cf">function</span>(b, X, y) <span class="fu">sum</span>((y <span class="sc">-</span> X <span class="sc">%*%</span> b)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="dv">10</span><span class="sc">*</span>b <span class="sc">%*%</span> b, X, y),</span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">length</span>(beta1), <span class="fu">length</span>(beta2))</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a>    bh <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> <span class="dv">10</span><span class="sc">*</span><span class="fu">diag</span>(<span class="dv">2</span>)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the ridge solution</span></span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(bh[<span class="dv">1</span>], bh[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a>    <span class="fu">box</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-89-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>We can check the ridge solution over many simulation runs</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the truth</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate many datasets in a simulation </span></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>)</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.99</span>, <span class="fl">0.99</span>, <span class="dv">1</span>), <span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> X[,<span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>])</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># betahat &lt;- solve(t(X) %*% X + 2*diag(2)) %*% t(X) %*% y</span></span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>    betahat <span class="ot">&lt;-</span> <span class="fu">lm.ridge</span>(y <span class="sc">~</span> X <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lambda =</span> <span class="dv">2</span>)<span class="sc">$</span>coef</span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(betahat[<span class="dv">1</span>], betahat[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-90-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>This effect is gradually changing as we increase the penalty level. The following simulation shows how the variation of <span class="math inline">\(\boldsymbol \beta\)</span> changes. We show this with two penalty values, and see how the estimated parameters are away from the truth.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># small penalty</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate many datasets in a simulation </span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>)</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.99</span>, <span class="fl">0.99</span>, <span class="dv">1</span>), <span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> X[,<span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>])</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>    betahat <span class="ot">&lt;-</span> <span class="fu">lm.ridge</span>(y <span class="sc">~</span> X <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lambda =</span> <span class="dv">2</span>)<span class="sc">$</span>coef</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(betahat[<span class="dv">1</span>], betahat[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># large penalty</span></span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>) </span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate many datasets in a simulation </span></span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>)</span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.99</span>, <span class="fl">0.99</span>, <span class="dv">1</span>), <span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> X[,<span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>])</span>
<span id="cb73-26"><a href="#cb73-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb73-27"><a href="#cb73-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># betahat &lt;- solve(t(X) %*% X + 30*diag(2)) %*% t(X) %*% y</span></span>
<span id="cb73-28"><a href="#cb73-28" aria-hidden="true" tabindex="-1"></a>    betahat <span class="ot">&lt;-</span> <span class="fu">lm.ridge</span>(y <span class="sc">~</span> X <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lambda =</span> <span class="dv">30</span>)<span class="sc">$</span>coef</span>
<span id="cb73-29"><a href="#cb73-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(betahat[<span class="dv">1</span>], betahat[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb73-30"><a href="#cb73-30" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-91-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="bias-and-variance-of-ridge-regression" class="section level2 hasAnchor" number="6.3">
<h2 class="hasAnchor"><span class="header-section-number">6.3</span> Bias and Variance of Ridge Regression<a href="#bias-and-variance-of-ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can set a relationship between Ridge and OLS, assuming that the OLS estimator exist.</p>
<p><span class="math display">\[\begin{align}
\widehat{\boldsymbol \beta}^\text{ridge} =&amp; (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y}\\
=&amp; (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} (\mathbf{X}^\text{T}\mathbf{X}) \color{OrangeRed}{(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}\mathbf{y}}\\
=&amp; (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} (\mathbf{X}^\text{T}\mathbf{X}) \color{OrangeRed}{\widehat{\boldsymbol \beta}^\text{ols}}
\end{align}\]</span></p>
<p>This leads to a biased estimator (since the OLS estimator is unbiased) if we use any nonzero <span class="math inline">\(\lambda\)</span>.</p>
<ul>
<li>As <span class="math inline">\(\lambda \rightarrow 0\)</span>, the ridge solution is eventually the same as OLS</li>
<li>As <span class="math inline">\(\lambda \rightarrow \infty\)</span>, <span class="math inline">\(\widehat{\boldsymbol \beta}^\text{ridge} \rightarrow 0\)</span></li>
</ul>
<p>It can be easier to analyze a case with <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}= n \mathbf{I}\)</span>, i.e, with standardized and orthogonal columns in <span class="math inline">\(\mathbf{X}\)</span>. Note that in this case, each <span class="math inline">\(\beta_j^{\text{ols}}\)</span> is just the projection of <span class="math inline">\(\mathbf{y}\)</span> onto <span class="math inline">\(\mathbf{x}_j\)</span>, the <span class="math inline">\(j\)</span>th column of the design matrix. We also have</p>
<p><span class="math display">\[\begin{align}
\widehat{\boldsymbol \beta}^\text{ridge} =&amp; (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} (\mathbf{X}^\text{T}\mathbf{X}) \widehat{\boldsymbol \beta}^\text{ols}\\
=&amp; (\mathbf{I}+ \lambda \mathbf{I})^{-1}\widehat{\boldsymbol \beta}^\text{ols}\\
=&amp; (1 + \lambda)^{-1} \widehat{\boldsymbol \beta}^\text{ols}\\

\Longrightarrow \beta_j^{\text{ridge}} =&amp; \frac{1}{1 + \lambda} \beta_j^\text{ols}
\end{align}\]</span></p>
<p>Then in this case, the bias and variance of the ridge estimator can be explicitly expressed:</p>
<ul>
<li><span class="math inline">\(\text{Bias}(\beta_j^{\text{ridge}}) = \frac{-\lambda}{1 + \lambda} \beta_j^\text{ols}\)</span> (not zero)</li>
<li><span class="math inline">\(\text{Var}(\beta_j^{\text{ridge}}) = \frac{1}{(1 + \lambda)^2} \text{Var}(\beta_j^\text{ols})\)</span> (reduced from OLS)</li>
</ul>
<p>Of course, we can ask the question: is it worth it? We could proceed with a simple analysis of the MSE of <span class="math inline">\(\beta\)</span> (dropping <span class="math inline">\(j\)</span>):</p>
<p><span class="math display">\[\begin{align}
\text{MSE}(\beta) &amp;= \text{E}(\widehat{\beta} - \beta)^2 \\
&amp;= \text{E}[\widehat{\beta} - \text{E}(\widehat{\beta})]^2 + \text{E}[\widehat{\beta} - \beta]^2 \\
&amp;= \text{E}[\widehat{\beta} - \text{E}(\widehat{\beta})]^2 + 0 + [\text{E}(\widehat{\beta}) - \beta]^2 \\
&amp;= \text{Var}(\widehat{\beta}) + \text{Bias}^2.
\end{align}\]</span></p>
<p>This bias-variance breakdown formula will appear multiple times. Now, plug-in the results developed earlier based on the orthogonal design matrix, and investigate the derivative of the MSE of the Ridge estimator, we have</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \text{MSE}(\widehat{\beta}^\text{ridge})}{ \partial \lambda} =&amp; \frac{\partial}{\partial \lambda} \left[ \frac{1}{(1+\lambda)^2} \text{Var}(\widehat{\beta}^\text{ols}) + \frac{\lambda^2}{(1 + \lambda)^2} \beta^2 \right] \\
=&amp; \frac{2}{(1+\lambda)^3} \left[ \lambda \beta^2 - \text{Var}(\widehat{\beta}^\text{ols}) \right]
\end{align}\]</span></p>
<p>Note that when the derivative is negative, increasing <span class="math inline">\(\lambda\)</span> would decrease the MSE. This implies that we can reduce the MSE by choosing a small <span class="math inline">\(\lambda\)</span>. Of course the situation is much more involving when the columns in <span class="math inline">\(\mathbf{X}\)</span> are not orthogonal. However, the following analysis helps to understand a non-orthogonal case. It is essentially re-organizing the columns of <span class="math inline">\(\mathbf{X}\)</span> into its principle components so that they are still orthogonal.</p>
<p>Let’s first take a singular value decomposition (SVD) of <span class="math inline">\(\mathbf{X}\)</span>, with <span class="math inline">\(\mathbf{X}= \mathbf{U}\mathbf{D}\mathbf{V}^\text{T}\)</span>, then the columns in <span class="math inline">\(\mathbf{U}\)</span> form an orthonormal basis and columns in <span class="math inline">\(\mathbf{U}\mathbf{D}\)</span> are the <strong>principal components</strong> and <span class="math inline">\(\mathbf{V}\)</span> defines the principle directions. In addition, we have <span class="math inline">\(n \widehat{\boldsymbol \Sigma} = \mathbf{X}^\text{T}\mathbf{X}= \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}\)</span>. Assuming that <span class="math inline">\(p &lt; n\)</span>, and <span class="math inline">\(\mathbf{X}\)</span> has full column ranks, then the Ridge estimator fitted <span class="math inline">\(\mathbf{y}\)</span> value can be decomposed as</p>
<p><span class="math display">\[\begin{align}
\widehat{\mathbf{y}}^\text{ridge} =&amp; \mathbf{X}\widehat{\beta}^\text{ridge} \\
=&amp; \mathbf{X}(\mathbf{X}^\text{T}\mathbf{X}+ n \lambda)^{-1} \mathbf{X}^\text{T}\mathbf{y}\\
=&amp; \mathbf{U}\mathbf{D}\mathbf{V}^\text{T}( \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}+ n \lambda \mathbf{V}\mathbf{V}^\text{T})^{-1} \mathbf{V}\mathbf{D}\mathbf{U}^\text{T}\mathbf{y}\\
=&amp; \mathbf{U}\mathbf{D}^2 (n \lambda + \mathbf{D}^2)^{-1} \mathbf{U}^\text{T}\mathbf{y}\\
=&amp; \sum_{j = 1}^p \mathbf{u}_j \left( \frac{d_j^2}{n \lambda + d_j^2} \mathbf{u}_j^\text{T}\mathbf{y}\right),
\end{align}\]</span></p>
<p>where <span class="math inline">\(d_j\)</span> is the <span class="math inline">\(j\)</span>th eigenvalue of the PCA. Hence, the Ridge regression fitted value can be understood as</p>
<ul>
<li>Perform PCA of <span class="math inline">\(\mathbf{X}\)</span></li>
<li>Project <span class="math inline">\(\mathbf{y}\)</span> onto the PCs</li>
<li>Shrink the projection <span class="math inline">\(\mathbf{u}_j^\text{T}\mathbf{y}\)</span> by the factor <span class="math inline">\(d_j^2 / (n \lambda + d_j^2)\)</span></li>
<li>Reassemble the PCs using all the shrunken length</li>
</ul>
<p>Hence, the bias-variance notion can be understood as the trade-off on these derived directions <span class="math inline">\(\mathbf{u}_j\)</span> and their corresponding parameters <span class="math inline">\(\mathbf{u}_j^\text{T}\mathbf{y}\)</span>.</p>
</div>
<div id="degrees-of-freedom" class="section level2 hasAnchor" number="6.4">
<h2 class="hasAnchor"><span class="header-section-number">6.4</span> Degrees of Freedom<a href="#degrees-of-freedom" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We know that for a linear model, the degrees of freedom (DF) is simply the number of parameters used. There is a formal definition, using</p>
<p><span class="math display">\[\begin{align}
\text{DF}(\widehat{f}) =&amp; \frac{1}{\sigma^2} \text{Cov}(\widehat{y}_i, y_i)\\
=&amp; \frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\widehat{\mathbf{y}}, \mathbf{y})]
\end{align}\]</span></p>
<p>We can check that for a linear regression (assuming the intercept is already included in <span class="math inline">\(\mathbf{X}\)</span>), the DF is</p>
<p><span class="math display">\[\begin{align}
\frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\widehat{\mathbf{y}}^\text{ols}, \mathbf{y})] =&amp; \frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}\mathbf{y}, \mathbf{y})] \\
=&amp; \text{Trace}(\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}) \\
=&amp; \text{Trace}(\mathbf{I}_{p\times p})\\
=&amp; p
\end{align}\]</span></p>
<p>For the Ridge regression, we can perform the same analysis on ridge regression.</p>
<p><span class="math display">\[\begin{align}
\frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\widehat{\mathbf{y}}^\text{ridge}, \mathbf{y})] =&amp; \frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X}+ n \lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y}, \mathbf{y})] \\
=&amp; \text{Trace}(\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}) \\
=&amp; \text{Trace}(\mathbf{U}\mathbf{D}\mathbf{V}^\text{T}( \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}+ n \lambda \mathbf{V}\mathbf{V}^\text{T})^{-1} \mathbf{V}\mathbf{D}\mathbf{U}^\text{T})\\
=&amp; \sum_{j = 1}^p \frac{d_j^2}{d_j^2 + n\lambda}
\end{align}\]</span></p>
<p>Note that this is smaller than <span class="math inline">\(p\)</span> as long as <span class="math inline">\(\lambda \neq 0\)</span>. This implies that the Ridge regression does not use the full potential of all <span class="math inline">\(p\)</span> variables, since there is a risk of over-fitting.</p>
</div>
<div id="using-the-lm.ridge-function" class="section level2 hasAnchor" number="6.5">
<h2 class="hasAnchor"><span class="header-section-number">6.5</span> Using the <code>lm.ridge()</code> function<a href="#using-the-lm.ridge-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have seen how the <code>lm.ridge()</code> can be used to fit a Ridge regression. However, keep in mind that the <code>lambda</code> parameter used in the function actually specifies the <span class="math inline">\(n \lambda\)</span> entirely we used in our notation. However, regardless, our goal is mainly to tune this parameter to achieve a good balance of bias-variance trade off. However, the difficulty here is to evaluate the performance without knowing the truth. Let’s first use a simulated example, in which we do know the truth and then introduce the cross-validation approach for real data where we do not know the truth.</p>
<p>We use the prostate cancer data <code>prostate</code> from the <code>ElemStatLearn</code> package. The dataset contains 8 explanatory variables and one outcome <code>lpsa</code>, the log prostate-specific antigen value.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># ElemStatLearn is currently archived, install a previous version</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># library(devtools)</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># install_version(&quot;ElemStatLearn&quot;, version = &quot;2015.6.26&quot;, repos = &quot;http://cran.r-project.org&quot;)</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>(prostate)</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a><span class="do">##       lcavol  lweight age      lbph svi       lcp gleason pgg45       lpsa train</span></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 -0.5798185 2.769459  50 -1.386294   0 -1.386294       6     0 -0.4307829  TRUE</span></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 -0.9942523 3.319626  58 -1.386294   0 -1.386294       6     0 -0.1625189  TRUE</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 3 -0.5108256 2.691243  74 -1.386294   0 -1.386294       7    20 -0.1625189  TRUE</span></span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 -1.2039728 3.282789  58 -1.386294   0 -1.386294       6     0 -0.1625189  TRUE</span></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a><span class="do">## 5  0.7514161 3.432373  62 -1.386294   0 -1.386294       6     0  0.3715636  TRUE</span></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a><span class="do">## 6 -1.0498221 3.228826  50 -1.386294   0 -1.386294       6     0  0.7654678  TRUE</span></span></code></pre></div>
<div id="scaling-issue" class="section level3 hasAnchor" number="6.5.1">
<h3 class="hasAnchor"><span class="header-section-number">6.5.1</span> Scaling Issue<a href="#scaling-issue" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can use <code>lm.ridge()</code> with a fixed <span class="math inline">\(\lambda\)</span> value, as we have shown in the previous example. Its syntax is again similar to the <code>lm()</code> function, with an additional argument <code>lambda</code>. We can also compare that with our own code.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># lm.ridge function from the MASS package</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm.ridge</span>(lpsa <span class="sc">~</span>., <span class="at">data =</span> prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>], <span class="at">lambda =</span> <span class="dv">1</span>)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="do">##                  lcavol     lweight         age        lbph         svi         lcp     gleason </span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  0.14716982  0.55209405  0.61998311 -0.02049376  0.09488234  0.74846397 -0.09399009  0.05227074 </span></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="do">##       pgg45 </span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a><span class="do">##  0.00424397</span></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># using our own code</span></span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="fu">data.matrix</span>(prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]))</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> prostate[, <span class="dv">9</span>]</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> <span class="fu">diag</span>(<span class="dv">9</span>)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a><span class="do">##                [,1]</span></span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a><span class="do">##          0.07941225</span></span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a><span class="do">## lcavol   0.55985143</span></span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a><span class="do">## lweight  0.60398302</span></span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a><span class="do">## age     -0.01957258</span></span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a><span class="do">## lbph     0.09395770</span></span>
<span id="cb75-18"><a href="#cb75-18" aria-hidden="true" tabindex="-1"></a><span class="do">## svi      0.68809341</span></span>
<span id="cb75-19"><a href="#cb75-19" aria-hidden="true" tabindex="-1"></a><span class="do">## lcp     -0.08863685</span></span>
<span id="cb75-20"><a href="#cb75-20" aria-hidden="true" tabindex="-1"></a><span class="do">## gleason  0.06288206</span></span>
<span id="cb75-21"><a href="#cb75-21" aria-hidden="true" tabindex="-1"></a><span class="do">## pgg45    0.00416878</span></span></code></pre></div>
<p>However, they look different. This is because ridge regression has a scaling issue: it would shrink parameters differently if the corresponding covariates have different scales. This can be seen from our previous development of the SVD analysis. Since the shrinkage is the same for all <span class="math inline">\(d_j\)</span>s, it would apply a larger shrinkage for small <span class="math inline">\(d_j\)</span>. A commonly used approach to deal with the scaling issue is to <strong>standardize all covariates</strong> such that they are treated the same way. In addition, we will also <strong>center both <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span></strong> before performing the ridge regression. An interesting consequence of centering is that we do not need the intercept anymore, since <span class="math inline">\(\mathbf{X}\boldsymbol \beta= \mathbf{0}\)</span> for all <span class="math inline">\(\boldsymbol \beta\)</span>. One last point is that when performing scaling, <code>lm.ridge()</code> use the <span class="math inline">\(n\)</span> factor instead of <span class="math inline">\(n-1\)</span> when calculating the standard deviation. Hence, incorporating all these, we have</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># perform centering and scaling</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">scale</span>(<span class="fu">data.matrix</span>(prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]), <span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># use n instead of (n-1) for standardization</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="fu">nrow</span>(X)</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> X <span class="sc">*</span> <span class="fu">sqrt</span>(n <span class="sc">/</span> (n<span class="dv">-1</span>))</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># center y but not scaling</span></span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">scale</span>(prostate[, <span class="dv">9</span>], <span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># getting the estimated parameter</span></span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>  mybeta <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> <span class="fu">diag</span>(<span class="dv">8</span>)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>  ridge.fit <span class="ot">=</span> <span class="fu">lm.ridge</span>(lpsa <span class="sc">~</span>., <span class="at">data =</span> prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>], <span class="at">lambda =</span> <span class="dv">1</span>)</span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># note that $coef obtains the coefficients internally from lm.ridge</span></span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># however coef() would transform these back to the original scale version</span></span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(mybeta, ridge.fit<span class="sc">$</span>coef)</span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a><span class="do">##                [,1]        [,2]</span></span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a><span class="do">## lcavol   0.64734891  0.64734891</span></span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a><span class="do">## lweight  0.26423507  0.26423507</span></span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a><span class="do">## age     -0.15178989 -0.15178989</span></span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a><span class="do">## lbph     0.13694453  0.13694453</span></span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a><span class="do">## svi      0.30825889  0.30825889</span></span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a><span class="do">## lcp     -0.13074243 -0.13074243</span></span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a><span class="do">## gleason  0.03755141  0.03755141</span></span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a><span class="do">## pgg45    0.11907848  0.11907848</span></span></code></pre></div>
</div>
<div id="multiple-lambda-values" class="section level3 hasAnchor" number="6.5.2">
<h3 class="hasAnchor"><span class="header-section-number">6.5.2</span> Multiple <span class="math inline">\(\lambda\)</span> values<a href="#multiple-lambda-values" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since we now face the problem of bias-variance trade-off, we can fit the model with multiple <span class="math inline">\(\lambda\)</span> values and select the best. This can be done using the following code.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(MASS)</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">=</span> <span class="fu">lm.ridge</span>(lpsa<span class="sc">~</span>.,  <span class="at">data =</span> prostate[, <span class="sc">-</span><span class="dv">10</span>], <span class="at">lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">100</span>, <span class="at">by=</span><span class="fl">0.2</span>))</span></code></pre></div>
<p>For each <span class="math inline">\(\lambda\)</span>, the coefficients of all variables are recorded. The plot shows how these coefficients change as a function of <span class="math inline">\(\lambda\)</span>. We can easily see that as <span class="math inline">\(\lambda\)</span> becomes larger, the coefficients are shrunken towards 0. This is consistent with our understanding of the bias. On the very left hand size of the plot, the value of each parameter corresponds to the OLS result since no penalty is applied. Be careful that the coefficients of the fitted objects <code>fit$coef</code> are scaled by the standard deviation of the covariates. If you need the original scale, make sure to use <code>coef(fit)</code>.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">matplot</span>(<span class="fu">coef</span>(fit)[, <span class="sc">-</span><span class="dv">1</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Lambda&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Coefficients&quot;</span>)</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">text</span>(<span class="fu">rep</span>(<span class="dv">50</span>, <span class="dv">8</span>), <span class="fu">coef</span>(fit)[<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>], <span class="fu">colnames</span>(prostate)[<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>])</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Prostate Cancer Data: Ridge Coefficients&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-97-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>To select the best <span class="math inline">\(\lambda\)</span> value, there can be several different methods. We will discuss two approaches among them: <span class="math inline">\(k\)</span>-fold cross-validation and generalized cross-validation (GCV).</p>
</div>
</div>
<div id="cross-validation" class="section level2 hasAnchor" number="6.6">
<h2 class="hasAnchor"><span class="header-section-number">6.6</span> Cross-validation<a href="#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Cross-validation (CV) is a technique to evaluate the performance of a model on an independent set of data. The essential idea is to separate out a subset of the data and do not use that part during the training, while using it for testing. We can then rotate to or sample a different subset as the testing data. Different cross-validation methods differs on the mechanisms of generating such testing data. <strong><span class="math inline">\(K\)</span>-fold cross-validation</strong> is probably the the most popular among them. The method works in the following steps:</p>
<ol style="list-style-type: decimal">
<li>Randomly split the data into <span class="math inline">\(K\)</span> equal portions</li>
<li>For each <span class="math inline">\(k\)</span> in <span class="math inline">\(1, \ldots, K\)</span>: use the <span class="math inline">\(k\)</span>th portion as the testing data and the rest as training data, obtain the testing error</li>
<li>Average all <span class="math inline">\(K\)</span> testing errors</li>
</ol>
<p>Here is a graphical demonstration of a <span class="math inline">\(10\)</span>-fold CV:</p>
<center>
<img src="images/kfoldcv.png" style="width:80.0%" />
</center>
<p>There are also many other cross-validation procedures, for example, the <strong>Monte Carlo cross-validation</strong> randomly splits the data into training and testing (instead of fix <span class="math inline">\(K\)</span> portions) each time and repeat the process as many times as we like. The benefit of such procedure is that if this is repeated enough times, the estimated testing error becomes fairly stable, and not affected much by the random mechanism. On the other hand, we can also repeat the entire <span class="math inline">\(K\)</span>-fold CV process many times, then average the errors. This is also trying to reduced the influence of randomness.</p>
</div>
<div id="leave-one-out-cross-validation" class="section level2 hasAnchor" number="6.7">
<h2 class="hasAnchor"><span class="header-section-number">6.7</span> Leave-one-out cross-validation<a href="#leave-one-out-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Regarding the randomness, the leave-one-out cross-validation is completely nonrandom. It is essentially the <span class="math inline">\(k\)</span>-fold CV approach, but with <span class="math inline">\(k\)</span> equal to <span class="math inline">\(n\)</span>, the sample size. A standard approach would require to re-fit the model <span class="math inline">\(n\)</span> times, however, some linear algebra can show that there is an equivalent form using the “Hat” matrix when fitting a linear regression:</p>
<p><span class="math display">\[\begin{align}
\text{CV}(n) =&amp; \frac{1}{n}\sum_{i=1}^n (y_i - \widehat{y}_{[i]})^2\\
=&amp; \frac{1}{n} \sum_{i=1}^n \left( \frac{y_i - \widehat{y}_i}{1 - \mathbf{H}_{ii}} \right)^2,
\end{align}\]</span>
where <span class="math inline">\(\widehat{y}_{i}\)</span> is the fitted value using the whole dataset, but <span class="math inline">\(\widehat{y}_{[i]}\)</span> is the prediction of <span class="math inline">\(i\)</span>th observation using the data without it when fitting the model. And <span class="math inline">\(\mathbf{H}_{ii}\)</span> is the <span class="math inline">\(i\)</span>th diagonal element of the hat matrix <span class="math inline">\(\mathbf{H}= \mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}\)</span>. The proof is essentially an application of the <a href="https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula">Sherman–Morrison–Woodbury (SMW)</a> formula, which is also used when deriving the rank-one update of a quasi-Newton optimization approach.</p>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>Denote <span class="math inline">\(\mathbf{X}_{[i]}\)</span> and <span class="math inline">\(\mathbf{y}_{[i]}\)</span> the data derived from <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, but with the <span class="math inline">\(i\)</span> observation (<span class="math inline">\(x_i\)</span>, <span class="math inline">\(y_i\)</span>) removed. We then have the properties that</p>
<p><span class="math display">\[\mathbf{X}_{[i]}^\text{T}\mathbf{X}_{[i]} = \mathbf{X}^\text{T}\mathbf{X}- x_i x_i^\text{T}, \]</span></p>
<p>and</p>
<p><span class="math display">\[\mathbf{H}_{ii} = x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i.\]</span></p>
<p>By the SMW formula, we have</p>
<p><span class="math display">\[(\mathbf{X}_{[i]}^\text{T}\mathbf{X}_{[i]})^{-1} = (\mathbf{X}^\text{T}\mathbf{X})^{-1} + \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1}x_i x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1}}{ 1 - \mathbf{H}_{ii}}, \]</span></p>
<p>Further notice that</p>
<p><span class="math display">\[\mathbf{X}_{[i]}^\text{T}\mathbf{y}_{[i]} = \mathbf{X}^\text{T}\mathbf{y}- x_i y_i, \]</span></p>
<p>we can then reconstruct the fitted parameter when observation <span class="math inline">\(i\)</span> is removed:</p>
<p><span class="math display">\[\begin{align}
\widehat{\boldsymbol \beta}_{[i]} =&amp; (\mathbf{X}_{[i]}^\text{T}\mathbf{X}_{[i]})^{-1} \mathbf{X}_{[i]}^\text{T}\mathbf{y}_{[i]} \\
=&amp; \left[ (\mathbf{X}^\text{T}\mathbf{X})^{-1} + \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1}x_i x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1}}{ 1 - \mathbf{H}_{ii}} \right] (\mathbf{X}^\text{T}\mathbf{y}- x_i y_i)\\
=&amp; (\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}\mathbf{y}+ \left[ - (\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i y_i +  \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1}x_i x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1}}{ 1 - \mathbf{H}_{ii}} (\mathbf{X}^\text{T}\mathbf{y}- x_i y_i) \right] \\
=&amp; \widehat{\boldsymbol \beta} - \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i}{1 - \mathbf{H}_{ii}} \left[ y_i (1 - \mathbf{H}_{ii}) - x_i^\text{T}\widehat{\boldsymbol \beta} + \mathbf{H}_{ii} y_i \right]\\
=&amp; \widehat{\boldsymbol \beta} - \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i}{1 - \mathbf{H}_{ii}} \left( y_i - x_i^\text{T}\widehat{\boldsymbol \beta} \right)
\end{align}\]</span></p>
<p>Then the error of the <span class="math inline">\(i\)</span>th obervation from the leave-one-out model is</p>
<p><span class="math display">\[\begin{align}
y _i - \widehat{y}_{[i]} =&amp; y _i - x_i^\text{T}\widehat{\boldsymbol \beta}_{[i]} \\
=&amp; y _i - x_i^\text{T}\left[ \widehat{\boldsymbol \beta} - \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i}{1 - \mathbf{H}_{ii}} \left( y_i - x_i^\text{T}\widehat{\boldsymbol \beta} \right)  \right]\\
=&amp; y _i - x_i^\text{T}\widehat{\boldsymbol \beta} + \frac{x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i}{1 - \mathbf{H}_{ii}} \left( y_i - x_i^\text{T}\widehat{\boldsymbol \beta} \right)\\
=&amp; y _i - x_i^\text{T}\widehat{\boldsymbol \beta} + \frac{\mathbf{H}_{ii}}{1 - \mathbf{H}_{ii}} \left( y_i - x_i^\text{T}\widehat{\boldsymbol \beta} \right)\\
=&amp; \frac{y_i - x_i^\text{T}\widehat{\boldsymbol \beta}}{1 - \mathbf{H}_{ii}}
\end{align}\]</span></p>
<p>This completes the proof.</p>
</div>
<div id="generalized-cross-validation" class="section level3 hasAnchor" number="6.7.1">
<h3 class="hasAnchor"><span class="header-section-number">6.7.1</span> Generalized cross-validation<a href="#generalized-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The generalized cross-validation (GCV, <span class="citation">Golub, Heath, and Wahba (<a href="#ref-golub1979generalized" role="doc-biblioref">1979</a>)</span>) is a modified version of the leave-one-out CV:</p>
<p><span class="math display">\[\text{GCV}(\lambda) = \frac{\sum_{i=1}^n (y_i - x_i^\text{T}\widehat{\boldsymbol \beta}^\text{ridge}_\lambda)}{(n - \text{Trace}(\mathbf{S}_\lambda))}\]</span>
where <span class="math inline">\(\mathbf{S}_\lambda\)</span> is the hat matrix corresponding to the ridge regression:</p>
<p><span class="math display">\[\mathbf{S}_\lambda = \mathbf{X}(\mathbf{X}^\text{T}\mathbf{X}+ \lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\]</span></p>
<p>The following plot shows how GCV value changes as a function of <span class="math inline">\(\lambda\)</span>.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use GCV to select the best lambda</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(fit<span class="sc">$</span>lambda[<span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>], fit<span class="sc">$</span>GCV[<span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, </span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">ylab =</span> <span class="st">&quot;GCV&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Lambda&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Prostate Cancer Data: GCV&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-99-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We can select the best <span class="math inline">\(\lambda\)</span> that produces the smallest GCV.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>    fit<span class="sc">$</span>lambda[<span class="fu">which.min</span>(fit<span class="sc">$</span>GCV)]</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 6.8</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">coef</span>(fit)[<span class="fu">which.min</span>(fit<span class="sc">$</span>GCV), ], <span class="dv">4</span>)</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a><span class="do">##          lcavol lweight     age    lbph     svi     lcp gleason   pgg45 </span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a><span class="do">##  0.0170  0.4949  0.6050 -0.0169  0.0863  0.6885 -0.0420  0.0634  0.0034</span></span></code></pre></div>
</div>
</div>
<div id="the-glmnet-package" class="section level2 hasAnchor" number="6.8">
<h2 class="hasAnchor"><span class="header-section-number">6.8</span> The <code>glmnet</code> package<a href="#the-glmnet-package" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <code>glmnet</code> package implements the <span class="math inline">\(k\)</span>-fold cross-validation. To perform a ridge regression with cross-validation, we need to use the <code>cv.glmnet()</code> function with <span class="math inline">\(alpha = 0\)</span>. Here, the <span class="math inline">\(\alpha\)</span> is a parameter that controls the <span class="math inline">\(\ell_2\)</span> and <span class="math inline">\(\ell_1\)</span> (Lasso) penalties. In addition, the lambda values are also automatically selected, on the log-scale.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(glmnet)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Loading required package: Matrix</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: package &#39;Matrix&#39; was built under R version 4.2.2</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Loaded glmnet 4.1-4</span></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">3</span>)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>  fit2 <span class="ot">=</span> <span class="fu">cv.glmnet</span>(<span class="fu">data.matrix</span>(prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]), prostate<span class="sc">$</span>lpsa, <span class="at">nfolds =</span> <span class="dv">10</span>, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(fit2<span class="sc">$</span>glmnet.fit, <span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-101-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>It is useful to plot the cross-validation error against the <span class="math inline">\(\lambda\)</span> values , then select the corresponding <span class="math inline">\(\lambda\)</span> with the smallest error. The corresponding coefficient values can be obtained using the <code>s = "lambda.min"</code> option in the <code>coef()</code> function. However, this can still be subject to over-fitting, and sometimes practitioners use <code>s = "lambda.1se"</code> to select a slightly heavier penalized version based on the variations observed from different folds.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(fit2)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-102-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>(fit2, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot;</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="do">##                       s1</span></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  0.011566731</span></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="do">## lcavol       0.492211875</span></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="do">## lweight      0.604155671</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="do">## age         -0.016727236</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a><span class="do">## lbph         0.085820464</span></span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a><span class="do">## svi          0.685477646</span></span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a><span class="do">## lcp         -0.039717080</span></span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a><span class="do">## gleason      0.063806235</span></span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a><span class="do">## pgg45        0.003411982</span></span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>(fit2, <span class="at">s =</span> <span class="st">&quot;lambda.1se&quot;</span>)</span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot;</span></span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a><span class="do">##                       s1</span></span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  0.035381749</span></span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a><span class="do">## lcavol       0.264613825</span></span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a><span class="do">## lweight      0.421408730</span></span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a><span class="do">## age         -0.002555681</span></span>
<span id="cb83-20"><a href="#cb83-20" aria-hidden="true" tabindex="-1"></a><span class="do">## lbph         0.049916919</span></span>
<span id="cb83-21"><a href="#cb83-21" aria-hidden="true" tabindex="-1"></a><span class="do">## svi          0.452500472</span></span>
<span id="cb83-22"><a href="#cb83-22" aria-hidden="true" tabindex="-1"></a><span class="do">## lcp          0.075346975</span></span>
<span id="cb83-23"><a href="#cb83-23" aria-hidden="true" tabindex="-1"></a><span class="do">## gleason      0.083894617</span></span>
<span id="cb83-24"><a href="#cb83-24" aria-hidden="true" tabindex="-1"></a><span class="do">## pgg45        0.002615235</span></span></code></pre></div>
<div id="scaling-issue-1" class="section level3 hasAnchor" number="6.8.1">
<h3 class="hasAnchor"><span class="header-section-number">6.8.1</span> Scaling Issue<a href="#scaling-issue-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>glmnet</code> package would using the same strategies for scaling: center and standardize <span class="math inline">\(\mathbf{X}\)</span> and center <span class="math inline">\(\mathbf{y}\)</span>. A slight difference is that it considers using <span class="math inline">\(1/(2n)\)</span> as the normalizing factor of the residual sum of squares, but also uses <span class="math inline">\(\lambda/2 \lVert \boldsymbol \beta\rVert_2^2\)</span> as the penalty. This does not affect our formulation since the <span class="math inline">\(1/2\)</span> cancels out. However, it would slightly affect the Lasso formulation introduced in the next Chapter since the <span class="math inline">\(\ell_1\)</span> penalty does not apply this <span class="math inline">\(1/2\)</span> factor. Nonetheless, we can check the (nearly) equivalence between <code>lm.ridge</code> and <code>glmnet()</code>:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">scale</span>(<span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p), n, p)))</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">scale</span>(X[, <span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>]<span class="sc">*</span><span class="fl">0.5</span> <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.5</span>)))</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>  lam <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="fl">0.1</span>)</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>  fit1 <span class="ot">&lt;-</span> <span class="fu">lm.ridge</span>(y <span class="sc">~</span> X, <span class="at">lambda =</span> lam)</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>  fit2 <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lam <span class="sc">/</span> <span class="fu">nrow</span>(X))</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the estimated parameters</span></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">matplot</span>(<span class="fu">apply</span>(<span class="fu">coef</span>(fit1), <span class="dv">2</span>, rev), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">main =</span> <span class="st">&quot;lm.ridge&quot;</span>)</span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">matplot</span>(<span class="fu">t</span>(<span class="fu">as.matrix</span>(<span class="fu">coef</span>(fit2))), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">main =</span> <span class="st">&quot;glmnet&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-103-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Check differences</span></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">max</span>(<span class="fu">abs</span>(<span class="fu">apply</span>(<span class="fu">coef</span>(fit1), <span class="dv">2</span>, rev) <span class="sc">-</span> <span class="fu">t</span>(<span class="fu">as.matrix</span>(<span class="fu">coef</span>(fit2)))))</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.0009968625</span></span></code></pre></div>
<!--chapter:end:02.2-ridge.Rmd-->
</div>
</div>
</div>
<div id="lasso" class="section level1 hasAnchor" number="7">
<h1 class="hasAnchor"><span class="header-section-number">7</span> Lasso<a href="#lasso" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Lasso <span class="citation">(<a href="#ref-tibshirani1996regression" role="doc-biblioref">Tibshirani 1996</a>)</span> is among the most popular machine learning models. Different from the Ridge regression, its adds <span class="math inline">\(\ell_1\)</span> penalty on the fitted parameters:</p>
<p><span class="math display">\[\begin{align}
\widehat{\boldsymbol \beta}^\text{lasso} =&amp; \mathop{\mathrm{arg\,min}}_{\boldsymbol \beta} (\mathbf{y}- \mathbf{X}\boldsymbol \beta)^\text{T}(\mathbf{y}- \mathbf{X}\boldsymbol \beta) + n \lambda \lVert\boldsymbol \beta\rVert_1\\
=&amp; \mathop{\mathrm{arg\,min}}_{\boldsymbol \beta} \frac{1}{n} \sum_{i=1}^n (y_i - x_i^\text{T}\boldsymbol \beta)^2 + \lambda \sum_{i=1}^p |\beta_j|,
\end{align}\]</span></p>
<p>The main advantage of adding such a penalty is that small <span class="math inline">\(\widehat{\beta}_j\)</span> values can be <strong>shrunk to zero</strong>. This may prevents over-fitting and also improve the interpretability especially when the number of variables is large. We will analyze the Lasso starting with a single variable case, and then discuss the application of coordinate descent algorithm to obtain the solution.</p>
<div id="one-variable-lasso-and-shrinkage" class="section level2 hasAnchor" number="7.1">
<h2 class="hasAnchor"><span class="header-section-number">7.1</span> One-Variable Lasso and Shrinkage<a href="#one-variable-lasso-and-shrinkage" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To illustrate how Lasso shrink a parameter estimate to zero, let’s consider an orthogonal design matrix case, i.e., <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}= n \mathbf{I}\)</span>, which will eventually reduce to a one-variable problem. Note that the intercept term is not essential because we can always pre-center the observed data <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span>s so that they can be recovered after this one variable problem. Our objective function is</p>
<p><span class="math display">\[\frac{1}{n}\lVert \mathbf{y}- \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert\boldsymbol \beta\rVert_1\]</span>
We are going to relate the solution the OLS solution, which exists in this case because <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span> is invertible. Hence, we have</p>
<p><span class="math display">\[\begin{align}
&amp;\frac{1}{n}\lVert \mathbf{y}- \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert\boldsymbol \beta\rVert_1\\
=&amp;\frac{1}{n}\lVert \mathbf{y}- \color{OrangeRed}{\mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} + \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols}} - \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert\boldsymbol \beta\rVert_1\\
=&amp;\frac{1}{n}\lVert \mathbf{y}- \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} \rVert^2 + \frac{1}{n} \lVert \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} - \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert\boldsymbol \beta\rVert_1
\end{align}\]</span></p>
<p>The cross-term is zero because the OLS residual term is orthogonal to the columns of <span class="math inline">\(\mathbf{X}\)</span>:</p>
<p><span class="math display">\[\begin{align}
&amp;2(\mathbf{y}- \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols})^\text{T}(\mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} - \mathbf{X}\boldsymbol \beta)\\
=&amp; 2\mathbf{r}^\text{T}\mathbf{X}(\widehat{\boldsymbol \beta}^\text{ols} - \boldsymbol \beta)\\
=&amp; 0
\end{align}\]</span></p>
<p>Then we just need to optimize the part that involves <span class="math inline">\(\boldsymbol \beta\)</span>:</p>
<p><span class="math display">\[\begin{align}
&amp;\underset{\boldsymbol \beta}{\mathop{\mathrm{arg\,min}}} \frac{1}{n}\lVert \mathbf{y}- \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} \rVert^2 + \frac{1}{n} \lVert \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} - \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert\boldsymbol \beta\rVert_1\\
=&amp;\underset{\boldsymbol \beta}{\mathop{\mathrm{arg\,min}}} \frac{1}{n} \lVert \mathbf{X}\widehat{\boldsymbol \beta}^\text{ols} - \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert\boldsymbol \beta\rVert_1\\
=&amp;\underset{\boldsymbol \beta}{\mathop{\mathrm{arg\,min}}} \frac{1}{n} (\widehat{\boldsymbol \beta}^\text{ols} - \boldsymbol \beta)^\text{T}\mathbf{X}^\text{T}\mathbf{X}(\widehat{\boldsymbol \beta}^\text{ols} - \boldsymbol \beta)  + \lambda \lVert\boldsymbol \beta\rVert_1\\
=&amp;\underset{\boldsymbol \beta}{\mathop{\mathrm{arg\,min}}} \frac{1}{n} (\widehat{\boldsymbol \beta}^\text{ols} - \boldsymbol \beta)^\text{T}n \mathbf{I}(\widehat{\boldsymbol \beta}^\text{ols} - \boldsymbol \beta)  + \lambda \lVert\boldsymbol \beta\rVert_1\\
=&amp;\underset{\boldsymbol \beta}{\mathop{\mathrm{arg\,min}}} \sum_{j = 1}^p (\widehat{\boldsymbol \beta}^\text{ols}_j - \boldsymbol \beta_j )^2 + \lambda \sum_j |\boldsymbol \beta_j|\\
\end{align}\]</span></p>
<p>This is a separable problem meaning that we can solve each <span class="math inline">\(\beta_j\)</span> independently since they do not interfere each other. Then the univariate problem is</p>
<p><span class="math display">\[\underset{\beta}{\mathop{\mathrm{arg\,min}}} \,\, (\beta - a)^2 + \lambda |\beta|\]</span>
We learned that to solve for an optimizer, we can set the gradient to be zero. However, the function is not everywhere differentiable. Still, we can separate this into two cases: <span class="math inline">\(\beta &gt; 0\)</span> and <span class="math inline">\(\beta &lt; 0\)</span>. For the positive side, we have</p>
<p><span class="math display">\[\begin{align}
0 =&amp; \frac{\partial}{\partial \beta} \,\, (\beta - a)^2 + \lambda |\beta| = 2 (\beta - a) + \lambda \\
\Longrightarrow \quad \beta =&amp;\, a - \lambda/2
\end{align}\]</span></p>
<p>However, this will maintain positive only when <span class="math inline">\(\beta\)</span> is greater than <span class="math inline">\(a - \lambda/2\)</span>. The negative size is similar. And whenever <span class="math inline">\(\beta\)</span> falls in between, it will be shrunk to zero. Overall, for our previous univariate optimization problem, the solution is</p>
<p><span class="math display">\[\begin{align}
\hat\beta_j^\text{lasso} &amp;=
        \begin{cases}
        \hat\beta_j^\text{ols} - \lambda/2 &amp; \text{if} \quad \hat\beta_j^\text{ols} &gt; \lambda/2 \\
        0 &amp; \text{if} \quad |\hat\beta_j^\text{ols}| &lt; \lambda/2 \\
        \hat\beta_j^\text{ols} + \lambda/2 &amp; \text{if} \quad \hat\beta_j^\text{ols} &lt; -\lambda/2 \\
        \end{cases}\\
        &amp;= \text{sign}(\hat\beta_j^\text{ols}) \left(|\hat\beta_j^\text{ols}| - \lambda/2 \right)_+ \\
        &amp;\doteq \text{SoftTH}(\hat\beta_j^\text{ols}, \lambda)
\end{align}\]</span></p>
<p>This is called a <strong>soft-thresholding function</strong>. This implies that when <span class="math inline">\(\lambda\)</span> is large enough, the estimated <span class="math inline">\(\beta\)</span> parameter of Lasso will be shrunk towards zero. The following animated figure demonstrates how adding an <span class="math inline">\(\ell_1\)</span> penalty can change the optimizer. The objective function is <span class="math inline">\(0.5 + (\beta - 1)^2\)</span> and based on our previous analysis, once the penalty is larger than 2, the optimizer would stay at 0.</p>
<div id="htmlwidget-4f2bc96119f020fb83b4" style="width:100%;height:576px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-4f2bc96119f020fb83b4">{"x":{"data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 0.0","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 0.0","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 0.0","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 0.0","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 0.0","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 0.0","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 0.0","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 0.0","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 0.0","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 0.0","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 0.0","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 0.0","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 0.0","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 0.0","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 0.0","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 0.0","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 0.0","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 0.0","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 0.0","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 0.0","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 0.0","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 0.0","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 0.0","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 0.0","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 0.0","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 0.0","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 0.0","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 0.0","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 0.0","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 0.0","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 0.0","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 0.0","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 0.0","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 0.0","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 0.0","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 0.0","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 0.0","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 0.0","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 0.0","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 0.0","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 0.0","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 0.0","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 0.0","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 0.0","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 0.0","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 0.0","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 0.0","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 0.0","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 0.0","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 0.0","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.0","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.0","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.0","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.0","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.0","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.0","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.0","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.0","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.0","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.0","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.0","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.0","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.0","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.0","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.0","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.0","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.0","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.0","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.0","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.0","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.0","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.0","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.0","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.0","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.0","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.0","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.0","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.0","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.0","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.0","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.0","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.0","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.0","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.0","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.0","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.0","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.0","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.0","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.0","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.0","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.0","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.0","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.0","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.0","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.0","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.0","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.0","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.0","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.0","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.0","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.0","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.0","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.0","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.0","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.0","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.0","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.0","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.0","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.0","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.0","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.0","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.0","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.0","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.0","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.0","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.0","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.0","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.0","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.0","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.0","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.0","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.0","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.0","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.0","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.0","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.0","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.0","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.0","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.0","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.0","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.0","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.0","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.0","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.0","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.0","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.0","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.0","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.0","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.0","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.0","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.0","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.0","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.0","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.0","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.0","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.0","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.0","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.0","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.0","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.0","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 0.0","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.0","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.0","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.0","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.0","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.0","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.0","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.0","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.0","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.0","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.0","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.0","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.0","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.0","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.0","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.0","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.0","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.0","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.0","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.0","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.0","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.0","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.0","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.0","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.0","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.0","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.0","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.0","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.0","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.0","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.0","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.0","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.0","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.0","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.0","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.0","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.0","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.0","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.0","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.0","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.0","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.0","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.0","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.0","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.0","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.0","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.0","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.0","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.0","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.0","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.0","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.0","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.0","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.0","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.0","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.0","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.0","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.0","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.0","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.0","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.0","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.0","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.0","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.0","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.0","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.0","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.0","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.0","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.0","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.0","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.0","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.0","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.0","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.0","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.0","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.0","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.0","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.0","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.0","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.0","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.0","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.0","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.0","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.0","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.0","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.0","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.0","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.0","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.0","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.0","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.0","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.0","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.0","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.0","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.0","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.0","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.0","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.0","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.0","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.0","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.0"],"frame":"0","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.49<br />value: 2.7201<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.48<br />value: 2.6904<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.47<br />value: 2.6609<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.46<br />value: 2.6316<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.45<br />value: 2.6025<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.44<br />value: 2.5736<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.43<br />value: 2.5449<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.42<br />value: 2.5164<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.41<br />value: 2.4881<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.40<br />value: 2.4600<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.39<br />value: 2.4321<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.38<br />value: 2.4044<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.37<br />value: 2.3769<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.36<br />value: 2.3496<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.35<br />value: 2.3225<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.34<br />value: 2.2956<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.33<br />value: 2.2689<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.32<br />value: 2.2424<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.31<br />value: 2.2161<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.30<br />value: 2.1900<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.29<br />value: 2.1641<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.28<br />value: 2.1384<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.27<br />value: 2.1129<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.26<br />value: 2.0876<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.25<br />value: 2.0625<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.24<br />value: 2.0376<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.23<br />value: 2.0129<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.22<br />value: 1.9884<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.21<br />value: 1.9641<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.20<br />value: 1.9400<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.19<br />value: 1.9161<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.18<br />value: 1.8924<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.17<br />value: 1.8689<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.16<br />value: 1.8456<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.15<br />value: 1.8225<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.14<br />value: 1.7996<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.13<br />value: 1.7769<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.12<br />value: 1.7544<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.11<br />value: 1.7321<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.10<br />value: 1.7100<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.09<br />value: 1.6881<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.08<br />value: 1.6664<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.07<br />value: 1.6449<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.06<br />value: 1.6236<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.05<br />value: 1.6025<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.04<br />value: 1.5816<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.03<br />value: 1.5609<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.02<br />value: 1.5404<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.01<br />value: 1.5201<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.01<br />value: 1.4801<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.02<br />value: 1.4604<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.03<br />value: 1.4409<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.04<br />value: 1.4216<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.05<br />value: 1.4025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.06<br />value: 1.3836<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.07<br />value: 1.3649<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.08<br />value: 1.3464<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.09<br />value: 1.3281<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.10<br />value: 1.3100<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.11<br />value: 1.2921<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.12<br />value: 1.2744<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.13<br />value: 1.2569<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.14<br />value: 1.2396<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.15<br />value: 1.2225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.16<br />value: 1.2056<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.17<br />value: 1.1889<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.18<br />value: 1.1724<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.19<br />value: 1.1561<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.20<br />value: 1.1400<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.21<br />value: 1.1241<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.22<br />value: 1.1084<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.23<br />value: 1.0929<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.24<br />value: 1.0776<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.25<br />value: 1.0625<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.26<br />value: 1.0476<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.27<br />value: 1.0329<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.28<br />value: 1.0184<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.29<br />value: 1.0041<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.30<br />value: 0.9900<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.31<br />value: 0.9761<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.32<br />value: 0.9624<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.33<br />value: 0.9489<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.34<br />value: 0.9356<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.35<br />value: 0.9225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.36<br />value: 0.9096<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.37<br />value: 0.8969<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.38<br />value: 0.8844<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.39<br />value: 0.8721<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.40<br />value: 0.8600<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.41<br />value: 0.8481<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.42<br />value: 0.8364<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.43<br />value: 0.8249<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.44<br />value: 0.8136<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.45<br />value: 0.8025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.46<br />value: 0.7916<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.47<br />value: 0.7809<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.48<br />value: 0.7704<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.49<br />value: 0.7601<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.50<br />value: 0.7500<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.51<br />value: 0.7401<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.52<br />value: 0.7304<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.53<br />value: 0.7209<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.54<br />value: 0.7116<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.55<br />value: 0.7025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.56<br />value: 0.6936<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.57<br />value: 0.6849<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.58<br />value: 0.6764<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.59<br />value: 0.6681<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.60<br />value: 0.6600<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.61<br />value: 0.6521<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.62<br />value: 0.6444<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.63<br />value: 0.6369<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.64<br />value: 0.6296<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.65<br />value: 0.6225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.66<br />value: 0.6156<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.67<br />value: 0.6089<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.68<br />value: 0.6024<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.69<br />value: 0.5961<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.70<br />value: 0.5900<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.71<br />value: 0.5841<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.72<br />value: 0.5784<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.73<br />value: 0.5729<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.74<br />value: 0.5676<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.75<br />value: 0.5625<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.76<br />value: 0.5576<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.77<br />value: 0.5529<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.78<br />value: 0.5484<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.79<br />value: 0.5441<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.80<br />value: 0.5400<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.81<br />value: 0.5361<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.82<br />value: 0.5324<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.83<br />value: 0.5289<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.84<br />value: 0.5256<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.85<br />value: 0.5225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.86<br />value: 0.5196<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.87<br />value: 0.5169<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.88<br />value: 0.5144<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.89<br />value: 0.5121<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.90<br />value: 0.5100<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.91<br />value: 0.5081<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.92<br />value: 0.5064<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.93<br />value: 0.5049<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.94<br />value: 0.5036<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.95<br />value: 0.5025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.96<br />value: 0.5016<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.97<br />value: 0.5009<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.98<br />value: 0.5004<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.99<br />value: 0.5001<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.00<br />value: 0.5000<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.01<br />value: 0.5001<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.02<br />value: 0.5004<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.03<br />value: 0.5009<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.04<br />value: 0.5016<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.05<br />value: 0.5025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.06<br />value: 0.5036<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.07<br />value: 0.5049<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.08<br />value: 0.5064<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.09<br />value: 0.5081<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.10<br />value: 0.5100<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.11<br />value: 0.5121<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.12<br />value: 0.5144<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.13<br />value: 0.5169<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.14<br />value: 0.5196<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.15<br />value: 0.5225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.16<br />value: 0.5256<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.17<br />value: 0.5289<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.18<br />value: 0.5324<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.19<br />value: 0.5361<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.20<br />value: 0.5400<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.21<br />value: 0.5441<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.22<br />value: 0.5484<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.23<br />value: 0.5529<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.24<br />value: 0.5576<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.25<br />value: 0.5625<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.26<br />value: 0.5676<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.27<br />value: 0.5729<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.28<br />value: 0.5784<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.29<br />value: 0.5841<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.30<br />value: 0.5900<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.31<br />value: 0.5961<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.32<br />value: 0.6024<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.33<br />value: 0.6089<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.34<br />value: 0.6156<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.35<br />value: 0.6225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.36<br />value: 0.6296<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.37<br />value: 0.6369<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.38<br />value: 0.6444<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.39<br />value: 0.6521<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.40<br />value: 0.6600<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.41<br />value: 0.6681<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.42<br />value: 0.6764<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.43<br />value: 0.6849<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.44<br />value: 0.6936<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.45<br />value: 0.7025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.46<br />value: 0.7116<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.47<br />value: 0.7209<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.48<br />value: 0.7304<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.49<br />value: 0.7401<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.50<br />value: 0.7500<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.51<br />value: 0.7601<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.52<br />value: 0.7704<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.53<br />value: 0.7809<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.54<br />value: 0.7916<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.55<br />value: 0.8025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.56<br />value: 0.8136<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.57<br />value: 0.8249<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.58<br />value: 0.8364<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.59<br />value: 0.8481<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.60<br />value: 0.8600<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.61<br />value: 0.8721<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.62<br />value: 0.8844<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.63<br />value: 0.8969<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.64<br />value: 0.9096<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.65<br />value: 0.9225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.66<br />value: 0.9356<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.67<br />value: 0.9489<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.68<br />value: 0.9624<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.69<br />value: 0.9761<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.70<br />value: 0.9900<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.71<br />value: 1.0041<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.72<br />value: 1.0184<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.73<br />value: 1.0329<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.74<br />value: 1.0476<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.75<br />value: 1.0625<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.76<br />value: 1.0776<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.77<br />value: 1.0929<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.78<br />value: 1.1084<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.79<br />value: 1.1241<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.80<br />value: 1.1400<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.81<br />value: 1.1561<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.82<br />value: 1.1724<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.83<br />value: 1.1889<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.84<br />value: 1.2056<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.85<br />value: 1.2225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.86<br />value: 1.2396<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.87<br />value: 1.2569<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.88<br />value: 1.2744<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.89<br />value: 1.2921<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.90<br />value: 1.3100<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.91<br />value: 1.3281<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.92<br />value: 1.3464<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.93<br />value: 1.3649<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.94<br />value: 1.3836<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.95<br />value: 1.4025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.96<br />value: 1.4216<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.97<br />value: 1.4409<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.98<br />value: 1.4604<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.99<br />value: 1.4801<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  2.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.0"],"frame":"0","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"text":["b: -0.50<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.49<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.48<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.47<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.46<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.45<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.44<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.43<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.42<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.41<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.40<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.39<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.38<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.37<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.36<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.35<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.34<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.33<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.32<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.31<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.30<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.29<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.28<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.27<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.26<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.25<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.24<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.23<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.22<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.21<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.20<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.19<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.18<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.17<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.16<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.15<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.14<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.13<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.12<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.11<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.10<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.09<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.08<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.07<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.06<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.05<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.04<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.03<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.02<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.01<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.01<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.02<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.03<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.04<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.05<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.06<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.07<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.08<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.09<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.10<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.11<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.12<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.13<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.14<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.15<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.16<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.17<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.18<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.19<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.20<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.21<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.22<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.23<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.24<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.25<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.26<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.27<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.28<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.29<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.30<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.31<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.32<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.33<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.34<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.35<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.36<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.37<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.38<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.39<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.40<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.41<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.42<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.43<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.44<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.45<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.46<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.47<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.48<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.49<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.50<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.51<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.52<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.53<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.54<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.55<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.56<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.57<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.58<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.59<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.60<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.61<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.62<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.63<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.64<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.65<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.66<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.67<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.68<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.69<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.70<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.71<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.72<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.73<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.74<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.75<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.76<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.77<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.78<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.79<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.80<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.81<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.82<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.83<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.84<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.85<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.86<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.87<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.88<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.89<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.90<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.91<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.92<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.93<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.94<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.95<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.96<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.97<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.98<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.99<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.01<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.02<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.03<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.04<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.05<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.06<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.07<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.08<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.09<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.10<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.11<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.12<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.13<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.14<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.15<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.16<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.17<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.18<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.19<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.20<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.21<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.22<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.23<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.24<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.25<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.26<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.27<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.28<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.29<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.30<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.31<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.32<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.33<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.34<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.35<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.36<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.37<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.38<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.39<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.40<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.41<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.42<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.43<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.44<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.45<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.46<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.47<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.48<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.49<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.50<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.51<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.52<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.53<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.54<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.55<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.56<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.57<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.58<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.59<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.60<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.61<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.62<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.63<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.64<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.65<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.66<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.67<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.68<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.69<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.70<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.71<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.72<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.73<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.74<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.75<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.76<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.77<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.78<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.79<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.80<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.81<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.82<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.83<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.84<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.85<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.86<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.87<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.88<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.89<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.90<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.91<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.92<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.93<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.94<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.95<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.96<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.97<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.98<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.99<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  2.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0"],"frame":"0","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"layout":{"margin":{"t":25.7412480974125,"r":7.30593607305936,"b":39.6955859969559,"l":31.4155251141553},"plot_bgcolor":"transparent","paper_bgcolor":"transparent","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"xaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-0.625,2.125],"tickmode":"array","ticktext":["-0.5","0.0","0.5","1.0","1.5","2.0"],"tickvals":[-0.5,0,0.5,1,1.5,2],"categoryorder":"array","categoryarray":["-0.5","0.0","0.5","1.0","1.5","2.0"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"y","title":{"text":"Beta","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":false,"range":[-0.2,4.2],"tickmode":"array","ticktext":["0","1","2","3","4"],"tickvals":[0,1,2,3,4],"categoryorder":"array","categoryarray":["0","1","2","3","4"],"nticks":null,"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(255,255,255,1)","gridwidth":0.66417600664176,"zeroline":false,"anchor":"x","title":{"text":"Function Value","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":true,"legend":{"bgcolor":"transparent","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895},"title":{"text":"Function","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187}}},"hovermode":"closest","barmode":"relative","sliders":[{"currentvalue":{"prefix":"~Lambda: ","xanchor":"right","font":{"size":16,"color":"rgba(204,204,204,1)"}},"steps":[{"method":"animate","args":[["0"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"0","value":"0"},{"method":"animate","args":[["0.1"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"0.1","value":"0.1"},{"method":"animate","args":[["0.2"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"0.2","value":"0.2"},{"method":"animate","args":[["0.3"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"0.3","value":"0.3"},{"method":"animate","args":[["0.4"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"0.4","value":"0.4"},{"method":"animate","args":[["0.5"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"0.5","value":"0.5"},{"method":"animate","args":[["0.6"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"0.6","value":"0.6"},{"method":"animate","args":[["0.7"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"0.7","value":"0.7"},{"method":"animate","args":[["0.8"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"0.8","value":"0.8"},{"method":"animate","args":[["0.9"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"0.9","value":"0.9"},{"method":"animate","args":[["1"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"1","value":"1"},{"method":"animate","args":[["1.1"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"1.1","value":"1.1"},{"method":"animate","args":[["1.2"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"1.2","value":"1.2"},{"method":"animate","args":[["1.3"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"1.3","value":"1.3"},{"method":"animate","args":[["1.4"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"1.4","value":"1.4"},{"method":"animate","args":[["1.5"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"1.5","value":"1.5"},{"method":"animate","args":[["1.6"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"1.6","value":"1.6"},{"method":"animate","args":[["1.7"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"1.7","value":"1.7"},{"method":"animate","args":[["1.8"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"1.8","value":"1.8"},{"method":"animate","args":[["1.9"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"1.9","value":"1.9"},{"method":"animate","args":[["2"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"2","value":"2"},{"method":"animate","args":[["2.1"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"2.1","value":"2.1"},{"method":"animate","args":[["2.2"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"2.2","value":"2.2"},{"method":"animate","args":[["2.3"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"2.3","value":"2.3"},{"method":"animate","args":[["2.4"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"2.4","value":"2.4"},{"method":"animate","args":[["2.5"],{"transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false},"mode":"immediate"}],"label":"2.5","value":"2.5"}],"visible":true,"pad":{"t":40}}],"updatemenus":[{"type":"buttons","direction":"right","showactive":false,"y":0,"x":0,"yanchor":"top","xanchor":"right","pad":{"t":60,"r":5},"buttons":[{"label":"Play","method":"animate","args":[null,{"fromcurrent":true,"mode":"immediate","transition":{"duration":500,"easing":"linear"},"frame":{"duration":500,"redraw":false}}]}]}]},"config":{"doubleClick":"reset","modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"source":"A","attrs":{"46f4689f7162":{"x":{},"y":{},"colour":{},"frame":{},"type":"scatter"}},"cur_data":"46f4689f7162","visdat":{"46f4689f7162":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"frames":[{"name":"0","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 0.0","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 0.0","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 0.0","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 0.0","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 0.0","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 0.0","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 0.0","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 0.0","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 0.0","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 0.0","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 0.0","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 0.0","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 0.0","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 0.0","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 0.0","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 0.0","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 0.0","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 0.0","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 0.0","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 0.0","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 0.0","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 0.0","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 0.0","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 0.0","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 0.0","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 0.0","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 0.0","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 0.0","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 0.0","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 0.0","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 0.0","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 0.0","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 0.0","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 0.0","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 0.0","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 0.0","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 0.0","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 0.0","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 0.0","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 0.0","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 0.0","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 0.0","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 0.0","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 0.0","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 0.0","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 0.0","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 0.0","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 0.0","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 0.0","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 0.0","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.0","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.0","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.0","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.0","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.0","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.0","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.0","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.0","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.0","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.0","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.0","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.0","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.0","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.0","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.0","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.0","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.0","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.0","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.0","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.0","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.0","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.0","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.0","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.0","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.0","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.0","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.0","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.0","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.0","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.0","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.0","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.0","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.0","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.0","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.0","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.0","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.0","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.0","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.0","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.0","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.0","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.0","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.0","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.0","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.0","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.0","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.0","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.0","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.0","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.0","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.0","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.0","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.0","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.0","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.0","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.0","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.0","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.0","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.0","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.0","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.0","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.0","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.0","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.0","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.0","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.0","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.0","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.0","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.0","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.0","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.0","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.0","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.0","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.0","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.0","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.0","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.0","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.0","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.0","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.0","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.0","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.0","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.0","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.0","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.0","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.0","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.0","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.0","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.0","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.0","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.0","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.0","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.0","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.0","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.0","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.0","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.0","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.0","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.0","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.0","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 0.0","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.0","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.0","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.0","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.0","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.0","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.0","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.0","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.0","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.0","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.0","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.0","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.0","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.0","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.0","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.0","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.0","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.0","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.0","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.0","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.0","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.0","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.0","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.0","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.0","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.0","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.0","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.0","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.0","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.0","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.0","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.0","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.0","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.0","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.0","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.0","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.0","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.0","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.0","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.0","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.0","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.0","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.0","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.0","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.0","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.0","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.0","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.0","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.0","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.0","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.0","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.0","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.0","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.0","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.0","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.0","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.0","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.0","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.0","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.0","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.0","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.0","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.0","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.0","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.0","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.0","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.0","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.0","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.0","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.0","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.0","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.0","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.0","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.0","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.0","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.0","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.0","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.0","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.0","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.0","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.0","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.0","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.0","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.0","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.0","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.0","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.0","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.0","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.0","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.0","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.0","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.0","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.0","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.0","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.0","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.0","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.0","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.0","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.0","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.0","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.0"],"frame":"0","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.49<br />value: 2.7201<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.48<br />value: 2.6904<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.47<br />value: 2.6609<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.46<br />value: 2.6316<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.45<br />value: 2.6025<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.44<br />value: 2.5736<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.43<br />value: 2.5449<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.42<br />value: 2.5164<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.41<br />value: 2.4881<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.40<br />value: 2.4600<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.39<br />value: 2.4321<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.38<br />value: 2.4044<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.37<br />value: 2.3769<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.36<br />value: 2.3496<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.35<br />value: 2.3225<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.34<br />value: 2.2956<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.33<br />value: 2.2689<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.32<br />value: 2.2424<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.31<br />value: 2.2161<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.30<br />value: 2.1900<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.29<br />value: 2.1641<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.28<br />value: 2.1384<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.27<br />value: 2.1129<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.26<br />value: 2.0876<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.25<br />value: 2.0625<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.24<br />value: 2.0376<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.23<br />value: 2.0129<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.22<br />value: 1.9884<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.21<br />value: 1.9641<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.20<br />value: 1.9400<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.19<br />value: 1.9161<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.18<br />value: 1.8924<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.17<br />value: 1.8689<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.16<br />value: 1.8456<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.15<br />value: 1.8225<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.14<br />value: 1.7996<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.13<br />value: 1.7769<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.12<br />value: 1.7544<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.11<br />value: 1.7321<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.10<br />value: 1.7100<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.09<br />value: 1.6881<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.08<br />value: 1.6664<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.07<br />value: 1.6449<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.06<br />value: 1.6236<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.05<br />value: 1.6025<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.04<br />value: 1.5816<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.03<br />value: 1.5609<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.02<br />value: 1.5404<br />Function: Loss + Penalty<br />Lambda: 0.0","b: -0.01<br />value: 1.5201<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.01<br />value: 1.4801<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.02<br />value: 1.4604<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.03<br />value: 1.4409<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.04<br />value: 1.4216<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.05<br />value: 1.4025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.06<br />value: 1.3836<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.07<br />value: 1.3649<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.08<br />value: 1.3464<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.09<br />value: 1.3281<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.10<br />value: 1.3100<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.11<br />value: 1.2921<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.12<br />value: 1.2744<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.13<br />value: 1.2569<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.14<br />value: 1.2396<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.15<br />value: 1.2225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.16<br />value: 1.2056<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.17<br />value: 1.1889<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.18<br />value: 1.1724<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.19<br />value: 1.1561<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.20<br />value: 1.1400<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.21<br />value: 1.1241<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.22<br />value: 1.1084<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.23<br />value: 1.0929<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.24<br />value: 1.0776<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.25<br />value: 1.0625<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.26<br />value: 1.0476<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.27<br />value: 1.0329<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.28<br />value: 1.0184<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.29<br />value: 1.0041<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.30<br />value: 0.9900<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.31<br />value: 0.9761<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.32<br />value: 0.9624<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.33<br />value: 0.9489<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.34<br />value: 0.9356<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.35<br />value: 0.9225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.36<br />value: 0.9096<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.37<br />value: 0.8969<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.38<br />value: 0.8844<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.39<br />value: 0.8721<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.40<br />value: 0.8600<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.41<br />value: 0.8481<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.42<br />value: 0.8364<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.43<br />value: 0.8249<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.44<br />value: 0.8136<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.45<br />value: 0.8025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.46<br />value: 0.7916<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.47<br />value: 0.7809<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.48<br />value: 0.7704<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.49<br />value: 0.7601<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.50<br />value: 0.7500<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.51<br />value: 0.7401<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.52<br />value: 0.7304<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.53<br />value: 0.7209<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.54<br />value: 0.7116<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.55<br />value: 0.7025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.56<br />value: 0.6936<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.57<br />value: 0.6849<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.58<br />value: 0.6764<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.59<br />value: 0.6681<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.60<br />value: 0.6600<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.61<br />value: 0.6521<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.62<br />value: 0.6444<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.63<br />value: 0.6369<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.64<br />value: 0.6296<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.65<br />value: 0.6225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.66<br />value: 0.6156<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.67<br />value: 0.6089<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.68<br />value: 0.6024<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.69<br />value: 0.5961<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.70<br />value: 0.5900<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.71<br />value: 0.5841<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.72<br />value: 0.5784<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.73<br />value: 0.5729<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.74<br />value: 0.5676<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.75<br />value: 0.5625<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.76<br />value: 0.5576<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.77<br />value: 0.5529<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.78<br />value: 0.5484<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.79<br />value: 0.5441<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.80<br />value: 0.5400<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.81<br />value: 0.5361<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.82<br />value: 0.5324<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.83<br />value: 0.5289<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.84<br />value: 0.5256<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.85<br />value: 0.5225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.86<br />value: 0.5196<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.87<br />value: 0.5169<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.88<br />value: 0.5144<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.89<br />value: 0.5121<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.90<br />value: 0.5100<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.91<br />value: 0.5081<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.92<br />value: 0.5064<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.93<br />value: 0.5049<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.94<br />value: 0.5036<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.95<br />value: 0.5025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.96<br />value: 0.5016<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.97<br />value: 0.5009<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.98<br />value: 0.5004<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  0.99<br />value: 0.5001<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.00<br />value: 0.5000<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.01<br />value: 0.5001<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.02<br />value: 0.5004<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.03<br />value: 0.5009<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.04<br />value: 0.5016<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.05<br />value: 0.5025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.06<br />value: 0.5036<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.07<br />value: 0.5049<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.08<br />value: 0.5064<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.09<br />value: 0.5081<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.10<br />value: 0.5100<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.11<br />value: 0.5121<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.12<br />value: 0.5144<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.13<br />value: 0.5169<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.14<br />value: 0.5196<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.15<br />value: 0.5225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.16<br />value: 0.5256<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.17<br />value: 0.5289<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.18<br />value: 0.5324<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.19<br />value: 0.5361<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.20<br />value: 0.5400<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.21<br />value: 0.5441<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.22<br />value: 0.5484<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.23<br />value: 0.5529<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.24<br />value: 0.5576<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.25<br />value: 0.5625<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.26<br />value: 0.5676<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.27<br />value: 0.5729<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.28<br />value: 0.5784<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.29<br />value: 0.5841<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.30<br />value: 0.5900<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.31<br />value: 0.5961<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.32<br />value: 0.6024<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.33<br />value: 0.6089<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.34<br />value: 0.6156<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.35<br />value: 0.6225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.36<br />value: 0.6296<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.37<br />value: 0.6369<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.38<br />value: 0.6444<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.39<br />value: 0.6521<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.40<br />value: 0.6600<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.41<br />value: 0.6681<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.42<br />value: 0.6764<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.43<br />value: 0.6849<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.44<br />value: 0.6936<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.45<br />value: 0.7025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.46<br />value: 0.7116<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.47<br />value: 0.7209<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.48<br />value: 0.7304<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.49<br />value: 0.7401<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.50<br />value: 0.7500<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.51<br />value: 0.7601<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.52<br />value: 0.7704<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.53<br />value: 0.7809<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.54<br />value: 0.7916<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.55<br />value: 0.8025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.56<br />value: 0.8136<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.57<br />value: 0.8249<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.58<br />value: 0.8364<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.59<br />value: 0.8481<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.60<br />value: 0.8600<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.61<br />value: 0.8721<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.62<br />value: 0.8844<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.63<br />value: 0.8969<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.64<br />value: 0.9096<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.65<br />value: 0.9225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.66<br />value: 0.9356<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.67<br />value: 0.9489<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.68<br />value: 0.9624<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.69<br />value: 0.9761<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.70<br />value: 0.9900<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.71<br />value: 1.0041<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.72<br />value: 1.0184<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.73<br />value: 1.0329<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.74<br />value: 1.0476<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.75<br />value: 1.0625<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.76<br />value: 1.0776<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.77<br />value: 1.0929<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.78<br />value: 1.1084<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.79<br />value: 1.1241<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.80<br />value: 1.1400<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.81<br />value: 1.1561<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.82<br />value: 1.1724<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.83<br />value: 1.1889<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.84<br />value: 1.2056<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.85<br />value: 1.2225<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.86<br />value: 1.2396<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.87<br />value: 1.2569<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.88<br />value: 1.2744<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.89<br />value: 1.2921<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.90<br />value: 1.3100<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.91<br />value: 1.3281<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.92<br />value: 1.3464<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.93<br />value: 1.3649<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.94<br />value: 1.3836<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.95<br />value: 1.4025<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.96<br />value: 1.4216<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.97<br />value: 1.4409<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.98<br />value: 1.4604<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  1.99<br />value: 1.4801<br />Function: Loss + Penalty<br />Lambda: 0.0","b:  2.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.0"],"frame":"0","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"text":["b: -0.50<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.49<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.48<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.47<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.46<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.45<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.44<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.43<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.42<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.41<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.40<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.39<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.38<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.37<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.36<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.35<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.34<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.33<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.32<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.31<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.30<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.29<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.28<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.27<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.26<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.25<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.24<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.23<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.22<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.21<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.20<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.19<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.18<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.17<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.16<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.15<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.14<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.13<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.12<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.11<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.10<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.09<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.08<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.07<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.06<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.05<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.04<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.03<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.02<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b: -0.01<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.01<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.02<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.03<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.04<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.05<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.06<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.07<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.08<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.09<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.10<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.11<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.12<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.13<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.14<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.15<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.16<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.17<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.18<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.19<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.20<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.21<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.22<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.23<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.24<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.25<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.26<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.27<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.28<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.29<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.30<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.31<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.32<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.33<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.34<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.35<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.36<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.37<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.38<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.39<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.40<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.41<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.42<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.43<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.44<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.45<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.46<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.47<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.48<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.49<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.50<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.51<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.52<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.53<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.54<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.55<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.56<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.57<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.58<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.59<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.60<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.61<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.62<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.63<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.64<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.65<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.66<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.67<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.68<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.69<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.70<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.71<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.72<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.73<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.74<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.75<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.76<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.77<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.78<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.79<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.80<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.81<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.82<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.83<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.84<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.85<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.86<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.87<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.88<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.89<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.90<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.91<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.92<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.93<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.94<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.95<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.96<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.97<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.98<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  0.99<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.01<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.02<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.03<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.04<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.05<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.06<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.07<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.08<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.09<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.10<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.11<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.12<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.13<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.14<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.15<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.16<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.17<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.18<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.19<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.20<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.21<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.22<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.23<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.24<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.25<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.26<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.27<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.28<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.29<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.30<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.31<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.32<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.33<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.34<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.35<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.36<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.37<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.38<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.39<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.40<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.41<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.42<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.43<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.44<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.45<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.46<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.47<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.48<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.49<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.50<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.51<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.52<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.53<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.54<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.55<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.56<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.57<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.58<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.59<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.60<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.61<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.62<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.63<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.64<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.65<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.66<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.67<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.68<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.69<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.70<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.71<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.72<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.73<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.74<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.75<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.76<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.77<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.78<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.79<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.80<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.81<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.82<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.83<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.84<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.85<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.86<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.87<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.88<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.89<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.90<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.91<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.92<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.93<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.94<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.95<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.96<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.97<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.98<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  1.99<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0","b:  2.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.0"],"frame":"0","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"0.1","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 0.1","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 0.1","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 0.1","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 0.1","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 0.1","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 0.1","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 0.1","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 0.1","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 0.1","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 0.1","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 0.1","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 0.1","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 0.1","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 0.1","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 0.1","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 0.1","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 0.1","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 0.1","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 0.1","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 0.1","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 0.1","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 0.1","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 0.1","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 0.1","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 0.1","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 0.1","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 0.1","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 0.1","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 0.1","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 0.1","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 0.1","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 0.1","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 0.1","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 0.1","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 0.1","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 0.1","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 0.1","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 0.1","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 0.1","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 0.1","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 0.1","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 0.1","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 0.1","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 0.1","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 0.1","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 0.1","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 0.1","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 0.1","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 0.1","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 0.1","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.1","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.1","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.1","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.1","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.1","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.1","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.1","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.1","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.1","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.1","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.1","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.1","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.1","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.1","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.1","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.1","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.1","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.1","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.1","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.1","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.1","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.1","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.1","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.1","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.1","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.1","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.1","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.1","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.1","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.1","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.1","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.1","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.1","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.1","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.1","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.1","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.1","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.1","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.1","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.1","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.1","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.1","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.1","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.1","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.1","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.1","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.1","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.1","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.1","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.1","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.1","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.1","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.1","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.1","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.1","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.1","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.1","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.1","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.1","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.1","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.1","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.1","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.1","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.1","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.1","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.1","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.1","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.1","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.1","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.1","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.1","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.1","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.1","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.1","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.1","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.1","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.1","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.1","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.1","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.1","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.1","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.1","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.1","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.1","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.1","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.1","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.1","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.1","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.1","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.1","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.1","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.1","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.1","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.1","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.1","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.1","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.1","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.1","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.1","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.1","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 0.1","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.1","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.1","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.1","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.1","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.1","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.1","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.1","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.1","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.1","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.1","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.1","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.1","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.1","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.1","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.1","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.1","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.1","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.1","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.1","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.1","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.1","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.1","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.1","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.1","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.1","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.1","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.1","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.1","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.1","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.1","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.1","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.1","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.1","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.1","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.1","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.1","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.1","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.1","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.1","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.1","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.1","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.1","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.1","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.1","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.1","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.1","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.1","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.1","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.1","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.1","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.1","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.1","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.1","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.1","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.1","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.1","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.1","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.1","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.1","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.1","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.1","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.1","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.1","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.1","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.1","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.1","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.1","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.1","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.1","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.1","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.1","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.1","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.1","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.1","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.1","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.1","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.1","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.1","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.1","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.1","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.1","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.1","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.1","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.1","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.1","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.1","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.1","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.1","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.1","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.1","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.1","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.1","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.1","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.1","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.1","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.1","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.1","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.1","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.1","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.1"],"frame":"0.1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.8,2.7691,2.7384,2.7079,2.6776,2.6475,2.6176,2.5879,2.5584,2.5291,2.5,2.4711,2.4424,2.4139,2.3856,2.3575,2.3296,2.3019,2.2744,2.2471,2.22,2.1931,2.1664,2.1399,2.1136,2.0875,2.0616,2.0359,2.0104,1.9851,1.96,1.9351,1.9104,1.8859,1.8616,1.8375,1.8136,1.7899,1.7664,1.7431,1.72,1.6971,1.6744,1.6519,1.6296,1.6075,1.5856,1.5639,1.5424,1.5211,1.5,1.4811,1.4624,1.4439,1.4256,1.4075,1.3896,1.3719,1.3544,1.3371,1.32,1.3031,1.2864,1.2699,1.2536,1.2375,1.2216,1.2059,1.1904,1.1751,1.16,1.1451,1.1304,1.1159,1.1016,1.0875,1.0736,1.0599,1.0464,1.0331,1.02,1.0071,0.9944,0.9819,0.9696,0.9575,0.9456,0.9339,0.9224,0.9111,0.9,0.8891,0.8784,0.8679,0.8576,0.8475,0.8376,0.8279,0.8184,0.8091,0.8,0.7911,0.7824,0.7739,0.7656,0.7575,0.7496,0.7419,0.7344,0.7271,0.72,0.7131,0.7064,0.6999,0.6936,0.6875,0.6816,0.6759,0.6704,0.6651,0.66,0.6551,0.6504,0.6459,0.6416,0.6375,0.6336,0.6299,0.6264,0.6231,0.62,0.6171,0.6144,0.6119,0.6096,0.6075,0.6056,0.6039,0.6024,0.6011,0.6,0.5991,0.5984,0.5979,0.5976,0.5975,0.5976,0.5979,0.5984,0.5991,0.6,0.6011,0.6024,0.6039,0.6056,0.6075,0.6096,0.6119,0.6144,0.6171,0.62,0.6231,0.6264,0.6299,0.6336,0.6375,0.6416,0.6459,0.6504,0.6551,0.66,0.6651,0.6704,0.6759,0.6816,0.6875,0.6936,0.6999,0.7064,0.7131,0.72,0.7271,0.7344,0.7419,0.7496,0.7575,0.7656,0.7739,0.7824,0.7911,0.8,0.8091,0.8184,0.8279,0.8376,0.8475,0.8576,0.8679,0.8784,0.8891,0.9,0.9111,0.9224,0.9339,0.9456,0.9575,0.9696,0.9819,0.9944,1.0071,1.02,1.0331,1.0464,1.0599,1.0736,1.0875,1.1016,1.1159,1.1304,1.1451,1.16,1.1751,1.1904,1.2059,1.2216,1.2375,1.2536,1.2699,1.2864,1.3031,1.32,1.3371,1.3544,1.3719,1.3896,1.4075,1.4256,1.4439,1.4624,1.4811,1.5,1.5191,1.5384,1.5579,1.5776,1.5975,1.6176,1.6379,1.6584,1.6791,1.7],"text":["b: -0.50<br />value: 2.8000<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.49<br />value: 2.7691<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.48<br />value: 2.7384<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.47<br />value: 2.7079<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.46<br />value: 2.6776<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.45<br />value: 2.6475<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.44<br />value: 2.6176<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.43<br />value: 2.5879<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.42<br />value: 2.5584<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.41<br />value: 2.5291<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.40<br />value: 2.5000<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.39<br />value: 2.4711<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.38<br />value: 2.4424<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.37<br />value: 2.4139<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.36<br />value: 2.3856<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.35<br />value: 2.3575<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.34<br />value: 2.3296<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.33<br />value: 2.3019<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.32<br />value: 2.2744<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.31<br />value: 2.2471<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.30<br />value: 2.2200<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.29<br />value: 2.1931<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.28<br />value: 2.1664<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.27<br />value: 2.1399<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.26<br />value: 2.1136<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.25<br />value: 2.0875<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.24<br />value: 2.0616<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.23<br />value: 2.0359<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.22<br />value: 2.0104<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.21<br />value: 1.9851<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.20<br />value: 1.9600<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.19<br />value: 1.9351<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.18<br />value: 1.9104<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.17<br />value: 1.8859<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.16<br />value: 1.8616<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.15<br />value: 1.8375<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.14<br />value: 1.8136<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.13<br />value: 1.7899<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.12<br />value: 1.7664<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.11<br />value: 1.7431<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.10<br />value: 1.7200<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.09<br />value: 1.6971<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.08<br />value: 1.6744<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.07<br />value: 1.6519<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.06<br />value: 1.6296<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.05<br />value: 1.6075<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.04<br />value: 1.5856<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.03<br />value: 1.5639<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.02<br />value: 1.5424<br />Function: Loss + Penalty<br />Lambda: 0.1","b: -0.01<br />value: 1.5211<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.01<br />value: 1.4811<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.02<br />value: 1.4624<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.03<br />value: 1.4439<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.04<br />value: 1.4256<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.05<br />value: 1.4075<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.06<br />value: 1.3896<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.07<br />value: 1.3719<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.08<br />value: 1.3544<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.09<br />value: 1.3371<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.10<br />value: 1.3200<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.11<br />value: 1.3031<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.12<br />value: 1.2864<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.13<br />value: 1.2699<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.14<br />value: 1.2536<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.15<br />value: 1.2375<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.16<br />value: 1.2216<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.17<br />value: 1.2059<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.18<br />value: 1.1904<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.19<br />value: 1.1751<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.20<br />value: 1.1600<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.21<br />value: 1.1451<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.22<br />value: 1.1304<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.23<br />value: 1.1159<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.24<br />value: 1.1016<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.25<br />value: 1.0875<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.26<br />value: 1.0736<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.27<br />value: 1.0599<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.28<br />value: 1.0464<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.29<br />value: 1.0331<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.30<br />value: 1.0200<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.31<br />value: 1.0071<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.32<br />value: 0.9944<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.33<br />value: 0.9819<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.34<br />value: 0.9696<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.35<br />value: 0.9575<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.36<br />value: 0.9456<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.37<br />value: 0.9339<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.38<br />value: 0.9224<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.39<br />value: 0.9111<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.40<br />value: 0.9000<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.41<br />value: 0.8891<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.42<br />value: 0.8784<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.43<br />value: 0.8679<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.44<br />value: 0.8576<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.45<br />value: 0.8475<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.46<br />value: 0.8376<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.47<br />value: 0.8279<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.48<br />value: 0.8184<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.49<br />value: 0.8091<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.50<br />value: 0.8000<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.51<br />value: 0.7911<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.52<br />value: 0.7824<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.53<br />value: 0.7739<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.54<br />value: 0.7656<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.55<br />value: 0.7575<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.56<br />value: 0.7496<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.57<br />value: 0.7419<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.58<br />value: 0.7344<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.59<br />value: 0.7271<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.60<br />value: 0.7200<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.61<br />value: 0.7131<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.62<br />value: 0.7064<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.63<br />value: 0.6999<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.64<br />value: 0.6936<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.65<br />value: 0.6875<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.66<br />value: 0.6816<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.67<br />value: 0.6759<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.68<br />value: 0.6704<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.69<br />value: 0.6651<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.70<br />value: 0.6600<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.71<br />value: 0.6551<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.72<br />value: 0.6504<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.73<br />value: 0.6459<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.74<br />value: 0.6416<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.75<br />value: 0.6375<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.76<br />value: 0.6336<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.77<br />value: 0.6299<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.78<br />value: 0.6264<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.79<br />value: 0.6231<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.80<br />value: 0.6200<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.81<br />value: 0.6171<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.82<br />value: 0.6144<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.83<br />value: 0.6119<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.84<br />value: 0.6096<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.85<br />value: 0.6075<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.86<br />value: 0.6056<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.87<br />value: 0.6039<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.88<br />value: 0.6024<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.89<br />value: 0.6011<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.90<br />value: 0.6000<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.91<br />value: 0.5991<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.92<br />value: 0.5984<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.93<br />value: 0.5979<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.94<br />value: 0.5976<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.95<br />value: 0.5975<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.96<br />value: 0.5976<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.97<br />value: 0.5979<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.98<br />value: 0.5984<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  0.99<br />value: 0.5991<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.00<br />value: 0.6000<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.01<br />value: 0.6011<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.02<br />value: 0.6024<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.03<br />value: 0.6039<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.04<br />value: 0.6056<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.05<br />value: 0.6075<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.06<br />value: 0.6096<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.07<br />value: 0.6119<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.08<br />value: 0.6144<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.09<br />value: 0.6171<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.10<br />value: 0.6200<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.11<br />value: 0.6231<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.12<br />value: 0.6264<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.13<br />value: 0.6299<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.14<br />value: 0.6336<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.15<br />value: 0.6375<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.16<br />value: 0.6416<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.17<br />value: 0.6459<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.18<br />value: 0.6504<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.19<br />value: 0.6551<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.20<br />value: 0.6600<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.21<br />value: 0.6651<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.22<br />value: 0.6704<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.23<br />value: 0.6759<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.24<br />value: 0.6816<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.25<br />value: 0.6875<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.26<br />value: 0.6936<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.27<br />value: 0.6999<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.28<br />value: 0.7064<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.29<br />value: 0.7131<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.30<br />value: 0.7200<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.31<br />value: 0.7271<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.32<br />value: 0.7344<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.33<br />value: 0.7419<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.34<br />value: 0.7496<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.35<br />value: 0.7575<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.36<br />value: 0.7656<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.37<br />value: 0.7739<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.38<br />value: 0.7824<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.39<br />value: 0.7911<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.40<br />value: 0.8000<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.41<br />value: 0.8091<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.42<br />value: 0.8184<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.43<br />value: 0.8279<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.44<br />value: 0.8376<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.45<br />value: 0.8475<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.46<br />value: 0.8576<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.47<br />value: 0.8679<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.48<br />value: 0.8784<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.49<br />value: 0.8891<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.50<br />value: 0.9000<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.51<br />value: 0.9111<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.52<br />value: 0.9224<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.53<br />value: 0.9339<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.54<br />value: 0.9456<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.55<br />value: 0.9575<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.56<br />value: 0.9696<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.57<br />value: 0.9819<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.58<br />value: 0.9944<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.59<br />value: 1.0071<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.60<br />value: 1.0200<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.61<br />value: 1.0331<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.62<br />value: 1.0464<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.63<br />value: 1.0599<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.64<br />value: 1.0736<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.65<br />value: 1.0875<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.66<br />value: 1.1016<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.67<br />value: 1.1159<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.68<br />value: 1.1304<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.69<br />value: 1.1451<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.70<br />value: 1.1600<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.71<br />value: 1.1751<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.72<br />value: 1.1904<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.73<br />value: 1.2059<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.74<br />value: 1.2216<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.75<br />value: 1.2375<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.76<br />value: 1.2536<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.77<br />value: 1.2699<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.78<br />value: 1.2864<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.79<br />value: 1.3031<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.80<br />value: 1.3200<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.81<br />value: 1.3371<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.82<br />value: 1.3544<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.83<br />value: 1.3719<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.84<br />value: 1.3896<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.85<br />value: 1.4075<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.86<br />value: 1.4256<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.87<br />value: 1.4439<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.88<br />value: 1.4624<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.89<br />value: 1.4811<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.90<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.91<br />value: 1.5191<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.92<br />value: 1.5384<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.93<br />value: 1.5579<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.94<br />value: 1.5776<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.95<br />value: 1.5975<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.96<br />value: 1.6176<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.97<br />value: 1.6379<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.98<br />value: 1.6584<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  1.99<br />value: 1.6791<br />Function: Loss + Penalty<br />Lambda: 0.1","b:  2.00<br />value: 1.7000<br />Function: Loss + Penalty<br />Lambda: 0.1"],"frame":"0.1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.05,0.049,0.048,0.047,0.046,0.045,0.044,0.043,0.042,0.041,0.04,0.039,0.038,0.037,0.036,0.035,0.034,0.033,0.032,0.031,0.03,0.029,0.028,0.027,0.026,0.025,0.024,0.023,0.022,0.021,0.02,0.019,0.018,0.017,0.016,0.015,0.014,0.013,0.012,0.011,0.01,0.009,0.008,0.007,0.006,0.005,0.004,0.003,0.002,0.001,0,0.001,0.002,0.003,0.004,0.005,0.00600000000000001,0.00700000000000001,0.008,0.009,0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02,0.021,0.022,0.023,0.024,0.025,0.026,0.027,0.028,0.029,0.03,0.031,0.032,0.033,0.034,0.035,0.036,0.037,0.038,0.039,0.04,0.041,0.042,0.043,0.044,0.045,0.046,0.047,0.048,0.049,0.05,0.051,0.052,0.053,0.054,0.055,0.056,0.057,0.058,0.059,0.06,0.061,0.062,0.063,0.064,0.065,0.066,0.067,0.068,0.069,0.07,0.071,0.072,0.073,0.074,0.075,0.076,0.077,0.078,0.079,0.08,0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09,0.091,0.092,0.093,0.094,0.095,0.096,0.097,0.098,0.099,0.1,0.101,0.102,0.103,0.104,0.105,0.106,0.107,0.108,0.109,0.11,0.111,0.112,0.113,0.114,0.115,0.116,0.117,0.118,0.119,0.12,0.121,0.122,0.123,0.124,0.125,0.126,0.127,0.128,0.129,0.13,0.131,0.132,0.133,0.134,0.135,0.136,0.137,0.138,0.139,0.14,0.141,0.142,0.143,0.144,0.145,0.146,0.147,0.148,0.149,0.15,0.151,0.152,0.153,0.154,0.155,0.156,0.157,0.158,0.159,0.16,0.161,0.162,0.163,0.164,0.165,0.166,0.167,0.168,0.169,0.17,0.171,0.172,0.173,0.174,0.175,0.176,0.177,0.178,0.179,0.18,0.181,0.182,0.183,0.184,0.185,0.186,0.187,0.188,0.189,0.19,0.191,0.192,0.193,0.194,0.195,0.196,0.197,0.198,0.199,0.2],"text":["b: -0.50<br />value: 0.0500<br />Function: Penalty<br />Lambda: 0.1","b: -0.49<br />value: 0.0490<br />Function: Penalty<br />Lambda: 0.1","b: -0.48<br />value: 0.0480<br />Function: Penalty<br />Lambda: 0.1","b: -0.47<br />value: 0.0470<br />Function: Penalty<br />Lambda: 0.1","b: -0.46<br />value: 0.0460<br />Function: Penalty<br />Lambda: 0.1","b: -0.45<br />value: 0.0450<br />Function: Penalty<br />Lambda: 0.1","b: -0.44<br />value: 0.0440<br />Function: Penalty<br />Lambda: 0.1","b: -0.43<br />value: 0.0430<br />Function: Penalty<br />Lambda: 0.1","b: -0.42<br />value: 0.0420<br />Function: Penalty<br />Lambda: 0.1","b: -0.41<br />value: 0.0410<br />Function: Penalty<br />Lambda: 0.1","b: -0.40<br />value: 0.0400<br />Function: Penalty<br />Lambda: 0.1","b: -0.39<br />value: 0.0390<br />Function: Penalty<br />Lambda: 0.1","b: -0.38<br />value: 0.0380<br />Function: Penalty<br />Lambda: 0.1","b: -0.37<br />value: 0.0370<br />Function: Penalty<br />Lambda: 0.1","b: -0.36<br />value: 0.0360<br />Function: Penalty<br />Lambda: 0.1","b: -0.35<br />value: 0.0350<br />Function: Penalty<br />Lambda: 0.1","b: -0.34<br />value: 0.0340<br />Function: Penalty<br />Lambda: 0.1","b: -0.33<br />value: 0.0330<br />Function: Penalty<br />Lambda: 0.1","b: -0.32<br />value: 0.0320<br />Function: Penalty<br />Lambda: 0.1","b: -0.31<br />value: 0.0310<br />Function: Penalty<br />Lambda: 0.1","b: -0.30<br />value: 0.0300<br />Function: Penalty<br />Lambda: 0.1","b: -0.29<br />value: 0.0290<br />Function: Penalty<br />Lambda: 0.1","b: -0.28<br />value: 0.0280<br />Function: Penalty<br />Lambda: 0.1","b: -0.27<br />value: 0.0270<br />Function: Penalty<br />Lambda: 0.1","b: -0.26<br />value: 0.0260<br />Function: Penalty<br />Lambda: 0.1","b: -0.25<br />value: 0.0250<br />Function: Penalty<br />Lambda: 0.1","b: -0.24<br />value: 0.0240<br />Function: Penalty<br />Lambda: 0.1","b: -0.23<br />value: 0.0230<br />Function: Penalty<br />Lambda: 0.1","b: -0.22<br />value: 0.0220<br />Function: Penalty<br />Lambda: 0.1","b: -0.21<br />value: 0.0210<br />Function: Penalty<br />Lambda: 0.1","b: -0.20<br />value: 0.0200<br />Function: Penalty<br />Lambda: 0.1","b: -0.19<br />value: 0.0190<br />Function: Penalty<br />Lambda: 0.1","b: -0.18<br />value: 0.0180<br />Function: Penalty<br />Lambda: 0.1","b: -0.17<br />value: 0.0170<br />Function: Penalty<br />Lambda: 0.1","b: -0.16<br />value: 0.0160<br />Function: Penalty<br />Lambda: 0.1","b: -0.15<br />value: 0.0150<br />Function: Penalty<br />Lambda: 0.1","b: -0.14<br />value: 0.0140<br />Function: Penalty<br />Lambda: 0.1","b: -0.13<br />value: 0.0130<br />Function: Penalty<br />Lambda: 0.1","b: -0.12<br />value: 0.0120<br />Function: Penalty<br />Lambda: 0.1","b: -0.11<br />value: 0.0110<br />Function: Penalty<br />Lambda: 0.1","b: -0.10<br />value: 0.0100<br />Function: Penalty<br />Lambda: 0.1","b: -0.09<br />value: 0.0090<br />Function: Penalty<br />Lambda: 0.1","b: -0.08<br />value: 0.0080<br />Function: Penalty<br />Lambda: 0.1","b: -0.07<br />value: 0.0070<br />Function: Penalty<br />Lambda: 0.1","b: -0.06<br />value: 0.0060<br />Function: Penalty<br />Lambda: 0.1","b: -0.05<br />value: 0.0050<br />Function: Penalty<br />Lambda: 0.1","b: -0.04<br />value: 0.0040<br />Function: Penalty<br />Lambda: 0.1","b: -0.03<br />value: 0.0030<br />Function: Penalty<br />Lambda: 0.1","b: -0.02<br />value: 0.0020<br />Function: Penalty<br />Lambda: 0.1","b: -0.01<br />value: 0.0010<br />Function: Penalty<br />Lambda: 0.1","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.1","b:  0.01<br />value: 0.0010<br />Function: Penalty<br />Lambda: 0.1","b:  0.02<br />value: 0.0020<br />Function: Penalty<br />Lambda: 0.1","b:  0.03<br />value: 0.0030<br />Function: Penalty<br />Lambda: 0.1","b:  0.04<br />value: 0.0040<br />Function: Penalty<br />Lambda: 0.1","b:  0.05<br />value: 0.0050<br />Function: Penalty<br />Lambda: 0.1","b:  0.06<br />value: 0.0060<br />Function: Penalty<br />Lambda: 0.1","b:  0.07<br />value: 0.0070<br />Function: Penalty<br />Lambda: 0.1","b:  0.08<br />value: 0.0080<br />Function: Penalty<br />Lambda: 0.1","b:  0.09<br />value: 0.0090<br />Function: Penalty<br />Lambda: 0.1","b:  0.10<br />value: 0.0100<br />Function: Penalty<br />Lambda: 0.1","b:  0.11<br />value: 0.0110<br />Function: Penalty<br />Lambda: 0.1","b:  0.12<br />value: 0.0120<br />Function: Penalty<br />Lambda: 0.1","b:  0.13<br />value: 0.0130<br />Function: Penalty<br />Lambda: 0.1","b:  0.14<br />value: 0.0140<br />Function: Penalty<br />Lambda: 0.1","b:  0.15<br />value: 0.0150<br />Function: Penalty<br />Lambda: 0.1","b:  0.16<br />value: 0.0160<br />Function: Penalty<br />Lambda: 0.1","b:  0.17<br />value: 0.0170<br />Function: Penalty<br />Lambda: 0.1","b:  0.18<br />value: 0.0180<br />Function: Penalty<br />Lambda: 0.1","b:  0.19<br />value: 0.0190<br />Function: Penalty<br />Lambda: 0.1","b:  0.20<br />value: 0.0200<br />Function: Penalty<br />Lambda: 0.1","b:  0.21<br />value: 0.0210<br />Function: Penalty<br />Lambda: 0.1","b:  0.22<br />value: 0.0220<br />Function: Penalty<br />Lambda: 0.1","b:  0.23<br />value: 0.0230<br />Function: Penalty<br />Lambda: 0.1","b:  0.24<br />value: 0.0240<br />Function: Penalty<br />Lambda: 0.1","b:  0.25<br />value: 0.0250<br />Function: Penalty<br />Lambda: 0.1","b:  0.26<br />value: 0.0260<br />Function: Penalty<br />Lambda: 0.1","b:  0.27<br />value: 0.0270<br />Function: Penalty<br />Lambda: 0.1","b:  0.28<br />value: 0.0280<br />Function: Penalty<br />Lambda: 0.1","b:  0.29<br />value: 0.0290<br />Function: Penalty<br />Lambda: 0.1","b:  0.30<br />value: 0.0300<br />Function: Penalty<br />Lambda: 0.1","b:  0.31<br />value: 0.0310<br />Function: Penalty<br />Lambda: 0.1","b:  0.32<br />value: 0.0320<br />Function: Penalty<br />Lambda: 0.1","b:  0.33<br />value: 0.0330<br />Function: Penalty<br />Lambda: 0.1","b:  0.34<br />value: 0.0340<br />Function: Penalty<br />Lambda: 0.1","b:  0.35<br />value: 0.0350<br />Function: Penalty<br />Lambda: 0.1","b:  0.36<br />value: 0.0360<br />Function: Penalty<br />Lambda: 0.1","b:  0.37<br />value: 0.0370<br />Function: Penalty<br />Lambda: 0.1","b:  0.38<br />value: 0.0380<br />Function: Penalty<br />Lambda: 0.1","b:  0.39<br />value: 0.0390<br />Function: Penalty<br />Lambda: 0.1","b:  0.40<br />value: 0.0400<br />Function: Penalty<br />Lambda: 0.1","b:  0.41<br />value: 0.0410<br />Function: Penalty<br />Lambda: 0.1","b:  0.42<br />value: 0.0420<br />Function: Penalty<br />Lambda: 0.1","b:  0.43<br />value: 0.0430<br />Function: Penalty<br />Lambda: 0.1","b:  0.44<br />value: 0.0440<br />Function: Penalty<br />Lambda: 0.1","b:  0.45<br />value: 0.0450<br />Function: Penalty<br />Lambda: 0.1","b:  0.46<br />value: 0.0460<br />Function: Penalty<br />Lambda: 0.1","b:  0.47<br />value: 0.0470<br />Function: Penalty<br />Lambda: 0.1","b:  0.48<br />value: 0.0480<br />Function: Penalty<br />Lambda: 0.1","b:  0.49<br />value: 0.0490<br />Function: Penalty<br />Lambda: 0.1","b:  0.50<br />value: 0.0500<br />Function: Penalty<br />Lambda: 0.1","b:  0.51<br />value: 0.0510<br />Function: Penalty<br />Lambda: 0.1","b:  0.52<br />value: 0.0520<br />Function: Penalty<br />Lambda: 0.1","b:  0.53<br />value: 0.0530<br />Function: Penalty<br />Lambda: 0.1","b:  0.54<br />value: 0.0540<br />Function: Penalty<br />Lambda: 0.1","b:  0.55<br />value: 0.0550<br />Function: Penalty<br />Lambda: 0.1","b:  0.56<br />value: 0.0560<br />Function: Penalty<br />Lambda: 0.1","b:  0.57<br />value: 0.0570<br />Function: Penalty<br />Lambda: 0.1","b:  0.58<br />value: 0.0580<br />Function: Penalty<br />Lambda: 0.1","b:  0.59<br />value: 0.0590<br />Function: Penalty<br />Lambda: 0.1","b:  0.60<br />value: 0.0600<br />Function: Penalty<br />Lambda: 0.1","b:  0.61<br />value: 0.0610<br />Function: Penalty<br />Lambda: 0.1","b:  0.62<br />value: 0.0620<br />Function: Penalty<br />Lambda: 0.1","b:  0.63<br />value: 0.0630<br />Function: Penalty<br />Lambda: 0.1","b:  0.64<br />value: 0.0640<br />Function: Penalty<br />Lambda: 0.1","b:  0.65<br />value: 0.0650<br />Function: Penalty<br />Lambda: 0.1","b:  0.66<br />value: 0.0660<br />Function: Penalty<br />Lambda: 0.1","b:  0.67<br />value: 0.0670<br />Function: Penalty<br />Lambda: 0.1","b:  0.68<br />value: 0.0680<br />Function: Penalty<br />Lambda: 0.1","b:  0.69<br />value: 0.0690<br />Function: Penalty<br />Lambda: 0.1","b:  0.70<br />value: 0.0700<br />Function: Penalty<br />Lambda: 0.1","b:  0.71<br />value: 0.0710<br />Function: Penalty<br />Lambda: 0.1","b:  0.72<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.1","b:  0.73<br />value: 0.0730<br />Function: Penalty<br />Lambda: 0.1","b:  0.74<br />value: 0.0740<br />Function: Penalty<br />Lambda: 0.1","b:  0.75<br />value: 0.0750<br />Function: Penalty<br />Lambda: 0.1","b:  0.76<br />value: 0.0760<br />Function: Penalty<br />Lambda: 0.1","b:  0.77<br />value: 0.0770<br />Function: Penalty<br />Lambda: 0.1","b:  0.78<br />value: 0.0780<br />Function: Penalty<br />Lambda: 0.1","b:  0.79<br />value: 0.0790<br />Function: Penalty<br />Lambda: 0.1","b:  0.80<br />value: 0.0800<br />Function: Penalty<br />Lambda: 0.1","b:  0.81<br />value: 0.0810<br />Function: Penalty<br />Lambda: 0.1","b:  0.82<br />value: 0.0820<br />Function: Penalty<br />Lambda: 0.1","b:  0.83<br />value: 0.0830<br />Function: Penalty<br />Lambda: 0.1","b:  0.84<br />value: 0.0840<br />Function: Penalty<br />Lambda: 0.1","b:  0.85<br />value: 0.0850<br />Function: Penalty<br />Lambda: 0.1","b:  0.86<br />value: 0.0860<br />Function: Penalty<br />Lambda: 0.1","b:  0.87<br />value: 0.0870<br />Function: Penalty<br />Lambda: 0.1","b:  0.88<br />value: 0.0880<br />Function: Penalty<br />Lambda: 0.1","b:  0.89<br />value: 0.0890<br />Function: Penalty<br />Lambda: 0.1","b:  0.90<br />value: 0.0900<br />Function: Penalty<br />Lambda: 0.1","b:  0.91<br />value: 0.0910<br />Function: Penalty<br />Lambda: 0.1","b:  0.92<br />value: 0.0920<br />Function: Penalty<br />Lambda: 0.1","b:  0.93<br />value: 0.0930<br />Function: Penalty<br />Lambda: 0.1","b:  0.94<br />value: 0.0940<br />Function: Penalty<br />Lambda: 0.1","b:  0.95<br />value: 0.0950<br />Function: Penalty<br />Lambda: 0.1","b:  0.96<br />value: 0.0960<br />Function: Penalty<br />Lambda: 0.1","b:  0.97<br />value: 0.0970<br />Function: Penalty<br />Lambda: 0.1","b:  0.98<br />value: 0.0980<br />Function: Penalty<br />Lambda: 0.1","b:  0.99<br />value: 0.0990<br />Function: Penalty<br />Lambda: 0.1","b:  1.00<br />value: 0.1000<br />Function: Penalty<br />Lambda: 0.1","b:  1.01<br />value: 0.1010<br />Function: Penalty<br />Lambda: 0.1","b:  1.02<br />value: 0.1020<br />Function: Penalty<br />Lambda: 0.1","b:  1.03<br />value: 0.1030<br />Function: Penalty<br />Lambda: 0.1","b:  1.04<br />value: 0.1040<br />Function: Penalty<br />Lambda: 0.1","b:  1.05<br />value: 0.1050<br />Function: Penalty<br />Lambda: 0.1","b:  1.06<br />value: 0.1060<br />Function: Penalty<br />Lambda: 0.1","b:  1.07<br />value: 0.1070<br />Function: Penalty<br />Lambda: 0.1","b:  1.08<br />value: 0.1080<br />Function: Penalty<br />Lambda: 0.1","b:  1.09<br />value: 0.1090<br />Function: Penalty<br />Lambda: 0.1","b:  1.10<br />value: 0.1100<br />Function: Penalty<br />Lambda: 0.1","b:  1.11<br />value: 0.1110<br />Function: Penalty<br />Lambda: 0.1","b:  1.12<br />value: 0.1120<br />Function: Penalty<br />Lambda: 0.1","b:  1.13<br />value: 0.1130<br />Function: Penalty<br />Lambda: 0.1","b:  1.14<br />value: 0.1140<br />Function: Penalty<br />Lambda: 0.1","b:  1.15<br />value: 0.1150<br />Function: Penalty<br />Lambda: 0.1","b:  1.16<br />value: 0.1160<br />Function: Penalty<br />Lambda: 0.1","b:  1.17<br />value: 0.1170<br />Function: Penalty<br />Lambda: 0.1","b:  1.18<br />value: 0.1180<br />Function: Penalty<br />Lambda: 0.1","b:  1.19<br />value: 0.1190<br />Function: Penalty<br />Lambda: 0.1","b:  1.20<br />value: 0.1200<br />Function: Penalty<br />Lambda: 0.1","b:  1.21<br />value: 0.1210<br />Function: Penalty<br />Lambda: 0.1","b:  1.22<br />value: 0.1220<br />Function: Penalty<br />Lambda: 0.1","b:  1.23<br />value: 0.1230<br />Function: Penalty<br />Lambda: 0.1","b:  1.24<br />value: 0.1240<br />Function: Penalty<br />Lambda: 0.1","b:  1.25<br />value: 0.1250<br />Function: Penalty<br />Lambda: 0.1","b:  1.26<br />value: 0.1260<br />Function: Penalty<br />Lambda: 0.1","b:  1.27<br />value: 0.1270<br />Function: Penalty<br />Lambda: 0.1","b:  1.28<br />value: 0.1280<br />Function: Penalty<br />Lambda: 0.1","b:  1.29<br />value: 0.1290<br />Function: Penalty<br />Lambda: 0.1","b:  1.30<br />value: 0.1300<br />Function: Penalty<br />Lambda: 0.1","b:  1.31<br />value: 0.1310<br />Function: Penalty<br />Lambda: 0.1","b:  1.32<br />value: 0.1320<br />Function: Penalty<br />Lambda: 0.1","b:  1.33<br />value: 0.1330<br />Function: Penalty<br />Lambda: 0.1","b:  1.34<br />value: 0.1340<br />Function: Penalty<br />Lambda: 0.1","b:  1.35<br />value: 0.1350<br />Function: Penalty<br />Lambda: 0.1","b:  1.36<br />value: 0.1360<br />Function: Penalty<br />Lambda: 0.1","b:  1.37<br />value: 0.1370<br />Function: Penalty<br />Lambda: 0.1","b:  1.38<br />value: 0.1380<br />Function: Penalty<br />Lambda: 0.1","b:  1.39<br />value: 0.1390<br />Function: Penalty<br />Lambda: 0.1","b:  1.40<br />value: 0.1400<br />Function: Penalty<br />Lambda: 0.1","b:  1.41<br />value: 0.1410<br />Function: Penalty<br />Lambda: 0.1","b:  1.42<br />value: 0.1420<br />Function: Penalty<br />Lambda: 0.1","b:  1.43<br />value: 0.1430<br />Function: Penalty<br />Lambda: 0.1","b:  1.44<br />value: 0.1440<br />Function: Penalty<br />Lambda: 0.1","b:  1.45<br />value: 0.1450<br />Function: Penalty<br />Lambda: 0.1","b:  1.46<br />value: 0.1460<br />Function: Penalty<br />Lambda: 0.1","b:  1.47<br />value: 0.1470<br />Function: Penalty<br />Lambda: 0.1","b:  1.48<br />value: 0.1480<br />Function: Penalty<br />Lambda: 0.1","b:  1.49<br />value: 0.1490<br />Function: Penalty<br />Lambda: 0.1","b:  1.50<br />value: 0.1500<br />Function: Penalty<br />Lambda: 0.1","b:  1.51<br />value: 0.1510<br />Function: Penalty<br />Lambda: 0.1","b:  1.52<br />value: 0.1520<br />Function: Penalty<br />Lambda: 0.1","b:  1.53<br />value: 0.1530<br />Function: Penalty<br />Lambda: 0.1","b:  1.54<br />value: 0.1540<br />Function: Penalty<br />Lambda: 0.1","b:  1.55<br />value: 0.1550<br />Function: Penalty<br />Lambda: 0.1","b:  1.56<br />value: 0.1560<br />Function: Penalty<br />Lambda: 0.1","b:  1.57<br />value: 0.1570<br />Function: Penalty<br />Lambda: 0.1","b:  1.58<br />value: 0.1580<br />Function: Penalty<br />Lambda: 0.1","b:  1.59<br />value: 0.1590<br />Function: Penalty<br />Lambda: 0.1","b:  1.60<br />value: 0.1600<br />Function: Penalty<br />Lambda: 0.1","b:  1.61<br />value: 0.1610<br />Function: Penalty<br />Lambda: 0.1","b:  1.62<br />value: 0.1620<br />Function: Penalty<br />Lambda: 0.1","b:  1.63<br />value: 0.1630<br />Function: Penalty<br />Lambda: 0.1","b:  1.64<br />value: 0.1640<br />Function: Penalty<br />Lambda: 0.1","b:  1.65<br />value: 0.1650<br />Function: Penalty<br />Lambda: 0.1","b:  1.66<br />value: 0.1660<br />Function: Penalty<br />Lambda: 0.1","b:  1.67<br />value: 0.1670<br />Function: Penalty<br />Lambda: 0.1","b:  1.68<br />value: 0.1680<br />Function: Penalty<br />Lambda: 0.1","b:  1.69<br />value: 0.1690<br />Function: Penalty<br />Lambda: 0.1","b:  1.70<br />value: 0.1700<br />Function: Penalty<br />Lambda: 0.1","b:  1.71<br />value: 0.1710<br />Function: Penalty<br />Lambda: 0.1","b:  1.72<br />value: 0.1720<br />Function: Penalty<br />Lambda: 0.1","b:  1.73<br />value: 0.1730<br />Function: Penalty<br />Lambda: 0.1","b:  1.74<br />value: 0.1740<br />Function: Penalty<br />Lambda: 0.1","b:  1.75<br />value: 0.1750<br />Function: Penalty<br />Lambda: 0.1","b:  1.76<br />value: 0.1760<br />Function: Penalty<br />Lambda: 0.1","b:  1.77<br />value: 0.1770<br />Function: Penalty<br />Lambda: 0.1","b:  1.78<br />value: 0.1780<br />Function: Penalty<br />Lambda: 0.1","b:  1.79<br />value: 0.1790<br />Function: Penalty<br />Lambda: 0.1","b:  1.80<br />value: 0.1800<br />Function: Penalty<br />Lambda: 0.1","b:  1.81<br />value: 0.1810<br />Function: Penalty<br />Lambda: 0.1","b:  1.82<br />value: 0.1820<br />Function: Penalty<br />Lambda: 0.1","b:  1.83<br />value: 0.1830<br />Function: Penalty<br />Lambda: 0.1","b:  1.84<br />value: 0.1840<br />Function: Penalty<br />Lambda: 0.1","b:  1.85<br />value: 0.1850<br />Function: Penalty<br />Lambda: 0.1","b:  1.86<br />value: 0.1860<br />Function: Penalty<br />Lambda: 0.1","b:  1.87<br />value: 0.1870<br />Function: Penalty<br />Lambda: 0.1","b:  1.88<br />value: 0.1880<br />Function: Penalty<br />Lambda: 0.1","b:  1.89<br />value: 0.1890<br />Function: Penalty<br />Lambda: 0.1","b:  1.90<br />value: 0.1900<br />Function: Penalty<br />Lambda: 0.1","b:  1.91<br />value: 0.1910<br />Function: Penalty<br />Lambda: 0.1","b:  1.92<br />value: 0.1920<br />Function: Penalty<br />Lambda: 0.1","b:  1.93<br />value: 0.1930<br />Function: Penalty<br />Lambda: 0.1","b:  1.94<br />value: 0.1940<br />Function: Penalty<br />Lambda: 0.1","b:  1.95<br />value: 0.1950<br />Function: Penalty<br />Lambda: 0.1","b:  1.96<br />value: 0.1960<br />Function: Penalty<br />Lambda: 0.1","b:  1.97<br />value: 0.1970<br />Function: Penalty<br />Lambda: 0.1","b:  1.98<br />value: 0.1980<br />Function: Penalty<br />Lambda: 0.1","b:  1.99<br />value: 0.1990<br />Function: Penalty<br />Lambda: 0.1","b:  2.00<br />value: 0.2000<br />Function: Penalty<br />Lambda: 0.1"],"frame":"0.1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"0.2","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 0.2","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 0.2","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 0.2","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 0.2","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 0.2","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 0.2","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 0.2","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 0.2","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 0.2","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 0.2","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 0.2","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 0.2","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 0.2","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 0.2","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 0.2","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 0.2","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 0.2","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 0.2","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 0.2","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 0.2","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 0.2","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 0.2","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 0.2","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 0.2","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 0.2","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 0.2","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 0.2","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 0.2","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 0.2","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 0.2","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 0.2","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 0.2","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 0.2","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 0.2","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 0.2","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 0.2","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 0.2","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 0.2","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 0.2","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 0.2","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 0.2","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 0.2","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 0.2","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 0.2","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 0.2","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 0.2","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 0.2","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 0.2","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 0.2","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 0.2","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.2","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.2","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.2","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.2","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.2","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.2","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.2","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.2","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.2","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.2","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.2","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.2","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.2","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.2","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.2","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.2","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.2","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.2","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.2","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.2","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.2","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.2","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.2","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.2","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.2","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.2","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.2","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.2","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.2","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.2","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.2","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.2","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.2","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.2","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.2","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.2","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.2","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.2","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.2","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.2","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.2","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.2","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.2","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.2","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.2","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.2","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.2","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.2","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.2","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.2","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.2","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.2","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.2","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.2","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.2","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.2","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.2","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.2","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.2","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.2","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.2","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.2","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.2","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.2","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.2","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.2","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.2","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.2","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.2","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.2","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.2","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.2","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.2","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.2","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.2","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.2","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.2","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.2","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.2","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.2","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.2","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.2","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.2","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.2","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.2","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.2","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.2","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.2","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.2","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.2","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.2","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.2","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.2","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.2","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.2","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.2","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.2","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.2","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.2","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.2","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 0.2","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.2","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.2","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.2","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.2","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.2","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.2","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.2","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.2","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.2","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.2","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.2","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.2","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.2","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.2","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.2","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.2","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.2","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.2","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.2","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.2","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.2","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.2","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.2","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.2","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.2","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.2","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.2","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.2","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.2","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.2","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.2","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.2","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.2","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.2","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.2","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.2","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.2","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.2","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.2","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.2","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.2","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.2","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.2","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.2","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.2","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.2","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.2","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.2","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.2","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.2","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.2","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.2","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.2","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.2","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.2","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.2","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.2","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.2","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.2","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.2","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.2","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.2","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.2","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.2","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.2","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.2","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.2","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.2","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.2","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.2","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.2","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.2","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.2","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.2","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.2","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.2","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.2","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.2","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.2","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.2","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.2","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.2","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.2","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.2","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.2","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.2","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.2","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.2","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.2","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.2","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.2","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.2","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.2","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.2","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.2","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.2","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.2","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.2","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.2","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.2"],"frame":"0.2","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.85,2.8181,2.7864,2.7549,2.7236,2.6925,2.6616,2.6309,2.6004,2.5701,2.54,2.5101,2.4804,2.4509,2.4216,2.3925,2.3636,2.3349,2.3064,2.2781,2.25,2.2221,2.1944,2.1669,2.1396,2.1125,2.0856,2.0589,2.0324,2.0061,1.98,1.9541,1.9284,1.9029,1.8776,1.8525,1.8276,1.8029,1.7784,1.7541,1.73,1.7061,1.6824,1.6589,1.6356,1.6125,1.5896,1.5669,1.5444,1.5221,1.5,1.4821,1.4644,1.4469,1.4296,1.4125,1.3956,1.3789,1.3624,1.3461,1.33,1.3141,1.2984,1.2829,1.2676,1.2525,1.2376,1.2229,1.2084,1.1941,1.18,1.1661,1.1524,1.1389,1.1256,1.1125,1.0996,1.0869,1.0744,1.0621,1.05,1.0381,1.0264,1.0149,1.0036,0.9925,0.9816,0.9709,0.9604,0.9501,0.94,0.9301,0.9204,0.9109,0.9016,0.8925,0.8836,0.8749,0.8664,0.8581,0.85,0.8421,0.8344,0.8269,0.8196,0.8125,0.8056,0.7989,0.7924,0.7861,0.78,0.7741,0.7684,0.7629,0.7576,0.7525,0.7476,0.7429,0.7384,0.7341,0.73,0.7261,0.7224,0.7189,0.7156,0.7125,0.7096,0.7069,0.7044,0.7021,0.7,0.6981,0.6964,0.6949,0.6936,0.6925,0.6916,0.6909,0.6904,0.6901,0.69,0.6901,0.6904,0.6909,0.6916,0.6925,0.6936,0.6949,0.6964,0.6981,0.7,0.7021,0.7044,0.7069,0.7096,0.7125,0.7156,0.7189,0.7224,0.7261,0.73,0.7341,0.7384,0.7429,0.7476,0.7525,0.7576,0.7629,0.7684,0.7741,0.78,0.7861,0.7924,0.7989,0.8056,0.8125,0.8196,0.8269,0.8344,0.8421,0.85,0.8581,0.8664,0.8749,0.8836,0.8925,0.9016,0.9109,0.9204,0.9301,0.94,0.9501,0.9604,0.9709,0.9816,0.9925,1.0036,1.0149,1.0264,1.0381,1.05,1.0621,1.0744,1.0869,1.0996,1.1125,1.1256,1.1389,1.1524,1.1661,1.18,1.1941,1.2084,1.2229,1.2376,1.2525,1.2676,1.2829,1.2984,1.3141,1.33,1.3461,1.3624,1.3789,1.3956,1.4125,1.4296,1.4469,1.4644,1.4821,1.5,1.5181,1.5364,1.5549,1.5736,1.5925,1.6116,1.6309,1.6504,1.6701,1.69,1.7101,1.7304,1.7509,1.7716,1.7925,1.8136,1.8349,1.8564,1.8781,1.9],"text":["b: -0.50<br />value: 2.8500<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.49<br />value: 2.8181<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.48<br />value: 2.7864<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.47<br />value: 2.7549<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.46<br />value: 2.7236<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.45<br />value: 2.6925<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.44<br />value: 2.6616<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.43<br />value: 2.6309<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.42<br />value: 2.6004<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.41<br />value: 2.5701<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.40<br />value: 2.5400<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.39<br />value: 2.5101<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.38<br />value: 2.4804<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.37<br />value: 2.4509<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.36<br />value: 2.4216<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.35<br />value: 2.3925<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.34<br />value: 2.3636<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.33<br />value: 2.3349<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.32<br />value: 2.3064<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.31<br />value: 2.2781<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.30<br />value: 2.2500<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.29<br />value: 2.2221<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.28<br />value: 2.1944<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.27<br />value: 2.1669<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.26<br />value: 2.1396<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.25<br />value: 2.1125<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.24<br />value: 2.0856<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.23<br />value: 2.0589<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.22<br />value: 2.0324<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.21<br />value: 2.0061<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.20<br />value: 1.9800<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.19<br />value: 1.9541<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.18<br />value: 1.9284<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.17<br />value: 1.9029<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.16<br />value: 1.8776<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.15<br />value: 1.8525<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.14<br />value: 1.8276<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.13<br />value: 1.8029<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.12<br />value: 1.7784<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.11<br />value: 1.7541<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.10<br />value: 1.7300<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.09<br />value: 1.7061<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.08<br />value: 1.6824<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.07<br />value: 1.6589<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.06<br />value: 1.6356<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.05<br />value: 1.6125<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.04<br />value: 1.5896<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.03<br />value: 1.5669<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.02<br />value: 1.5444<br />Function: Loss + Penalty<br />Lambda: 0.2","b: -0.01<br />value: 1.5221<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.01<br />value: 1.4821<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.02<br />value: 1.4644<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.03<br />value: 1.4469<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.04<br />value: 1.4296<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.05<br />value: 1.4125<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.06<br />value: 1.3956<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.07<br />value: 1.3789<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.08<br />value: 1.3624<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.09<br />value: 1.3461<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.10<br />value: 1.3300<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.11<br />value: 1.3141<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.12<br />value: 1.2984<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.13<br />value: 1.2829<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.14<br />value: 1.2676<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.15<br />value: 1.2525<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.16<br />value: 1.2376<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.17<br />value: 1.2229<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.18<br />value: 1.2084<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.19<br />value: 1.1941<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.20<br />value: 1.1800<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.21<br />value: 1.1661<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.22<br />value: 1.1524<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.23<br />value: 1.1389<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.24<br />value: 1.1256<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.25<br />value: 1.1125<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.26<br />value: 1.0996<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.27<br />value: 1.0869<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.28<br />value: 1.0744<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.29<br />value: 1.0621<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.30<br />value: 1.0500<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.31<br />value: 1.0381<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.32<br />value: 1.0264<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.33<br />value: 1.0149<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.34<br />value: 1.0036<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.35<br />value: 0.9925<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.36<br />value: 0.9816<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.37<br />value: 0.9709<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.38<br />value: 0.9604<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.39<br />value: 0.9501<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.40<br />value: 0.9400<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.41<br />value: 0.9301<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.42<br />value: 0.9204<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.43<br />value: 0.9109<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.44<br />value: 0.9016<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.45<br />value: 0.8925<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.46<br />value: 0.8836<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.47<br />value: 0.8749<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.48<br />value: 0.8664<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.49<br />value: 0.8581<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.50<br />value: 0.8500<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.51<br />value: 0.8421<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.52<br />value: 0.8344<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.53<br />value: 0.8269<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.54<br />value: 0.8196<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.55<br />value: 0.8125<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.56<br />value: 0.8056<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.57<br />value: 0.7989<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.58<br />value: 0.7924<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.59<br />value: 0.7861<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.60<br />value: 0.7800<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.61<br />value: 0.7741<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.62<br />value: 0.7684<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.63<br />value: 0.7629<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.64<br />value: 0.7576<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.65<br />value: 0.7525<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.66<br />value: 0.7476<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.67<br />value: 0.7429<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.68<br />value: 0.7384<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.69<br />value: 0.7341<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.70<br />value: 0.7300<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.71<br />value: 0.7261<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.72<br />value: 0.7224<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.73<br />value: 0.7189<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.74<br />value: 0.7156<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.75<br />value: 0.7125<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.76<br />value: 0.7096<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.77<br />value: 0.7069<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.78<br />value: 0.7044<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.79<br />value: 0.7021<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.80<br />value: 0.7000<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.81<br />value: 0.6981<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.82<br />value: 0.6964<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.83<br />value: 0.6949<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.84<br />value: 0.6936<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.85<br />value: 0.6925<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.86<br />value: 0.6916<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.87<br />value: 0.6909<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.88<br />value: 0.6904<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.89<br />value: 0.6901<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.90<br />value: 0.6900<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.91<br />value: 0.6901<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.92<br />value: 0.6904<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.93<br />value: 0.6909<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.94<br />value: 0.6916<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.95<br />value: 0.6925<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.96<br />value: 0.6936<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.97<br />value: 0.6949<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.98<br />value: 0.6964<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  0.99<br />value: 0.6981<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.00<br />value: 0.7000<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.01<br />value: 0.7021<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.02<br />value: 0.7044<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.03<br />value: 0.7069<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.04<br />value: 0.7096<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.05<br />value: 0.7125<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.06<br />value: 0.7156<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.07<br />value: 0.7189<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.08<br />value: 0.7224<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.09<br />value: 0.7261<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.10<br />value: 0.7300<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.11<br />value: 0.7341<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.12<br />value: 0.7384<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.13<br />value: 0.7429<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.14<br />value: 0.7476<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.15<br />value: 0.7525<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.16<br />value: 0.7576<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.17<br />value: 0.7629<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.18<br />value: 0.7684<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.19<br />value: 0.7741<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.20<br />value: 0.7800<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.21<br />value: 0.7861<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.22<br />value: 0.7924<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.23<br />value: 0.7989<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.24<br />value: 0.8056<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.25<br />value: 0.8125<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.26<br />value: 0.8196<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.27<br />value: 0.8269<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.28<br />value: 0.8344<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.29<br />value: 0.8421<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.30<br />value: 0.8500<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.31<br />value: 0.8581<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.32<br />value: 0.8664<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.33<br />value: 0.8749<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.34<br />value: 0.8836<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.35<br />value: 0.8925<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.36<br />value: 0.9016<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.37<br />value: 0.9109<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.38<br />value: 0.9204<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.39<br />value: 0.9301<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.40<br />value: 0.9400<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.41<br />value: 0.9501<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.42<br />value: 0.9604<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.43<br />value: 0.9709<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.44<br />value: 0.9816<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.45<br />value: 0.9925<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.46<br />value: 1.0036<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.47<br />value: 1.0149<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.48<br />value: 1.0264<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.49<br />value: 1.0381<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.50<br />value: 1.0500<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.51<br />value: 1.0621<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.52<br />value: 1.0744<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.53<br />value: 1.0869<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.54<br />value: 1.0996<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.55<br />value: 1.1125<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.56<br />value: 1.1256<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.57<br />value: 1.1389<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.58<br />value: 1.1524<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.59<br />value: 1.1661<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.60<br />value: 1.1800<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.61<br />value: 1.1941<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.62<br />value: 1.2084<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.63<br />value: 1.2229<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.64<br />value: 1.2376<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.65<br />value: 1.2525<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.66<br />value: 1.2676<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.67<br />value: 1.2829<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.68<br />value: 1.2984<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.69<br />value: 1.3141<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.70<br />value: 1.3300<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.71<br />value: 1.3461<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.72<br />value: 1.3624<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.73<br />value: 1.3789<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.74<br />value: 1.3956<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.75<br />value: 1.4125<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.76<br />value: 1.4296<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.77<br />value: 1.4469<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.78<br />value: 1.4644<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.79<br />value: 1.4821<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.80<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.81<br />value: 1.5181<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.82<br />value: 1.5364<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.83<br />value: 1.5549<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.84<br />value: 1.5736<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.85<br />value: 1.5925<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.86<br />value: 1.6116<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.87<br />value: 1.6309<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.88<br />value: 1.6504<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.89<br />value: 1.6701<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.90<br />value: 1.6900<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.91<br />value: 1.7101<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.92<br />value: 1.7304<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.93<br />value: 1.7509<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.94<br />value: 1.7716<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.95<br />value: 1.7925<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.96<br />value: 1.8136<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.97<br />value: 1.8349<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.98<br />value: 1.8564<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  1.99<br />value: 1.8781<br />Function: Loss + Penalty<br />Lambda: 0.2","b:  2.00<br />value: 1.9000<br />Function: Loss + Penalty<br />Lambda: 0.2"],"frame":"0.2","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.1,0.098,0.096,0.094,0.092,0.09,0.088,0.086,0.084,0.082,0.08,0.078,0.076,0.074,0.072,0.07,0.068,0.066,0.064,0.062,0.06,0.058,0.056,0.054,0.052,0.05,0.048,0.046,0.044,0.042,0.04,0.038,0.036,0.034,0.032,0.03,0.028,0.026,0.024,0.022,0.02,0.018,0.016,0.014,0.012,0.01,0.008,0.00599999999999999,0.004,0.002,0,0.002,0.004,0.00600000000000001,0.00800000000000001,0.01,0.012,0.014,0.016,0.018,0.02,0.022,0.024,0.026,0.028,0.03,0.032,0.034,0.036,0.038,0.04,0.042,0.044,0.046,0.048,0.05,0.052,0.054,0.056,0.058,0.06,0.062,0.064,0.066,0.068,0.07,0.072,0.074,0.076,0.078,0.08,0.082,0.084,0.086,0.088,0.09,0.092,0.094,0.096,0.098,0.1,0.102,0.104,0.106,0.108,0.11,0.112,0.114,0.116,0.118,0.12,0.122,0.124,0.126,0.128,0.13,0.132,0.134,0.136,0.138,0.14,0.142,0.144,0.146,0.148,0.15,0.152,0.154,0.156,0.158,0.16,0.162,0.164,0.166,0.168,0.17,0.172,0.174,0.176,0.178,0.18,0.182,0.184,0.186,0.188,0.19,0.192,0.194,0.196,0.198,0.2,0.202,0.204,0.206,0.208,0.21,0.212,0.214,0.216,0.218,0.22,0.222,0.224,0.226,0.228,0.23,0.232,0.234,0.236,0.238,0.24,0.242,0.244,0.246,0.248,0.25,0.252,0.254,0.256,0.258,0.26,0.262,0.264,0.266,0.268,0.27,0.272,0.274,0.276,0.278,0.28,0.282,0.284,0.286,0.288,0.29,0.292,0.294,0.296,0.298,0.3,0.302,0.304,0.306,0.308,0.31,0.312,0.314,0.316,0.318,0.32,0.322,0.324,0.326,0.328,0.33,0.332,0.334,0.336,0.338,0.34,0.342,0.344,0.346,0.348,0.35,0.352,0.354,0.356,0.358,0.36,0.362,0.364,0.366,0.368,0.37,0.372,0.374,0.376,0.378,0.38,0.382,0.384,0.386,0.388,0.39,0.392,0.394,0.396,0.398,0.4],"text":["b: -0.50<br />value: 0.1000<br />Function: Penalty<br />Lambda: 0.2","b: -0.49<br />value: 0.0980<br />Function: Penalty<br />Lambda: 0.2","b: -0.48<br />value: 0.0960<br />Function: Penalty<br />Lambda: 0.2","b: -0.47<br />value: 0.0940<br />Function: Penalty<br />Lambda: 0.2","b: -0.46<br />value: 0.0920<br />Function: Penalty<br />Lambda: 0.2","b: -0.45<br />value: 0.0900<br />Function: Penalty<br />Lambda: 0.2","b: -0.44<br />value: 0.0880<br />Function: Penalty<br />Lambda: 0.2","b: -0.43<br />value: 0.0860<br />Function: Penalty<br />Lambda: 0.2","b: -0.42<br />value: 0.0840<br />Function: Penalty<br />Lambda: 0.2","b: -0.41<br />value: 0.0820<br />Function: Penalty<br />Lambda: 0.2","b: -0.40<br />value: 0.0800<br />Function: Penalty<br />Lambda: 0.2","b: -0.39<br />value: 0.0780<br />Function: Penalty<br />Lambda: 0.2","b: -0.38<br />value: 0.0760<br />Function: Penalty<br />Lambda: 0.2","b: -0.37<br />value: 0.0740<br />Function: Penalty<br />Lambda: 0.2","b: -0.36<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.2","b: -0.35<br />value: 0.0700<br />Function: Penalty<br />Lambda: 0.2","b: -0.34<br />value: 0.0680<br />Function: Penalty<br />Lambda: 0.2","b: -0.33<br />value: 0.0660<br />Function: Penalty<br />Lambda: 0.2","b: -0.32<br />value: 0.0640<br />Function: Penalty<br />Lambda: 0.2","b: -0.31<br />value: 0.0620<br />Function: Penalty<br />Lambda: 0.2","b: -0.30<br />value: 0.0600<br />Function: Penalty<br />Lambda: 0.2","b: -0.29<br />value: 0.0580<br />Function: Penalty<br />Lambda: 0.2","b: -0.28<br />value: 0.0560<br />Function: Penalty<br />Lambda: 0.2","b: -0.27<br />value: 0.0540<br />Function: Penalty<br />Lambda: 0.2","b: -0.26<br />value: 0.0520<br />Function: Penalty<br />Lambda: 0.2","b: -0.25<br />value: 0.0500<br />Function: Penalty<br />Lambda: 0.2","b: -0.24<br />value: 0.0480<br />Function: Penalty<br />Lambda: 0.2","b: -0.23<br />value: 0.0460<br />Function: Penalty<br />Lambda: 0.2","b: -0.22<br />value: 0.0440<br />Function: Penalty<br />Lambda: 0.2","b: -0.21<br />value: 0.0420<br />Function: Penalty<br />Lambda: 0.2","b: -0.20<br />value: 0.0400<br />Function: Penalty<br />Lambda: 0.2","b: -0.19<br />value: 0.0380<br />Function: Penalty<br />Lambda: 0.2","b: -0.18<br />value: 0.0360<br />Function: Penalty<br />Lambda: 0.2","b: -0.17<br />value: 0.0340<br />Function: Penalty<br />Lambda: 0.2","b: -0.16<br />value: 0.0320<br />Function: Penalty<br />Lambda: 0.2","b: -0.15<br />value: 0.0300<br />Function: Penalty<br />Lambda: 0.2","b: -0.14<br />value: 0.0280<br />Function: Penalty<br />Lambda: 0.2","b: -0.13<br />value: 0.0260<br />Function: Penalty<br />Lambda: 0.2","b: -0.12<br />value: 0.0240<br />Function: Penalty<br />Lambda: 0.2","b: -0.11<br />value: 0.0220<br />Function: Penalty<br />Lambda: 0.2","b: -0.10<br />value: 0.0200<br />Function: Penalty<br />Lambda: 0.2","b: -0.09<br />value: 0.0180<br />Function: Penalty<br />Lambda: 0.2","b: -0.08<br />value: 0.0160<br />Function: Penalty<br />Lambda: 0.2","b: -0.07<br />value: 0.0140<br />Function: Penalty<br />Lambda: 0.2","b: -0.06<br />value: 0.0120<br />Function: Penalty<br />Lambda: 0.2","b: -0.05<br />value: 0.0100<br />Function: Penalty<br />Lambda: 0.2","b: -0.04<br />value: 0.0080<br />Function: Penalty<br />Lambda: 0.2","b: -0.03<br />value: 0.0060<br />Function: Penalty<br />Lambda: 0.2","b: -0.02<br />value: 0.0040<br />Function: Penalty<br />Lambda: 0.2","b: -0.01<br />value: 0.0020<br />Function: Penalty<br />Lambda: 0.2","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.2","b:  0.01<br />value: 0.0020<br />Function: Penalty<br />Lambda: 0.2","b:  0.02<br />value: 0.0040<br />Function: Penalty<br />Lambda: 0.2","b:  0.03<br />value: 0.0060<br />Function: Penalty<br />Lambda: 0.2","b:  0.04<br />value: 0.0080<br />Function: Penalty<br />Lambda: 0.2","b:  0.05<br />value: 0.0100<br />Function: Penalty<br />Lambda: 0.2","b:  0.06<br />value: 0.0120<br />Function: Penalty<br />Lambda: 0.2","b:  0.07<br />value: 0.0140<br />Function: Penalty<br />Lambda: 0.2","b:  0.08<br />value: 0.0160<br />Function: Penalty<br />Lambda: 0.2","b:  0.09<br />value: 0.0180<br />Function: Penalty<br />Lambda: 0.2","b:  0.10<br />value: 0.0200<br />Function: Penalty<br />Lambda: 0.2","b:  0.11<br />value: 0.0220<br />Function: Penalty<br />Lambda: 0.2","b:  0.12<br />value: 0.0240<br />Function: Penalty<br />Lambda: 0.2","b:  0.13<br />value: 0.0260<br />Function: Penalty<br />Lambda: 0.2","b:  0.14<br />value: 0.0280<br />Function: Penalty<br />Lambda: 0.2","b:  0.15<br />value: 0.0300<br />Function: Penalty<br />Lambda: 0.2","b:  0.16<br />value: 0.0320<br />Function: Penalty<br />Lambda: 0.2","b:  0.17<br />value: 0.0340<br />Function: Penalty<br />Lambda: 0.2","b:  0.18<br />value: 0.0360<br />Function: Penalty<br />Lambda: 0.2","b:  0.19<br />value: 0.0380<br />Function: Penalty<br />Lambda: 0.2","b:  0.20<br />value: 0.0400<br />Function: Penalty<br />Lambda: 0.2","b:  0.21<br />value: 0.0420<br />Function: Penalty<br />Lambda: 0.2","b:  0.22<br />value: 0.0440<br />Function: Penalty<br />Lambda: 0.2","b:  0.23<br />value: 0.0460<br />Function: Penalty<br />Lambda: 0.2","b:  0.24<br />value: 0.0480<br />Function: Penalty<br />Lambda: 0.2","b:  0.25<br />value: 0.0500<br />Function: Penalty<br />Lambda: 0.2","b:  0.26<br />value: 0.0520<br />Function: Penalty<br />Lambda: 0.2","b:  0.27<br />value: 0.0540<br />Function: Penalty<br />Lambda: 0.2","b:  0.28<br />value: 0.0560<br />Function: Penalty<br />Lambda: 0.2","b:  0.29<br />value: 0.0580<br />Function: Penalty<br />Lambda: 0.2","b:  0.30<br />value: 0.0600<br />Function: Penalty<br />Lambda: 0.2","b:  0.31<br />value: 0.0620<br />Function: Penalty<br />Lambda: 0.2","b:  0.32<br />value: 0.0640<br />Function: Penalty<br />Lambda: 0.2","b:  0.33<br />value: 0.0660<br />Function: Penalty<br />Lambda: 0.2","b:  0.34<br />value: 0.0680<br />Function: Penalty<br />Lambda: 0.2","b:  0.35<br />value: 0.0700<br />Function: Penalty<br />Lambda: 0.2","b:  0.36<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.2","b:  0.37<br />value: 0.0740<br />Function: Penalty<br />Lambda: 0.2","b:  0.38<br />value: 0.0760<br />Function: Penalty<br />Lambda: 0.2","b:  0.39<br />value: 0.0780<br />Function: Penalty<br />Lambda: 0.2","b:  0.40<br />value: 0.0800<br />Function: Penalty<br />Lambda: 0.2","b:  0.41<br />value: 0.0820<br />Function: Penalty<br />Lambda: 0.2","b:  0.42<br />value: 0.0840<br />Function: Penalty<br />Lambda: 0.2","b:  0.43<br />value: 0.0860<br />Function: Penalty<br />Lambda: 0.2","b:  0.44<br />value: 0.0880<br />Function: Penalty<br />Lambda: 0.2","b:  0.45<br />value: 0.0900<br />Function: Penalty<br />Lambda: 0.2","b:  0.46<br />value: 0.0920<br />Function: Penalty<br />Lambda: 0.2","b:  0.47<br />value: 0.0940<br />Function: Penalty<br />Lambda: 0.2","b:  0.48<br />value: 0.0960<br />Function: Penalty<br />Lambda: 0.2","b:  0.49<br />value: 0.0980<br />Function: Penalty<br />Lambda: 0.2","b:  0.50<br />value: 0.1000<br />Function: Penalty<br />Lambda: 0.2","b:  0.51<br />value: 0.1020<br />Function: Penalty<br />Lambda: 0.2","b:  0.52<br />value: 0.1040<br />Function: Penalty<br />Lambda: 0.2","b:  0.53<br />value: 0.1060<br />Function: Penalty<br />Lambda: 0.2","b:  0.54<br />value: 0.1080<br />Function: Penalty<br />Lambda: 0.2","b:  0.55<br />value: 0.1100<br />Function: Penalty<br />Lambda: 0.2","b:  0.56<br />value: 0.1120<br />Function: Penalty<br />Lambda: 0.2","b:  0.57<br />value: 0.1140<br />Function: Penalty<br />Lambda: 0.2","b:  0.58<br />value: 0.1160<br />Function: Penalty<br />Lambda: 0.2","b:  0.59<br />value: 0.1180<br />Function: Penalty<br />Lambda: 0.2","b:  0.60<br />value: 0.1200<br />Function: Penalty<br />Lambda: 0.2","b:  0.61<br />value: 0.1220<br />Function: Penalty<br />Lambda: 0.2","b:  0.62<br />value: 0.1240<br />Function: Penalty<br />Lambda: 0.2","b:  0.63<br />value: 0.1260<br />Function: Penalty<br />Lambda: 0.2","b:  0.64<br />value: 0.1280<br />Function: Penalty<br />Lambda: 0.2","b:  0.65<br />value: 0.1300<br />Function: Penalty<br />Lambda: 0.2","b:  0.66<br />value: 0.1320<br />Function: Penalty<br />Lambda: 0.2","b:  0.67<br />value: 0.1340<br />Function: Penalty<br />Lambda: 0.2","b:  0.68<br />value: 0.1360<br />Function: Penalty<br />Lambda: 0.2","b:  0.69<br />value: 0.1380<br />Function: Penalty<br />Lambda: 0.2","b:  0.70<br />value: 0.1400<br />Function: Penalty<br />Lambda: 0.2","b:  0.71<br />value: 0.1420<br />Function: Penalty<br />Lambda: 0.2","b:  0.72<br />value: 0.1440<br />Function: Penalty<br />Lambda: 0.2","b:  0.73<br />value: 0.1460<br />Function: Penalty<br />Lambda: 0.2","b:  0.74<br />value: 0.1480<br />Function: Penalty<br />Lambda: 0.2","b:  0.75<br />value: 0.1500<br />Function: Penalty<br />Lambda: 0.2","b:  0.76<br />value: 0.1520<br />Function: Penalty<br />Lambda: 0.2","b:  0.77<br />value: 0.1540<br />Function: Penalty<br />Lambda: 0.2","b:  0.78<br />value: 0.1560<br />Function: Penalty<br />Lambda: 0.2","b:  0.79<br />value: 0.1580<br />Function: Penalty<br />Lambda: 0.2","b:  0.80<br />value: 0.1600<br />Function: Penalty<br />Lambda: 0.2","b:  0.81<br />value: 0.1620<br />Function: Penalty<br />Lambda: 0.2","b:  0.82<br />value: 0.1640<br />Function: Penalty<br />Lambda: 0.2","b:  0.83<br />value: 0.1660<br />Function: Penalty<br />Lambda: 0.2","b:  0.84<br />value: 0.1680<br />Function: Penalty<br />Lambda: 0.2","b:  0.85<br />value: 0.1700<br />Function: Penalty<br />Lambda: 0.2","b:  0.86<br />value: 0.1720<br />Function: Penalty<br />Lambda: 0.2","b:  0.87<br />value: 0.1740<br />Function: Penalty<br />Lambda: 0.2","b:  0.88<br />value: 0.1760<br />Function: Penalty<br />Lambda: 0.2","b:  0.89<br />value: 0.1780<br />Function: Penalty<br />Lambda: 0.2","b:  0.90<br />value: 0.1800<br />Function: Penalty<br />Lambda: 0.2","b:  0.91<br />value: 0.1820<br />Function: Penalty<br />Lambda: 0.2","b:  0.92<br />value: 0.1840<br />Function: Penalty<br />Lambda: 0.2","b:  0.93<br />value: 0.1860<br />Function: Penalty<br />Lambda: 0.2","b:  0.94<br />value: 0.1880<br />Function: Penalty<br />Lambda: 0.2","b:  0.95<br />value: 0.1900<br />Function: Penalty<br />Lambda: 0.2","b:  0.96<br />value: 0.1920<br />Function: Penalty<br />Lambda: 0.2","b:  0.97<br />value: 0.1940<br />Function: Penalty<br />Lambda: 0.2","b:  0.98<br />value: 0.1960<br />Function: Penalty<br />Lambda: 0.2","b:  0.99<br />value: 0.1980<br />Function: Penalty<br />Lambda: 0.2","b:  1.00<br />value: 0.2000<br />Function: Penalty<br />Lambda: 0.2","b:  1.01<br />value: 0.2020<br />Function: Penalty<br />Lambda: 0.2","b:  1.02<br />value: 0.2040<br />Function: Penalty<br />Lambda: 0.2","b:  1.03<br />value: 0.2060<br />Function: Penalty<br />Lambda: 0.2","b:  1.04<br />value: 0.2080<br />Function: Penalty<br />Lambda: 0.2","b:  1.05<br />value: 0.2100<br />Function: Penalty<br />Lambda: 0.2","b:  1.06<br />value: 0.2120<br />Function: Penalty<br />Lambda: 0.2","b:  1.07<br />value: 0.2140<br />Function: Penalty<br />Lambda: 0.2","b:  1.08<br />value: 0.2160<br />Function: Penalty<br />Lambda: 0.2","b:  1.09<br />value: 0.2180<br />Function: Penalty<br />Lambda: 0.2","b:  1.10<br />value: 0.2200<br />Function: Penalty<br />Lambda: 0.2","b:  1.11<br />value: 0.2220<br />Function: Penalty<br />Lambda: 0.2","b:  1.12<br />value: 0.2240<br />Function: Penalty<br />Lambda: 0.2","b:  1.13<br />value: 0.2260<br />Function: Penalty<br />Lambda: 0.2","b:  1.14<br />value: 0.2280<br />Function: Penalty<br />Lambda: 0.2","b:  1.15<br />value: 0.2300<br />Function: Penalty<br />Lambda: 0.2","b:  1.16<br />value: 0.2320<br />Function: Penalty<br />Lambda: 0.2","b:  1.17<br />value: 0.2340<br />Function: Penalty<br />Lambda: 0.2","b:  1.18<br />value: 0.2360<br />Function: Penalty<br />Lambda: 0.2","b:  1.19<br />value: 0.2380<br />Function: Penalty<br />Lambda: 0.2","b:  1.20<br />value: 0.2400<br />Function: Penalty<br />Lambda: 0.2","b:  1.21<br />value: 0.2420<br />Function: Penalty<br />Lambda: 0.2","b:  1.22<br />value: 0.2440<br />Function: Penalty<br />Lambda: 0.2","b:  1.23<br />value: 0.2460<br />Function: Penalty<br />Lambda: 0.2","b:  1.24<br />value: 0.2480<br />Function: Penalty<br />Lambda: 0.2","b:  1.25<br />value: 0.2500<br />Function: Penalty<br />Lambda: 0.2","b:  1.26<br />value: 0.2520<br />Function: Penalty<br />Lambda: 0.2","b:  1.27<br />value: 0.2540<br />Function: Penalty<br />Lambda: 0.2","b:  1.28<br />value: 0.2560<br />Function: Penalty<br />Lambda: 0.2","b:  1.29<br />value: 0.2580<br />Function: Penalty<br />Lambda: 0.2","b:  1.30<br />value: 0.2600<br />Function: Penalty<br />Lambda: 0.2","b:  1.31<br />value: 0.2620<br />Function: Penalty<br />Lambda: 0.2","b:  1.32<br />value: 0.2640<br />Function: Penalty<br />Lambda: 0.2","b:  1.33<br />value: 0.2660<br />Function: Penalty<br />Lambda: 0.2","b:  1.34<br />value: 0.2680<br />Function: Penalty<br />Lambda: 0.2","b:  1.35<br />value: 0.2700<br />Function: Penalty<br />Lambda: 0.2","b:  1.36<br />value: 0.2720<br />Function: Penalty<br />Lambda: 0.2","b:  1.37<br />value: 0.2740<br />Function: Penalty<br />Lambda: 0.2","b:  1.38<br />value: 0.2760<br />Function: Penalty<br />Lambda: 0.2","b:  1.39<br />value: 0.2780<br />Function: Penalty<br />Lambda: 0.2","b:  1.40<br />value: 0.2800<br />Function: Penalty<br />Lambda: 0.2","b:  1.41<br />value: 0.2820<br />Function: Penalty<br />Lambda: 0.2","b:  1.42<br />value: 0.2840<br />Function: Penalty<br />Lambda: 0.2","b:  1.43<br />value: 0.2860<br />Function: Penalty<br />Lambda: 0.2","b:  1.44<br />value: 0.2880<br />Function: Penalty<br />Lambda: 0.2","b:  1.45<br />value: 0.2900<br />Function: Penalty<br />Lambda: 0.2","b:  1.46<br />value: 0.2920<br />Function: Penalty<br />Lambda: 0.2","b:  1.47<br />value: 0.2940<br />Function: Penalty<br />Lambda: 0.2","b:  1.48<br />value: 0.2960<br />Function: Penalty<br />Lambda: 0.2","b:  1.49<br />value: 0.2980<br />Function: Penalty<br />Lambda: 0.2","b:  1.50<br />value: 0.3000<br />Function: Penalty<br />Lambda: 0.2","b:  1.51<br />value: 0.3020<br />Function: Penalty<br />Lambda: 0.2","b:  1.52<br />value: 0.3040<br />Function: Penalty<br />Lambda: 0.2","b:  1.53<br />value: 0.3060<br />Function: Penalty<br />Lambda: 0.2","b:  1.54<br />value: 0.3080<br />Function: Penalty<br />Lambda: 0.2","b:  1.55<br />value: 0.3100<br />Function: Penalty<br />Lambda: 0.2","b:  1.56<br />value: 0.3120<br />Function: Penalty<br />Lambda: 0.2","b:  1.57<br />value: 0.3140<br />Function: Penalty<br />Lambda: 0.2","b:  1.58<br />value: 0.3160<br />Function: Penalty<br />Lambda: 0.2","b:  1.59<br />value: 0.3180<br />Function: Penalty<br />Lambda: 0.2","b:  1.60<br />value: 0.3200<br />Function: Penalty<br />Lambda: 0.2","b:  1.61<br />value: 0.3220<br />Function: Penalty<br />Lambda: 0.2","b:  1.62<br />value: 0.3240<br />Function: Penalty<br />Lambda: 0.2","b:  1.63<br />value: 0.3260<br />Function: Penalty<br />Lambda: 0.2","b:  1.64<br />value: 0.3280<br />Function: Penalty<br />Lambda: 0.2","b:  1.65<br />value: 0.3300<br />Function: Penalty<br />Lambda: 0.2","b:  1.66<br />value: 0.3320<br />Function: Penalty<br />Lambda: 0.2","b:  1.67<br />value: 0.3340<br />Function: Penalty<br />Lambda: 0.2","b:  1.68<br />value: 0.3360<br />Function: Penalty<br />Lambda: 0.2","b:  1.69<br />value: 0.3380<br />Function: Penalty<br />Lambda: 0.2","b:  1.70<br />value: 0.3400<br />Function: Penalty<br />Lambda: 0.2","b:  1.71<br />value: 0.3420<br />Function: Penalty<br />Lambda: 0.2","b:  1.72<br />value: 0.3440<br />Function: Penalty<br />Lambda: 0.2","b:  1.73<br />value: 0.3460<br />Function: Penalty<br />Lambda: 0.2","b:  1.74<br />value: 0.3480<br />Function: Penalty<br />Lambda: 0.2","b:  1.75<br />value: 0.3500<br />Function: Penalty<br />Lambda: 0.2","b:  1.76<br />value: 0.3520<br />Function: Penalty<br />Lambda: 0.2","b:  1.77<br />value: 0.3540<br />Function: Penalty<br />Lambda: 0.2","b:  1.78<br />value: 0.3560<br />Function: Penalty<br />Lambda: 0.2","b:  1.79<br />value: 0.3580<br />Function: Penalty<br />Lambda: 0.2","b:  1.80<br />value: 0.3600<br />Function: Penalty<br />Lambda: 0.2","b:  1.81<br />value: 0.3620<br />Function: Penalty<br />Lambda: 0.2","b:  1.82<br />value: 0.3640<br />Function: Penalty<br />Lambda: 0.2","b:  1.83<br />value: 0.3660<br />Function: Penalty<br />Lambda: 0.2","b:  1.84<br />value: 0.3680<br />Function: Penalty<br />Lambda: 0.2","b:  1.85<br />value: 0.3700<br />Function: Penalty<br />Lambda: 0.2","b:  1.86<br />value: 0.3720<br />Function: Penalty<br />Lambda: 0.2","b:  1.87<br />value: 0.3740<br />Function: Penalty<br />Lambda: 0.2","b:  1.88<br />value: 0.3760<br />Function: Penalty<br />Lambda: 0.2","b:  1.89<br />value: 0.3780<br />Function: Penalty<br />Lambda: 0.2","b:  1.90<br />value: 0.3800<br />Function: Penalty<br />Lambda: 0.2","b:  1.91<br />value: 0.3820<br />Function: Penalty<br />Lambda: 0.2","b:  1.92<br />value: 0.3840<br />Function: Penalty<br />Lambda: 0.2","b:  1.93<br />value: 0.3860<br />Function: Penalty<br />Lambda: 0.2","b:  1.94<br />value: 0.3880<br />Function: Penalty<br />Lambda: 0.2","b:  1.95<br />value: 0.3900<br />Function: Penalty<br />Lambda: 0.2","b:  1.96<br />value: 0.3920<br />Function: Penalty<br />Lambda: 0.2","b:  1.97<br />value: 0.3940<br />Function: Penalty<br />Lambda: 0.2","b:  1.98<br />value: 0.3960<br />Function: Penalty<br />Lambda: 0.2","b:  1.99<br />value: 0.3980<br />Function: Penalty<br />Lambda: 0.2","b:  2.00<br />value: 0.4000<br />Function: Penalty<br />Lambda: 0.2"],"frame":"0.2","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"0.3","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 0.3","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 0.3","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 0.3","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 0.3","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 0.3","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 0.3","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 0.3","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 0.3","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 0.3","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 0.3","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 0.3","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 0.3","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 0.3","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 0.3","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 0.3","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 0.3","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 0.3","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 0.3","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 0.3","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 0.3","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 0.3","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 0.3","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 0.3","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 0.3","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 0.3","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 0.3","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 0.3","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 0.3","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 0.3","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 0.3","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 0.3","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 0.3","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 0.3","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 0.3","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 0.3","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 0.3","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 0.3","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 0.3","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 0.3","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 0.3","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 0.3","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 0.3","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 0.3","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 0.3","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 0.3","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 0.3","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 0.3","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 0.3","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 0.3","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 0.3","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.3","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.3","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.3","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.3","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.3","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.3","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.3","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.3","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.3","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.3","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.3","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.3","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.3","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.3","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.3","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.3","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.3","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.3","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.3","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.3","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.3","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.3","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.3","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.3","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.3","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.3","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.3","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.3","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.3","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.3","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.3","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.3","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.3","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.3","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.3","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.3","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.3","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.3","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.3","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.3","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.3","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.3","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.3","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.3","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.3","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.3","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.3","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.3","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.3","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.3","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.3","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.3","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.3","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.3","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.3","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.3","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.3","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.3","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.3","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.3","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.3","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.3","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.3","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.3","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.3","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.3","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.3","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.3","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.3","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.3","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.3","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.3","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.3","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.3","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.3","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.3","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.3","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.3","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.3","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.3","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.3","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.3","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.3","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.3","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.3","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.3","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.3","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.3","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.3","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.3","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.3","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.3","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.3","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.3","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.3","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.3","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.3","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.3","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.3","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.3","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 0.3","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.3","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.3","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.3","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.3","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.3","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.3","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.3","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.3","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.3","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.3","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.3","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.3","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.3","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.3","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.3","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.3","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.3","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.3","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.3","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.3","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.3","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.3","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.3","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.3","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.3","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.3","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.3","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.3","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.3","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.3","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.3","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.3","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.3","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.3","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.3","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.3","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.3","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.3","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.3","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.3","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.3","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.3","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.3","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.3","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.3","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.3","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.3","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.3","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.3","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.3","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.3","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.3","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.3","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.3","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.3","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.3","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.3","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.3","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.3","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.3","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.3","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.3","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.3","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.3","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.3","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.3","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.3","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.3","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.3","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.3","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.3","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.3","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.3","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.3","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.3","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.3","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.3","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.3","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.3","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.3","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.3","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.3","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.3","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.3","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.3","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.3","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.3","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.3","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.3","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.3","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.3","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.3","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.3","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.3","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.3","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.3","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.3","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.3","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.3","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.3"],"frame":"0.3","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.9,2.8671,2.8344,2.8019,2.7696,2.7375,2.7056,2.6739,2.6424,2.6111,2.58,2.5491,2.5184,2.4879,2.4576,2.4275,2.3976,2.3679,2.3384,2.3091,2.28,2.2511,2.2224,2.1939,2.1656,2.1375,2.1096,2.0819,2.0544,2.0271,2,1.9731,1.9464,1.9199,1.8936,1.8675,1.8416,1.8159,1.7904,1.7651,1.74,1.7151,1.6904,1.6659,1.6416,1.6175,1.5936,1.5699,1.5464,1.5231,1.5,1.4831,1.4664,1.4499,1.4336,1.4175,1.4016,1.3859,1.3704,1.3551,1.34,1.3251,1.3104,1.2959,1.2816,1.2675,1.2536,1.2399,1.2264,1.2131,1.2,1.1871,1.1744,1.1619,1.1496,1.1375,1.1256,1.1139,1.1024,1.0911,1.08,1.0691,1.0584,1.0479,1.0376,1.0275,1.0176,1.0079,0.9984,0.9891,0.98,0.9711,0.9624,0.9539,0.9456,0.9375,0.9296,0.9219,0.9144,0.9071,0.9,0.8931,0.8864,0.8799,0.8736,0.8675,0.8616,0.8559,0.8504,0.8451,0.84,0.8351,0.8304,0.8259,0.8216,0.8175,0.8136,0.8099,0.8064,0.8031,0.8,0.7971,0.7944,0.7919,0.7896,0.7875,0.7856,0.7839,0.7824,0.7811,0.78,0.7791,0.7784,0.7779,0.7776,0.7775,0.7776,0.7779,0.7784,0.7791,0.78,0.7811,0.7824,0.7839,0.7856,0.7875,0.7896,0.7919,0.7944,0.7971,0.8,0.8031,0.8064,0.8099,0.8136,0.8175,0.8216,0.8259,0.8304,0.8351,0.84,0.8451,0.8504,0.8559,0.8616,0.8675,0.8736,0.8799,0.8864,0.8931,0.9,0.9071,0.9144,0.9219,0.9296,0.9375,0.9456,0.9539,0.9624,0.9711,0.98,0.9891,0.9984,1.0079,1.0176,1.0275,1.0376,1.0479,1.0584,1.0691,1.08,1.0911,1.1024,1.1139,1.1256,1.1375,1.1496,1.1619,1.1744,1.1871,1.2,1.2131,1.2264,1.2399,1.2536,1.2675,1.2816,1.2959,1.3104,1.3251,1.34,1.3551,1.3704,1.3859,1.4016,1.4175,1.4336,1.4499,1.4664,1.4831,1.5,1.5171,1.5344,1.5519,1.5696,1.5875,1.6056,1.6239,1.6424,1.6611,1.68,1.6991,1.7184,1.7379,1.7576,1.7775,1.7976,1.8179,1.8384,1.8591,1.88,1.9011,1.9224,1.9439,1.9656,1.9875,2.0096,2.0319,2.0544,2.0771,2.1],"text":["b: -0.50<br />value: 2.9000<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.49<br />value: 2.8671<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.48<br />value: 2.8344<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.47<br />value: 2.8019<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.46<br />value: 2.7696<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.45<br />value: 2.7375<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.44<br />value: 2.7056<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.43<br />value: 2.6739<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.42<br />value: 2.6424<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.41<br />value: 2.6111<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.40<br />value: 2.5800<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.39<br />value: 2.5491<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.38<br />value: 2.5184<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.37<br />value: 2.4879<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.36<br />value: 2.4576<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.35<br />value: 2.4275<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.34<br />value: 2.3976<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.33<br />value: 2.3679<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.32<br />value: 2.3384<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.31<br />value: 2.3091<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.30<br />value: 2.2800<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.29<br />value: 2.2511<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.28<br />value: 2.2224<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.27<br />value: 2.1939<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.26<br />value: 2.1656<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.25<br />value: 2.1375<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.24<br />value: 2.1096<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.23<br />value: 2.0819<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.22<br />value: 2.0544<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.21<br />value: 2.0271<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.20<br />value: 2.0000<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.19<br />value: 1.9731<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.18<br />value: 1.9464<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.17<br />value: 1.9199<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.16<br />value: 1.8936<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.15<br />value: 1.8675<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.14<br />value: 1.8416<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.13<br />value: 1.8159<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.12<br />value: 1.7904<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.11<br />value: 1.7651<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.10<br />value: 1.7400<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.09<br />value: 1.7151<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.08<br />value: 1.6904<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.07<br />value: 1.6659<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.06<br />value: 1.6416<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.05<br />value: 1.6175<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.04<br />value: 1.5936<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.03<br />value: 1.5699<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.02<br />value: 1.5464<br />Function: Loss + Penalty<br />Lambda: 0.3","b: -0.01<br />value: 1.5231<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.01<br />value: 1.4831<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.02<br />value: 1.4664<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.03<br />value: 1.4499<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.04<br />value: 1.4336<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.05<br />value: 1.4175<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.06<br />value: 1.4016<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.07<br />value: 1.3859<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.08<br />value: 1.3704<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.09<br />value: 1.3551<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.10<br />value: 1.3400<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.11<br />value: 1.3251<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.12<br />value: 1.3104<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.13<br />value: 1.2959<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.14<br />value: 1.2816<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.15<br />value: 1.2675<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.16<br />value: 1.2536<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.17<br />value: 1.2399<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.18<br />value: 1.2264<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.19<br />value: 1.2131<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.20<br />value: 1.2000<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.21<br />value: 1.1871<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.22<br />value: 1.1744<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.23<br />value: 1.1619<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.24<br />value: 1.1496<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.25<br />value: 1.1375<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.26<br />value: 1.1256<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.27<br />value: 1.1139<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.28<br />value: 1.1024<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.29<br />value: 1.0911<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.30<br />value: 1.0800<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.31<br />value: 1.0691<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.32<br />value: 1.0584<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.33<br />value: 1.0479<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.34<br />value: 1.0376<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.35<br />value: 1.0275<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.36<br />value: 1.0176<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.37<br />value: 1.0079<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.38<br />value: 0.9984<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.39<br />value: 0.9891<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.40<br />value: 0.9800<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.41<br />value: 0.9711<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.42<br />value: 0.9624<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.43<br />value: 0.9539<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.44<br />value: 0.9456<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.45<br />value: 0.9375<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.46<br />value: 0.9296<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.47<br />value: 0.9219<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.48<br />value: 0.9144<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.49<br />value: 0.9071<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.50<br />value: 0.9000<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.51<br />value: 0.8931<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.52<br />value: 0.8864<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.53<br />value: 0.8799<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.54<br />value: 0.8736<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.55<br />value: 0.8675<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.56<br />value: 0.8616<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.57<br />value: 0.8559<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.58<br />value: 0.8504<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.59<br />value: 0.8451<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.60<br />value: 0.8400<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.61<br />value: 0.8351<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.62<br />value: 0.8304<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.63<br />value: 0.8259<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.64<br />value: 0.8216<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.65<br />value: 0.8175<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.66<br />value: 0.8136<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.67<br />value: 0.8099<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.68<br />value: 0.8064<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.69<br />value: 0.8031<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.70<br />value: 0.8000<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.71<br />value: 0.7971<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.72<br />value: 0.7944<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.73<br />value: 0.7919<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.74<br />value: 0.7896<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.75<br />value: 0.7875<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.76<br />value: 0.7856<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.77<br />value: 0.7839<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.78<br />value: 0.7824<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.79<br />value: 0.7811<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.80<br />value: 0.7800<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.81<br />value: 0.7791<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.82<br />value: 0.7784<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.83<br />value: 0.7779<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.84<br />value: 0.7776<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.85<br />value: 0.7775<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.86<br />value: 0.7776<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.87<br />value: 0.7779<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.88<br />value: 0.7784<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.89<br />value: 0.7791<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.90<br />value: 0.7800<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.91<br />value: 0.7811<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.92<br />value: 0.7824<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.93<br />value: 0.7839<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.94<br />value: 0.7856<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.95<br />value: 0.7875<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.96<br />value: 0.7896<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.97<br />value: 0.7919<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.98<br />value: 0.7944<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  0.99<br />value: 0.7971<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.00<br />value: 0.8000<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.01<br />value: 0.8031<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.02<br />value: 0.8064<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.03<br />value: 0.8099<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.04<br />value: 0.8136<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.05<br />value: 0.8175<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.06<br />value: 0.8216<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.07<br />value: 0.8259<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.08<br />value: 0.8304<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.09<br />value: 0.8351<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.10<br />value: 0.8400<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.11<br />value: 0.8451<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.12<br />value: 0.8504<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.13<br />value: 0.8559<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.14<br />value: 0.8616<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.15<br />value: 0.8675<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.16<br />value: 0.8736<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.17<br />value: 0.8799<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.18<br />value: 0.8864<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.19<br />value: 0.8931<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.20<br />value: 0.9000<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.21<br />value: 0.9071<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.22<br />value: 0.9144<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.23<br />value: 0.9219<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.24<br />value: 0.9296<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.25<br />value: 0.9375<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.26<br />value: 0.9456<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.27<br />value: 0.9539<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.28<br />value: 0.9624<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.29<br />value: 0.9711<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.30<br />value: 0.9800<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.31<br />value: 0.9891<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.32<br />value: 0.9984<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.33<br />value: 1.0079<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.34<br />value: 1.0176<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.35<br />value: 1.0275<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.36<br />value: 1.0376<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.37<br />value: 1.0479<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.38<br />value: 1.0584<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.39<br />value: 1.0691<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.40<br />value: 1.0800<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.41<br />value: 1.0911<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.42<br />value: 1.1024<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.43<br />value: 1.1139<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.44<br />value: 1.1256<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.45<br />value: 1.1375<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.46<br />value: 1.1496<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.47<br />value: 1.1619<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.48<br />value: 1.1744<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.49<br />value: 1.1871<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.50<br />value: 1.2000<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.51<br />value: 1.2131<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.52<br />value: 1.2264<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.53<br />value: 1.2399<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.54<br />value: 1.2536<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.55<br />value: 1.2675<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.56<br />value: 1.2816<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.57<br />value: 1.2959<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.58<br />value: 1.3104<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.59<br />value: 1.3251<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.60<br />value: 1.3400<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.61<br />value: 1.3551<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.62<br />value: 1.3704<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.63<br />value: 1.3859<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.64<br />value: 1.4016<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.65<br />value: 1.4175<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.66<br />value: 1.4336<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.67<br />value: 1.4499<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.68<br />value: 1.4664<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.69<br />value: 1.4831<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.70<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.71<br />value: 1.5171<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.72<br />value: 1.5344<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.73<br />value: 1.5519<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.74<br />value: 1.5696<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.75<br />value: 1.5875<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.76<br />value: 1.6056<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.77<br />value: 1.6239<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.78<br />value: 1.6424<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.79<br />value: 1.6611<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.80<br />value: 1.6800<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.81<br />value: 1.6991<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.82<br />value: 1.7184<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.83<br />value: 1.7379<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.84<br />value: 1.7576<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.85<br />value: 1.7775<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.86<br />value: 1.7976<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.87<br />value: 1.8179<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.88<br />value: 1.8384<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.89<br />value: 1.8591<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.90<br />value: 1.8800<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.91<br />value: 1.9011<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.92<br />value: 1.9224<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.93<br />value: 1.9439<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.94<br />value: 1.9656<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.95<br />value: 1.9875<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.96<br />value: 2.0096<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.97<br />value: 2.0319<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.98<br />value: 2.0544<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  1.99<br />value: 2.0771<br />Function: Loss + Penalty<br />Lambda: 0.3","b:  2.00<br />value: 2.1000<br />Function: Loss + Penalty<br />Lambda: 0.3"],"frame":"0.3","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.15,0.147,0.144,0.141,0.138,0.135,0.132,0.129,0.126,0.123,0.12,0.117,0.114,0.111,0.108,0.105,0.102,0.099,0.096,0.093,0.09,0.087,0.084,0.081,0.078,0.075,0.072,0.069,0.066,0.063,0.06,0.057,0.054,0.051,0.048,0.045,0.042,0.039,0.036,0.033,0.03,0.027,0.024,0.021,0.018,0.015,0.012,0.00899999999999999,0.00600000000000001,0.003,0,0.003,0.00600000000000001,0.00900000000000001,0.012,0.015,0.018,0.021,0.024,0.027,0.03,0.033,0.036,0.039,0.042,0.045,0.048,0.051,0.054,0.057,0.06,0.063,0.066,0.069,0.072,0.075,0.078,0.081,0.084,0.087,0.09,0.093,0.096,0.099,0.102,0.105,0.108,0.111,0.114,0.117,0.12,0.123,0.126,0.129,0.132,0.135,0.138,0.141,0.144,0.147,0.15,0.153,0.156,0.159,0.162,0.165,0.168,0.171,0.174,0.177,0.18,0.183,0.186,0.189,0.192,0.195,0.198,0.201,0.204,0.207,0.21,0.213,0.216,0.219,0.222,0.225,0.228,0.231,0.234,0.237,0.24,0.243,0.246,0.249,0.252,0.255,0.258,0.261,0.264,0.267,0.27,0.273,0.276,0.279,0.282,0.285,0.288,0.291,0.294,0.297,0.3,0.303,0.306,0.309,0.312,0.315,0.318,0.321,0.324,0.327,0.33,0.333,0.336,0.339,0.342,0.345,0.348,0.351,0.354,0.357,0.36,0.363,0.366,0.369,0.372,0.375,0.378,0.381,0.384,0.387,0.39,0.393,0.396,0.399,0.402,0.405,0.408,0.411,0.414,0.417,0.42,0.423,0.426,0.429,0.432,0.435,0.438,0.441,0.444,0.447,0.45,0.453,0.456,0.459,0.462,0.465,0.468,0.471,0.474,0.477,0.48,0.483,0.486,0.489,0.492,0.495,0.498,0.501,0.504,0.507,0.51,0.513,0.516,0.519,0.522,0.525,0.528,0.531,0.534,0.537,0.54,0.543,0.546,0.549,0.552,0.555,0.558,0.561,0.564,0.567,0.57,0.573,0.576,0.579,0.582,0.585,0.588,0.591,0.594,0.597,0.6],"text":["b: -0.50<br />value: 0.1500<br />Function: Penalty<br />Lambda: 0.3","b: -0.49<br />value: 0.1470<br />Function: Penalty<br />Lambda: 0.3","b: -0.48<br />value: 0.1440<br />Function: Penalty<br />Lambda: 0.3","b: -0.47<br />value: 0.1410<br />Function: Penalty<br />Lambda: 0.3","b: -0.46<br />value: 0.1380<br />Function: Penalty<br />Lambda: 0.3","b: -0.45<br />value: 0.1350<br />Function: Penalty<br />Lambda: 0.3","b: -0.44<br />value: 0.1320<br />Function: Penalty<br />Lambda: 0.3","b: -0.43<br />value: 0.1290<br />Function: Penalty<br />Lambda: 0.3","b: -0.42<br />value: 0.1260<br />Function: Penalty<br />Lambda: 0.3","b: -0.41<br />value: 0.1230<br />Function: Penalty<br />Lambda: 0.3","b: -0.40<br />value: 0.1200<br />Function: Penalty<br />Lambda: 0.3","b: -0.39<br />value: 0.1170<br />Function: Penalty<br />Lambda: 0.3","b: -0.38<br />value: 0.1140<br />Function: Penalty<br />Lambda: 0.3","b: -0.37<br />value: 0.1110<br />Function: Penalty<br />Lambda: 0.3","b: -0.36<br />value: 0.1080<br />Function: Penalty<br />Lambda: 0.3","b: -0.35<br />value: 0.1050<br />Function: Penalty<br />Lambda: 0.3","b: -0.34<br />value: 0.1020<br />Function: Penalty<br />Lambda: 0.3","b: -0.33<br />value: 0.0990<br />Function: Penalty<br />Lambda: 0.3","b: -0.32<br />value: 0.0960<br />Function: Penalty<br />Lambda: 0.3","b: -0.31<br />value: 0.0930<br />Function: Penalty<br />Lambda: 0.3","b: -0.30<br />value: 0.0900<br />Function: Penalty<br />Lambda: 0.3","b: -0.29<br />value: 0.0870<br />Function: Penalty<br />Lambda: 0.3","b: -0.28<br />value: 0.0840<br />Function: Penalty<br />Lambda: 0.3","b: -0.27<br />value: 0.0810<br />Function: Penalty<br />Lambda: 0.3","b: -0.26<br />value: 0.0780<br />Function: Penalty<br />Lambda: 0.3","b: -0.25<br />value: 0.0750<br />Function: Penalty<br />Lambda: 0.3","b: -0.24<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.3","b: -0.23<br />value: 0.0690<br />Function: Penalty<br />Lambda: 0.3","b: -0.22<br />value: 0.0660<br />Function: Penalty<br />Lambda: 0.3","b: -0.21<br />value: 0.0630<br />Function: Penalty<br />Lambda: 0.3","b: -0.20<br />value: 0.0600<br />Function: Penalty<br />Lambda: 0.3","b: -0.19<br />value: 0.0570<br />Function: Penalty<br />Lambda: 0.3","b: -0.18<br />value: 0.0540<br />Function: Penalty<br />Lambda: 0.3","b: -0.17<br />value: 0.0510<br />Function: Penalty<br />Lambda: 0.3","b: -0.16<br />value: 0.0480<br />Function: Penalty<br />Lambda: 0.3","b: -0.15<br />value: 0.0450<br />Function: Penalty<br />Lambda: 0.3","b: -0.14<br />value: 0.0420<br />Function: Penalty<br />Lambda: 0.3","b: -0.13<br />value: 0.0390<br />Function: Penalty<br />Lambda: 0.3","b: -0.12<br />value: 0.0360<br />Function: Penalty<br />Lambda: 0.3","b: -0.11<br />value: 0.0330<br />Function: Penalty<br />Lambda: 0.3","b: -0.10<br />value: 0.0300<br />Function: Penalty<br />Lambda: 0.3","b: -0.09<br />value: 0.0270<br />Function: Penalty<br />Lambda: 0.3","b: -0.08<br />value: 0.0240<br />Function: Penalty<br />Lambda: 0.3","b: -0.07<br />value: 0.0210<br />Function: Penalty<br />Lambda: 0.3","b: -0.06<br />value: 0.0180<br />Function: Penalty<br />Lambda: 0.3","b: -0.05<br />value: 0.0150<br />Function: Penalty<br />Lambda: 0.3","b: -0.04<br />value: 0.0120<br />Function: Penalty<br />Lambda: 0.3","b: -0.03<br />value: 0.0090<br />Function: Penalty<br />Lambda: 0.3","b: -0.02<br />value: 0.0060<br />Function: Penalty<br />Lambda: 0.3","b: -0.01<br />value: 0.0030<br />Function: Penalty<br />Lambda: 0.3","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.3","b:  0.01<br />value: 0.0030<br />Function: Penalty<br />Lambda: 0.3","b:  0.02<br />value: 0.0060<br />Function: Penalty<br />Lambda: 0.3","b:  0.03<br />value: 0.0090<br />Function: Penalty<br />Lambda: 0.3","b:  0.04<br />value: 0.0120<br />Function: Penalty<br />Lambda: 0.3","b:  0.05<br />value: 0.0150<br />Function: Penalty<br />Lambda: 0.3","b:  0.06<br />value: 0.0180<br />Function: Penalty<br />Lambda: 0.3","b:  0.07<br />value: 0.0210<br />Function: Penalty<br />Lambda: 0.3","b:  0.08<br />value: 0.0240<br />Function: Penalty<br />Lambda: 0.3","b:  0.09<br />value: 0.0270<br />Function: Penalty<br />Lambda: 0.3","b:  0.10<br />value: 0.0300<br />Function: Penalty<br />Lambda: 0.3","b:  0.11<br />value: 0.0330<br />Function: Penalty<br />Lambda: 0.3","b:  0.12<br />value: 0.0360<br />Function: Penalty<br />Lambda: 0.3","b:  0.13<br />value: 0.0390<br />Function: Penalty<br />Lambda: 0.3","b:  0.14<br />value: 0.0420<br />Function: Penalty<br />Lambda: 0.3","b:  0.15<br />value: 0.0450<br />Function: Penalty<br />Lambda: 0.3","b:  0.16<br />value: 0.0480<br />Function: Penalty<br />Lambda: 0.3","b:  0.17<br />value: 0.0510<br />Function: Penalty<br />Lambda: 0.3","b:  0.18<br />value: 0.0540<br />Function: Penalty<br />Lambda: 0.3","b:  0.19<br />value: 0.0570<br />Function: Penalty<br />Lambda: 0.3","b:  0.20<br />value: 0.0600<br />Function: Penalty<br />Lambda: 0.3","b:  0.21<br />value: 0.0630<br />Function: Penalty<br />Lambda: 0.3","b:  0.22<br />value: 0.0660<br />Function: Penalty<br />Lambda: 0.3","b:  0.23<br />value: 0.0690<br />Function: Penalty<br />Lambda: 0.3","b:  0.24<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.3","b:  0.25<br />value: 0.0750<br />Function: Penalty<br />Lambda: 0.3","b:  0.26<br />value: 0.0780<br />Function: Penalty<br />Lambda: 0.3","b:  0.27<br />value: 0.0810<br />Function: Penalty<br />Lambda: 0.3","b:  0.28<br />value: 0.0840<br />Function: Penalty<br />Lambda: 0.3","b:  0.29<br />value: 0.0870<br />Function: Penalty<br />Lambda: 0.3","b:  0.30<br />value: 0.0900<br />Function: Penalty<br />Lambda: 0.3","b:  0.31<br />value: 0.0930<br />Function: Penalty<br />Lambda: 0.3","b:  0.32<br />value: 0.0960<br />Function: Penalty<br />Lambda: 0.3","b:  0.33<br />value: 0.0990<br />Function: Penalty<br />Lambda: 0.3","b:  0.34<br />value: 0.1020<br />Function: Penalty<br />Lambda: 0.3","b:  0.35<br />value: 0.1050<br />Function: Penalty<br />Lambda: 0.3","b:  0.36<br />value: 0.1080<br />Function: Penalty<br />Lambda: 0.3","b:  0.37<br />value: 0.1110<br />Function: Penalty<br />Lambda: 0.3","b:  0.38<br />value: 0.1140<br />Function: Penalty<br />Lambda: 0.3","b:  0.39<br />value: 0.1170<br />Function: Penalty<br />Lambda: 0.3","b:  0.40<br />value: 0.1200<br />Function: Penalty<br />Lambda: 0.3","b:  0.41<br />value: 0.1230<br />Function: Penalty<br />Lambda: 0.3","b:  0.42<br />value: 0.1260<br />Function: Penalty<br />Lambda: 0.3","b:  0.43<br />value: 0.1290<br />Function: Penalty<br />Lambda: 0.3","b:  0.44<br />value: 0.1320<br />Function: Penalty<br />Lambda: 0.3","b:  0.45<br />value: 0.1350<br />Function: Penalty<br />Lambda: 0.3","b:  0.46<br />value: 0.1380<br />Function: Penalty<br />Lambda: 0.3","b:  0.47<br />value: 0.1410<br />Function: Penalty<br />Lambda: 0.3","b:  0.48<br />value: 0.1440<br />Function: Penalty<br />Lambda: 0.3","b:  0.49<br />value: 0.1470<br />Function: Penalty<br />Lambda: 0.3","b:  0.50<br />value: 0.1500<br />Function: Penalty<br />Lambda: 0.3","b:  0.51<br />value: 0.1530<br />Function: Penalty<br />Lambda: 0.3","b:  0.52<br />value: 0.1560<br />Function: Penalty<br />Lambda: 0.3","b:  0.53<br />value: 0.1590<br />Function: Penalty<br />Lambda: 0.3","b:  0.54<br />value: 0.1620<br />Function: Penalty<br />Lambda: 0.3","b:  0.55<br />value: 0.1650<br />Function: Penalty<br />Lambda: 0.3","b:  0.56<br />value: 0.1680<br />Function: Penalty<br />Lambda: 0.3","b:  0.57<br />value: 0.1710<br />Function: Penalty<br />Lambda: 0.3","b:  0.58<br />value: 0.1740<br />Function: Penalty<br />Lambda: 0.3","b:  0.59<br />value: 0.1770<br />Function: Penalty<br />Lambda: 0.3","b:  0.60<br />value: 0.1800<br />Function: Penalty<br />Lambda: 0.3","b:  0.61<br />value: 0.1830<br />Function: Penalty<br />Lambda: 0.3","b:  0.62<br />value: 0.1860<br />Function: Penalty<br />Lambda: 0.3","b:  0.63<br />value: 0.1890<br />Function: Penalty<br />Lambda: 0.3","b:  0.64<br />value: 0.1920<br />Function: Penalty<br />Lambda: 0.3","b:  0.65<br />value: 0.1950<br />Function: Penalty<br />Lambda: 0.3","b:  0.66<br />value: 0.1980<br />Function: Penalty<br />Lambda: 0.3","b:  0.67<br />value: 0.2010<br />Function: Penalty<br />Lambda: 0.3","b:  0.68<br />value: 0.2040<br />Function: Penalty<br />Lambda: 0.3","b:  0.69<br />value: 0.2070<br />Function: Penalty<br />Lambda: 0.3","b:  0.70<br />value: 0.2100<br />Function: Penalty<br />Lambda: 0.3","b:  0.71<br />value: 0.2130<br />Function: Penalty<br />Lambda: 0.3","b:  0.72<br />value: 0.2160<br />Function: Penalty<br />Lambda: 0.3","b:  0.73<br />value: 0.2190<br />Function: Penalty<br />Lambda: 0.3","b:  0.74<br />value: 0.2220<br />Function: Penalty<br />Lambda: 0.3","b:  0.75<br />value: 0.2250<br />Function: Penalty<br />Lambda: 0.3","b:  0.76<br />value: 0.2280<br />Function: Penalty<br />Lambda: 0.3","b:  0.77<br />value: 0.2310<br />Function: Penalty<br />Lambda: 0.3","b:  0.78<br />value: 0.2340<br />Function: Penalty<br />Lambda: 0.3","b:  0.79<br />value: 0.2370<br />Function: Penalty<br />Lambda: 0.3","b:  0.80<br />value: 0.2400<br />Function: Penalty<br />Lambda: 0.3","b:  0.81<br />value: 0.2430<br />Function: Penalty<br />Lambda: 0.3","b:  0.82<br />value: 0.2460<br />Function: Penalty<br />Lambda: 0.3","b:  0.83<br />value: 0.2490<br />Function: Penalty<br />Lambda: 0.3","b:  0.84<br />value: 0.2520<br />Function: Penalty<br />Lambda: 0.3","b:  0.85<br />value: 0.2550<br />Function: Penalty<br />Lambda: 0.3","b:  0.86<br />value: 0.2580<br />Function: Penalty<br />Lambda: 0.3","b:  0.87<br />value: 0.2610<br />Function: Penalty<br />Lambda: 0.3","b:  0.88<br />value: 0.2640<br />Function: Penalty<br />Lambda: 0.3","b:  0.89<br />value: 0.2670<br />Function: Penalty<br />Lambda: 0.3","b:  0.90<br />value: 0.2700<br />Function: Penalty<br />Lambda: 0.3","b:  0.91<br />value: 0.2730<br />Function: Penalty<br />Lambda: 0.3","b:  0.92<br />value: 0.2760<br />Function: Penalty<br />Lambda: 0.3","b:  0.93<br />value: 0.2790<br />Function: Penalty<br />Lambda: 0.3","b:  0.94<br />value: 0.2820<br />Function: Penalty<br />Lambda: 0.3","b:  0.95<br />value: 0.2850<br />Function: Penalty<br />Lambda: 0.3","b:  0.96<br />value: 0.2880<br />Function: Penalty<br />Lambda: 0.3","b:  0.97<br />value: 0.2910<br />Function: Penalty<br />Lambda: 0.3","b:  0.98<br />value: 0.2940<br />Function: Penalty<br />Lambda: 0.3","b:  0.99<br />value: 0.2970<br />Function: Penalty<br />Lambda: 0.3","b:  1.00<br />value: 0.3000<br />Function: Penalty<br />Lambda: 0.3","b:  1.01<br />value: 0.3030<br />Function: Penalty<br />Lambda: 0.3","b:  1.02<br />value: 0.3060<br />Function: Penalty<br />Lambda: 0.3","b:  1.03<br />value: 0.3090<br />Function: Penalty<br />Lambda: 0.3","b:  1.04<br />value: 0.3120<br />Function: Penalty<br />Lambda: 0.3","b:  1.05<br />value: 0.3150<br />Function: Penalty<br />Lambda: 0.3","b:  1.06<br />value: 0.3180<br />Function: Penalty<br />Lambda: 0.3","b:  1.07<br />value: 0.3210<br />Function: Penalty<br />Lambda: 0.3","b:  1.08<br />value: 0.3240<br />Function: Penalty<br />Lambda: 0.3","b:  1.09<br />value: 0.3270<br />Function: Penalty<br />Lambda: 0.3","b:  1.10<br />value: 0.3300<br />Function: Penalty<br />Lambda: 0.3","b:  1.11<br />value: 0.3330<br />Function: Penalty<br />Lambda: 0.3","b:  1.12<br />value: 0.3360<br />Function: Penalty<br />Lambda: 0.3","b:  1.13<br />value: 0.3390<br />Function: Penalty<br />Lambda: 0.3","b:  1.14<br />value: 0.3420<br />Function: Penalty<br />Lambda: 0.3","b:  1.15<br />value: 0.3450<br />Function: Penalty<br />Lambda: 0.3","b:  1.16<br />value: 0.3480<br />Function: Penalty<br />Lambda: 0.3","b:  1.17<br />value: 0.3510<br />Function: Penalty<br />Lambda: 0.3","b:  1.18<br />value: 0.3540<br />Function: Penalty<br />Lambda: 0.3","b:  1.19<br />value: 0.3570<br />Function: Penalty<br />Lambda: 0.3","b:  1.20<br />value: 0.3600<br />Function: Penalty<br />Lambda: 0.3","b:  1.21<br />value: 0.3630<br />Function: Penalty<br />Lambda: 0.3","b:  1.22<br />value: 0.3660<br />Function: Penalty<br />Lambda: 0.3","b:  1.23<br />value: 0.3690<br />Function: Penalty<br />Lambda: 0.3","b:  1.24<br />value: 0.3720<br />Function: Penalty<br />Lambda: 0.3","b:  1.25<br />value: 0.3750<br />Function: Penalty<br />Lambda: 0.3","b:  1.26<br />value: 0.3780<br />Function: Penalty<br />Lambda: 0.3","b:  1.27<br />value: 0.3810<br />Function: Penalty<br />Lambda: 0.3","b:  1.28<br />value: 0.3840<br />Function: Penalty<br />Lambda: 0.3","b:  1.29<br />value: 0.3870<br />Function: Penalty<br />Lambda: 0.3","b:  1.30<br />value: 0.3900<br />Function: Penalty<br />Lambda: 0.3","b:  1.31<br />value: 0.3930<br />Function: Penalty<br />Lambda: 0.3","b:  1.32<br />value: 0.3960<br />Function: Penalty<br />Lambda: 0.3","b:  1.33<br />value: 0.3990<br />Function: Penalty<br />Lambda: 0.3","b:  1.34<br />value: 0.4020<br />Function: Penalty<br />Lambda: 0.3","b:  1.35<br />value: 0.4050<br />Function: Penalty<br />Lambda: 0.3","b:  1.36<br />value: 0.4080<br />Function: Penalty<br />Lambda: 0.3","b:  1.37<br />value: 0.4110<br />Function: Penalty<br />Lambda: 0.3","b:  1.38<br />value: 0.4140<br />Function: Penalty<br />Lambda: 0.3","b:  1.39<br />value: 0.4170<br />Function: Penalty<br />Lambda: 0.3","b:  1.40<br />value: 0.4200<br />Function: Penalty<br />Lambda: 0.3","b:  1.41<br />value: 0.4230<br />Function: Penalty<br />Lambda: 0.3","b:  1.42<br />value: 0.4260<br />Function: Penalty<br />Lambda: 0.3","b:  1.43<br />value: 0.4290<br />Function: Penalty<br />Lambda: 0.3","b:  1.44<br />value: 0.4320<br />Function: Penalty<br />Lambda: 0.3","b:  1.45<br />value: 0.4350<br />Function: Penalty<br />Lambda: 0.3","b:  1.46<br />value: 0.4380<br />Function: Penalty<br />Lambda: 0.3","b:  1.47<br />value: 0.4410<br />Function: Penalty<br />Lambda: 0.3","b:  1.48<br />value: 0.4440<br />Function: Penalty<br />Lambda: 0.3","b:  1.49<br />value: 0.4470<br />Function: Penalty<br />Lambda: 0.3","b:  1.50<br />value: 0.4500<br />Function: Penalty<br />Lambda: 0.3","b:  1.51<br />value: 0.4530<br />Function: Penalty<br />Lambda: 0.3","b:  1.52<br />value: 0.4560<br />Function: Penalty<br />Lambda: 0.3","b:  1.53<br />value: 0.4590<br />Function: Penalty<br />Lambda: 0.3","b:  1.54<br />value: 0.4620<br />Function: Penalty<br />Lambda: 0.3","b:  1.55<br />value: 0.4650<br />Function: Penalty<br />Lambda: 0.3","b:  1.56<br />value: 0.4680<br />Function: Penalty<br />Lambda: 0.3","b:  1.57<br />value: 0.4710<br />Function: Penalty<br />Lambda: 0.3","b:  1.58<br />value: 0.4740<br />Function: Penalty<br />Lambda: 0.3","b:  1.59<br />value: 0.4770<br />Function: Penalty<br />Lambda: 0.3","b:  1.60<br />value: 0.4800<br />Function: Penalty<br />Lambda: 0.3","b:  1.61<br />value: 0.4830<br />Function: Penalty<br />Lambda: 0.3","b:  1.62<br />value: 0.4860<br />Function: Penalty<br />Lambda: 0.3","b:  1.63<br />value: 0.4890<br />Function: Penalty<br />Lambda: 0.3","b:  1.64<br />value: 0.4920<br />Function: Penalty<br />Lambda: 0.3","b:  1.65<br />value: 0.4950<br />Function: Penalty<br />Lambda: 0.3","b:  1.66<br />value: 0.4980<br />Function: Penalty<br />Lambda: 0.3","b:  1.67<br />value: 0.5010<br />Function: Penalty<br />Lambda: 0.3","b:  1.68<br />value: 0.5040<br />Function: Penalty<br />Lambda: 0.3","b:  1.69<br />value: 0.5070<br />Function: Penalty<br />Lambda: 0.3","b:  1.70<br />value: 0.5100<br />Function: Penalty<br />Lambda: 0.3","b:  1.71<br />value: 0.5130<br />Function: Penalty<br />Lambda: 0.3","b:  1.72<br />value: 0.5160<br />Function: Penalty<br />Lambda: 0.3","b:  1.73<br />value: 0.5190<br />Function: Penalty<br />Lambda: 0.3","b:  1.74<br />value: 0.5220<br />Function: Penalty<br />Lambda: 0.3","b:  1.75<br />value: 0.5250<br />Function: Penalty<br />Lambda: 0.3","b:  1.76<br />value: 0.5280<br />Function: Penalty<br />Lambda: 0.3","b:  1.77<br />value: 0.5310<br />Function: Penalty<br />Lambda: 0.3","b:  1.78<br />value: 0.5340<br />Function: Penalty<br />Lambda: 0.3","b:  1.79<br />value: 0.5370<br />Function: Penalty<br />Lambda: 0.3","b:  1.80<br />value: 0.5400<br />Function: Penalty<br />Lambda: 0.3","b:  1.81<br />value: 0.5430<br />Function: Penalty<br />Lambda: 0.3","b:  1.82<br />value: 0.5460<br />Function: Penalty<br />Lambda: 0.3","b:  1.83<br />value: 0.5490<br />Function: Penalty<br />Lambda: 0.3","b:  1.84<br />value: 0.5520<br />Function: Penalty<br />Lambda: 0.3","b:  1.85<br />value: 0.5550<br />Function: Penalty<br />Lambda: 0.3","b:  1.86<br />value: 0.5580<br />Function: Penalty<br />Lambda: 0.3","b:  1.87<br />value: 0.5610<br />Function: Penalty<br />Lambda: 0.3","b:  1.88<br />value: 0.5640<br />Function: Penalty<br />Lambda: 0.3","b:  1.89<br />value: 0.5670<br />Function: Penalty<br />Lambda: 0.3","b:  1.90<br />value: 0.5700<br />Function: Penalty<br />Lambda: 0.3","b:  1.91<br />value: 0.5730<br />Function: Penalty<br />Lambda: 0.3","b:  1.92<br />value: 0.5760<br />Function: Penalty<br />Lambda: 0.3","b:  1.93<br />value: 0.5790<br />Function: Penalty<br />Lambda: 0.3","b:  1.94<br />value: 0.5820<br />Function: Penalty<br />Lambda: 0.3","b:  1.95<br />value: 0.5850<br />Function: Penalty<br />Lambda: 0.3","b:  1.96<br />value: 0.5880<br />Function: Penalty<br />Lambda: 0.3","b:  1.97<br />value: 0.5910<br />Function: Penalty<br />Lambda: 0.3","b:  1.98<br />value: 0.5940<br />Function: Penalty<br />Lambda: 0.3","b:  1.99<br />value: 0.5970<br />Function: Penalty<br />Lambda: 0.3","b:  2.00<br />value: 0.6000<br />Function: Penalty<br />Lambda: 0.3"],"frame":"0.3","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"0.4","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 0.4","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 0.4","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 0.4","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 0.4","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 0.4","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 0.4","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 0.4","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 0.4","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 0.4","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 0.4","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 0.4","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 0.4","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 0.4","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 0.4","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 0.4","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 0.4","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 0.4","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 0.4","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 0.4","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 0.4","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 0.4","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 0.4","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 0.4","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 0.4","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 0.4","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 0.4","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 0.4","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 0.4","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 0.4","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 0.4","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 0.4","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 0.4","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 0.4","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 0.4","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 0.4","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 0.4","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 0.4","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 0.4","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 0.4","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 0.4","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 0.4","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 0.4","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 0.4","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 0.4","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 0.4","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 0.4","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 0.4","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 0.4","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 0.4","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 0.4","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.4","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.4","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.4","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.4","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.4","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.4","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.4","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.4","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.4","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.4","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.4","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.4","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.4","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.4","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.4","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.4","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.4","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.4","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.4","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.4","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.4","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.4","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.4","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.4","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.4","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.4","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.4","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.4","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.4","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.4","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.4","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.4","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.4","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.4","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.4","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.4","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.4","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.4","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.4","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.4","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.4","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.4","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.4","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.4","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.4","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.4","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.4","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.4","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.4","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.4","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.4","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.4","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.4","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.4","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.4","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.4","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.4","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.4","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.4","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.4","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.4","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.4","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.4","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.4","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.4","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.4","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.4","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.4","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.4","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.4","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.4","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.4","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.4","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.4","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.4","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.4","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.4","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.4","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.4","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.4","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.4","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.4","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.4","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.4","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.4","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.4","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.4","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.4","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.4","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.4","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.4","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.4","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.4","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.4","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.4","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.4","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.4","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.4","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.4","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.4","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 0.4","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.4","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.4","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.4","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.4","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.4","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.4","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.4","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.4","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.4","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.4","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.4","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.4","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.4","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.4","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.4","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.4","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.4","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.4","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.4","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.4","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.4","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.4","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.4","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.4","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.4","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.4","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.4","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.4","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.4","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.4","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.4","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.4","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.4","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.4","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.4","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.4","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.4","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.4","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.4","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.4","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.4","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.4","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.4","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.4","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.4","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.4","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.4","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.4","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.4","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.4","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.4","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.4","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.4","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.4","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.4","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.4","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.4","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.4","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.4","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.4","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.4","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.4","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.4","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.4","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.4","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.4","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.4","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.4","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.4","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.4","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.4","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.4","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.4","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.4","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.4","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.4","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.4","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.4","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.4","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.4","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.4","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.4","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.4","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.4","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.4","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.4","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.4","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.4","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.4","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.4","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.4","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.4","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.4","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.4","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.4","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.4","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.4","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.4","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.4","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.4"],"frame":"0.4","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.95,2.9161,2.8824,2.8489,2.8156,2.7825,2.7496,2.7169,2.6844,2.6521,2.62,2.5881,2.5564,2.5249,2.4936,2.4625,2.4316,2.4009,2.3704,2.3401,2.31,2.2801,2.2504,2.2209,2.1916,2.1625,2.1336,2.1049,2.0764,2.0481,2.02,1.9921,1.9644,1.9369,1.9096,1.8825,1.8556,1.8289,1.8024,1.7761,1.75,1.7241,1.6984,1.6729,1.6476,1.6225,1.5976,1.5729,1.5484,1.5241,1.5,1.4841,1.4684,1.4529,1.4376,1.4225,1.4076,1.3929,1.3784,1.3641,1.35,1.3361,1.3224,1.3089,1.2956,1.2825,1.2696,1.2569,1.2444,1.2321,1.22,1.2081,1.1964,1.1849,1.1736,1.1625,1.1516,1.1409,1.1304,1.1201,1.11,1.1001,1.0904,1.0809,1.0716,1.0625,1.0536,1.0449,1.0364,1.0281,1.02,1.0121,1.0044,0.9969,0.9896,0.9825,0.9756,0.9689,0.9624,0.9561,0.95,0.9441,0.9384,0.9329,0.9276,0.9225,0.9176,0.9129,0.9084,0.9041,0.9,0.8961,0.8924,0.8889,0.8856,0.8825,0.8796,0.8769,0.8744,0.8721,0.87,0.8681,0.8664,0.8649,0.8636,0.8625,0.8616,0.8609,0.8604,0.8601,0.86,0.8601,0.8604,0.8609,0.8616,0.8625,0.8636,0.8649,0.8664,0.8681,0.87,0.8721,0.8744,0.8769,0.8796,0.8825,0.8856,0.8889,0.8924,0.8961,0.9,0.9041,0.9084,0.9129,0.9176,0.9225,0.9276,0.9329,0.9384,0.9441,0.95,0.9561,0.9624,0.9689,0.9756,0.9825,0.9896,0.9969,1.0044,1.0121,1.02,1.0281,1.0364,1.0449,1.0536,1.0625,1.0716,1.0809,1.0904,1.1001,1.11,1.1201,1.1304,1.1409,1.1516,1.1625,1.1736,1.1849,1.1964,1.2081,1.22,1.2321,1.2444,1.2569,1.2696,1.2825,1.2956,1.3089,1.3224,1.3361,1.35,1.3641,1.3784,1.3929,1.4076,1.4225,1.4376,1.4529,1.4684,1.4841,1.5,1.5161,1.5324,1.5489,1.5656,1.5825,1.5996,1.6169,1.6344,1.6521,1.67,1.6881,1.7064,1.7249,1.7436,1.7625,1.7816,1.8009,1.8204,1.8401,1.86,1.8801,1.9004,1.9209,1.9416,1.9625,1.9836,2.0049,2.0264,2.0481,2.07,2.0921,2.1144,2.1369,2.1596,2.1825,2.2056,2.2289,2.2524,2.2761,2.3],"text":["b: -0.50<br />value: 2.9500<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.49<br />value: 2.9161<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.48<br />value: 2.8824<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.47<br />value: 2.8489<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.46<br />value: 2.8156<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.45<br />value: 2.7825<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.44<br />value: 2.7496<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.43<br />value: 2.7169<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.42<br />value: 2.6844<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.41<br />value: 2.6521<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.40<br />value: 2.6200<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.39<br />value: 2.5881<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.38<br />value: 2.5564<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.37<br />value: 2.5249<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.36<br />value: 2.4936<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.35<br />value: 2.4625<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.34<br />value: 2.4316<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.33<br />value: 2.4009<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.32<br />value: 2.3704<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.31<br />value: 2.3401<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.30<br />value: 2.3100<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.29<br />value: 2.2801<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.28<br />value: 2.2504<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.27<br />value: 2.2209<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.26<br />value: 2.1916<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.25<br />value: 2.1625<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.24<br />value: 2.1336<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.23<br />value: 2.1049<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.22<br />value: 2.0764<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.21<br />value: 2.0481<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.20<br />value: 2.0200<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.19<br />value: 1.9921<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.18<br />value: 1.9644<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.17<br />value: 1.9369<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.16<br />value: 1.9096<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.15<br />value: 1.8825<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.14<br />value: 1.8556<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.13<br />value: 1.8289<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.12<br />value: 1.8024<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.11<br />value: 1.7761<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.10<br />value: 1.7500<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.09<br />value: 1.7241<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.08<br />value: 1.6984<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.07<br />value: 1.6729<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.06<br />value: 1.6476<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.05<br />value: 1.6225<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.04<br />value: 1.5976<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.03<br />value: 1.5729<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.02<br />value: 1.5484<br />Function: Loss + Penalty<br />Lambda: 0.4","b: -0.01<br />value: 1.5241<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.01<br />value: 1.4841<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.02<br />value: 1.4684<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.03<br />value: 1.4529<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.04<br />value: 1.4376<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.05<br />value: 1.4225<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.06<br />value: 1.4076<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.07<br />value: 1.3929<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.08<br />value: 1.3784<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.09<br />value: 1.3641<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.10<br />value: 1.3500<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.11<br />value: 1.3361<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.12<br />value: 1.3224<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.13<br />value: 1.3089<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.14<br />value: 1.2956<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.15<br />value: 1.2825<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.16<br />value: 1.2696<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.17<br />value: 1.2569<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.18<br />value: 1.2444<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.19<br />value: 1.2321<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.20<br />value: 1.2200<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.21<br />value: 1.2081<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.22<br />value: 1.1964<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.23<br />value: 1.1849<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.24<br />value: 1.1736<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.25<br />value: 1.1625<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.26<br />value: 1.1516<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.27<br />value: 1.1409<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.28<br />value: 1.1304<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.29<br />value: 1.1201<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.30<br />value: 1.1100<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.31<br />value: 1.1001<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.32<br />value: 1.0904<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.33<br />value: 1.0809<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.34<br />value: 1.0716<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.35<br />value: 1.0625<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.36<br />value: 1.0536<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.37<br />value: 1.0449<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.38<br />value: 1.0364<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.39<br />value: 1.0281<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.40<br />value: 1.0200<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.41<br />value: 1.0121<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.42<br />value: 1.0044<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.43<br />value: 0.9969<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.44<br />value: 0.9896<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.45<br />value: 0.9825<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.46<br />value: 0.9756<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.47<br />value: 0.9689<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.48<br />value: 0.9624<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.49<br />value: 0.9561<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.50<br />value: 0.9500<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.51<br />value: 0.9441<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.52<br />value: 0.9384<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.53<br />value: 0.9329<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.54<br />value: 0.9276<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.55<br />value: 0.9225<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.56<br />value: 0.9176<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.57<br />value: 0.9129<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.58<br />value: 0.9084<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.59<br />value: 0.9041<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.60<br />value: 0.9000<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.61<br />value: 0.8961<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.62<br />value: 0.8924<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.63<br />value: 0.8889<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.64<br />value: 0.8856<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.65<br />value: 0.8825<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.66<br />value: 0.8796<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.67<br />value: 0.8769<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.68<br />value: 0.8744<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.69<br />value: 0.8721<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.70<br />value: 0.8700<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.71<br />value: 0.8681<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.72<br />value: 0.8664<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.73<br />value: 0.8649<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.74<br />value: 0.8636<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.75<br />value: 0.8625<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.76<br />value: 0.8616<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.77<br />value: 0.8609<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.78<br />value: 0.8604<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.79<br />value: 0.8601<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.80<br />value: 0.8600<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.81<br />value: 0.8601<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.82<br />value: 0.8604<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.83<br />value: 0.8609<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.84<br />value: 0.8616<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.85<br />value: 0.8625<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.86<br />value: 0.8636<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.87<br />value: 0.8649<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.88<br />value: 0.8664<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.89<br />value: 0.8681<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.90<br />value: 0.8700<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.91<br />value: 0.8721<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.92<br />value: 0.8744<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.93<br />value: 0.8769<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.94<br />value: 0.8796<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.95<br />value: 0.8825<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.96<br />value: 0.8856<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.97<br />value: 0.8889<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.98<br />value: 0.8924<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  0.99<br />value: 0.8961<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.00<br />value: 0.9000<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.01<br />value: 0.9041<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.02<br />value: 0.9084<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.03<br />value: 0.9129<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.04<br />value: 0.9176<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.05<br />value: 0.9225<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.06<br />value: 0.9276<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.07<br />value: 0.9329<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.08<br />value: 0.9384<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.09<br />value: 0.9441<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.10<br />value: 0.9500<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.11<br />value: 0.9561<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.12<br />value: 0.9624<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.13<br />value: 0.9689<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.14<br />value: 0.9756<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.15<br />value: 0.9825<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.16<br />value: 0.9896<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.17<br />value: 0.9969<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.18<br />value: 1.0044<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.19<br />value: 1.0121<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.20<br />value: 1.0200<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.21<br />value: 1.0281<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.22<br />value: 1.0364<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.23<br />value: 1.0449<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.24<br />value: 1.0536<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.25<br />value: 1.0625<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.26<br />value: 1.0716<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.27<br />value: 1.0809<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.28<br />value: 1.0904<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.29<br />value: 1.1001<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.30<br />value: 1.1100<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.31<br />value: 1.1201<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.32<br />value: 1.1304<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.33<br />value: 1.1409<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.34<br />value: 1.1516<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.35<br />value: 1.1625<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.36<br />value: 1.1736<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.37<br />value: 1.1849<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.38<br />value: 1.1964<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.39<br />value: 1.2081<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.40<br />value: 1.2200<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.41<br />value: 1.2321<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.42<br />value: 1.2444<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.43<br />value: 1.2569<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.44<br />value: 1.2696<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.45<br />value: 1.2825<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.46<br />value: 1.2956<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.47<br />value: 1.3089<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.48<br />value: 1.3224<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.49<br />value: 1.3361<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.50<br />value: 1.3500<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.51<br />value: 1.3641<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.52<br />value: 1.3784<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.53<br />value: 1.3929<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.54<br />value: 1.4076<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.55<br />value: 1.4225<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.56<br />value: 1.4376<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.57<br />value: 1.4529<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.58<br />value: 1.4684<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.59<br />value: 1.4841<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.60<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.61<br />value: 1.5161<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.62<br />value: 1.5324<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.63<br />value: 1.5489<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.64<br />value: 1.5656<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.65<br />value: 1.5825<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.66<br />value: 1.5996<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.67<br />value: 1.6169<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.68<br />value: 1.6344<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.69<br />value: 1.6521<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.70<br />value: 1.6700<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.71<br />value: 1.6881<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.72<br />value: 1.7064<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.73<br />value: 1.7249<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.74<br />value: 1.7436<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.75<br />value: 1.7625<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.76<br />value: 1.7816<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.77<br />value: 1.8009<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.78<br />value: 1.8204<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.79<br />value: 1.8401<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.80<br />value: 1.8600<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.81<br />value: 1.8801<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.82<br />value: 1.9004<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.83<br />value: 1.9209<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.84<br />value: 1.9416<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.85<br />value: 1.9625<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.86<br />value: 1.9836<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.87<br />value: 2.0049<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.88<br />value: 2.0264<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.89<br />value: 2.0481<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.90<br />value: 2.0700<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.91<br />value: 2.0921<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.92<br />value: 2.1144<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.93<br />value: 2.1369<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.94<br />value: 2.1596<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.95<br />value: 2.1825<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.96<br />value: 2.2056<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.97<br />value: 2.2289<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.98<br />value: 2.2524<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  1.99<br />value: 2.2761<br />Function: Loss + Penalty<br />Lambda: 0.4","b:  2.00<br />value: 2.3000<br />Function: Loss + Penalty<br />Lambda: 0.4"],"frame":"0.4","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.2,0.196,0.192,0.188,0.184,0.18,0.176,0.172,0.168,0.164,0.16,0.156,0.152,0.148,0.144,0.14,0.136,0.132,0.128,0.124,0.12,0.116,0.112,0.108,0.104,0.1,0.096,0.092,0.088,0.084,0.08,0.076,0.072,0.068,0.064,0.06,0.056,0.052,0.048,0.044,0.04,0.036,0.032,0.028,0.024,0.02,0.016,0.012,0.00800000000000001,0.004,0,0.004,0.00800000000000001,0.012,0.016,0.02,0.024,0.028,0.032,0.036,0.04,0.044,0.048,0.052,0.056,0.06,0.064,0.068,0.072,0.076,0.08,0.084,0.088,0.092,0.096,0.1,0.104,0.108,0.112,0.116,0.12,0.124,0.128,0.132,0.136,0.14,0.144,0.148,0.152,0.156,0.16,0.164,0.168,0.172,0.176,0.18,0.184,0.188,0.192,0.196,0.2,0.204,0.208,0.212,0.216,0.22,0.224,0.228,0.232,0.236,0.24,0.244,0.248,0.252,0.256,0.26,0.264,0.268,0.272,0.276,0.28,0.284,0.288,0.292,0.296,0.3,0.304,0.308,0.312,0.316,0.32,0.324,0.328,0.332,0.336,0.34,0.344,0.348,0.352,0.356,0.36,0.364,0.368,0.372,0.376,0.38,0.384,0.388,0.392,0.396,0.4,0.404,0.408,0.412,0.416,0.42,0.424,0.428,0.432,0.436,0.44,0.444,0.448,0.452,0.456,0.46,0.464,0.468,0.472,0.476,0.48,0.484,0.488,0.492,0.496,0.5,0.504,0.508,0.512,0.516,0.52,0.524,0.528,0.532,0.536,0.54,0.544,0.548,0.552,0.556,0.56,0.564,0.568,0.572,0.576,0.58,0.584,0.588,0.592,0.596,0.6,0.604,0.608,0.612,0.616,0.62,0.624,0.628,0.632,0.636,0.64,0.644,0.648,0.652,0.656,0.66,0.664,0.668,0.672,0.676,0.68,0.684,0.688,0.692,0.696,0.7,0.704,0.708,0.712,0.716,0.72,0.724,0.728,0.732,0.736,0.74,0.744,0.748,0.752,0.756,0.76,0.764,0.768,0.772,0.776,0.78,0.784,0.788,0.792,0.796,0.8],"text":["b: -0.50<br />value: 0.2000<br />Function: Penalty<br />Lambda: 0.4","b: -0.49<br />value: 0.1960<br />Function: Penalty<br />Lambda: 0.4","b: -0.48<br />value: 0.1920<br />Function: Penalty<br />Lambda: 0.4","b: -0.47<br />value: 0.1880<br />Function: Penalty<br />Lambda: 0.4","b: -0.46<br />value: 0.1840<br />Function: Penalty<br />Lambda: 0.4","b: -0.45<br />value: 0.1800<br />Function: Penalty<br />Lambda: 0.4","b: -0.44<br />value: 0.1760<br />Function: Penalty<br />Lambda: 0.4","b: -0.43<br />value: 0.1720<br />Function: Penalty<br />Lambda: 0.4","b: -0.42<br />value: 0.1680<br />Function: Penalty<br />Lambda: 0.4","b: -0.41<br />value: 0.1640<br />Function: Penalty<br />Lambda: 0.4","b: -0.40<br />value: 0.1600<br />Function: Penalty<br />Lambda: 0.4","b: -0.39<br />value: 0.1560<br />Function: Penalty<br />Lambda: 0.4","b: -0.38<br />value: 0.1520<br />Function: Penalty<br />Lambda: 0.4","b: -0.37<br />value: 0.1480<br />Function: Penalty<br />Lambda: 0.4","b: -0.36<br />value: 0.1440<br />Function: Penalty<br />Lambda: 0.4","b: -0.35<br />value: 0.1400<br />Function: Penalty<br />Lambda: 0.4","b: -0.34<br />value: 0.1360<br />Function: Penalty<br />Lambda: 0.4","b: -0.33<br />value: 0.1320<br />Function: Penalty<br />Lambda: 0.4","b: -0.32<br />value: 0.1280<br />Function: Penalty<br />Lambda: 0.4","b: -0.31<br />value: 0.1240<br />Function: Penalty<br />Lambda: 0.4","b: -0.30<br />value: 0.1200<br />Function: Penalty<br />Lambda: 0.4","b: -0.29<br />value: 0.1160<br />Function: Penalty<br />Lambda: 0.4","b: -0.28<br />value: 0.1120<br />Function: Penalty<br />Lambda: 0.4","b: -0.27<br />value: 0.1080<br />Function: Penalty<br />Lambda: 0.4","b: -0.26<br />value: 0.1040<br />Function: Penalty<br />Lambda: 0.4","b: -0.25<br />value: 0.1000<br />Function: Penalty<br />Lambda: 0.4","b: -0.24<br />value: 0.0960<br />Function: Penalty<br />Lambda: 0.4","b: -0.23<br />value: 0.0920<br />Function: Penalty<br />Lambda: 0.4","b: -0.22<br />value: 0.0880<br />Function: Penalty<br />Lambda: 0.4","b: -0.21<br />value: 0.0840<br />Function: Penalty<br />Lambda: 0.4","b: -0.20<br />value: 0.0800<br />Function: Penalty<br />Lambda: 0.4","b: -0.19<br />value: 0.0760<br />Function: Penalty<br />Lambda: 0.4","b: -0.18<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.4","b: -0.17<br />value: 0.0680<br />Function: Penalty<br />Lambda: 0.4","b: -0.16<br />value: 0.0640<br />Function: Penalty<br />Lambda: 0.4","b: -0.15<br />value: 0.0600<br />Function: Penalty<br />Lambda: 0.4","b: -0.14<br />value: 0.0560<br />Function: Penalty<br />Lambda: 0.4","b: -0.13<br />value: 0.0520<br />Function: Penalty<br />Lambda: 0.4","b: -0.12<br />value: 0.0480<br />Function: Penalty<br />Lambda: 0.4","b: -0.11<br />value: 0.0440<br />Function: Penalty<br />Lambda: 0.4","b: -0.10<br />value: 0.0400<br />Function: Penalty<br />Lambda: 0.4","b: -0.09<br />value: 0.0360<br />Function: Penalty<br />Lambda: 0.4","b: -0.08<br />value: 0.0320<br />Function: Penalty<br />Lambda: 0.4","b: -0.07<br />value: 0.0280<br />Function: Penalty<br />Lambda: 0.4","b: -0.06<br />value: 0.0240<br />Function: Penalty<br />Lambda: 0.4","b: -0.05<br />value: 0.0200<br />Function: Penalty<br />Lambda: 0.4","b: -0.04<br />value: 0.0160<br />Function: Penalty<br />Lambda: 0.4","b: -0.03<br />value: 0.0120<br />Function: Penalty<br />Lambda: 0.4","b: -0.02<br />value: 0.0080<br />Function: Penalty<br />Lambda: 0.4","b: -0.01<br />value: 0.0040<br />Function: Penalty<br />Lambda: 0.4","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.4","b:  0.01<br />value: 0.0040<br />Function: Penalty<br />Lambda: 0.4","b:  0.02<br />value: 0.0080<br />Function: Penalty<br />Lambda: 0.4","b:  0.03<br />value: 0.0120<br />Function: Penalty<br />Lambda: 0.4","b:  0.04<br />value: 0.0160<br />Function: Penalty<br />Lambda: 0.4","b:  0.05<br />value: 0.0200<br />Function: Penalty<br />Lambda: 0.4","b:  0.06<br />value: 0.0240<br />Function: Penalty<br />Lambda: 0.4","b:  0.07<br />value: 0.0280<br />Function: Penalty<br />Lambda: 0.4","b:  0.08<br />value: 0.0320<br />Function: Penalty<br />Lambda: 0.4","b:  0.09<br />value: 0.0360<br />Function: Penalty<br />Lambda: 0.4","b:  0.10<br />value: 0.0400<br />Function: Penalty<br />Lambda: 0.4","b:  0.11<br />value: 0.0440<br />Function: Penalty<br />Lambda: 0.4","b:  0.12<br />value: 0.0480<br />Function: Penalty<br />Lambda: 0.4","b:  0.13<br />value: 0.0520<br />Function: Penalty<br />Lambda: 0.4","b:  0.14<br />value: 0.0560<br />Function: Penalty<br />Lambda: 0.4","b:  0.15<br />value: 0.0600<br />Function: Penalty<br />Lambda: 0.4","b:  0.16<br />value: 0.0640<br />Function: Penalty<br />Lambda: 0.4","b:  0.17<br />value: 0.0680<br />Function: Penalty<br />Lambda: 0.4","b:  0.18<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.4","b:  0.19<br />value: 0.0760<br />Function: Penalty<br />Lambda: 0.4","b:  0.20<br />value: 0.0800<br />Function: Penalty<br />Lambda: 0.4","b:  0.21<br />value: 0.0840<br />Function: Penalty<br />Lambda: 0.4","b:  0.22<br />value: 0.0880<br />Function: Penalty<br />Lambda: 0.4","b:  0.23<br />value: 0.0920<br />Function: Penalty<br />Lambda: 0.4","b:  0.24<br />value: 0.0960<br />Function: Penalty<br />Lambda: 0.4","b:  0.25<br />value: 0.1000<br />Function: Penalty<br />Lambda: 0.4","b:  0.26<br />value: 0.1040<br />Function: Penalty<br />Lambda: 0.4","b:  0.27<br />value: 0.1080<br />Function: Penalty<br />Lambda: 0.4","b:  0.28<br />value: 0.1120<br />Function: Penalty<br />Lambda: 0.4","b:  0.29<br />value: 0.1160<br />Function: Penalty<br />Lambda: 0.4","b:  0.30<br />value: 0.1200<br />Function: Penalty<br />Lambda: 0.4","b:  0.31<br />value: 0.1240<br />Function: Penalty<br />Lambda: 0.4","b:  0.32<br />value: 0.1280<br />Function: Penalty<br />Lambda: 0.4","b:  0.33<br />value: 0.1320<br />Function: Penalty<br />Lambda: 0.4","b:  0.34<br />value: 0.1360<br />Function: Penalty<br />Lambda: 0.4","b:  0.35<br />value: 0.1400<br />Function: Penalty<br />Lambda: 0.4","b:  0.36<br />value: 0.1440<br />Function: Penalty<br />Lambda: 0.4","b:  0.37<br />value: 0.1480<br />Function: Penalty<br />Lambda: 0.4","b:  0.38<br />value: 0.1520<br />Function: Penalty<br />Lambda: 0.4","b:  0.39<br />value: 0.1560<br />Function: Penalty<br />Lambda: 0.4","b:  0.40<br />value: 0.1600<br />Function: Penalty<br />Lambda: 0.4","b:  0.41<br />value: 0.1640<br />Function: Penalty<br />Lambda: 0.4","b:  0.42<br />value: 0.1680<br />Function: Penalty<br />Lambda: 0.4","b:  0.43<br />value: 0.1720<br />Function: Penalty<br />Lambda: 0.4","b:  0.44<br />value: 0.1760<br />Function: Penalty<br />Lambda: 0.4","b:  0.45<br />value: 0.1800<br />Function: Penalty<br />Lambda: 0.4","b:  0.46<br />value: 0.1840<br />Function: Penalty<br />Lambda: 0.4","b:  0.47<br />value: 0.1880<br />Function: Penalty<br />Lambda: 0.4","b:  0.48<br />value: 0.1920<br />Function: Penalty<br />Lambda: 0.4","b:  0.49<br />value: 0.1960<br />Function: Penalty<br />Lambda: 0.4","b:  0.50<br />value: 0.2000<br />Function: Penalty<br />Lambda: 0.4","b:  0.51<br />value: 0.2040<br />Function: Penalty<br />Lambda: 0.4","b:  0.52<br />value: 0.2080<br />Function: Penalty<br />Lambda: 0.4","b:  0.53<br />value: 0.2120<br />Function: Penalty<br />Lambda: 0.4","b:  0.54<br />value: 0.2160<br />Function: Penalty<br />Lambda: 0.4","b:  0.55<br />value: 0.2200<br />Function: Penalty<br />Lambda: 0.4","b:  0.56<br />value: 0.2240<br />Function: Penalty<br />Lambda: 0.4","b:  0.57<br />value: 0.2280<br />Function: Penalty<br />Lambda: 0.4","b:  0.58<br />value: 0.2320<br />Function: Penalty<br />Lambda: 0.4","b:  0.59<br />value: 0.2360<br />Function: Penalty<br />Lambda: 0.4","b:  0.60<br />value: 0.2400<br />Function: Penalty<br />Lambda: 0.4","b:  0.61<br />value: 0.2440<br />Function: Penalty<br />Lambda: 0.4","b:  0.62<br />value: 0.2480<br />Function: Penalty<br />Lambda: 0.4","b:  0.63<br />value: 0.2520<br />Function: Penalty<br />Lambda: 0.4","b:  0.64<br />value: 0.2560<br />Function: Penalty<br />Lambda: 0.4","b:  0.65<br />value: 0.2600<br />Function: Penalty<br />Lambda: 0.4","b:  0.66<br />value: 0.2640<br />Function: Penalty<br />Lambda: 0.4","b:  0.67<br />value: 0.2680<br />Function: Penalty<br />Lambda: 0.4","b:  0.68<br />value: 0.2720<br />Function: Penalty<br />Lambda: 0.4","b:  0.69<br />value: 0.2760<br />Function: Penalty<br />Lambda: 0.4","b:  0.70<br />value: 0.2800<br />Function: Penalty<br />Lambda: 0.4","b:  0.71<br />value: 0.2840<br />Function: Penalty<br />Lambda: 0.4","b:  0.72<br />value: 0.2880<br />Function: Penalty<br />Lambda: 0.4","b:  0.73<br />value: 0.2920<br />Function: Penalty<br />Lambda: 0.4","b:  0.74<br />value: 0.2960<br />Function: Penalty<br />Lambda: 0.4","b:  0.75<br />value: 0.3000<br />Function: Penalty<br />Lambda: 0.4","b:  0.76<br />value: 0.3040<br />Function: Penalty<br />Lambda: 0.4","b:  0.77<br />value: 0.3080<br />Function: Penalty<br />Lambda: 0.4","b:  0.78<br />value: 0.3120<br />Function: Penalty<br />Lambda: 0.4","b:  0.79<br />value: 0.3160<br />Function: Penalty<br />Lambda: 0.4","b:  0.80<br />value: 0.3200<br />Function: Penalty<br />Lambda: 0.4","b:  0.81<br />value: 0.3240<br />Function: Penalty<br />Lambda: 0.4","b:  0.82<br />value: 0.3280<br />Function: Penalty<br />Lambda: 0.4","b:  0.83<br />value: 0.3320<br />Function: Penalty<br />Lambda: 0.4","b:  0.84<br />value: 0.3360<br />Function: Penalty<br />Lambda: 0.4","b:  0.85<br />value: 0.3400<br />Function: Penalty<br />Lambda: 0.4","b:  0.86<br />value: 0.3440<br />Function: Penalty<br />Lambda: 0.4","b:  0.87<br />value: 0.3480<br />Function: Penalty<br />Lambda: 0.4","b:  0.88<br />value: 0.3520<br />Function: Penalty<br />Lambda: 0.4","b:  0.89<br />value: 0.3560<br />Function: Penalty<br />Lambda: 0.4","b:  0.90<br />value: 0.3600<br />Function: Penalty<br />Lambda: 0.4","b:  0.91<br />value: 0.3640<br />Function: Penalty<br />Lambda: 0.4","b:  0.92<br />value: 0.3680<br />Function: Penalty<br />Lambda: 0.4","b:  0.93<br />value: 0.3720<br />Function: Penalty<br />Lambda: 0.4","b:  0.94<br />value: 0.3760<br />Function: Penalty<br />Lambda: 0.4","b:  0.95<br />value: 0.3800<br />Function: Penalty<br />Lambda: 0.4","b:  0.96<br />value: 0.3840<br />Function: Penalty<br />Lambda: 0.4","b:  0.97<br />value: 0.3880<br />Function: Penalty<br />Lambda: 0.4","b:  0.98<br />value: 0.3920<br />Function: Penalty<br />Lambda: 0.4","b:  0.99<br />value: 0.3960<br />Function: Penalty<br />Lambda: 0.4","b:  1.00<br />value: 0.4000<br />Function: Penalty<br />Lambda: 0.4","b:  1.01<br />value: 0.4040<br />Function: Penalty<br />Lambda: 0.4","b:  1.02<br />value: 0.4080<br />Function: Penalty<br />Lambda: 0.4","b:  1.03<br />value: 0.4120<br />Function: Penalty<br />Lambda: 0.4","b:  1.04<br />value: 0.4160<br />Function: Penalty<br />Lambda: 0.4","b:  1.05<br />value: 0.4200<br />Function: Penalty<br />Lambda: 0.4","b:  1.06<br />value: 0.4240<br />Function: Penalty<br />Lambda: 0.4","b:  1.07<br />value: 0.4280<br />Function: Penalty<br />Lambda: 0.4","b:  1.08<br />value: 0.4320<br />Function: Penalty<br />Lambda: 0.4","b:  1.09<br />value: 0.4360<br />Function: Penalty<br />Lambda: 0.4","b:  1.10<br />value: 0.4400<br />Function: Penalty<br />Lambda: 0.4","b:  1.11<br />value: 0.4440<br />Function: Penalty<br />Lambda: 0.4","b:  1.12<br />value: 0.4480<br />Function: Penalty<br />Lambda: 0.4","b:  1.13<br />value: 0.4520<br />Function: Penalty<br />Lambda: 0.4","b:  1.14<br />value: 0.4560<br />Function: Penalty<br />Lambda: 0.4","b:  1.15<br />value: 0.4600<br />Function: Penalty<br />Lambda: 0.4","b:  1.16<br />value: 0.4640<br />Function: Penalty<br />Lambda: 0.4","b:  1.17<br />value: 0.4680<br />Function: Penalty<br />Lambda: 0.4","b:  1.18<br />value: 0.4720<br />Function: Penalty<br />Lambda: 0.4","b:  1.19<br />value: 0.4760<br />Function: Penalty<br />Lambda: 0.4","b:  1.20<br />value: 0.4800<br />Function: Penalty<br />Lambda: 0.4","b:  1.21<br />value: 0.4840<br />Function: Penalty<br />Lambda: 0.4","b:  1.22<br />value: 0.4880<br />Function: Penalty<br />Lambda: 0.4","b:  1.23<br />value: 0.4920<br />Function: Penalty<br />Lambda: 0.4","b:  1.24<br />value: 0.4960<br />Function: Penalty<br />Lambda: 0.4","b:  1.25<br />value: 0.5000<br />Function: Penalty<br />Lambda: 0.4","b:  1.26<br />value: 0.5040<br />Function: Penalty<br />Lambda: 0.4","b:  1.27<br />value: 0.5080<br />Function: Penalty<br />Lambda: 0.4","b:  1.28<br />value: 0.5120<br />Function: Penalty<br />Lambda: 0.4","b:  1.29<br />value: 0.5160<br />Function: Penalty<br />Lambda: 0.4","b:  1.30<br />value: 0.5200<br />Function: Penalty<br />Lambda: 0.4","b:  1.31<br />value: 0.5240<br />Function: Penalty<br />Lambda: 0.4","b:  1.32<br />value: 0.5280<br />Function: Penalty<br />Lambda: 0.4","b:  1.33<br />value: 0.5320<br />Function: Penalty<br />Lambda: 0.4","b:  1.34<br />value: 0.5360<br />Function: Penalty<br />Lambda: 0.4","b:  1.35<br />value: 0.5400<br />Function: Penalty<br />Lambda: 0.4","b:  1.36<br />value: 0.5440<br />Function: Penalty<br />Lambda: 0.4","b:  1.37<br />value: 0.5480<br />Function: Penalty<br />Lambda: 0.4","b:  1.38<br />value: 0.5520<br />Function: Penalty<br />Lambda: 0.4","b:  1.39<br />value: 0.5560<br />Function: Penalty<br />Lambda: 0.4","b:  1.40<br />value: 0.5600<br />Function: Penalty<br />Lambda: 0.4","b:  1.41<br />value: 0.5640<br />Function: Penalty<br />Lambda: 0.4","b:  1.42<br />value: 0.5680<br />Function: Penalty<br />Lambda: 0.4","b:  1.43<br />value: 0.5720<br />Function: Penalty<br />Lambda: 0.4","b:  1.44<br />value: 0.5760<br />Function: Penalty<br />Lambda: 0.4","b:  1.45<br />value: 0.5800<br />Function: Penalty<br />Lambda: 0.4","b:  1.46<br />value: 0.5840<br />Function: Penalty<br />Lambda: 0.4","b:  1.47<br />value: 0.5880<br />Function: Penalty<br />Lambda: 0.4","b:  1.48<br />value: 0.5920<br />Function: Penalty<br />Lambda: 0.4","b:  1.49<br />value: 0.5960<br />Function: Penalty<br />Lambda: 0.4","b:  1.50<br />value: 0.6000<br />Function: Penalty<br />Lambda: 0.4","b:  1.51<br />value: 0.6040<br />Function: Penalty<br />Lambda: 0.4","b:  1.52<br />value: 0.6080<br />Function: Penalty<br />Lambda: 0.4","b:  1.53<br />value: 0.6120<br />Function: Penalty<br />Lambda: 0.4","b:  1.54<br />value: 0.6160<br />Function: Penalty<br />Lambda: 0.4","b:  1.55<br />value: 0.6200<br />Function: Penalty<br />Lambda: 0.4","b:  1.56<br />value: 0.6240<br />Function: Penalty<br />Lambda: 0.4","b:  1.57<br />value: 0.6280<br />Function: Penalty<br />Lambda: 0.4","b:  1.58<br />value: 0.6320<br />Function: Penalty<br />Lambda: 0.4","b:  1.59<br />value: 0.6360<br />Function: Penalty<br />Lambda: 0.4","b:  1.60<br />value: 0.6400<br />Function: Penalty<br />Lambda: 0.4","b:  1.61<br />value: 0.6440<br />Function: Penalty<br />Lambda: 0.4","b:  1.62<br />value: 0.6480<br />Function: Penalty<br />Lambda: 0.4","b:  1.63<br />value: 0.6520<br />Function: Penalty<br />Lambda: 0.4","b:  1.64<br />value: 0.6560<br />Function: Penalty<br />Lambda: 0.4","b:  1.65<br />value: 0.6600<br />Function: Penalty<br />Lambda: 0.4","b:  1.66<br />value: 0.6640<br />Function: Penalty<br />Lambda: 0.4","b:  1.67<br />value: 0.6680<br />Function: Penalty<br />Lambda: 0.4","b:  1.68<br />value: 0.6720<br />Function: Penalty<br />Lambda: 0.4","b:  1.69<br />value: 0.6760<br />Function: Penalty<br />Lambda: 0.4","b:  1.70<br />value: 0.6800<br />Function: Penalty<br />Lambda: 0.4","b:  1.71<br />value: 0.6840<br />Function: Penalty<br />Lambda: 0.4","b:  1.72<br />value: 0.6880<br />Function: Penalty<br />Lambda: 0.4","b:  1.73<br />value: 0.6920<br />Function: Penalty<br />Lambda: 0.4","b:  1.74<br />value: 0.6960<br />Function: Penalty<br />Lambda: 0.4","b:  1.75<br />value: 0.7000<br />Function: Penalty<br />Lambda: 0.4","b:  1.76<br />value: 0.7040<br />Function: Penalty<br />Lambda: 0.4","b:  1.77<br />value: 0.7080<br />Function: Penalty<br />Lambda: 0.4","b:  1.78<br />value: 0.7120<br />Function: Penalty<br />Lambda: 0.4","b:  1.79<br />value: 0.7160<br />Function: Penalty<br />Lambda: 0.4","b:  1.80<br />value: 0.7200<br />Function: Penalty<br />Lambda: 0.4","b:  1.81<br />value: 0.7240<br />Function: Penalty<br />Lambda: 0.4","b:  1.82<br />value: 0.7280<br />Function: Penalty<br />Lambda: 0.4","b:  1.83<br />value: 0.7320<br />Function: Penalty<br />Lambda: 0.4","b:  1.84<br />value: 0.7360<br />Function: Penalty<br />Lambda: 0.4","b:  1.85<br />value: 0.7400<br />Function: Penalty<br />Lambda: 0.4","b:  1.86<br />value: 0.7440<br />Function: Penalty<br />Lambda: 0.4","b:  1.87<br />value: 0.7480<br />Function: Penalty<br />Lambda: 0.4","b:  1.88<br />value: 0.7520<br />Function: Penalty<br />Lambda: 0.4","b:  1.89<br />value: 0.7560<br />Function: Penalty<br />Lambda: 0.4","b:  1.90<br />value: 0.7600<br />Function: Penalty<br />Lambda: 0.4","b:  1.91<br />value: 0.7640<br />Function: Penalty<br />Lambda: 0.4","b:  1.92<br />value: 0.7680<br />Function: Penalty<br />Lambda: 0.4","b:  1.93<br />value: 0.7720<br />Function: Penalty<br />Lambda: 0.4","b:  1.94<br />value: 0.7760<br />Function: Penalty<br />Lambda: 0.4","b:  1.95<br />value: 0.7800<br />Function: Penalty<br />Lambda: 0.4","b:  1.96<br />value: 0.7840<br />Function: Penalty<br />Lambda: 0.4","b:  1.97<br />value: 0.7880<br />Function: Penalty<br />Lambda: 0.4","b:  1.98<br />value: 0.7920<br />Function: Penalty<br />Lambda: 0.4","b:  1.99<br />value: 0.7960<br />Function: Penalty<br />Lambda: 0.4","b:  2.00<br />value: 0.8000<br />Function: Penalty<br />Lambda: 0.4"],"frame":"0.4","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"0.5","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 0.5","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 0.5","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 0.5","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 0.5","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 0.5","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 0.5","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 0.5","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 0.5","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 0.5","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 0.5","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 0.5","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 0.5","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 0.5","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 0.5","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 0.5","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 0.5","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 0.5","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 0.5","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 0.5","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 0.5","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 0.5","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 0.5","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 0.5","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 0.5","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 0.5","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 0.5","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 0.5","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 0.5","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 0.5","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 0.5","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 0.5","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 0.5","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 0.5","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 0.5","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 0.5","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 0.5","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 0.5","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 0.5","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 0.5","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 0.5","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 0.5","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 0.5","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 0.5","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 0.5","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 0.5","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 0.5","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 0.5","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 0.5","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 0.5","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 0.5","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.5","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.5","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.5","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.5","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.5","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.5","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.5","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.5","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.5","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.5","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.5","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.5","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.5","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.5","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.5","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.5","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.5","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.5","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.5","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.5","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.5","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.5","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.5","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.5","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.5","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.5","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.5","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.5","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.5","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.5","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.5","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.5","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.5","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.5","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.5","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.5","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.5","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.5","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.5","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.5","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.5","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.5","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.5","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.5","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.5","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.5","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.5","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.5","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.5","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.5","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.5","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.5","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.5","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.5","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.5","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.5","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.5","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.5","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.5","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.5","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.5","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.5","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.5","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.5","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.5","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.5","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.5","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.5","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.5","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.5","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.5","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.5","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.5","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.5","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.5","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.5","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.5","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.5","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.5","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.5","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.5","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.5","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.5","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.5","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.5","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.5","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.5","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.5","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.5","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.5","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.5","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.5","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.5","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.5","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.5","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.5","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.5","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.5","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.5","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.5","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 0.5","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.5","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.5","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.5","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.5","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.5","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.5","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.5","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.5","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.5","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.5","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.5","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.5","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.5","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.5","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.5","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.5","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.5","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.5","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.5","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.5","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.5","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.5","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.5","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.5","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.5","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.5","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.5","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.5","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.5","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.5","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.5","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.5","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.5","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.5","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.5","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.5","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.5","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.5","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.5","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.5","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.5","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.5","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.5","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.5","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.5","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.5","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.5","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.5","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.5","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.5","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.5","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.5","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.5","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.5","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.5","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.5","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.5","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.5","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.5","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.5","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.5","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.5","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.5","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.5","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.5","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.5","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.5","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.5","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.5","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.5","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.5","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.5","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.5","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.5","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.5","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.5","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.5","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.5","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.5","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.5","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.5","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.5","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.5","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.5","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.5","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.5","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.5","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.5","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.5","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.5","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.5","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.5","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.5","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.5","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.5","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.5","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.5","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.5","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.5","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.5"],"frame":"0.5","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3,2.9651,2.9304,2.8959,2.8616,2.8275,2.7936,2.7599,2.7264,2.6931,2.66,2.6271,2.5944,2.5619,2.5296,2.4975,2.4656,2.4339,2.4024,2.3711,2.34,2.3091,2.2784,2.2479,2.2176,2.1875,2.1576,2.1279,2.0984,2.0691,2.04,2.0111,1.9824,1.9539,1.9256,1.8975,1.8696,1.8419,1.8144,1.7871,1.76,1.7331,1.7064,1.6799,1.6536,1.6275,1.6016,1.5759,1.5504,1.5251,1.5,1.4851,1.4704,1.4559,1.4416,1.4275,1.4136,1.3999,1.3864,1.3731,1.36,1.3471,1.3344,1.3219,1.3096,1.2975,1.2856,1.2739,1.2624,1.2511,1.24,1.2291,1.2184,1.2079,1.1976,1.1875,1.1776,1.1679,1.1584,1.1491,1.14,1.1311,1.1224,1.1139,1.1056,1.0975,1.0896,1.0819,1.0744,1.0671,1.06,1.0531,1.0464,1.0399,1.0336,1.0275,1.0216,1.0159,1.0104,1.0051,1,0.9951,0.9904,0.9859,0.9816,0.9775,0.9736,0.9699,0.9664,0.9631,0.96,0.9571,0.9544,0.9519,0.9496,0.9475,0.9456,0.9439,0.9424,0.9411,0.94,0.9391,0.9384,0.9379,0.9376,0.9375,0.9376,0.9379,0.9384,0.9391,0.94,0.9411,0.9424,0.9439,0.9456,0.9475,0.9496,0.9519,0.9544,0.9571,0.96,0.9631,0.9664,0.9699,0.9736,0.9775,0.9816,0.9859,0.9904,0.9951,1,1.0051,1.0104,1.0159,1.0216,1.0275,1.0336,1.0399,1.0464,1.0531,1.06,1.0671,1.0744,1.0819,1.0896,1.0975,1.1056,1.1139,1.1224,1.1311,1.14,1.1491,1.1584,1.1679,1.1776,1.1875,1.1976,1.2079,1.2184,1.2291,1.24,1.2511,1.2624,1.2739,1.2856,1.2975,1.3096,1.3219,1.3344,1.3471,1.36,1.3731,1.3864,1.3999,1.4136,1.4275,1.4416,1.4559,1.4704,1.4851,1.5,1.5151,1.5304,1.5459,1.5616,1.5775,1.5936,1.6099,1.6264,1.6431,1.66,1.6771,1.6944,1.7119,1.7296,1.7475,1.7656,1.7839,1.8024,1.8211,1.84,1.8591,1.8784,1.8979,1.9176,1.9375,1.9576,1.9779,1.9984,2.0191,2.04,2.0611,2.0824,2.1039,2.1256,2.1475,2.1696,2.1919,2.2144,2.2371,2.26,2.2831,2.3064,2.3299,2.3536,2.3775,2.4016,2.4259,2.4504,2.4751,2.5],"text":["b: -0.50<br />value: 3.0000<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.49<br />value: 2.9651<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.48<br />value: 2.9304<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.47<br />value: 2.8959<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.46<br />value: 2.8616<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.45<br />value: 2.8275<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.44<br />value: 2.7936<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.43<br />value: 2.7599<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.42<br />value: 2.7264<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.41<br />value: 2.6931<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.40<br />value: 2.6600<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.39<br />value: 2.6271<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.38<br />value: 2.5944<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.37<br />value: 2.5619<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.36<br />value: 2.5296<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.35<br />value: 2.4975<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.34<br />value: 2.4656<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.33<br />value: 2.4339<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.32<br />value: 2.4024<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.31<br />value: 2.3711<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.30<br />value: 2.3400<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.29<br />value: 2.3091<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.28<br />value: 2.2784<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.27<br />value: 2.2479<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.26<br />value: 2.2176<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.25<br />value: 2.1875<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.24<br />value: 2.1576<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.23<br />value: 2.1279<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.22<br />value: 2.0984<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.21<br />value: 2.0691<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.20<br />value: 2.0400<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.19<br />value: 2.0111<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.18<br />value: 1.9824<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.17<br />value: 1.9539<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.16<br />value: 1.9256<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.15<br />value: 1.8975<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.14<br />value: 1.8696<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.13<br />value: 1.8419<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.12<br />value: 1.8144<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.11<br />value: 1.7871<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.10<br />value: 1.7600<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.09<br />value: 1.7331<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.08<br />value: 1.7064<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.07<br />value: 1.6799<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.06<br />value: 1.6536<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.05<br />value: 1.6275<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.04<br />value: 1.6016<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.03<br />value: 1.5759<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.02<br />value: 1.5504<br />Function: Loss + Penalty<br />Lambda: 0.5","b: -0.01<br />value: 1.5251<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.01<br />value: 1.4851<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.02<br />value: 1.4704<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.03<br />value: 1.4559<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.04<br />value: 1.4416<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.05<br />value: 1.4275<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.06<br />value: 1.4136<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.07<br />value: 1.3999<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.08<br />value: 1.3864<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.09<br />value: 1.3731<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.10<br />value: 1.3600<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.11<br />value: 1.3471<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.12<br />value: 1.3344<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.13<br />value: 1.3219<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.14<br />value: 1.3096<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.15<br />value: 1.2975<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.16<br />value: 1.2856<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.17<br />value: 1.2739<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.18<br />value: 1.2624<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.19<br />value: 1.2511<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.20<br />value: 1.2400<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.21<br />value: 1.2291<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.22<br />value: 1.2184<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.23<br />value: 1.2079<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.24<br />value: 1.1976<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.25<br />value: 1.1875<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.26<br />value: 1.1776<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.27<br />value: 1.1679<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.28<br />value: 1.1584<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.29<br />value: 1.1491<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.30<br />value: 1.1400<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.31<br />value: 1.1311<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.32<br />value: 1.1224<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.33<br />value: 1.1139<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.34<br />value: 1.1056<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.35<br />value: 1.0975<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.36<br />value: 1.0896<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.37<br />value: 1.0819<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.38<br />value: 1.0744<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.39<br />value: 1.0671<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.40<br />value: 1.0600<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.41<br />value: 1.0531<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.42<br />value: 1.0464<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.43<br />value: 1.0399<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.44<br />value: 1.0336<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.45<br />value: 1.0275<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.46<br />value: 1.0216<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.47<br />value: 1.0159<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.48<br />value: 1.0104<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.49<br />value: 1.0051<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.50<br />value: 1.0000<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.51<br />value: 0.9951<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.52<br />value: 0.9904<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.53<br />value: 0.9859<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.54<br />value: 0.9816<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.55<br />value: 0.9775<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.56<br />value: 0.9736<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.57<br />value: 0.9699<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.58<br />value: 0.9664<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.59<br />value: 0.9631<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.60<br />value: 0.9600<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.61<br />value: 0.9571<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.62<br />value: 0.9544<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.63<br />value: 0.9519<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.64<br />value: 0.9496<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.65<br />value: 0.9475<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.66<br />value: 0.9456<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.67<br />value: 0.9439<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.68<br />value: 0.9424<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.69<br />value: 0.9411<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.70<br />value: 0.9400<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.71<br />value: 0.9391<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.72<br />value: 0.9384<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.73<br />value: 0.9379<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.74<br />value: 0.9376<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.75<br />value: 0.9375<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.76<br />value: 0.9376<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.77<br />value: 0.9379<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.78<br />value: 0.9384<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.79<br />value: 0.9391<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.80<br />value: 0.9400<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.81<br />value: 0.9411<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.82<br />value: 0.9424<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.83<br />value: 0.9439<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.84<br />value: 0.9456<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.85<br />value: 0.9475<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.86<br />value: 0.9496<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.87<br />value: 0.9519<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.88<br />value: 0.9544<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.89<br />value: 0.9571<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.90<br />value: 0.9600<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.91<br />value: 0.9631<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.92<br />value: 0.9664<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.93<br />value: 0.9699<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.94<br />value: 0.9736<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.95<br />value: 0.9775<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.96<br />value: 0.9816<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.97<br />value: 0.9859<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.98<br />value: 0.9904<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  0.99<br />value: 0.9951<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.00<br />value: 1.0000<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.01<br />value: 1.0051<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.02<br />value: 1.0104<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.03<br />value: 1.0159<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.04<br />value: 1.0216<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.05<br />value: 1.0275<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.06<br />value: 1.0336<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.07<br />value: 1.0399<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.08<br />value: 1.0464<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.09<br />value: 1.0531<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.10<br />value: 1.0600<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.11<br />value: 1.0671<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.12<br />value: 1.0744<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.13<br />value: 1.0819<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.14<br />value: 1.0896<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.15<br />value: 1.0975<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.16<br />value: 1.1056<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.17<br />value: 1.1139<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.18<br />value: 1.1224<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.19<br />value: 1.1311<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.20<br />value: 1.1400<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.21<br />value: 1.1491<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.22<br />value: 1.1584<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.23<br />value: 1.1679<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.24<br />value: 1.1776<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.25<br />value: 1.1875<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.26<br />value: 1.1976<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.27<br />value: 1.2079<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.28<br />value: 1.2184<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.29<br />value: 1.2291<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.30<br />value: 1.2400<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.31<br />value: 1.2511<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.32<br />value: 1.2624<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.33<br />value: 1.2739<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.34<br />value: 1.2856<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.35<br />value: 1.2975<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.36<br />value: 1.3096<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.37<br />value: 1.3219<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.38<br />value: 1.3344<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.39<br />value: 1.3471<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.40<br />value: 1.3600<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.41<br />value: 1.3731<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.42<br />value: 1.3864<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.43<br />value: 1.3999<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.44<br />value: 1.4136<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.45<br />value: 1.4275<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.46<br />value: 1.4416<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.47<br />value: 1.4559<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.48<br />value: 1.4704<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.49<br />value: 1.4851<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.50<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.51<br />value: 1.5151<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.52<br />value: 1.5304<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.53<br />value: 1.5459<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.54<br />value: 1.5616<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.55<br />value: 1.5775<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.56<br />value: 1.5936<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.57<br />value: 1.6099<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.58<br />value: 1.6264<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.59<br />value: 1.6431<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.60<br />value: 1.6600<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.61<br />value: 1.6771<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.62<br />value: 1.6944<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.63<br />value: 1.7119<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.64<br />value: 1.7296<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.65<br />value: 1.7475<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.66<br />value: 1.7656<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.67<br />value: 1.7839<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.68<br />value: 1.8024<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.69<br />value: 1.8211<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.70<br />value: 1.8400<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.71<br />value: 1.8591<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.72<br />value: 1.8784<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.73<br />value: 1.8979<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.74<br />value: 1.9176<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.75<br />value: 1.9375<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.76<br />value: 1.9576<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.77<br />value: 1.9779<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.78<br />value: 1.9984<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.79<br />value: 2.0191<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.80<br />value: 2.0400<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.81<br />value: 2.0611<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.82<br />value: 2.0824<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.83<br />value: 2.1039<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.84<br />value: 2.1256<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.85<br />value: 2.1475<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.86<br />value: 2.1696<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.87<br />value: 2.1919<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.88<br />value: 2.2144<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.89<br />value: 2.2371<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.90<br />value: 2.2600<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.91<br />value: 2.2831<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.92<br />value: 2.3064<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.93<br />value: 2.3299<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.94<br />value: 2.3536<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.95<br />value: 2.3775<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.96<br />value: 2.4016<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.97<br />value: 2.4259<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.98<br />value: 2.4504<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  1.99<br />value: 2.4751<br />Function: Loss + Penalty<br />Lambda: 0.5","b:  2.00<br />value: 2.5000<br />Function: Loss + Penalty<br />Lambda: 0.5"],"frame":"0.5","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.25,0.245,0.24,0.235,0.23,0.225,0.22,0.215,0.21,0.205,0.2,0.195,0.19,0.185,0.18,0.175,0.17,0.165,0.16,0.155,0.15,0.145,0.14,0.135,0.13,0.125,0.12,0.115,0.11,0.105,0.1,0.095,0.09,0.085,0.08,0.075,0.07,0.065,0.06,0.055,0.05,0.045,0.04,0.035,0.03,0.025,0.02,0.015,0.01,0.005,0,0.005,0.01,0.015,0.02,0.025,0.03,0.035,0.04,0.045,0.05,0.055,0.06,0.065,0.07,0.075,0.08,0.085,0.09,0.095,0.1,0.105,0.11,0.115,0.12,0.125,0.13,0.135,0.14,0.145,0.15,0.155,0.16,0.165,0.17,0.175,0.18,0.185,0.19,0.195,0.2,0.205,0.21,0.215,0.22,0.225,0.23,0.235,0.24,0.245,0.25,0.255,0.26,0.265,0.27,0.275,0.28,0.285,0.29,0.295,0.3,0.305,0.31,0.315,0.32,0.325,0.33,0.335,0.34,0.345,0.35,0.355,0.36,0.365,0.37,0.375,0.38,0.385,0.39,0.395,0.4,0.405,0.41,0.415,0.42,0.425,0.43,0.435,0.44,0.445,0.45,0.455,0.46,0.465,0.47,0.475,0.48,0.485,0.49,0.495,0.5,0.505,0.51,0.515,0.52,0.525,0.53,0.535,0.54,0.545,0.55,0.555,0.56,0.565,0.57,0.575,0.58,0.585,0.59,0.595,0.6,0.605,0.61,0.615,0.62,0.625,0.63,0.635,0.64,0.645,0.65,0.655,0.66,0.665,0.67,0.675,0.68,0.685,0.69,0.695,0.7,0.705,0.71,0.715,0.72,0.725,0.73,0.735,0.74,0.745,0.75,0.755,0.76,0.765,0.77,0.775,0.78,0.785,0.79,0.795,0.8,0.805,0.81,0.815,0.82,0.825,0.83,0.835,0.84,0.845,0.85,0.855,0.86,0.865,0.87,0.875,0.88,0.885,0.89,0.895,0.9,0.905,0.91,0.915,0.92,0.925,0.93,0.935,0.94,0.945,0.95,0.955,0.96,0.965,0.97,0.975,0.98,0.985,0.99,0.995,1],"text":["b: -0.50<br />value: 0.2500<br />Function: Penalty<br />Lambda: 0.5","b: -0.49<br />value: 0.2450<br />Function: Penalty<br />Lambda: 0.5","b: -0.48<br />value: 0.2400<br />Function: Penalty<br />Lambda: 0.5","b: -0.47<br />value: 0.2350<br />Function: Penalty<br />Lambda: 0.5","b: -0.46<br />value: 0.2300<br />Function: Penalty<br />Lambda: 0.5","b: -0.45<br />value: 0.2250<br />Function: Penalty<br />Lambda: 0.5","b: -0.44<br />value: 0.2200<br />Function: Penalty<br />Lambda: 0.5","b: -0.43<br />value: 0.2150<br />Function: Penalty<br />Lambda: 0.5","b: -0.42<br />value: 0.2100<br />Function: Penalty<br />Lambda: 0.5","b: -0.41<br />value: 0.2050<br />Function: Penalty<br />Lambda: 0.5","b: -0.40<br />value: 0.2000<br />Function: Penalty<br />Lambda: 0.5","b: -0.39<br />value: 0.1950<br />Function: Penalty<br />Lambda: 0.5","b: -0.38<br />value: 0.1900<br />Function: Penalty<br />Lambda: 0.5","b: -0.37<br />value: 0.1850<br />Function: Penalty<br />Lambda: 0.5","b: -0.36<br />value: 0.1800<br />Function: Penalty<br />Lambda: 0.5","b: -0.35<br />value: 0.1750<br />Function: Penalty<br />Lambda: 0.5","b: -0.34<br />value: 0.1700<br />Function: Penalty<br />Lambda: 0.5","b: -0.33<br />value: 0.1650<br />Function: Penalty<br />Lambda: 0.5","b: -0.32<br />value: 0.1600<br />Function: Penalty<br />Lambda: 0.5","b: -0.31<br />value: 0.1550<br />Function: Penalty<br />Lambda: 0.5","b: -0.30<br />value: 0.1500<br />Function: Penalty<br />Lambda: 0.5","b: -0.29<br />value: 0.1450<br />Function: Penalty<br />Lambda: 0.5","b: -0.28<br />value: 0.1400<br />Function: Penalty<br />Lambda: 0.5","b: -0.27<br />value: 0.1350<br />Function: Penalty<br />Lambda: 0.5","b: -0.26<br />value: 0.1300<br />Function: Penalty<br />Lambda: 0.5","b: -0.25<br />value: 0.1250<br />Function: Penalty<br />Lambda: 0.5","b: -0.24<br />value: 0.1200<br />Function: Penalty<br />Lambda: 0.5","b: -0.23<br />value: 0.1150<br />Function: Penalty<br />Lambda: 0.5","b: -0.22<br />value: 0.1100<br />Function: Penalty<br />Lambda: 0.5","b: -0.21<br />value: 0.1050<br />Function: Penalty<br />Lambda: 0.5","b: -0.20<br />value: 0.1000<br />Function: Penalty<br />Lambda: 0.5","b: -0.19<br />value: 0.0950<br />Function: Penalty<br />Lambda: 0.5","b: -0.18<br />value: 0.0900<br />Function: Penalty<br />Lambda: 0.5","b: -0.17<br />value: 0.0850<br />Function: Penalty<br />Lambda: 0.5","b: -0.16<br />value: 0.0800<br />Function: Penalty<br />Lambda: 0.5","b: -0.15<br />value: 0.0750<br />Function: Penalty<br />Lambda: 0.5","b: -0.14<br />value: 0.0700<br />Function: Penalty<br />Lambda: 0.5","b: -0.13<br />value: 0.0650<br />Function: Penalty<br />Lambda: 0.5","b: -0.12<br />value: 0.0600<br />Function: Penalty<br />Lambda: 0.5","b: -0.11<br />value: 0.0550<br />Function: Penalty<br />Lambda: 0.5","b: -0.10<br />value: 0.0500<br />Function: Penalty<br />Lambda: 0.5","b: -0.09<br />value: 0.0450<br />Function: Penalty<br />Lambda: 0.5","b: -0.08<br />value: 0.0400<br />Function: Penalty<br />Lambda: 0.5","b: -0.07<br />value: 0.0350<br />Function: Penalty<br />Lambda: 0.5","b: -0.06<br />value: 0.0300<br />Function: Penalty<br />Lambda: 0.5","b: -0.05<br />value: 0.0250<br />Function: Penalty<br />Lambda: 0.5","b: -0.04<br />value: 0.0200<br />Function: Penalty<br />Lambda: 0.5","b: -0.03<br />value: 0.0150<br />Function: Penalty<br />Lambda: 0.5","b: -0.02<br />value: 0.0100<br />Function: Penalty<br />Lambda: 0.5","b: -0.01<br />value: 0.0050<br />Function: Penalty<br />Lambda: 0.5","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.5","b:  0.01<br />value: 0.0050<br />Function: Penalty<br />Lambda: 0.5","b:  0.02<br />value: 0.0100<br />Function: Penalty<br />Lambda: 0.5","b:  0.03<br />value: 0.0150<br />Function: Penalty<br />Lambda: 0.5","b:  0.04<br />value: 0.0200<br />Function: Penalty<br />Lambda: 0.5","b:  0.05<br />value: 0.0250<br />Function: Penalty<br />Lambda: 0.5","b:  0.06<br />value: 0.0300<br />Function: Penalty<br />Lambda: 0.5","b:  0.07<br />value: 0.0350<br />Function: Penalty<br />Lambda: 0.5","b:  0.08<br />value: 0.0400<br />Function: Penalty<br />Lambda: 0.5","b:  0.09<br />value: 0.0450<br />Function: Penalty<br />Lambda: 0.5","b:  0.10<br />value: 0.0500<br />Function: Penalty<br />Lambda: 0.5","b:  0.11<br />value: 0.0550<br />Function: Penalty<br />Lambda: 0.5","b:  0.12<br />value: 0.0600<br />Function: Penalty<br />Lambda: 0.5","b:  0.13<br />value: 0.0650<br />Function: Penalty<br />Lambda: 0.5","b:  0.14<br />value: 0.0700<br />Function: Penalty<br />Lambda: 0.5","b:  0.15<br />value: 0.0750<br />Function: Penalty<br />Lambda: 0.5","b:  0.16<br />value: 0.0800<br />Function: Penalty<br />Lambda: 0.5","b:  0.17<br />value: 0.0850<br />Function: Penalty<br />Lambda: 0.5","b:  0.18<br />value: 0.0900<br />Function: Penalty<br />Lambda: 0.5","b:  0.19<br />value: 0.0950<br />Function: Penalty<br />Lambda: 0.5","b:  0.20<br />value: 0.1000<br />Function: Penalty<br />Lambda: 0.5","b:  0.21<br />value: 0.1050<br />Function: Penalty<br />Lambda: 0.5","b:  0.22<br />value: 0.1100<br />Function: Penalty<br />Lambda: 0.5","b:  0.23<br />value: 0.1150<br />Function: Penalty<br />Lambda: 0.5","b:  0.24<br />value: 0.1200<br />Function: Penalty<br />Lambda: 0.5","b:  0.25<br />value: 0.1250<br />Function: Penalty<br />Lambda: 0.5","b:  0.26<br />value: 0.1300<br />Function: Penalty<br />Lambda: 0.5","b:  0.27<br />value: 0.1350<br />Function: Penalty<br />Lambda: 0.5","b:  0.28<br />value: 0.1400<br />Function: Penalty<br />Lambda: 0.5","b:  0.29<br />value: 0.1450<br />Function: Penalty<br />Lambda: 0.5","b:  0.30<br />value: 0.1500<br />Function: Penalty<br />Lambda: 0.5","b:  0.31<br />value: 0.1550<br />Function: Penalty<br />Lambda: 0.5","b:  0.32<br />value: 0.1600<br />Function: Penalty<br />Lambda: 0.5","b:  0.33<br />value: 0.1650<br />Function: Penalty<br />Lambda: 0.5","b:  0.34<br />value: 0.1700<br />Function: Penalty<br />Lambda: 0.5","b:  0.35<br />value: 0.1750<br />Function: Penalty<br />Lambda: 0.5","b:  0.36<br />value: 0.1800<br />Function: Penalty<br />Lambda: 0.5","b:  0.37<br />value: 0.1850<br />Function: Penalty<br />Lambda: 0.5","b:  0.38<br />value: 0.1900<br />Function: Penalty<br />Lambda: 0.5","b:  0.39<br />value: 0.1950<br />Function: Penalty<br />Lambda: 0.5","b:  0.40<br />value: 0.2000<br />Function: Penalty<br />Lambda: 0.5","b:  0.41<br />value: 0.2050<br />Function: Penalty<br />Lambda: 0.5","b:  0.42<br />value: 0.2100<br />Function: Penalty<br />Lambda: 0.5","b:  0.43<br />value: 0.2150<br />Function: Penalty<br />Lambda: 0.5","b:  0.44<br />value: 0.2200<br />Function: Penalty<br />Lambda: 0.5","b:  0.45<br />value: 0.2250<br />Function: Penalty<br />Lambda: 0.5","b:  0.46<br />value: 0.2300<br />Function: Penalty<br />Lambda: 0.5","b:  0.47<br />value: 0.2350<br />Function: Penalty<br />Lambda: 0.5","b:  0.48<br />value: 0.2400<br />Function: Penalty<br />Lambda: 0.5","b:  0.49<br />value: 0.2450<br />Function: Penalty<br />Lambda: 0.5","b:  0.50<br />value: 0.2500<br />Function: Penalty<br />Lambda: 0.5","b:  0.51<br />value: 0.2550<br />Function: Penalty<br />Lambda: 0.5","b:  0.52<br />value: 0.2600<br />Function: Penalty<br />Lambda: 0.5","b:  0.53<br />value: 0.2650<br />Function: Penalty<br />Lambda: 0.5","b:  0.54<br />value: 0.2700<br />Function: Penalty<br />Lambda: 0.5","b:  0.55<br />value: 0.2750<br />Function: Penalty<br />Lambda: 0.5","b:  0.56<br />value: 0.2800<br />Function: Penalty<br />Lambda: 0.5","b:  0.57<br />value: 0.2850<br />Function: Penalty<br />Lambda: 0.5","b:  0.58<br />value: 0.2900<br />Function: Penalty<br />Lambda: 0.5","b:  0.59<br />value: 0.2950<br />Function: Penalty<br />Lambda: 0.5","b:  0.60<br />value: 0.3000<br />Function: Penalty<br />Lambda: 0.5","b:  0.61<br />value: 0.3050<br />Function: Penalty<br />Lambda: 0.5","b:  0.62<br />value: 0.3100<br />Function: Penalty<br />Lambda: 0.5","b:  0.63<br />value: 0.3150<br />Function: Penalty<br />Lambda: 0.5","b:  0.64<br />value: 0.3200<br />Function: Penalty<br />Lambda: 0.5","b:  0.65<br />value: 0.3250<br />Function: Penalty<br />Lambda: 0.5","b:  0.66<br />value: 0.3300<br />Function: Penalty<br />Lambda: 0.5","b:  0.67<br />value: 0.3350<br />Function: Penalty<br />Lambda: 0.5","b:  0.68<br />value: 0.3400<br />Function: Penalty<br />Lambda: 0.5","b:  0.69<br />value: 0.3450<br />Function: Penalty<br />Lambda: 0.5","b:  0.70<br />value: 0.3500<br />Function: Penalty<br />Lambda: 0.5","b:  0.71<br />value: 0.3550<br />Function: Penalty<br />Lambda: 0.5","b:  0.72<br />value: 0.3600<br />Function: Penalty<br />Lambda: 0.5","b:  0.73<br />value: 0.3650<br />Function: Penalty<br />Lambda: 0.5","b:  0.74<br />value: 0.3700<br />Function: Penalty<br />Lambda: 0.5","b:  0.75<br />value: 0.3750<br />Function: Penalty<br />Lambda: 0.5","b:  0.76<br />value: 0.3800<br />Function: Penalty<br />Lambda: 0.5","b:  0.77<br />value: 0.3850<br />Function: Penalty<br />Lambda: 0.5","b:  0.78<br />value: 0.3900<br />Function: Penalty<br />Lambda: 0.5","b:  0.79<br />value: 0.3950<br />Function: Penalty<br />Lambda: 0.5","b:  0.80<br />value: 0.4000<br />Function: Penalty<br />Lambda: 0.5","b:  0.81<br />value: 0.4050<br />Function: Penalty<br />Lambda: 0.5","b:  0.82<br />value: 0.4100<br />Function: Penalty<br />Lambda: 0.5","b:  0.83<br />value: 0.4150<br />Function: Penalty<br />Lambda: 0.5","b:  0.84<br />value: 0.4200<br />Function: Penalty<br />Lambda: 0.5","b:  0.85<br />value: 0.4250<br />Function: Penalty<br />Lambda: 0.5","b:  0.86<br />value: 0.4300<br />Function: Penalty<br />Lambda: 0.5","b:  0.87<br />value: 0.4350<br />Function: Penalty<br />Lambda: 0.5","b:  0.88<br />value: 0.4400<br />Function: Penalty<br />Lambda: 0.5","b:  0.89<br />value: 0.4450<br />Function: Penalty<br />Lambda: 0.5","b:  0.90<br />value: 0.4500<br />Function: Penalty<br />Lambda: 0.5","b:  0.91<br />value: 0.4550<br />Function: Penalty<br />Lambda: 0.5","b:  0.92<br />value: 0.4600<br />Function: Penalty<br />Lambda: 0.5","b:  0.93<br />value: 0.4650<br />Function: Penalty<br />Lambda: 0.5","b:  0.94<br />value: 0.4700<br />Function: Penalty<br />Lambda: 0.5","b:  0.95<br />value: 0.4750<br />Function: Penalty<br />Lambda: 0.5","b:  0.96<br />value: 0.4800<br />Function: Penalty<br />Lambda: 0.5","b:  0.97<br />value: 0.4850<br />Function: Penalty<br />Lambda: 0.5","b:  0.98<br />value: 0.4900<br />Function: Penalty<br />Lambda: 0.5","b:  0.99<br />value: 0.4950<br />Function: Penalty<br />Lambda: 0.5","b:  1.00<br />value: 0.5000<br />Function: Penalty<br />Lambda: 0.5","b:  1.01<br />value: 0.5050<br />Function: Penalty<br />Lambda: 0.5","b:  1.02<br />value: 0.5100<br />Function: Penalty<br />Lambda: 0.5","b:  1.03<br />value: 0.5150<br />Function: Penalty<br />Lambda: 0.5","b:  1.04<br />value: 0.5200<br />Function: Penalty<br />Lambda: 0.5","b:  1.05<br />value: 0.5250<br />Function: Penalty<br />Lambda: 0.5","b:  1.06<br />value: 0.5300<br />Function: Penalty<br />Lambda: 0.5","b:  1.07<br />value: 0.5350<br />Function: Penalty<br />Lambda: 0.5","b:  1.08<br />value: 0.5400<br />Function: Penalty<br />Lambda: 0.5","b:  1.09<br />value: 0.5450<br />Function: Penalty<br />Lambda: 0.5","b:  1.10<br />value: 0.5500<br />Function: Penalty<br />Lambda: 0.5","b:  1.11<br />value: 0.5550<br />Function: Penalty<br />Lambda: 0.5","b:  1.12<br />value: 0.5600<br />Function: Penalty<br />Lambda: 0.5","b:  1.13<br />value: 0.5650<br />Function: Penalty<br />Lambda: 0.5","b:  1.14<br />value: 0.5700<br />Function: Penalty<br />Lambda: 0.5","b:  1.15<br />value: 0.5750<br />Function: Penalty<br />Lambda: 0.5","b:  1.16<br />value: 0.5800<br />Function: Penalty<br />Lambda: 0.5","b:  1.17<br />value: 0.5850<br />Function: Penalty<br />Lambda: 0.5","b:  1.18<br />value: 0.5900<br />Function: Penalty<br />Lambda: 0.5","b:  1.19<br />value: 0.5950<br />Function: Penalty<br />Lambda: 0.5","b:  1.20<br />value: 0.6000<br />Function: Penalty<br />Lambda: 0.5","b:  1.21<br />value: 0.6050<br />Function: Penalty<br />Lambda: 0.5","b:  1.22<br />value: 0.6100<br />Function: Penalty<br />Lambda: 0.5","b:  1.23<br />value: 0.6150<br />Function: Penalty<br />Lambda: 0.5","b:  1.24<br />value: 0.6200<br />Function: Penalty<br />Lambda: 0.5","b:  1.25<br />value: 0.6250<br />Function: Penalty<br />Lambda: 0.5","b:  1.26<br />value: 0.6300<br />Function: Penalty<br />Lambda: 0.5","b:  1.27<br />value: 0.6350<br />Function: Penalty<br />Lambda: 0.5","b:  1.28<br />value: 0.6400<br />Function: Penalty<br />Lambda: 0.5","b:  1.29<br />value: 0.6450<br />Function: Penalty<br />Lambda: 0.5","b:  1.30<br />value: 0.6500<br />Function: Penalty<br />Lambda: 0.5","b:  1.31<br />value: 0.6550<br />Function: Penalty<br />Lambda: 0.5","b:  1.32<br />value: 0.6600<br />Function: Penalty<br />Lambda: 0.5","b:  1.33<br />value: 0.6650<br />Function: Penalty<br />Lambda: 0.5","b:  1.34<br />value: 0.6700<br />Function: Penalty<br />Lambda: 0.5","b:  1.35<br />value: 0.6750<br />Function: Penalty<br />Lambda: 0.5","b:  1.36<br />value: 0.6800<br />Function: Penalty<br />Lambda: 0.5","b:  1.37<br />value: 0.6850<br />Function: Penalty<br />Lambda: 0.5","b:  1.38<br />value: 0.6900<br />Function: Penalty<br />Lambda: 0.5","b:  1.39<br />value: 0.6950<br />Function: Penalty<br />Lambda: 0.5","b:  1.40<br />value: 0.7000<br />Function: Penalty<br />Lambda: 0.5","b:  1.41<br />value: 0.7050<br />Function: Penalty<br />Lambda: 0.5","b:  1.42<br />value: 0.7100<br />Function: Penalty<br />Lambda: 0.5","b:  1.43<br />value: 0.7150<br />Function: Penalty<br />Lambda: 0.5","b:  1.44<br />value: 0.7200<br />Function: Penalty<br />Lambda: 0.5","b:  1.45<br />value: 0.7250<br />Function: Penalty<br />Lambda: 0.5","b:  1.46<br />value: 0.7300<br />Function: Penalty<br />Lambda: 0.5","b:  1.47<br />value: 0.7350<br />Function: Penalty<br />Lambda: 0.5","b:  1.48<br />value: 0.7400<br />Function: Penalty<br />Lambda: 0.5","b:  1.49<br />value: 0.7450<br />Function: Penalty<br />Lambda: 0.5","b:  1.50<br />value: 0.7500<br />Function: Penalty<br />Lambda: 0.5","b:  1.51<br />value: 0.7550<br />Function: Penalty<br />Lambda: 0.5","b:  1.52<br />value: 0.7600<br />Function: Penalty<br />Lambda: 0.5","b:  1.53<br />value: 0.7650<br />Function: Penalty<br />Lambda: 0.5","b:  1.54<br />value: 0.7700<br />Function: Penalty<br />Lambda: 0.5","b:  1.55<br />value: 0.7750<br />Function: Penalty<br />Lambda: 0.5","b:  1.56<br />value: 0.7800<br />Function: Penalty<br />Lambda: 0.5","b:  1.57<br />value: 0.7850<br />Function: Penalty<br />Lambda: 0.5","b:  1.58<br />value: 0.7900<br />Function: Penalty<br />Lambda: 0.5","b:  1.59<br />value: 0.7950<br />Function: Penalty<br />Lambda: 0.5","b:  1.60<br />value: 0.8000<br />Function: Penalty<br />Lambda: 0.5","b:  1.61<br />value: 0.8050<br />Function: Penalty<br />Lambda: 0.5","b:  1.62<br />value: 0.8100<br />Function: Penalty<br />Lambda: 0.5","b:  1.63<br />value: 0.8150<br />Function: Penalty<br />Lambda: 0.5","b:  1.64<br />value: 0.8200<br />Function: Penalty<br />Lambda: 0.5","b:  1.65<br />value: 0.8250<br />Function: Penalty<br />Lambda: 0.5","b:  1.66<br />value: 0.8300<br />Function: Penalty<br />Lambda: 0.5","b:  1.67<br />value: 0.8350<br />Function: Penalty<br />Lambda: 0.5","b:  1.68<br />value: 0.8400<br />Function: Penalty<br />Lambda: 0.5","b:  1.69<br />value: 0.8450<br />Function: Penalty<br />Lambda: 0.5","b:  1.70<br />value: 0.8500<br />Function: Penalty<br />Lambda: 0.5","b:  1.71<br />value: 0.8550<br />Function: Penalty<br />Lambda: 0.5","b:  1.72<br />value: 0.8600<br />Function: Penalty<br />Lambda: 0.5","b:  1.73<br />value: 0.8650<br />Function: Penalty<br />Lambda: 0.5","b:  1.74<br />value: 0.8700<br />Function: Penalty<br />Lambda: 0.5","b:  1.75<br />value: 0.8750<br />Function: Penalty<br />Lambda: 0.5","b:  1.76<br />value: 0.8800<br />Function: Penalty<br />Lambda: 0.5","b:  1.77<br />value: 0.8850<br />Function: Penalty<br />Lambda: 0.5","b:  1.78<br />value: 0.8900<br />Function: Penalty<br />Lambda: 0.5","b:  1.79<br />value: 0.8950<br />Function: Penalty<br />Lambda: 0.5","b:  1.80<br />value: 0.9000<br />Function: Penalty<br />Lambda: 0.5","b:  1.81<br />value: 0.9050<br />Function: Penalty<br />Lambda: 0.5","b:  1.82<br />value: 0.9100<br />Function: Penalty<br />Lambda: 0.5","b:  1.83<br />value: 0.9150<br />Function: Penalty<br />Lambda: 0.5","b:  1.84<br />value: 0.9200<br />Function: Penalty<br />Lambda: 0.5","b:  1.85<br />value: 0.9250<br />Function: Penalty<br />Lambda: 0.5","b:  1.86<br />value: 0.9300<br />Function: Penalty<br />Lambda: 0.5","b:  1.87<br />value: 0.9350<br />Function: Penalty<br />Lambda: 0.5","b:  1.88<br />value: 0.9400<br />Function: Penalty<br />Lambda: 0.5","b:  1.89<br />value: 0.9450<br />Function: Penalty<br />Lambda: 0.5","b:  1.90<br />value: 0.9500<br />Function: Penalty<br />Lambda: 0.5","b:  1.91<br />value: 0.9550<br />Function: Penalty<br />Lambda: 0.5","b:  1.92<br />value: 0.9600<br />Function: Penalty<br />Lambda: 0.5","b:  1.93<br />value: 0.9650<br />Function: Penalty<br />Lambda: 0.5","b:  1.94<br />value: 0.9700<br />Function: Penalty<br />Lambda: 0.5","b:  1.95<br />value: 0.9750<br />Function: Penalty<br />Lambda: 0.5","b:  1.96<br />value: 0.9800<br />Function: Penalty<br />Lambda: 0.5","b:  1.97<br />value: 0.9850<br />Function: Penalty<br />Lambda: 0.5","b:  1.98<br />value: 0.9900<br />Function: Penalty<br />Lambda: 0.5","b:  1.99<br />value: 0.9950<br />Function: Penalty<br />Lambda: 0.5","b:  2.00<br />value: 1.0000<br />Function: Penalty<br />Lambda: 0.5"],"frame":"0.5","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"0.6","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 0.6","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 0.6","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 0.6","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 0.6","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 0.6","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 0.6","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 0.6","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 0.6","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 0.6","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 0.6","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 0.6","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 0.6","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 0.6","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 0.6","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 0.6","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 0.6","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 0.6","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 0.6","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 0.6","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 0.6","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 0.6","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 0.6","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 0.6","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 0.6","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 0.6","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 0.6","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 0.6","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 0.6","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 0.6","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 0.6","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 0.6","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 0.6","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 0.6","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 0.6","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 0.6","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 0.6","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 0.6","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 0.6","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 0.6","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 0.6","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 0.6","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 0.6","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 0.6","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 0.6","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 0.6","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 0.6","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 0.6","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 0.6","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 0.6","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 0.6","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.6","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.6","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.6","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.6","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.6","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.6","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.6","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.6","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.6","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.6","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.6","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.6","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.6","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.6","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.6","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.6","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.6","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.6","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.6","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.6","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.6","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.6","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.6","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.6","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.6","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.6","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.6","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.6","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.6","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.6","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.6","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.6","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.6","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.6","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.6","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.6","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.6","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.6","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.6","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.6","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.6","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.6","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.6","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.6","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.6","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.6","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.6","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.6","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.6","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.6","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.6","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.6","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.6","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.6","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.6","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.6","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.6","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.6","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.6","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.6","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.6","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.6","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.6","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.6","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.6","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.6","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.6","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.6","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.6","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.6","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.6","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.6","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.6","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.6","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.6","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.6","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.6","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.6","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.6","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.6","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.6","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.6","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.6","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.6","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.6","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.6","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.6","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.6","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.6","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.6","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.6","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.6","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.6","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.6","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.6","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.6","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.6","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.6","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.6","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.6","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 0.6","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.6","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.6","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.6","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.6","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.6","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.6","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.6","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.6","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.6","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.6","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.6","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.6","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.6","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.6","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.6","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.6","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.6","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.6","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.6","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.6","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.6","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.6","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.6","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.6","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.6","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.6","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.6","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.6","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.6","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.6","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.6","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.6","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.6","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.6","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.6","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.6","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.6","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.6","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.6","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.6","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.6","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.6","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.6","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.6","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.6","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.6","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.6","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.6","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.6","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.6","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.6","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.6","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.6","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.6","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.6","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.6","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.6","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.6","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.6","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.6","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.6","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.6","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.6","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.6","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.6","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.6","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.6","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.6","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.6","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.6","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.6","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.6","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.6","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.6","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.6","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.6","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.6","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.6","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.6","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.6","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.6","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.6","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.6","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.6","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.6","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.6","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.6","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.6","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.6","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.6","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.6","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.6","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.6","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.6","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.6","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.6","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.6","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.6","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.6","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.6"],"frame":"0.6","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.05,3.0141,2.9784,2.9429,2.9076,2.8725,2.8376,2.8029,2.7684,2.7341,2.7,2.6661,2.6324,2.5989,2.5656,2.5325,2.4996,2.4669,2.4344,2.4021,2.37,2.3381,2.3064,2.2749,2.2436,2.2125,2.1816,2.1509,2.1204,2.0901,2.06,2.0301,2.0004,1.9709,1.9416,1.9125,1.8836,1.8549,1.8264,1.7981,1.77,1.7421,1.7144,1.6869,1.6596,1.6325,1.6056,1.5789,1.5524,1.5261,1.5,1.4861,1.4724,1.4589,1.4456,1.4325,1.4196,1.4069,1.3944,1.3821,1.37,1.3581,1.3464,1.3349,1.3236,1.3125,1.3016,1.2909,1.2804,1.2701,1.26,1.2501,1.2404,1.2309,1.2216,1.2125,1.2036,1.1949,1.1864,1.1781,1.17,1.1621,1.1544,1.1469,1.1396,1.1325,1.1256,1.1189,1.1124,1.1061,1.1,1.0941,1.0884,1.0829,1.0776,1.0725,1.0676,1.0629,1.0584,1.0541,1.05,1.0461,1.0424,1.0389,1.0356,1.0325,1.0296,1.0269,1.0244,1.0221,1.02,1.0181,1.0164,1.0149,1.0136,1.0125,1.0116,1.0109,1.0104,1.0101,1.01,1.0101,1.0104,1.0109,1.0116,1.0125,1.0136,1.0149,1.0164,1.0181,1.02,1.0221,1.0244,1.0269,1.0296,1.0325,1.0356,1.0389,1.0424,1.0461,1.05,1.0541,1.0584,1.0629,1.0676,1.0725,1.0776,1.0829,1.0884,1.0941,1.1,1.1061,1.1124,1.1189,1.1256,1.1325,1.1396,1.1469,1.1544,1.1621,1.17,1.1781,1.1864,1.1949,1.2036,1.2125,1.2216,1.2309,1.2404,1.2501,1.26,1.2701,1.2804,1.2909,1.3016,1.3125,1.3236,1.3349,1.3464,1.3581,1.37,1.3821,1.3944,1.4069,1.4196,1.4325,1.4456,1.4589,1.4724,1.4861,1.5,1.5141,1.5284,1.5429,1.5576,1.5725,1.5876,1.6029,1.6184,1.6341,1.65,1.6661,1.6824,1.6989,1.7156,1.7325,1.7496,1.7669,1.7844,1.8021,1.82,1.8381,1.8564,1.8749,1.8936,1.9125,1.9316,1.9509,1.9704,1.9901,2.01,2.0301,2.0504,2.0709,2.0916,2.1125,2.1336,2.1549,2.1764,2.1981,2.22,2.2421,2.2644,2.2869,2.3096,2.3325,2.3556,2.3789,2.4024,2.4261,2.45,2.4741,2.4984,2.5229,2.5476,2.5725,2.5976,2.6229,2.6484,2.6741,2.7],"text":["b: -0.50<br />value: 3.0500<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.49<br />value: 3.0141<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.48<br />value: 2.9784<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.47<br />value: 2.9429<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.46<br />value: 2.9076<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.45<br />value: 2.8725<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.44<br />value: 2.8376<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.43<br />value: 2.8029<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.42<br />value: 2.7684<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.41<br />value: 2.7341<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.40<br />value: 2.7000<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.39<br />value: 2.6661<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.38<br />value: 2.6324<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.37<br />value: 2.5989<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.36<br />value: 2.5656<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.35<br />value: 2.5325<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.34<br />value: 2.4996<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.33<br />value: 2.4669<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.32<br />value: 2.4344<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.31<br />value: 2.4021<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.30<br />value: 2.3700<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.29<br />value: 2.3381<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.28<br />value: 2.3064<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.27<br />value: 2.2749<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.26<br />value: 2.2436<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.25<br />value: 2.2125<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.24<br />value: 2.1816<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.23<br />value: 2.1509<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.22<br />value: 2.1204<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.21<br />value: 2.0901<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.20<br />value: 2.0600<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.19<br />value: 2.0301<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.18<br />value: 2.0004<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.17<br />value: 1.9709<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.16<br />value: 1.9416<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.15<br />value: 1.9125<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.14<br />value: 1.8836<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.13<br />value: 1.8549<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.12<br />value: 1.8264<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.11<br />value: 1.7981<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.10<br />value: 1.7700<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.09<br />value: 1.7421<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.08<br />value: 1.7144<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.07<br />value: 1.6869<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.06<br />value: 1.6596<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.05<br />value: 1.6325<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.04<br />value: 1.6056<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.03<br />value: 1.5789<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.02<br />value: 1.5524<br />Function: Loss + Penalty<br />Lambda: 0.6","b: -0.01<br />value: 1.5261<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.01<br />value: 1.4861<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.02<br />value: 1.4724<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.03<br />value: 1.4589<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.04<br />value: 1.4456<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.05<br />value: 1.4325<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.06<br />value: 1.4196<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.07<br />value: 1.4069<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.08<br />value: 1.3944<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.09<br />value: 1.3821<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.10<br />value: 1.3700<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.11<br />value: 1.3581<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.12<br />value: 1.3464<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.13<br />value: 1.3349<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.14<br />value: 1.3236<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.15<br />value: 1.3125<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.16<br />value: 1.3016<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.17<br />value: 1.2909<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.18<br />value: 1.2804<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.19<br />value: 1.2701<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.20<br />value: 1.2600<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.21<br />value: 1.2501<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.22<br />value: 1.2404<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.23<br />value: 1.2309<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.24<br />value: 1.2216<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.25<br />value: 1.2125<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.26<br />value: 1.2036<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.27<br />value: 1.1949<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.28<br />value: 1.1864<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.29<br />value: 1.1781<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.30<br />value: 1.1700<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.31<br />value: 1.1621<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.32<br />value: 1.1544<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.33<br />value: 1.1469<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.34<br />value: 1.1396<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.35<br />value: 1.1325<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.36<br />value: 1.1256<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.37<br />value: 1.1189<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.38<br />value: 1.1124<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.39<br />value: 1.1061<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.40<br />value: 1.1000<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.41<br />value: 1.0941<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.42<br />value: 1.0884<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.43<br />value: 1.0829<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.44<br />value: 1.0776<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.45<br />value: 1.0725<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.46<br />value: 1.0676<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.47<br />value: 1.0629<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.48<br />value: 1.0584<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.49<br />value: 1.0541<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.50<br />value: 1.0500<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.51<br />value: 1.0461<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.52<br />value: 1.0424<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.53<br />value: 1.0389<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.54<br />value: 1.0356<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.55<br />value: 1.0325<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.56<br />value: 1.0296<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.57<br />value: 1.0269<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.58<br />value: 1.0244<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.59<br />value: 1.0221<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.60<br />value: 1.0200<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.61<br />value: 1.0181<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.62<br />value: 1.0164<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.63<br />value: 1.0149<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.64<br />value: 1.0136<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.65<br />value: 1.0125<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.66<br />value: 1.0116<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.67<br />value: 1.0109<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.68<br />value: 1.0104<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.69<br />value: 1.0101<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.70<br />value: 1.0100<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.71<br />value: 1.0101<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.72<br />value: 1.0104<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.73<br />value: 1.0109<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.74<br />value: 1.0116<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.75<br />value: 1.0125<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.76<br />value: 1.0136<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.77<br />value: 1.0149<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.78<br />value: 1.0164<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.79<br />value: 1.0181<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.80<br />value: 1.0200<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.81<br />value: 1.0221<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.82<br />value: 1.0244<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.83<br />value: 1.0269<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.84<br />value: 1.0296<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.85<br />value: 1.0325<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.86<br />value: 1.0356<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.87<br />value: 1.0389<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.88<br />value: 1.0424<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.89<br />value: 1.0461<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.90<br />value: 1.0500<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.91<br />value: 1.0541<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.92<br />value: 1.0584<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.93<br />value: 1.0629<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.94<br />value: 1.0676<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.95<br />value: 1.0725<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.96<br />value: 1.0776<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.97<br />value: 1.0829<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.98<br />value: 1.0884<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  0.99<br />value: 1.0941<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.00<br />value: 1.1000<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.01<br />value: 1.1061<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.02<br />value: 1.1124<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.03<br />value: 1.1189<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.04<br />value: 1.1256<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.05<br />value: 1.1325<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.06<br />value: 1.1396<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.07<br />value: 1.1469<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.08<br />value: 1.1544<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.09<br />value: 1.1621<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.10<br />value: 1.1700<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.11<br />value: 1.1781<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.12<br />value: 1.1864<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.13<br />value: 1.1949<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.14<br />value: 1.2036<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.15<br />value: 1.2125<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.16<br />value: 1.2216<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.17<br />value: 1.2309<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.18<br />value: 1.2404<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.19<br />value: 1.2501<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.20<br />value: 1.2600<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.21<br />value: 1.2701<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.22<br />value: 1.2804<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.23<br />value: 1.2909<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.24<br />value: 1.3016<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.25<br />value: 1.3125<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.26<br />value: 1.3236<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.27<br />value: 1.3349<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.28<br />value: 1.3464<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.29<br />value: 1.3581<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.30<br />value: 1.3700<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.31<br />value: 1.3821<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.32<br />value: 1.3944<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.33<br />value: 1.4069<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.34<br />value: 1.4196<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.35<br />value: 1.4325<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.36<br />value: 1.4456<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.37<br />value: 1.4589<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.38<br />value: 1.4724<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.39<br />value: 1.4861<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.40<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.41<br />value: 1.5141<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.42<br />value: 1.5284<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.43<br />value: 1.5429<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.44<br />value: 1.5576<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.45<br />value: 1.5725<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.46<br />value: 1.5876<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.47<br />value: 1.6029<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.48<br />value: 1.6184<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.49<br />value: 1.6341<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.50<br />value: 1.6500<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.51<br />value: 1.6661<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.52<br />value: 1.6824<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.53<br />value: 1.6989<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.54<br />value: 1.7156<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.55<br />value: 1.7325<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.56<br />value: 1.7496<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.57<br />value: 1.7669<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.58<br />value: 1.7844<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.59<br />value: 1.8021<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.60<br />value: 1.8200<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.61<br />value: 1.8381<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.62<br />value: 1.8564<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.63<br />value: 1.8749<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.64<br />value: 1.8936<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.65<br />value: 1.9125<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.66<br />value: 1.9316<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.67<br />value: 1.9509<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.68<br />value: 1.9704<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.69<br />value: 1.9901<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.70<br />value: 2.0100<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.71<br />value: 2.0301<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.72<br />value: 2.0504<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.73<br />value: 2.0709<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.74<br />value: 2.0916<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.75<br />value: 2.1125<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.76<br />value: 2.1336<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.77<br />value: 2.1549<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.78<br />value: 2.1764<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.79<br />value: 2.1981<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.80<br />value: 2.2200<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.81<br />value: 2.2421<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.82<br />value: 2.2644<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.83<br />value: 2.2869<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.84<br />value: 2.3096<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.85<br />value: 2.3325<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.86<br />value: 2.3556<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.87<br />value: 2.3789<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.88<br />value: 2.4024<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.89<br />value: 2.4261<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.90<br />value: 2.4500<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.91<br />value: 2.4741<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.92<br />value: 2.4984<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.93<br />value: 2.5229<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.94<br />value: 2.5476<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.95<br />value: 2.5725<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.96<br />value: 2.5976<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.97<br />value: 2.6229<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.98<br />value: 2.6484<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  1.99<br />value: 2.6741<br />Function: Loss + Penalty<br />Lambda: 0.6","b:  2.00<br />value: 2.7000<br />Function: Loss + Penalty<br />Lambda: 0.6"],"frame":"0.6","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.3,0.294,0.288,0.282,0.276,0.27,0.264,0.258,0.252,0.246,0.24,0.234,0.228,0.222,0.216,0.21,0.204,0.198,0.192,0.186,0.18,0.174,0.168,0.162,0.156,0.15,0.144,0.138,0.132,0.126,0.12,0.114,0.108,0.102,0.096,0.09,0.084,0.078,0.072,0.066,0.06,0.054,0.048,0.042,0.036,0.03,0.024,0.018,0.012,0.00600000000000001,0,0.00600000000000001,0.012,0.018,0.024,0.03,0.036,0.042,0.048,0.054,0.06,0.066,0.072,0.078,0.084,0.09,0.096,0.102,0.108,0.114,0.12,0.126,0.132,0.138,0.144,0.15,0.156,0.162,0.168,0.174,0.18,0.186,0.192,0.198,0.204,0.21,0.216,0.222,0.228,0.234,0.24,0.246,0.252,0.258,0.264,0.27,0.276,0.282,0.288,0.294,0.3,0.306,0.312,0.318,0.324,0.33,0.336,0.342,0.348,0.354,0.36,0.366,0.372,0.378,0.384,0.39,0.396,0.402,0.408,0.414,0.42,0.426,0.432,0.438,0.444,0.45,0.456,0.462,0.468,0.474,0.48,0.486,0.492,0.498,0.504,0.51,0.516,0.522,0.528,0.534,0.54,0.546,0.552,0.558,0.564,0.57,0.576,0.582,0.588,0.594,0.6,0.606,0.612,0.618,0.624,0.63,0.636,0.642,0.648,0.654,0.66,0.666,0.672,0.678,0.684,0.69,0.696,0.702,0.708,0.714,0.72,0.726,0.732,0.738,0.744,0.75,0.756,0.762,0.768,0.774,0.78,0.786,0.792,0.798,0.804,0.81,0.816,0.822,0.828,0.834,0.84,0.846,0.852,0.858,0.864,0.87,0.876,0.882,0.888,0.894,0.9,0.906,0.912,0.918,0.924,0.93,0.936,0.942,0.948,0.954,0.96,0.966,0.972,0.978,0.984,0.99,0.996,1.002,1.008,1.014,1.02,1.026,1.032,1.038,1.044,1.05,1.056,1.062,1.068,1.074,1.08,1.086,1.092,1.098,1.104,1.11,1.116,1.122,1.128,1.134,1.14,1.146,1.152,1.158,1.164,1.17,1.176,1.182,1.188,1.194,1.2],"text":["b: -0.50<br />value: 0.3000<br />Function: Penalty<br />Lambda: 0.6","b: -0.49<br />value: 0.2940<br />Function: Penalty<br />Lambda: 0.6","b: -0.48<br />value: 0.2880<br />Function: Penalty<br />Lambda: 0.6","b: -0.47<br />value: 0.2820<br />Function: Penalty<br />Lambda: 0.6","b: -0.46<br />value: 0.2760<br />Function: Penalty<br />Lambda: 0.6","b: -0.45<br />value: 0.2700<br />Function: Penalty<br />Lambda: 0.6","b: -0.44<br />value: 0.2640<br />Function: Penalty<br />Lambda: 0.6","b: -0.43<br />value: 0.2580<br />Function: Penalty<br />Lambda: 0.6","b: -0.42<br />value: 0.2520<br />Function: Penalty<br />Lambda: 0.6","b: -0.41<br />value: 0.2460<br />Function: Penalty<br />Lambda: 0.6","b: -0.40<br />value: 0.2400<br />Function: Penalty<br />Lambda: 0.6","b: -0.39<br />value: 0.2340<br />Function: Penalty<br />Lambda: 0.6","b: -0.38<br />value: 0.2280<br />Function: Penalty<br />Lambda: 0.6","b: -0.37<br />value: 0.2220<br />Function: Penalty<br />Lambda: 0.6","b: -0.36<br />value: 0.2160<br />Function: Penalty<br />Lambda: 0.6","b: -0.35<br />value: 0.2100<br />Function: Penalty<br />Lambda: 0.6","b: -0.34<br />value: 0.2040<br />Function: Penalty<br />Lambda: 0.6","b: -0.33<br />value: 0.1980<br />Function: Penalty<br />Lambda: 0.6","b: -0.32<br />value: 0.1920<br />Function: Penalty<br />Lambda: 0.6","b: -0.31<br />value: 0.1860<br />Function: Penalty<br />Lambda: 0.6","b: -0.30<br />value: 0.1800<br />Function: Penalty<br />Lambda: 0.6","b: -0.29<br />value: 0.1740<br />Function: Penalty<br />Lambda: 0.6","b: -0.28<br />value: 0.1680<br />Function: Penalty<br />Lambda: 0.6","b: -0.27<br />value: 0.1620<br />Function: Penalty<br />Lambda: 0.6","b: -0.26<br />value: 0.1560<br />Function: Penalty<br />Lambda: 0.6","b: -0.25<br />value: 0.1500<br />Function: Penalty<br />Lambda: 0.6","b: -0.24<br />value: 0.1440<br />Function: Penalty<br />Lambda: 0.6","b: -0.23<br />value: 0.1380<br />Function: Penalty<br />Lambda: 0.6","b: -0.22<br />value: 0.1320<br />Function: Penalty<br />Lambda: 0.6","b: -0.21<br />value: 0.1260<br />Function: Penalty<br />Lambda: 0.6","b: -0.20<br />value: 0.1200<br />Function: Penalty<br />Lambda: 0.6","b: -0.19<br />value: 0.1140<br />Function: Penalty<br />Lambda: 0.6","b: -0.18<br />value: 0.1080<br />Function: Penalty<br />Lambda: 0.6","b: -0.17<br />value: 0.1020<br />Function: Penalty<br />Lambda: 0.6","b: -0.16<br />value: 0.0960<br />Function: Penalty<br />Lambda: 0.6","b: -0.15<br />value: 0.0900<br />Function: Penalty<br />Lambda: 0.6","b: -0.14<br />value: 0.0840<br />Function: Penalty<br />Lambda: 0.6","b: -0.13<br />value: 0.0780<br />Function: Penalty<br />Lambda: 0.6","b: -0.12<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.6","b: -0.11<br />value: 0.0660<br />Function: Penalty<br />Lambda: 0.6","b: -0.10<br />value: 0.0600<br />Function: Penalty<br />Lambda: 0.6","b: -0.09<br />value: 0.0540<br />Function: Penalty<br />Lambda: 0.6","b: -0.08<br />value: 0.0480<br />Function: Penalty<br />Lambda: 0.6","b: -0.07<br />value: 0.0420<br />Function: Penalty<br />Lambda: 0.6","b: -0.06<br />value: 0.0360<br />Function: Penalty<br />Lambda: 0.6","b: -0.05<br />value: 0.0300<br />Function: Penalty<br />Lambda: 0.6","b: -0.04<br />value: 0.0240<br />Function: Penalty<br />Lambda: 0.6","b: -0.03<br />value: 0.0180<br />Function: Penalty<br />Lambda: 0.6","b: -0.02<br />value: 0.0120<br />Function: Penalty<br />Lambda: 0.6","b: -0.01<br />value: 0.0060<br />Function: Penalty<br />Lambda: 0.6","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.6","b:  0.01<br />value: 0.0060<br />Function: Penalty<br />Lambda: 0.6","b:  0.02<br />value: 0.0120<br />Function: Penalty<br />Lambda: 0.6","b:  0.03<br />value: 0.0180<br />Function: Penalty<br />Lambda: 0.6","b:  0.04<br />value: 0.0240<br />Function: Penalty<br />Lambda: 0.6","b:  0.05<br />value: 0.0300<br />Function: Penalty<br />Lambda: 0.6","b:  0.06<br />value: 0.0360<br />Function: Penalty<br />Lambda: 0.6","b:  0.07<br />value: 0.0420<br />Function: Penalty<br />Lambda: 0.6","b:  0.08<br />value: 0.0480<br />Function: Penalty<br />Lambda: 0.6","b:  0.09<br />value: 0.0540<br />Function: Penalty<br />Lambda: 0.6","b:  0.10<br />value: 0.0600<br />Function: Penalty<br />Lambda: 0.6","b:  0.11<br />value: 0.0660<br />Function: Penalty<br />Lambda: 0.6","b:  0.12<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.6","b:  0.13<br />value: 0.0780<br />Function: Penalty<br />Lambda: 0.6","b:  0.14<br />value: 0.0840<br />Function: Penalty<br />Lambda: 0.6","b:  0.15<br />value: 0.0900<br />Function: Penalty<br />Lambda: 0.6","b:  0.16<br />value: 0.0960<br />Function: Penalty<br />Lambda: 0.6","b:  0.17<br />value: 0.1020<br />Function: Penalty<br />Lambda: 0.6","b:  0.18<br />value: 0.1080<br />Function: Penalty<br />Lambda: 0.6","b:  0.19<br />value: 0.1140<br />Function: Penalty<br />Lambda: 0.6","b:  0.20<br />value: 0.1200<br />Function: Penalty<br />Lambda: 0.6","b:  0.21<br />value: 0.1260<br />Function: Penalty<br />Lambda: 0.6","b:  0.22<br />value: 0.1320<br />Function: Penalty<br />Lambda: 0.6","b:  0.23<br />value: 0.1380<br />Function: Penalty<br />Lambda: 0.6","b:  0.24<br />value: 0.1440<br />Function: Penalty<br />Lambda: 0.6","b:  0.25<br />value: 0.1500<br />Function: Penalty<br />Lambda: 0.6","b:  0.26<br />value: 0.1560<br />Function: Penalty<br />Lambda: 0.6","b:  0.27<br />value: 0.1620<br />Function: Penalty<br />Lambda: 0.6","b:  0.28<br />value: 0.1680<br />Function: Penalty<br />Lambda: 0.6","b:  0.29<br />value: 0.1740<br />Function: Penalty<br />Lambda: 0.6","b:  0.30<br />value: 0.1800<br />Function: Penalty<br />Lambda: 0.6","b:  0.31<br />value: 0.1860<br />Function: Penalty<br />Lambda: 0.6","b:  0.32<br />value: 0.1920<br />Function: Penalty<br />Lambda: 0.6","b:  0.33<br />value: 0.1980<br />Function: Penalty<br />Lambda: 0.6","b:  0.34<br />value: 0.2040<br />Function: Penalty<br />Lambda: 0.6","b:  0.35<br />value: 0.2100<br />Function: Penalty<br />Lambda: 0.6","b:  0.36<br />value: 0.2160<br />Function: Penalty<br />Lambda: 0.6","b:  0.37<br />value: 0.2220<br />Function: Penalty<br />Lambda: 0.6","b:  0.38<br />value: 0.2280<br />Function: Penalty<br />Lambda: 0.6","b:  0.39<br />value: 0.2340<br />Function: Penalty<br />Lambda: 0.6","b:  0.40<br />value: 0.2400<br />Function: Penalty<br />Lambda: 0.6","b:  0.41<br />value: 0.2460<br />Function: Penalty<br />Lambda: 0.6","b:  0.42<br />value: 0.2520<br />Function: Penalty<br />Lambda: 0.6","b:  0.43<br />value: 0.2580<br />Function: Penalty<br />Lambda: 0.6","b:  0.44<br />value: 0.2640<br />Function: Penalty<br />Lambda: 0.6","b:  0.45<br />value: 0.2700<br />Function: Penalty<br />Lambda: 0.6","b:  0.46<br />value: 0.2760<br />Function: Penalty<br />Lambda: 0.6","b:  0.47<br />value: 0.2820<br />Function: Penalty<br />Lambda: 0.6","b:  0.48<br />value: 0.2880<br />Function: Penalty<br />Lambda: 0.6","b:  0.49<br />value: 0.2940<br />Function: Penalty<br />Lambda: 0.6","b:  0.50<br />value: 0.3000<br />Function: Penalty<br />Lambda: 0.6","b:  0.51<br />value: 0.3060<br />Function: Penalty<br />Lambda: 0.6","b:  0.52<br />value: 0.3120<br />Function: Penalty<br />Lambda: 0.6","b:  0.53<br />value: 0.3180<br />Function: Penalty<br />Lambda: 0.6","b:  0.54<br />value: 0.3240<br />Function: Penalty<br />Lambda: 0.6","b:  0.55<br />value: 0.3300<br />Function: Penalty<br />Lambda: 0.6","b:  0.56<br />value: 0.3360<br />Function: Penalty<br />Lambda: 0.6","b:  0.57<br />value: 0.3420<br />Function: Penalty<br />Lambda: 0.6","b:  0.58<br />value: 0.3480<br />Function: Penalty<br />Lambda: 0.6","b:  0.59<br />value: 0.3540<br />Function: Penalty<br />Lambda: 0.6","b:  0.60<br />value: 0.3600<br />Function: Penalty<br />Lambda: 0.6","b:  0.61<br />value: 0.3660<br />Function: Penalty<br />Lambda: 0.6","b:  0.62<br />value: 0.3720<br />Function: Penalty<br />Lambda: 0.6","b:  0.63<br />value: 0.3780<br />Function: Penalty<br />Lambda: 0.6","b:  0.64<br />value: 0.3840<br />Function: Penalty<br />Lambda: 0.6","b:  0.65<br />value: 0.3900<br />Function: Penalty<br />Lambda: 0.6","b:  0.66<br />value: 0.3960<br />Function: Penalty<br />Lambda: 0.6","b:  0.67<br />value: 0.4020<br />Function: Penalty<br />Lambda: 0.6","b:  0.68<br />value: 0.4080<br />Function: Penalty<br />Lambda: 0.6","b:  0.69<br />value: 0.4140<br />Function: Penalty<br />Lambda: 0.6","b:  0.70<br />value: 0.4200<br />Function: Penalty<br />Lambda: 0.6","b:  0.71<br />value: 0.4260<br />Function: Penalty<br />Lambda: 0.6","b:  0.72<br />value: 0.4320<br />Function: Penalty<br />Lambda: 0.6","b:  0.73<br />value: 0.4380<br />Function: Penalty<br />Lambda: 0.6","b:  0.74<br />value: 0.4440<br />Function: Penalty<br />Lambda: 0.6","b:  0.75<br />value: 0.4500<br />Function: Penalty<br />Lambda: 0.6","b:  0.76<br />value: 0.4560<br />Function: Penalty<br />Lambda: 0.6","b:  0.77<br />value: 0.4620<br />Function: Penalty<br />Lambda: 0.6","b:  0.78<br />value: 0.4680<br />Function: Penalty<br />Lambda: 0.6","b:  0.79<br />value: 0.4740<br />Function: Penalty<br />Lambda: 0.6","b:  0.80<br />value: 0.4800<br />Function: Penalty<br />Lambda: 0.6","b:  0.81<br />value: 0.4860<br />Function: Penalty<br />Lambda: 0.6","b:  0.82<br />value: 0.4920<br />Function: Penalty<br />Lambda: 0.6","b:  0.83<br />value: 0.4980<br />Function: Penalty<br />Lambda: 0.6","b:  0.84<br />value: 0.5040<br />Function: Penalty<br />Lambda: 0.6","b:  0.85<br />value: 0.5100<br />Function: Penalty<br />Lambda: 0.6","b:  0.86<br />value: 0.5160<br />Function: Penalty<br />Lambda: 0.6","b:  0.87<br />value: 0.5220<br />Function: Penalty<br />Lambda: 0.6","b:  0.88<br />value: 0.5280<br />Function: Penalty<br />Lambda: 0.6","b:  0.89<br />value: 0.5340<br />Function: Penalty<br />Lambda: 0.6","b:  0.90<br />value: 0.5400<br />Function: Penalty<br />Lambda: 0.6","b:  0.91<br />value: 0.5460<br />Function: Penalty<br />Lambda: 0.6","b:  0.92<br />value: 0.5520<br />Function: Penalty<br />Lambda: 0.6","b:  0.93<br />value: 0.5580<br />Function: Penalty<br />Lambda: 0.6","b:  0.94<br />value: 0.5640<br />Function: Penalty<br />Lambda: 0.6","b:  0.95<br />value: 0.5700<br />Function: Penalty<br />Lambda: 0.6","b:  0.96<br />value: 0.5760<br />Function: Penalty<br />Lambda: 0.6","b:  0.97<br />value: 0.5820<br />Function: Penalty<br />Lambda: 0.6","b:  0.98<br />value: 0.5880<br />Function: Penalty<br />Lambda: 0.6","b:  0.99<br />value: 0.5940<br />Function: Penalty<br />Lambda: 0.6","b:  1.00<br />value: 0.6000<br />Function: Penalty<br />Lambda: 0.6","b:  1.01<br />value: 0.6060<br />Function: Penalty<br />Lambda: 0.6","b:  1.02<br />value: 0.6120<br />Function: Penalty<br />Lambda: 0.6","b:  1.03<br />value: 0.6180<br />Function: Penalty<br />Lambda: 0.6","b:  1.04<br />value: 0.6240<br />Function: Penalty<br />Lambda: 0.6","b:  1.05<br />value: 0.6300<br />Function: Penalty<br />Lambda: 0.6","b:  1.06<br />value: 0.6360<br />Function: Penalty<br />Lambda: 0.6","b:  1.07<br />value: 0.6420<br />Function: Penalty<br />Lambda: 0.6","b:  1.08<br />value: 0.6480<br />Function: Penalty<br />Lambda: 0.6","b:  1.09<br />value: 0.6540<br />Function: Penalty<br />Lambda: 0.6","b:  1.10<br />value: 0.6600<br />Function: Penalty<br />Lambda: 0.6","b:  1.11<br />value: 0.6660<br />Function: Penalty<br />Lambda: 0.6","b:  1.12<br />value: 0.6720<br />Function: Penalty<br />Lambda: 0.6","b:  1.13<br />value: 0.6780<br />Function: Penalty<br />Lambda: 0.6","b:  1.14<br />value: 0.6840<br />Function: Penalty<br />Lambda: 0.6","b:  1.15<br />value: 0.6900<br />Function: Penalty<br />Lambda: 0.6","b:  1.16<br />value: 0.6960<br />Function: Penalty<br />Lambda: 0.6","b:  1.17<br />value: 0.7020<br />Function: Penalty<br />Lambda: 0.6","b:  1.18<br />value: 0.7080<br />Function: Penalty<br />Lambda: 0.6","b:  1.19<br />value: 0.7140<br />Function: Penalty<br />Lambda: 0.6","b:  1.20<br />value: 0.7200<br />Function: Penalty<br />Lambda: 0.6","b:  1.21<br />value: 0.7260<br />Function: Penalty<br />Lambda: 0.6","b:  1.22<br />value: 0.7320<br />Function: Penalty<br />Lambda: 0.6","b:  1.23<br />value: 0.7380<br />Function: Penalty<br />Lambda: 0.6","b:  1.24<br />value: 0.7440<br />Function: Penalty<br />Lambda: 0.6","b:  1.25<br />value: 0.7500<br />Function: Penalty<br />Lambda: 0.6","b:  1.26<br />value: 0.7560<br />Function: Penalty<br />Lambda: 0.6","b:  1.27<br />value: 0.7620<br />Function: Penalty<br />Lambda: 0.6","b:  1.28<br />value: 0.7680<br />Function: Penalty<br />Lambda: 0.6","b:  1.29<br />value: 0.7740<br />Function: Penalty<br />Lambda: 0.6","b:  1.30<br />value: 0.7800<br />Function: Penalty<br />Lambda: 0.6","b:  1.31<br />value: 0.7860<br />Function: Penalty<br />Lambda: 0.6","b:  1.32<br />value: 0.7920<br />Function: Penalty<br />Lambda: 0.6","b:  1.33<br />value: 0.7980<br />Function: Penalty<br />Lambda: 0.6","b:  1.34<br />value: 0.8040<br />Function: Penalty<br />Lambda: 0.6","b:  1.35<br />value: 0.8100<br />Function: Penalty<br />Lambda: 0.6","b:  1.36<br />value: 0.8160<br />Function: Penalty<br />Lambda: 0.6","b:  1.37<br />value: 0.8220<br />Function: Penalty<br />Lambda: 0.6","b:  1.38<br />value: 0.8280<br />Function: Penalty<br />Lambda: 0.6","b:  1.39<br />value: 0.8340<br />Function: Penalty<br />Lambda: 0.6","b:  1.40<br />value: 0.8400<br />Function: Penalty<br />Lambda: 0.6","b:  1.41<br />value: 0.8460<br />Function: Penalty<br />Lambda: 0.6","b:  1.42<br />value: 0.8520<br />Function: Penalty<br />Lambda: 0.6","b:  1.43<br />value: 0.8580<br />Function: Penalty<br />Lambda: 0.6","b:  1.44<br />value: 0.8640<br />Function: Penalty<br />Lambda: 0.6","b:  1.45<br />value: 0.8700<br />Function: Penalty<br />Lambda: 0.6","b:  1.46<br />value: 0.8760<br />Function: Penalty<br />Lambda: 0.6","b:  1.47<br />value: 0.8820<br />Function: Penalty<br />Lambda: 0.6","b:  1.48<br />value: 0.8880<br />Function: Penalty<br />Lambda: 0.6","b:  1.49<br />value: 0.8940<br />Function: Penalty<br />Lambda: 0.6","b:  1.50<br />value: 0.9000<br />Function: Penalty<br />Lambda: 0.6","b:  1.51<br />value: 0.9060<br />Function: Penalty<br />Lambda: 0.6","b:  1.52<br />value: 0.9120<br />Function: Penalty<br />Lambda: 0.6","b:  1.53<br />value: 0.9180<br />Function: Penalty<br />Lambda: 0.6","b:  1.54<br />value: 0.9240<br />Function: Penalty<br />Lambda: 0.6","b:  1.55<br />value: 0.9300<br />Function: Penalty<br />Lambda: 0.6","b:  1.56<br />value: 0.9360<br />Function: Penalty<br />Lambda: 0.6","b:  1.57<br />value: 0.9420<br />Function: Penalty<br />Lambda: 0.6","b:  1.58<br />value: 0.9480<br />Function: Penalty<br />Lambda: 0.6","b:  1.59<br />value: 0.9540<br />Function: Penalty<br />Lambda: 0.6","b:  1.60<br />value: 0.9600<br />Function: Penalty<br />Lambda: 0.6","b:  1.61<br />value: 0.9660<br />Function: Penalty<br />Lambda: 0.6","b:  1.62<br />value: 0.9720<br />Function: Penalty<br />Lambda: 0.6","b:  1.63<br />value: 0.9780<br />Function: Penalty<br />Lambda: 0.6","b:  1.64<br />value: 0.9840<br />Function: Penalty<br />Lambda: 0.6","b:  1.65<br />value: 0.9900<br />Function: Penalty<br />Lambda: 0.6","b:  1.66<br />value: 0.9960<br />Function: Penalty<br />Lambda: 0.6","b:  1.67<br />value: 1.0020<br />Function: Penalty<br />Lambda: 0.6","b:  1.68<br />value: 1.0080<br />Function: Penalty<br />Lambda: 0.6","b:  1.69<br />value: 1.0140<br />Function: Penalty<br />Lambda: 0.6","b:  1.70<br />value: 1.0200<br />Function: Penalty<br />Lambda: 0.6","b:  1.71<br />value: 1.0260<br />Function: Penalty<br />Lambda: 0.6","b:  1.72<br />value: 1.0320<br />Function: Penalty<br />Lambda: 0.6","b:  1.73<br />value: 1.0380<br />Function: Penalty<br />Lambda: 0.6","b:  1.74<br />value: 1.0440<br />Function: Penalty<br />Lambda: 0.6","b:  1.75<br />value: 1.0500<br />Function: Penalty<br />Lambda: 0.6","b:  1.76<br />value: 1.0560<br />Function: Penalty<br />Lambda: 0.6","b:  1.77<br />value: 1.0620<br />Function: Penalty<br />Lambda: 0.6","b:  1.78<br />value: 1.0680<br />Function: Penalty<br />Lambda: 0.6","b:  1.79<br />value: 1.0740<br />Function: Penalty<br />Lambda: 0.6","b:  1.80<br />value: 1.0800<br />Function: Penalty<br />Lambda: 0.6","b:  1.81<br />value: 1.0860<br />Function: Penalty<br />Lambda: 0.6","b:  1.82<br />value: 1.0920<br />Function: Penalty<br />Lambda: 0.6","b:  1.83<br />value: 1.0980<br />Function: Penalty<br />Lambda: 0.6","b:  1.84<br />value: 1.1040<br />Function: Penalty<br />Lambda: 0.6","b:  1.85<br />value: 1.1100<br />Function: Penalty<br />Lambda: 0.6","b:  1.86<br />value: 1.1160<br />Function: Penalty<br />Lambda: 0.6","b:  1.87<br />value: 1.1220<br />Function: Penalty<br />Lambda: 0.6","b:  1.88<br />value: 1.1280<br />Function: Penalty<br />Lambda: 0.6","b:  1.89<br />value: 1.1340<br />Function: Penalty<br />Lambda: 0.6","b:  1.90<br />value: 1.1400<br />Function: Penalty<br />Lambda: 0.6","b:  1.91<br />value: 1.1460<br />Function: Penalty<br />Lambda: 0.6","b:  1.92<br />value: 1.1520<br />Function: Penalty<br />Lambda: 0.6","b:  1.93<br />value: 1.1580<br />Function: Penalty<br />Lambda: 0.6","b:  1.94<br />value: 1.1640<br />Function: Penalty<br />Lambda: 0.6","b:  1.95<br />value: 1.1700<br />Function: Penalty<br />Lambda: 0.6","b:  1.96<br />value: 1.1760<br />Function: Penalty<br />Lambda: 0.6","b:  1.97<br />value: 1.1820<br />Function: Penalty<br />Lambda: 0.6","b:  1.98<br />value: 1.1880<br />Function: Penalty<br />Lambda: 0.6","b:  1.99<br />value: 1.1940<br />Function: Penalty<br />Lambda: 0.6","b:  2.00<br />value: 1.2000<br />Function: Penalty<br />Lambda: 0.6"],"frame":"0.6","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"0.7","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 0.7","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 0.7","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 0.7","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 0.7","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 0.7","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 0.7","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 0.7","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 0.7","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 0.7","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 0.7","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 0.7","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 0.7","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 0.7","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 0.7","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 0.7","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 0.7","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 0.7","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 0.7","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 0.7","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 0.7","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 0.7","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 0.7","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 0.7","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 0.7","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 0.7","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 0.7","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 0.7","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 0.7","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 0.7","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 0.7","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 0.7","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 0.7","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 0.7","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 0.7","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 0.7","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 0.7","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 0.7","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 0.7","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 0.7","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 0.7","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 0.7","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 0.7","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 0.7","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 0.7","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 0.7","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 0.7","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 0.7","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 0.7","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 0.7","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 0.7","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.7","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.7","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.7","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.7","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.7","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.7","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.7","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.7","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.7","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.7","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.7","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.7","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.7","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.7","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.7","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.7","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.7","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.7","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.7","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.7","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.7","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.7","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.7","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.7","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.7","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.7","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.7","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.7","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.7","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.7","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.7","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.7","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.7","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.7","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.7","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.7","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.7","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.7","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.7","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.7","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.7","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.7","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.7","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.7","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.7","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.7","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.7","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.7","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.7","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.7","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.7","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.7","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.7","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.7","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.7","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.7","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.7","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.7","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.7","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.7","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.7","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.7","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.7","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.7","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.7","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.7","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.7","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.7","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.7","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.7","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.7","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.7","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.7","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.7","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.7","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.7","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.7","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.7","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.7","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.7","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.7","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.7","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.7","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.7","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.7","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.7","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.7","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.7","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.7","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.7","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.7","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.7","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.7","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.7","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.7","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.7","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.7","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.7","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.7","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.7","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 0.7","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.7","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.7","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.7","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.7","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.7","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.7","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.7","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.7","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.7","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.7","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.7","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.7","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.7","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.7","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.7","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.7","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.7","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.7","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.7","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.7","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.7","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.7","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.7","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.7","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.7","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.7","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.7","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.7","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.7","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.7","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.7","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.7","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.7","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.7","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.7","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.7","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.7","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.7","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.7","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.7","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.7","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.7","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.7","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.7","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.7","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.7","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.7","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.7","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.7","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.7","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.7","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.7","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.7","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.7","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.7","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.7","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.7","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.7","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.7","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.7","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.7","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.7","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.7","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.7","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.7","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.7","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.7","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.7","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.7","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.7","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.7","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.7","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.7","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.7","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.7","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.7","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.7","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.7","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.7","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.7","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.7","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.7","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.7","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.7","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.7","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.7","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.7","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.7","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.7","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.7","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.7","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.7","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.7","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.7","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.7","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.7","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.7","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.7","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.7","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.7"],"frame":"0.7","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.1,3.0631,3.0264,2.9899,2.9536,2.9175,2.8816,2.8459,2.8104,2.7751,2.74,2.7051,2.6704,2.6359,2.6016,2.5675,2.5336,2.4999,2.4664,2.4331,2.4,2.3671,2.3344,2.3019,2.2696,2.2375,2.2056,2.1739,2.1424,2.1111,2.08,2.0491,2.0184,1.9879,1.9576,1.9275,1.8976,1.8679,1.8384,1.8091,1.78,1.7511,1.7224,1.6939,1.6656,1.6375,1.6096,1.5819,1.5544,1.5271,1.5,1.4871,1.4744,1.4619,1.4496,1.4375,1.4256,1.4139,1.4024,1.3911,1.38,1.3691,1.3584,1.3479,1.3376,1.3275,1.3176,1.3079,1.2984,1.2891,1.28,1.2711,1.2624,1.2539,1.2456,1.2375,1.2296,1.2219,1.2144,1.2071,1.2,1.1931,1.1864,1.1799,1.1736,1.1675,1.1616,1.1559,1.1504,1.1451,1.14,1.1351,1.1304,1.1259,1.1216,1.1175,1.1136,1.1099,1.1064,1.1031,1.1,1.0971,1.0944,1.0919,1.0896,1.0875,1.0856,1.0839,1.0824,1.0811,1.08,1.0791,1.0784,1.0779,1.0776,1.0775,1.0776,1.0779,1.0784,1.0791,1.08,1.0811,1.0824,1.0839,1.0856,1.0875,1.0896,1.0919,1.0944,1.0971,1.1,1.1031,1.1064,1.1099,1.1136,1.1175,1.1216,1.1259,1.1304,1.1351,1.14,1.1451,1.1504,1.1559,1.1616,1.1675,1.1736,1.1799,1.1864,1.1931,1.2,1.2071,1.2144,1.2219,1.2296,1.2375,1.2456,1.2539,1.2624,1.2711,1.28,1.2891,1.2984,1.3079,1.3176,1.3275,1.3376,1.3479,1.3584,1.3691,1.38,1.3911,1.4024,1.4139,1.4256,1.4375,1.4496,1.4619,1.4744,1.4871,1.5,1.5131,1.5264,1.5399,1.5536,1.5675,1.5816,1.5959,1.6104,1.6251,1.64,1.6551,1.6704,1.6859,1.7016,1.7175,1.7336,1.7499,1.7664,1.7831,1.8,1.8171,1.8344,1.8519,1.8696,1.8875,1.9056,1.9239,1.9424,1.9611,1.98,1.9991,2.0184,2.0379,2.0576,2.0775,2.0976,2.1179,2.1384,2.1591,2.18,2.2011,2.2224,2.2439,2.2656,2.2875,2.3096,2.3319,2.3544,2.3771,2.4,2.4231,2.4464,2.4699,2.4936,2.5175,2.5416,2.5659,2.5904,2.6151,2.64,2.6651,2.6904,2.7159,2.7416,2.7675,2.7936,2.8199,2.8464,2.8731,2.9],"text":["b: -0.50<br />value: 3.1000<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.49<br />value: 3.0631<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.48<br />value: 3.0264<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.47<br />value: 2.9899<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.46<br />value: 2.9536<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.45<br />value: 2.9175<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.44<br />value: 2.8816<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.43<br />value: 2.8459<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.42<br />value: 2.8104<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.41<br />value: 2.7751<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.40<br />value: 2.7400<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.39<br />value: 2.7051<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.38<br />value: 2.6704<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.37<br />value: 2.6359<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.36<br />value: 2.6016<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.35<br />value: 2.5675<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.34<br />value: 2.5336<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.33<br />value: 2.4999<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.32<br />value: 2.4664<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.31<br />value: 2.4331<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.30<br />value: 2.4000<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.29<br />value: 2.3671<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.28<br />value: 2.3344<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.27<br />value: 2.3019<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.26<br />value: 2.2696<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.25<br />value: 2.2375<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.24<br />value: 2.2056<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.23<br />value: 2.1739<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.22<br />value: 2.1424<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.21<br />value: 2.1111<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.20<br />value: 2.0800<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.19<br />value: 2.0491<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.18<br />value: 2.0184<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.17<br />value: 1.9879<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.16<br />value: 1.9576<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.15<br />value: 1.9275<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.14<br />value: 1.8976<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.13<br />value: 1.8679<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.12<br />value: 1.8384<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.11<br />value: 1.8091<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.10<br />value: 1.7800<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.09<br />value: 1.7511<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.08<br />value: 1.7224<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.07<br />value: 1.6939<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.06<br />value: 1.6656<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.05<br />value: 1.6375<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.04<br />value: 1.6096<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.03<br />value: 1.5819<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.02<br />value: 1.5544<br />Function: Loss + Penalty<br />Lambda: 0.7","b: -0.01<br />value: 1.5271<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.01<br />value: 1.4871<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.02<br />value: 1.4744<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.03<br />value: 1.4619<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.04<br />value: 1.4496<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.05<br />value: 1.4375<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.06<br />value: 1.4256<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.07<br />value: 1.4139<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.08<br />value: 1.4024<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.09<br />value: 1.3911<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.10<br />value: 1.3800<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.11<br />value: 1.3691<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.12<br />value: 1.3584<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.13<br />value: 1.3479<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.14<br />value: 1.3376<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.15<br />value: 1.3275<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.16<br />value: 1.3176<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.17<br />value: 1.3079<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.18<br />value: 1.2984<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.19<br />value: 1.2891<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.20<br />value: 1.2800<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.21<br />value: 1.2711<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.22<br />value: 1.2624<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.23<br />value: 1.2539<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.24<br />value: 1.2456<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.25<br />value: 1.2375<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.26<br />value: 1.2296<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.27<br />value: 1.2219<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.28<br />value: 1.2144<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.29<br />value: 1.2071<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.30<br />value: 1.2000<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.31<br />value: 1.1931<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.32<br />value: 1.1864<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.33<br />value: 1.1799<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.34<br />value: 1.1736<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.35<br />value: 1.1675<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.36<br />value: 1.1616<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.37<br />value: 1.1559<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.38<br />value: 1.1504<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.39<br />value: 1.1451<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.40<br />value: 1.1400<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.41<br />value: 1.1351<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.42<br />value: 1.1304<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.43<br />value: 1.1259<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.44<br />value: 1.1216<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.45<br />value: 1.1175<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.46<br />value: 1.1136<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.47<br />value: 1.1099<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.48<br />value: 1.1064<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.49<br />value: 1.1031<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.50<br />value: 1.1000<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.51<br />value: 1.0971<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.52<br />value: 1.0944<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.53<br />value: 1.0919<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.54<br />value: 1.0896<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.55<br />value: 1.0875<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.56<br />value: 1.0856<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.57<br />value: 1.0839<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.58<br />value: 1.0824<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.59<br />value: 1.0811<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.60<br />value: 1.0800<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.61<br />value: 1.0791<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.62<br />value: 1.0784<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.63<br />value: 1.0779<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.64<br />value: 1.0776<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.65<br />value: 1.0775<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.66<br />value: 1.0776<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.67<br />value: 1.0779<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.68<br />value: 1.0784<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.69<br />value: 1.0791<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.70<br />value: 1.0800<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.71<br />value: 1.0811<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.72<br />value: 1.0824<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.73<br />value: 1.0839<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.74<br />value: 1.0856<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.75<br />value: 1.0875<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.76<br />value: 1.0896<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.77<br />value: 1.0919<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.78<br />value: 1.0944<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.79<br />value: 1.0971<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.80<br />value: 1.1000<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.81<br />value: 1.1031<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.82<br />value: 1.1064<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.83<br />value: 1.1099<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.84<br />value: 1.1136<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.85<br />value: 1.1175<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.86<br />value: 1.1216<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.87<br />value: 1.1259<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.88<br />value: 1.1304<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.89<br />value: 1.1351<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.90<br />value: 1.1400<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.91<br />value: 1.1451<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.92<br />value: 1.1504<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.93<br />value: 1.1559<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.94<br />value: 1.1616<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.95<br />value: 1.1675<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.96<br />value: 1.1736<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.97<br />value: 1.1799<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.98<br />value: 1.1864<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  0.99<br />value: 1.1931<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.00<br />value: 1.2000<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.01<br />value: 1.2071<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.02<br />value: 1.2144<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.03<br />value: 1.2219<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.04<br />value: 1.2296<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.05<br />value: 1.2375<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.06<br />value: 1.2456<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.07<br />value: 1.2539<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.08<br />value: 1.2624<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.09<br />value: 1.2711<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.10<br />value: 1.2800<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.11<br />value: 1.2891<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.12<br />value: 1.2984<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.13<br />value: 1.3079<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.14<br />value: 1.3176<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.15<br />value: 1.3275<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.16<br />value: 1.3376<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.17<br />value: 1.3479<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.18<br />value: 1.3584<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.19<br />value: 1.3691<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.20<br />value: 1.3800<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.21<br />value: 1.3911<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.22<br />value: 1.4024<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.23<br />value: 1.4139<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.24<br />value: 1.4256<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.25<br />value: 1.4375<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.26<br />value: 1.4496<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.27<br />value: 1.4619<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.28<br />value: 1.4744<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.29<br />value: 1.4871<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.30<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.31<br />value: 1.5131<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.32<br />value: 1.5264<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.33<br />value: 1.5399<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.34<br />value: 1.5536<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.35<br />value: 1.5675<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.36<br />value: 1.5816<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.37<br />value: 1.5959<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.38<br />value: 1.6104<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.39<br />value: 1.6251<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.40<br />value: 1.6400<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.41<br />value: 1.6551<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.42<br />value: 1.6704<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.43<br />value: 1.6859<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.44<br />value: 1.7016<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.45<br />value: 1.7175<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.46<br />value: 1.7336<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.47<br />value: 1.7499<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.48<br />value: 1.7664<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.49<br />value: 1.7831<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.50<br />value: 1.8000<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.51<br />value: 1.8171<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.52<br />value: 1.8344<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.53<br />value: 1.8519<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.54<br />value: 1.8696<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.55<br />value: 1.8875<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.56<br />value: 1.9056<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.57<br />value: 1.9239<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.58<br />value: 1.9424<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.59<br />value: 1.9611<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.60<br />value: 1.9800<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.61<br />value: 1.9991<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.62<br />value: 2.0184<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.63<br />value: 2.0379<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.64<br />value: 2.0576<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.65<br />value: 2.0775<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.66<br />value: 2.0976<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.67<br />value: 2.1179<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.68<br />value: 2.1384<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.69<br />value: 2.1591<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.70<br />value: 2.1800<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.71<br />value: 2.2011<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.72<br />value: 2.2224<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.73<br />value: 2.2439<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.74<br />value: 2.2656<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.75<br />value: 2.2875<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.76<br />value: 2.3096<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.77<br />value: 2.3319<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.78<br />value: 2.3544<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.79<br />value: 2.3771<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.80<br />value: 2.4000<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.81<br />value: 2.4231<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.82<br />value: 2.4464<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.83<br />value: 2.4699<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.84<br />value: 2.4936<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.85<br />value: 2.5175<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.86<br />value: 2.5416<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.87<br />value: 2.5659<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.88<br />value: 2.5904<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.89<br />value: 2.6151<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.90<br />value: 2.6400<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.91<br />value: 2.6651<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.92<br />value: 2.6904<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.93<br />value: 2.7159<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.94<br />value: 2.7416<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.95<br />value: 2.7675<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.96<br />value: 2.7936<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.97<br />value: 2.8199<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.98<br />value: 2.8464<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  1.99<br />value: 2.8731<br />Function: Loss + Penalty<br />Lambda: 0.7","b:  2.00<br />value: 2.9000<br />Function: Loss + Penalty<br />Lambda: 0.7"],"frame":"0.7","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.35,0.343,0.336,0.329,0.322,0.315,0.308,0.301,0.294,0.287,0.28,0.273,0.266,0.259,0.252,0.245,0.238,0.231,0.224,0.217,0.21,0.203,0.196,0.189,0.182,0.175,0.168,0.161,0.154,0.147,0.14,0.133,0.126,0.119,0.112,0.105,0.098,0.091,0.084,0.077,0.07,0.063,0.056,0.049,0.042,0.035,0.028,0.021,0.014,0.00700000000000001,0,0.00700000000000001,0.014,0.021,0.028,0.035,0.042,0.0490000000000001,0.056,0.063,0.07,0.077,0.084,0.091,0.098,0.105,0.112,0.119,0.126,0.133,0.14,0.147,0.154,0.161,0.168,0.175,0.182,0.189,0.196,0.203,0.21,0.217,0.224,0.231,0.238,0.245,0.252,0.259,0.266,0.273,0.28,0.287,0.294,0.301,0.308,0.315,0.322,0.329,0.336,0.343,0.35,0.357,0.364,0.371,0.378,0.385,0.392,0.399,0.406,0.413,0.42,0.427,0.434,0.441,0.448,0.455,0.462,0.469,0.476,0.483,0.49,0.497,0.504,0.511,0.518,0.525,0.532,0.539,0.546,0.553,0.56,0.567,0.574,0.581,0.588,0.595,0.602,0.609,0.616,0.623,0.63,0.637,0.644,0.651,0.658,0.665,0.672,0.679,0.686,0.693,0.7,0.707,0.714,0.721,0.728,0.735,0.742,0.749,0.756,0.763,0.77,0.777,0.784,0.791,0.798,0.805,0.812,0.819,0.826,0.833,0.84,0.847,0.854,0.861,0.868,0.875,0.882,0.889,0.896,0.903,0.91,0.917,0.924,0.931,0.938,0.945,0.952,0.959,0.966,0.973,0.98,0.987,0.994,1.001,1.008,1.015,1.022,1.029,1.036,1.043,1.05,1.057,1.064,1.071,1.078,1.085,1.092,1.099,1.106,1.113,1.12,1.127,1.134,1.141,1.148,1.155,1.162,1.169,1.176,1.183,1.19,1.197,1.204,1.211,1.218,1.225,1.232,1.239,1.246,1.253,1.26,1.267,1.274,1.281,1.288,1.295,1.302,1.309,1.316,1.323,1.33,1.337,1.344,1.351,1.358,1.365,1.372,1.379,1.386,1.393,1.4],"text":["b: -0.50<br />value: 0.3500<br />Function: Penalty<br />Lambda: 0.7","b: -0.49<br />value: 0.3430<br />Function: Penalty<br />Lambda: 0.7","b: -0.48<br />value: 0.3360<br />Function: Penalty<br />Lambda: 0.7","b: -0.47<br />value: 0.3290<br />Function: Penalty<br />Lambda: 0.7","b: -0.46<br />value: 0.3220<br />Function: Penalty<br />Lambda: 0.7","b: -0.45<br />value: 0.3150<br />Function: Penalty<br />Lambda: 0.7","b: -0.44<br />value: 0.3080<br />Function: Penalty<br />Lambda: 0.7","b: -0.43<br />value: 0.3010<br />Function: Penalty<br />Lambda: 0.7","b: -0.42<br />value: 0.2940<br />Function: Penalty<br />Lambda: 0.7","b: -0.41<br />value: 0.2870<br />Function: Penalty<br />Lambda: 0.7","b: -0.40<br />value: 0.2800<br />Function: Penalty<br />Lambda: 0.7","b: -0.39<br />value: 0.2730<br />Function: Penalty<br />Lambda: 0.7","b: -0.38<br />value: 0.2660<br />Function: Penalty<br />Lambda: 0.7","b: -0.37<br />value: 0.2590<br />Function: Penalty<br />Lambda: 0.7","b: -0.36<br />value: 0.2520<br />Function: Penalty<br />Lambda: 0.7","b: -0.35<br />value: 0.2450<br />Function: Penalty<br />Lambda: 0.7","b: -0.34<br />value: 0.2380<br />Function: Penalty<br />Lambda: 0.7","b: -0.33<br />value: 0.2310<br />Function: Penalty<br />Lambda: 0.7","b: -0.32<br />value: 0.2240<br />Function: Penalty<br />Lambda: 0.7","b: -0.31<br />value: 0.2170<br />Function: Penalty<br />Lambda: 0.7","b: -0.30<br />value: 0.2100<br />Function: Penalty<br />Lambda: 0.7","b: -0.29<br />value: 0.2030<br />Function: Penalty<br />Lambda: 0.7","b: -0.28<br />value: 0.1960<br />Function: Penalty<br />Lambda: 0.7","b: -0.27<br />value: 0.1890<br />Function: Penalty<br />Lambda: 0.7","b: -0.26<br />value: 0.1820<br />Function: Penalty<br />Lambda: 0.7","b: -0.25<br />value: 0.1750<br />Function: Penalty<br />Lambda: 0.7","b: -0.24<br />value: 0.1680<br />Function: Penalty<br />Lambda: 0.7","b: -0.23<br />value: 0.1610<br />Function: Penalty<br />Lambda: 0.7","b: -0.22<br />value: 0.1540<br />Function: Penalty<br />Lambda: 0.7","b: -0.21<br />value: 0.1470<br />Function: Penalty<br />Lambda: 0.7","b: -0.20<br />value: 0.1400<br />Function: Penalty<br />Lambda: 0.7","b: -0.19<br />value: 0.1330<br />Function: Penalty<br />Lambda: 0.7","b: -0.18<br />value: 0.1260<br />Function: Penalty<br />Lambda: 0.7","b: -0.17<br />value: 0.1190<br />Function: Penalty<br />Lambda: 0.7","b: -0.16<br />value: 0.1120<br />Function: Penalty<br />Lambda: 0.7","b: -0.15<br />value: 0.1050<br />Function: Penalty<br />Lambda: 0.7","b: -0.14<br />value: 0.0980<br />Function: Penalty<br />Lambda: 0.7","b: -0.13<br />value: 0.0910<br />Function: Penalty<br />Lambda: 0.7","b: -0.12<br />value: 0.0840<br />Function: Penalty<br />Lambda: 0.7","b: -0.11<br />value: 0.0770<br />Function: Penalty<br />Lambda: 0.7","b: -0.10<br />value: 0.0700<br />Function: Penalty<br />Lambda: 0.7","b: -0.09<br />value: 0.0630<br />Function: Penalty<br />Lambda: 0.7","b: -0.08<br />value: 0.0560<br />Function: Penalty<br />Lambda: 0.7","b: -0.07<br />value: 0.0490<br />Function: Penalty<br />Lambda: 0.7","b: -0.06<br />value: 0.0420<br />Function: Penalty<br />Lambda: 0.7","b: -0.05<br />value: 0.0350<br />Function: Penalty<br />Lambda: 0.7","b: -0.04<br />value: 0.0280<br />Function: Penalty<br />Lambda: 0.7","b: -0.03<br />value: 0.0210<br />Function: Penalty<br />Lambda: 0.7","b: -0.02<br />value: 0.0140<br />Function: Penalty<br />Lambda: 0.7","b: -0.01<br />value: 0.0070<br />Function: Penalty<br />Lambda: 0.7","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.7","b:  0.01<br />value: 0.0070<br />Function: Penalty<br />Lambda: 0.7","b:  0.02<br />value: 0.0140<br />Function: Penalty<br />Lambda: 0.7","b:  0.03<br />value: 0.0210<br />Function: Penalty<br />Lambda: 0.7","b:  0.04<br />value: 0.0280<br />Function: Penalty<br />Lambda: 0.7","b:  0.05<br />value: 0.0350<br />Function: Penalty<br />Lambda: 0.7","b:  0.06<br />value: 0.0420<br />Function: Penalty<br />Lambda: 0.7","b:  0.07<br />value: 0.0490<br />Function: Penalty<br />Lambda: 0.7","b:  0.08<br />value: 0.0560<br />Function: Penalty<br />Lambda: 0.7","b:  0.09<br />value: 0.0630<br />Function: Penalty<br />Lambda: 0.7","b:  0.10<br />value: 0.0700<br />Function: Penalty<br />Lambda: 0.7","b:  0.11<br />value: 0.0770<br />Function: Penalty<br />Lambda: 0.7","b:  0.12<br />value: 0.0840<br />Function: Penalty<br />Lambda: 0.7","b:  0.13<br />value: 0.0910<br />Function: Penalty<br />Lambda: 0.7","b:  0.14<br />value: 0.0980<br />Function: Penalty<br />Lambda: 0.7","b:  0.15<br />value: 0.1050<br />Function: Penalty<br />Lambda: 0.7","b:  0.16<br />value: 0.1120<br />Function: Penalty<br />Lambda: 0.7","b:  0.17<br />value: 0.1190<br />Function: Penalty<br />Lambda: 0.7","b:  0.18<br />value: 0.1260<br />Function: Penalty<br />Lambda: 0.7","b:  0.19<br />value: 0.1330<br />Function: Penalty<br />Lambda: 0.7","b:  0.20<br />value: 0.1400<br />Function: Penalty<br />Lambda: 0.7","b:  0.21<br />value: 0.1470<br />Function: Penalty<br />Lambda: 0.7","b:  0.22<br />value: 0.1540<br />Function: Penalty<br />Lambda: 0.7","b:  0.23<br />value: 0.1610<br />Function: Penalty<br />Lambda: 0.7","b:  0.24<br />value: 0.1680<br />Function: Penalty<br />Lambda: 0.7","b:  0.25<br />value: 0.1750<br />Function: Penalty<br />Lambda: 0.7","b:  0.26<br />value: 0.1820<br />Function: Penalty<br />Lambda: 0.7","b:  0.27<br />value: 0.1890<br />Function: Penalty<br />Lambda: 0.7","b:  0.28<br />value: 0.1960<br />Function: Penalty<br />Lambda: 0.7","b:  0.29<br />value: 0.2030<br />Function: Penalty<br />Lambda: 0.7","b:  0.30<br />value: 0.2100<br />Function: Penalty<br />Lambda: 0.7","b:  0.31<br />value: 0.2170<br />Function: Penalty<br />Lambda: 0.7","b:  0.32<br />value: 0.2240<br />Function: Penalty<br />Lambda: 0.7","b:  0.33<br />value: 0.2310<br />Function: Penalty<br />Lambda: 0.7","b:  0.34<br />value: 0.2380<br />Function: Penalty<br />Lambda: 0.7","b:  0.35<br />value: 0.2450<br />Function: Penalty<br />Lambda: 0.7","b:  0.36<br />value: 0.2520<br />Function: Penalty<br />Lambda: 0.7","b:  0.37<br />value: 0.2590<br />Function: Penalty<br />Lambda: 0.7","b:  0.38<br />value: 0.2660<br />Function: Penalty<br />Lambda: 0.7","b:  0.39<br />value: 0.2730<br />Function: Penalty<br />Lambda: 0.7","b:  0.40<br />value: 0.2800<br />Function: Penalty<br />Lambda: 0.7","b:  0.41<br />value: 0.2870<br />Function: Penalty<br />Lambda: 0.7","b:  0.42<br />value: 0.2940<br />Function: Penalty<br />Lambda: 0.7","b:  0.43<br />value: 0.3010<br />Function: Penalty<br />Lambda: 0.7","b:  0.44<br />value: 0.3080<br />Function: Penalty<br />Lambda: 0.7","b:  0.45<br />value: 0.3150<br />Function: Penalty<br />Lambda: 0.7","b:  0.46<br />value: 0.3220<br />Function: Penalty<br />Lambda: 0.7","b:  0.47<br />value: 0.3290<br />Function: Penalty<br />Lambda: 0.7","b:  0.48<br />value: 0.3360<br />Function: Penalty<br />Lambda: 0.7","b:  0.49<br />value: 0.3430<br />Function: Penalty<br />Lambda: 0.7","b:  0.50<br />value: 0.3500<br />Function: Penalty<br />Lambda: 0.7","b:  0.51<br />value: 0.3570<br />Function: Penalty<br />Lambda: 0.7","b:  0.52<br />value: 0.3640<br />Function: Penalty<br />Lambda: 0.7","b:  0.53<br />value: 0.3710<br />Function: Penalty<br />Lambda: 0.7","b:  0.54<br />value: 0.3780<br />Function: Penalty<br />Lambda: 0.7","b:  0.55<br />value: 0.3850<br />Function: Penalty<br />Lambda: 0.7","b:  0.56<br />value: 0.3920<br />Function: Penalty<br />Lambda: 0.7","b:  0.57<br />value: 0.3990<br />Function: Penalty<br />Lambda: 0.7","b:  0.58<br />value: 0.4060<br />Function: Penalty<br />Lambda: 0.7","b:  0.59<br />value: 0.4130<br />Function: Penalty<br />Lambda: 0.7","b:  0.60<br />value: 0.4200<br />Function: Penalty<br />Lambda: 0.7","b:  0.61<br />value: 0.4270<br />Function: Penalty<br />Lambda: 0.7","b:  0.62<br />value: 0.4340<br />Function: Penalty<br />Lambda: 0.7","b:  0.63<br />value: 0.4410<br />Function: Penalty<br />Lambda: 0.7","b:  0.64<br />value: 0.4480<br />Function: Penalty<br />Lambda: 0.7","b:  0.65<br />value: 0.4550<br />Function: Penalty<br />Lambda: 0.7","b:  0.66<br />value: 0.4620<br />Function: Penalty<br />Lambda: 0.7","b:  0.67<br />value: 0.4690<br />Function: Penalty<br />Lambda: 0.7","b:  0.68<br />value: 0.4760<br />Function: Penalty<br />Lambda: 0.7","b:  0.69<br />value: 0.4830<br />Function: Penalty<br />Lambda: 0.7","b:  0.70<br />value: 0.4900<br />Function: Penalty<br />Lambda: 0.7","b:  0.71<br />value: 0.4970<br />Function: Penalty<br />Lambda: 0.7","b:  0.72<br />value: 0.5040<br />Function: Penalty<br />Lambda: 0.7","b:  0.73<br />value: 0.5110<br />Function: Penalty<br />Lambda: 0.7","b:  0.74<br />value: 0.5180<br />Function: Penalty<br />Lambda: 0.7","b:  0.75<br />value: 0.5250<br />Function: Penalty<br />Lambda: 0.7","b:  0.76<br />value: 0.5320<br />Function: Penalty<br />Lambda: 0.7","b:  0.77<br />value: 0.5390<br />Function: Penalty<br />Lambda: 0.7","b:  0.78<br />value: 0.5460<br />Function: Penalty<br />Lambda: 0.7","b:  0.79<br />value: 0.5530<br />Function: Penalty<br />Lambda: 0.7","b:  0.80<br />value: 0.5600<br />Function: Penalty<br />Lambda: 0.7","b:  0.81<br />value: 0.5670<br />Function: Penalty<br />Lambda: 0.7","b:  0.82<br />value: 0.5740<br />Function: Penalty<br />Lambda: 0.7","b:  0.83<br />value: 0.5810<br />Function: Penalty<br />Lambda: 0.7","b:  0.84<br />value: 0.5880<br />Function: Penalty<br />Lambda: 0.7","b:  0.85<br />value: 0.5950<br />Function: Penalty<br />Lambda: 0.7","b:  0.86<br />value: 0.6020<br />Function: Penalty<br />Lambda: 0.7","b:  0.87<br />value: 0.6090<br />Function: Penalty<br />Lambda: 0.7","b:  0.88<br />value: 0.6160<br />Function: Penalty<br />Lambda: 0.7","b:  0.89<br />value: 0.6230<br />Function: Penalty<br />Lambda: 0.7","b:  0.90<br />value: 0.6300<br />Function: Penalty<br />Lambda: 0.7","b:  0.91<br />value: 0.6370<br />Function: Penalty<br />Lambda: 0.7","b:  0.92<br />value: 0.6440<br />Function: Penalty<br />Lambda: 0.7","b:  0.93<br />value: 0.6510<br />Function: Penalty<br />Lambda: 0.7","b:  0.94<br />value: 0.6580<br />Function: Penalty<br />Lambda: 0.7","b:  0.95<br />value: 0.6650<br />Function: Penalty<br />Lambda: 0.7","b:  0.96<br />value: 0.6720<br />Function: Penalty<br />Lambda: 0.7","b:  0.97<br />value: 0.6790<br />Function: Penalty<br />Lambda: 0.7","b:  0.98<br />value: 0.6860<br />Function: Penalty<br />Lambda: 0.7","b:  0.99<br />value: 0.6930<br />Function: Penalty<br />Lambda: 0.7","b:  1.00<br />value: 0.7000<br />Function: Penalty<br />Lambda: 0.7","b:  1.01<br />value: 0.7070<br />Function: Penalty<br />Lambda: 0.7","b:  1.02<br />value: 0.7140<br />Function: Penalty<br />Lambda: 0.7","b:  1.03<br />value: 0.7210<br />Function: Penalty<br />Lambda: 0.7","b:  1.04<br />value: 0.7280<br />Function: Penalty<br />Lambda: 0.7","b:  1.05<br />value: 0.7350<br />Function: Penalty<br />Lambda: 0.7","b:  1.06<br />value: 0.7420<br />Function: Penalty<br />Lambda: 0.7","b:  1.07<br />value: 0.7490<br />Function: Penalty<br />Lambda: 0.7","b:  1.08<br />value: 0.7560<br />Function: Penalty<br />Lambda: 0.7","b:  1.09<br />value: 0.7630<br />Function: Penalty<br />Lambda: 0.7","b:  1.10<br />value: 0.7700<br />Function: Penalty<br />Lambda: 0.7","b:  1.11<br />value: 0.7770<br />Function: Penalty<br />Lambda: 0.7","b:  1.12<br />value: 0.7840<br />Function: Penalty<br />Lambda: 0.7","b:  1.13<br />value: 0.7910<br />Function: Penalty<br />Lambda: 0.7","b:  1.14<br />value: 0.7980<br />Function: Penalty<br />Lambda: 0.7","b:  1.15<br />value: 0.8050<br />Function: Penalty<br />Lambda: 0.7","b:  1.16<br />value: 0.8120<br />Function: Penalty<br />Lambda: 0.7","b:  1.17<br />value: 0.8190<br />Function: Penalty<br />Lambda: 0.7","b:  1.18<br />value: 0.8260<br />Function: Penalty<br />Lambda: 0.7","b:  1.19<br />value: 0.8330<br />Function: Penalty<br />Lambda: 0.7","b:  1.20<br />value: 0.8400<br />Function: Penalty<br />Lambda: 0.7","b:  1.21<br />value: 0.8470<br />Function: Penalty<br />Lambda: 0.7","b:  1.22<br />value: 0.8540<br />Function: Penalty<br />Lambda: 0.7","b:  1.23<br />value: 0.8610<br />Function: Penalty<br />Lambda: 0.7","b:  1.24<br />value: 0.8680<br />Function: Penalty<br />Lambda: 0.7","b:  1.25<br />value: 0.8750<br />Function: Penalty<br />Lambda: 0.7","b:  1.26<br />value: 0.8820<br />Function: Penalty<br />Lambda: 0.7","b:  1.27<br />value: 0.8890<br />Function: Penalty<br />Lambda: 0.7","b:  1.28<br />value: 0.8960<br />Function: Penalty<br />Lambda: 0.7","b:  1.29<br />value: 0.9030<br />Function: Penalty<br />Lambda: 0.7","b:  1.30<br />value: 0.9100<br />Function: Penalty<br />Lambda: 0.7","b:  1.31<br />value: 0.9170<br />Function: Penalty<br />Lambda: 0.7","b:  1.32<br />value: 0.9240<br />Function: Penalty<br />Lambda: 0.7","b:  1.33<br />value: 0.9310<br />Function: Penalty<br />Lambda: 0.7","b:  1.34<br />value: 0.9380<br />Function: Penalty<br />Lambda: 0.7","b:  1.35<br />value: 0.9450<br />Function: Penalty<br />Lambda: 0.7","b:  1.36<br />value: 0.9520<br />Function: Penalty<br />Lambda: 0.7","b:  1.37<br />value: 0.9590<br />Function: Penalty<br />Lambda: 0.7","b:  1.38<br />value: 0.9660<br />Function: Penalty<br />Lambda: 0.7","b:  1.39<br />value: 0.9730<br />Function: Penalty<br />Lambda: 0.7","b:  1.40<br />value: 0.9800<br />Function: Penalty<br />Lambda: 0.7","b:  1.41<br />value: 0.9870<br />Function: Penalty<br />Lambda: 0.7","b:  1.42<br />value: 0.9940<br />Function: Penalty<br />Lambda: 0.7","b:  1.43<br />value: 1.0010<br />Function: Penalty<br />Lambda: 0.7","b:  1.44<br />value: 1.0080<br />Function: Penalty<br />Lambda: 0.7","b:  1.45<br />value: 1.0150<br />Function: Penalty<br />Lambda: 0.7","b:  1.46<br />value: 1.0220<br />Function: Penalty<br />Lambda: 0.7","b:  1.47<br />value: 1.0290<br />Function: Penalty<br />Lambda: 0.7","b:  1.48<br />value: 1.0360<br />Function: Penalty<br />Lambda: 0.7","b:  1.49<br />value: 1.0430<br />Function: Penalty<br />Lambda: 0.7","b:  1.50<br />value: 1.0500<br />Function: Penalty<br />Lambda: 0.7","b:  1.51<br />value: 1.0570<br />Function: Penalty<br />Lambda: 0.7","b:  1.52<br />value: 1.0640<br />Function: Penalty<br />Lambda: 0.7","b:  1.53<br />value: 1.0710<br />Function: Penalty<br />Lambda: 0.7","b:  1.54<br />value: 1.0780<br />Function: Penalty<br />Lambda: 0.7","b:  1.55<br />value: 1.0850<br />Function: Penalty<br />Lambda: 0.7","b:  1.56<br />value: 1.0920<br />Function: Penalty<br />Lambda: 0.7","b:  1.57<br />value: 1.0990<br />Function: Penalty<br />Lambda: 0.7","b:  1.58<br />value: 1.1060<br />Function: Penalty<br />Lambda: 0.7","b:  1.59<br />value: 1.1130<br />Function: Penalty<br />Lambda: 0.7","b:  1.60<br />value: 1.1200<br />Function: Penalty<br />Lambda: 0.7","b:  1.61<br />value: 1.1270<br />Function: Penalty<br />Lambda: 0.7","b:  1.62<br />value: 1.1340<br />Function: Penalty<br />Lambda: 0.7","b:  1.63<br />value: 1.1410<br />Function: Penalty<br />Lambda: 0.7","b:  1.64<br />value: 1.1480<br />Function: Penalty<br />Lambda: 0.7","b:  1.65<br />value: 1.1550<br />Function: Penalty<br />Lambda: 0.7","b:  1.66<br />value: 1.1620<br />Function: Penalty<br />Lambda: 0.7","b:  1.67<br />value: 1.1690<br />Function: Penalty<br />Lambda: 0.7","b:  1.68<br />value: 1.1760<br />Function: Penalty<br />Lambda: 0.7","b:  1.69<br />value: 1.1830<br />Function: Penalty<br />Lambda: 0.7","b:  1.70<br />value: 1.1900<br />Function: Penalty<br />Lambda: 0.7","b:  1.71<br />value: 1.1970<br />Function: Penalty<br />Lambda: 0.7","b:  1.72<br />value: 1.2040<br />Function: Penalty<br />Lambda: 0.7","b:  1.73<br />value: 1.2110<br />Function: Penalty<br />Lambda: 0.7","b:  1.74<br />value: 1.2180<br />Function: Penalty<br />Lambda: 0.7","b:  1.75<br />value: 1.2250<br />Function: Penalty<br />Lambda: 0.7","b:  1.76<br />value: 1.2320<br />Function: Penalty<br />Lambda: 0.7","b:  1.77<br />value: 1.2390<br />Function: Penalty<br />Lambda: 0.7","b:  1.78<br />value: 1.2460<br />Function: Penalty<br />Lambda: 0.7","b:  1.79<br />value: 1.2530<br />Function: Penalty<br />Lambda: 0.7","b:  1.80<br />value: 1.2600<br />Function: Penalty<br />Lambda: 0.7","b:  1.81<br />value: 1.2670<br />Function: Penalty<br />Lambda: 0.7","b:  1.82<br />value: 1.2740<br />Function: Penalty<br />Lambda: 0.7","b:  1.83<br />value: 1.2810<br />Function: Penalty<br />Lambda: 0.7","b:  1.84<br />value: 1.2880<br />Function: Penalty<br />Lambda: 0.7","b:  1.85<br />value: 1.2950<br />Function: Penalty<br />Lambda: 0.7","b:  1.86<br />value: 1.3020<br />Function: Penalty<br />Lambda: 0.7","b:  1.87<br />value: 1.3090<br />Function: Penalty<br />Lambda: 0.7","b:  1.88<br />value: 1.3160<br />Function: Penalty<br />Lambda: 0.7","b:  1.89<br />value: 1.3230<br />Function: Penalty<br />Lambda: 0.7","b:  1.90<br />value: 1.3300<br />Function: Penalty<br />Lambda: 0.7","b:  1.91<br />value: 1.3370<br />Function: Penalty<br />Lambda: 0.7","b:  1.92<br />value: 1.3440<br />Function: Penalty<br />Lambda: 0.7","b:  1.93<br />value: 1.3510<br />Function: Penalty<br />Lambda: 0.7","b:  1.94<br />value: 1.3580<br />Function: Penalty<br />Lambda: 0.7","b:  1.95<br />value: 1.3650<br />Function: Penalty<br />Lambda: 0.7","b:  1.96<br />value: 1.3720<br />Function: Penalty<br />Lambda: 0.7","b:  1.97<br />value: 1.3790<br />Function: Penalty<br />Lambda: 0.7","b:  1.98<br />value: 1.3860<br />Function: Penalty<br />Lambda: 0.7","b:  1.99<br />value: 1.3930<br />Function: Penalty<br />Lambda: 0.7","b:  2.00<br />value: 1.4000<br />Function: Penalty<br />Lambda: 0.7"],"frame":"0.7","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"0.8","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 0.8","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 0.8","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 0.8","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 0.8","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 0.8","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 0.8","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 0.8","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 0.8","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 0.8","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 0.8","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 0.8","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 0.8","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 0.8","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 0.8","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 0.8","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 0.8","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 0.8","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 0.8","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 0.8","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 0.8","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 0.8","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 0.8","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 0.8","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 0.8","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 0.8","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 0.8","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 0.8","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 0.8","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 0.8","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 0.8","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 0.8","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 0.8","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 0.8","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 0.8","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 0.8","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 0.8","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 0.8","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 0.8","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 0.8","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 0.8","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 0.8","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 0.8","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 0.8","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 0.8","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 0.8","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 0.8","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 0.8","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 0.8","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 0.8","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 0.8","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.8","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.8","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.8","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.8","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.8","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.8","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.8","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.8","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.8","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.8","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.8","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.8","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.8","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.8","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.8","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.8","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.8","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.8","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.8","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.8","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.8","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.8","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.8","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.8","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.8","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.8","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.8","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.8","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.8","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.8","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.8","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.8","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.8","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.8","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.8","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.8","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.8","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.8","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.8","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.8","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.8","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.8","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.8","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.8","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.8","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.8","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.8","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.8","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.8","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.8","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.8","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.8","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.8","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.8","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.8","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.8","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.8","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.8","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.8","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.8","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.8","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.8","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.8","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.8","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.8","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.8","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.8","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.8","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.8","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.8","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.8","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.8","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.8","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.8","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.8","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.8","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.8","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.8","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.8","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.8","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.8","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.8","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.8","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.8","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.8","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.8","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.8","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.8","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.8","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.8","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.8","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.8","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.8","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.8","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.8","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.8","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.8","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.8","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.8","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.8","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 0.8","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.8","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.8","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.8","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.8","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.8","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.8","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.8","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.8","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.8","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.8","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.8","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.8","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.8","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.8","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.8","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.8","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.8","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.8","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.8","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.8","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.8","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.8","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.8","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.8","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.8","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.8","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.8","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.8","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.8","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.8","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.8","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.8","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.8","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.8","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.8","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.8","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.8","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.8","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.8","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.8","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.8","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.8","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.8","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.8","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.8","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.8","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.8","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.8","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.8","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.8","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.8","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.8","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.8","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.8","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.8","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.8","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.8","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.8","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.8","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.8","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.8","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.8","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.8","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.8","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.8","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.8","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.8","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.8","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.8","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.8","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.8","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.8","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.8","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.8","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.8","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.8","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.8","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.8","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.8","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.8","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.8","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.8","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.8","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.8","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.8","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.8","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.8","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.8","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.8","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.8","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.8","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.8","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.8","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.8","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.8","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.8","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.8","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.8","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.8","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.8"],"frame":"0.8","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.15,3.1121,3.0744,3.0369,2.9996,2.9625,2.9256,2.8889,2.8524,2.8161,2.78,2.7441,2.7084,2.6729,2.6376,2.6025,2.5676,2.5329,2.4984,2.4641,2.43,2.3961,2.3624,2.3289,2.2956,2.2625,2.2296,2.1969,2.1644,2.1321,2.1,2.0681,2.0364,2.0049,1.9736,1.9425,1.9116,1.8809,1.8504,1.8201,1.79,1.7601,1.7304,1.7009,1.6716,1.6425,1.6136,1.5849,1.5564,1.5281,1.5,1.4881,1.4764,1.4649,1.4536,1.4425,1.4316,1.4209,1.4104,1.4001,1.39,1.3801,1.3704,1.3609,1.3516,1.3425,1.3336,1.3249,1.3164,1.3081,1.3,1.2921,1.2844,1.2769,1.2696,1.2625,1.2556,1.2489,1.2424,1.2361,1.23,1.2241,1.2184,1.2129,1.2076,1.2025,1.1976,1.1929,1.1884,1.1841,1.18,1.1761,1.1724,1.1689,1.1656,1.1625,1.1596,1.1569,1.1544,1.1521,1.15,1.1481,1.1464,1.1449,1.1436,1.1425,1.1416,1.1409,1.1404,1.1401,1.14,1.1401,1.1404,1.1409,1.1416,1.1425,1.1436,1.1449,1.1464,1.1481,1.15,1.1521,1.1544,1.1569,1.1596,1.1625,1.1656,1.1689,1.1724,1.1761,1.18,1.1841,1.1884,1.1929,1.1976,1.2025,1.2076,1.2129,1.2184,1.2241,1.23,1.2361,1.2424,1.2489,1.2556,1.2625,1.2696,1.2769,1.2844,1.2921,1.3,1.3081,1.3164,1.3249,1.3336,1.3425,1.3516,1.3609,1.3704,1.3801,1.39,1.4001,1.4104,1.4209,1.4316,1.4425,1.4536,1.4649,1.4764,1.4881,1.5,1.5121,1.5244,1.5369,1.5496,1.5625,1.5756,1.5889,1.6024,1.6161,1.63,1.6441,1.6584,1.6729,1.6876,1.7025,1.7176,1.7329,1.7484,1.7641,1.78,1.7961,1.8124,1.8289,1.8456,1.8625,1.8796,1.8969,1.9144,1.9321,1.95,1.9681,1.9864,2.0049,2.0236,2.0425,2.0616,2.0809,2.1004,2.1201,2.14,2.1601,2.1804,2.2009,2.2216,2.2425,2.2636,2.2849,2.3064,2.3281,2.35,2.3721,2.3944,2.4169,2.4396,2.4625,2.4856,2.5089,2.5324,2.5561,2.58,2.6041,2.6284,2.6529,2.6776,2.7025,2.7276,2.7529,2.7784,2.8041,2.83,2.8561,2.8824,2.9089,2.9356,2.9625,2.9896,3.0169,3.0444,3.0721,3.1],"text":["b: -0.50<br />value: 3.1500<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.49<br />value: 3.1121<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.48<br />value: 3.0744<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.47<br />value: 3.0369<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.46<br />value: 2.9996<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.45<br />value: 2.9625<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.44<br />value: 2.9256<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.43<br />value: 2.8889<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.42<br />value: 2.8524<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.41<br />value: 2.8161<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.40<br />value: 2.7800<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.39<br />value: 2.7441<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.38<br />value: 2.7084<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.37<br />value: 2.6729<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.36<br />value: 2.6376<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.35<br />value: 2.6025<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.34<br />value: 2.5676<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.33<br />value: 2.5329<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.32<br />value: 2.4984<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.31<br />value: 2.4641<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.30<br />value: 2.4300<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.29<br />value: 2.3961<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.28<br />value: 2.3624<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.27<br />value: 2.3289<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.26<br />value: 2.2956<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.25<br />value: 2.2625<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.24<br />value: 2.2296<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.23<br />value: 2.1969<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.22<br />value: 2.1644<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.21<br />value: 2.1321<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.20<br />value: 2.1000<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.19<br />value: 2.0681<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.18<br />value: 2.0364<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.17<br />value: 2.0049<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.16<br />value: 1.9736<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.15<br />value: 1.9425<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.14<br />value: 1.9116<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.13<br />value: 1.8809<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.12<br />value: 1.8504<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.11<br />value: 1.8201<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.10<br />value: 1.7900<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.09<br />value: 1.7601<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.08<br />value: 1.7304<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.07<br />value: 1.7009<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.06<br />value: 1.6716<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.05<br />value: 1.6425<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.04<br />value: 1.6136<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.03<br />value: 1.5849<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.02<br />value: 1.5564<br />Function: Loss + Penalty<br />Lambda: 0.8","b: -0.01<br />value: 1.5281<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.01<br />value: 1.4881<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.02<br />value: 1.4764<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.03<br />value: 1.4649<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.04<br />value: 1.4536<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.05<br />value: 1.4425<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.06<br />value: 1.4316<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.07<br />value: 1.4209<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.08<br />value: 1.4104<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.09<br />value: 1.4001<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.10<br />value: 1.3900<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.11<br />value: 1.3801<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.12<br />value: 1.3704<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.13<br />value: 1.3609<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.14<br />value: 1.3516<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.15<br />value: 1.3425<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.16<br />value: 1.3336<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.17<br />value: 1.3249<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.18<br />value: 1.3164<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.19<br />value: 1.3081<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.20<br />value: 1.3000<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.21<br />value: 1.2921<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.22<br />value: 1.2844<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.23<br />value: 1.2769<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.24<br />value: 1.2696<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.25<br />value: 1.2625<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.26<br />value: 1.2556<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.27<br />value: 1.2489<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.28<br />value: 1.2424<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.29<br />value: 1.2361<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.30<br />value: 1.2300<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.31<br />value: 1.2241<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.32<br />value: 1.2184<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.33<br />value: 1.2129<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.34<br />value: 1.2076<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.35<br />value: 1.2025<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.36<br />value: 1.1976<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.37<br />value: 1.1929<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.38<br />value: 1.1884<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.39<br />value: 1.1841<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.40<br />value: 1.1800<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.41<br />value: 1.1761<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.42<br />value: 1.1724<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.43<br />value: 1.1689<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.44<br />value: 1.1656<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.45<br />value: 1.1625<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.46<br />value: 1.1596<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.47<br />value: 1.1569<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.48<br />value: 1.1544<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.49<br />value: 1.1521<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.50<br />value: 1.1500<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.51<br />value: 1.1481<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.52<br />value: 1.1464<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.53<br />value: 1.1449<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.54<br />value: 1.1436<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.55<br />value: 1.1425<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.56<br />value: 1.1416<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.57<br />value: 1.1409<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.58<br />value: 1.1404<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.59<br />value: 1.1401<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.60<br />value: 1.1400<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.61<br />value: 1.1401<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.62<br />value: 1.1404<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.63<br />value: 1.1409<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.64<br />value: 1.1416<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.65<br />value: 1.1425<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.66<br />value: 1.1436<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.67<br />value: 1.1449<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.68<br />value: 1.1464<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.69<br />value: 1.1481<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.70<br />value: 1.1500<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.71<br />value: 1.1521<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.72<br />value: 1.1544<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.73<br />value: 1.1569<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.74<br />value: 1.1596<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.75<br />value: 1.1625<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.76<br />value: 1.1656<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.77<br />value: 1.1689<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.78<br />value: 1.1724<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.79<br />value: 1.1761<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.80<br />value: 1.1800<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.81<br />value: 1.1841<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.82<br />value: 1.1884<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.83<br />value: 1.1929<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.84<br />value: 1.1976<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.85<br />value: 1.2025<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.86<br />value: 1.2076<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.87<br />value: 1.2129<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.88<br />value: 1.2184<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.89<br />value: 1.2241<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.90<br />value: 1.2300<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.91<br />value: 1.2361<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.92<br />value: 1.2424<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.93<br />value: 1.2489<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.94<br />value: 1.2556<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.95<br />value: 1.2625<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.96<br />value: 1.2696<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.97<br />value: 1.2769<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.98<br />value: 1.2844<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  0.99<br />value: 1.2921<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.00<br />value: 1.3000<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.01<br />value: 1.3081<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.02<br />value: 1.3164<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.03<br />value: 1.3249<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.04<br />value: 1.3336<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.05<br />value: 1.3425<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.06<br />value: 1.3516<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.07<br />value: 1.3609<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.08<br />value: 1.3704<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.09<br />value: 1.3801<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.10<br />value: 1.3900<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.11<br />value: 1.4001<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.12<br />value: 1.4104<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.13<br />value: 1.4209<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.14<br />value: 1.4316<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.15<br />value: 1.4425<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.16<br />value: 1.4536<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.17<br />value: 1.4649<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.18<br />value: 1.4764<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.19<br />value: 1.4881<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.20<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.21<br />value: 1.5121<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.22<br />value: 1.5244<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.23<br />value: 1.5369<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.24<br />value: 1.5496<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.25<br />value: 1.5625<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.26<br />value: 1.5756<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.27<br />value: 1.5889<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.28<br />value: 1.6024<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.29<br />value: 1.6161<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.30<br />value: 1.6300<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.31<br />value: 1.6441<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.32<br />value: 1.6584<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.33<br />value: 1.6729<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.34<br />value: 1.6876<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.35<br />value: 1.7025<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.36<br />value: 1.7176<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.37<br />value: 1.7329<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.38<br />value: 1.7484<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.39<br />value: 1.7641<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.40<br />value: 1.7800<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.41<br />value: 1.7961<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.42<br />value: 1.8124<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.43<br />value: 1.8289<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.44<br />value: 1.8456<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.45<br />value: 1.8625<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.46<br />value: 1.8796<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.47<br />value: 1.8969<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.48<br />value: 1.9144<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.49<br />value: 1.9321<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.50<br />value: 1.9500<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.51<br />value: 1.9681<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.52<br />value: 1.9864<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.53<br />value: 2.0049<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.54<br />value: 2.0236<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.55<br />value: 2.0425<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.56<br />value: 2.0616<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.57<br />value: 2.0809<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.58<br />value: 2.1004<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.59<br />value: 2.1201<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.60<br />value: 2.1400<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.61<br />value: 2.1601<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.62<br />value: 2.1804<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.63<br />value: 2.2009<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.64<br />value: 2.2216<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.65<br />value: 2.2425<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.66<br />value: 2.2636<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.67<br />value: 2.2849<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.68<br />value: 2.3064<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.69<br />value: 2.3281<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.70<br />value: 2.3500<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.71<br />value: 2.3721<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.72<br />value: 2.3944<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.73<br />value: 2.4169<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.74<br />value: 2.4396<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.75<br />value: 2.4625<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.76<br />value: 2.4856<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.77<br />value: 2.5089<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.78<br />value: 2.5324<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.79<br />value: 2.5561<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.80<br />value: 2.5800<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.81<br />value: 2.6041<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.82<br />value: 2.6284<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.83<br />value: 2.6529<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.84<br />value: 2.6776<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.85<br />value: 2.7025<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.86<br />value: 2.7276<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.87<br />value: 2.7529<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.88<br />value: 2.7784<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.89<br />value: 2.8041<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.90<br />value: 2.8300<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.91<br />value: 2.8561<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.92<br />value: 2.8824<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.93<br />value: 2.9089<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.94<br />value: 2.9356<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.95<br />value: 2.9625<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.96<br />value: 2.9896<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.97<br />value: 3.0169<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.98<br />value: 3.0444<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  1.99<br />value: 3.0721<br />Function: Loss + Penalty<br />Lambda: 0.8","b:  2.00<br />value: 3.1000<br />Function: Loss + Penalty<br />Lambda: 0.8"],"frame":"0.8","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.4,0.392,0.384,0.376,0.368,0.36,0.352,0.344,0.336,0.328,0.32,0.312,0.304,0.296,0.288,0.28,0.272,0.264,0.256,0.248,0.24,0.232,0.224,0.216,0.208,0.2,0.192,0.184,0.176,0.168,0.16,0.152,0.144,0.136,0.128,0.12,0.112,0.104,0.096,0.088,0.08,0.072,0.064,0.056,0.048,0.04,0.032,0.024,0.016,0.00800000000000001,0,0.00800000000000001,0.016,0.024,0.032,0.04,0.048,0.056,0.064,0.072,0.08,0.088,0.096,0.104,0.112,0.12,0.128,0.136,0.144,0.152,0.16,0.168,0.176,0.184,0.192,0.2,0.208,0.216,0.224,0.232,0.24,0.248,0.256,0.264,0.272,0.28,0.288,0.296,0.304,0.312,0.32,0.328,0.336,0.344,0.352,0.36,0.368,0.376,0.384,0.392,0.4,0.408,0.416,0.424,0.432,0.44,0.448,0.456,0.464,0.472,0.48,0.488,0.496,0.504,0.512,0.52,0.528,0.536,0.544,0.552,0.56,0.568,0.576,0.584,0.592,0.6,0.608,0.616,0.624,0.632,0.64,0.648,0.656,0.664,0.672,0.68,0.688,0.696,0.704,0.712,0.72,0.728,0.736,0.744,0.752,0.76,0.768,0.776,0.784,0.792,0.8,0.808,0.816,0.824,0.832,0.84,0.848,0.856,0.864,0.872,0.88,0.888,0.896,0.904,0.912,0.92,0.928,0.936,0.944,0.952,0.96,0.968,0.976,0.984,0.992,1,1.008,1.016,1.024,1.032,1.04,1.048,1.056,1.064,1.072,1.08,1.088,1.096,1.104,1.112,1.12,1.128,1.136,1.144,1.152,1.16,1.168,1.176,1.184,1.192,1.2,1.208,1.216,1.224,1.232,1.24,1.248,1.256,1.264,1.272,1.28,1.288,1.296,1.304,1.312,1.32,1.328,1.336,1.344,1.352,1.36,1.368,1.376,1.384,1.392,1.4,1.408,1.416,1.424,1.432,1.44,1.448,1.456,1.464,1.472,1.48,1.488,1.496,1.504,1.512,1.52,1.528,1.536,1.544,1.552,1.56,1.568,1.576,1.584,1.592,1.6],"text":["b: -0.50<br />value: 0.4000<br />Function: Penalty<br />Lambda: 0.8","b: -0.49<br />value: 0.3920<br />Function: Penalty<br />Lambda: 0.8","b: -0.48<br />value: 0.3840<br />Function: Penalty<br />Lambda: 0.8","b: -0.47<br />value: 0.3760<br />Function: Penalty<br />Lambda: 0.8","b: -0.46<br />value: 0.3680<br />Function: Penalty<br />Lambda: 0.8","b: -0.45<br />value: 0.3600<br />Function: Penalty<br />Lambda: 0.8","b: -0.44<br />value: 0.3520<br />Function: Penalty<br />Lambda: 0.8","b: -0.43<br />value: 0.3440<br />Function: Penalty<br />Lambda: 0.8","b: -0.42<br />value: 0.3360<br />Function: Penalty<br />Lambda: 0.8","b: -0.41<br />value: 0.3280<br />Function: Penalty<br />Lambda: 0.8","b: -0.40<br />value: 0.3200<br />Function: Penalty<br />Lambda: 0.8","b: -0.39<br />value: 0.3120<br />Function: Penalty<br />Lambda: 0.8","b: -0.38<br />value: 0.3040<br />Function: Penalty<br />Lambda: 0.8","b: -0.37<br />value: 0.2960<br />Function: Penalty<br />Lambda: 0.8","b: -0.36<br />value: 0.2880<br />Function: Penalty<br />Lambda: 0.8","b: -0.35<br />value: 0.2800<br />Function: Penalty<br />Lambda: 0.8","b: -0.34<br />value: 0.2720<br />Function: Penalty<br />Lambda: 0.8","b: -0.33<br />value: 0.2640<br />Function: Penalty<br />Lambda: 0.8","b: -0.32<br />value: 0.2560<br />Function: Penalty<br />Lambda: 0.8","b: -0.31<br />value: 0.2480<br />Function: Penalty<br />Lambda: 0.8","b: -0.30<br />value: 0.2400<br />Function: Penalty<br />Lambda: 0.8","b: -0.29<br />value: 0.2320<br />Function: Penalty<br />Lambda: 0.8","b: -0.28<br />value: 0.2240<br />Function: Penalty<br />Lambda: 0.8","b: -0.27<br />value: 0.2160<br />Function: Penalty<br />Lambda: 0.8","b: -0.26<br />value: 0.2080<br />Function: Penalty<br />Lambda: 0.8","b: -0.25<br />value: 0.2000<br />Function: Penalty<br />Lambda: 0.8","b: -0.24<br />value: 0.1920<br />Function: Penalty<br />Lambda: 0.8","b: -0.23<br />value: 0.1840<br />Function: Penalty<br />Lambda: 0.8","b: -0.22<br />value: 0.1760<br />Function: Penalty<br />Lambda: 0.8","b: -0.21<br />value: 0.1680<br />Function: Penalty<br />Lambda: 0.8","b: -0.20<br />value: 0.1600<br />Function: Penalty<br />Lambda: 0.8","b: -0.19<br />value: 0.1520<br />Function: Penalty<br />Lambda: 0.8","b: -0.18<br />value: 0.1440<br />Function: Penalty<br />Lambda: 0.8","b: -0.17<br />value: 0.1360<br />Function: Penalty<br />Lambda: 0.8","b: -0.16<br />value: 0.1280<br />Function: Penalty<br />Lambda: 0.8","b: -0.15<br />value: 0.1200<br />Function: Penalty<br />Lambda: 0.8","b: -0.14<br />value: 0.1120<br />Function: Penalty<br />Lambda: 0.8","b: -0.13<br />value: 0.1040<br />Function: Penalty<br />Lambda: 0.8","b: -0.12<br />value: 0.0960<br />Function: Penalty<br />Lambda: 0.8","b: -0.11<br />value: 0.0880<br />Function: Penalty<br />Lambda: 0.8","b: -0.10<br />value: 0.0800<br />Function: Penalty<br />Lambda: 0.8","b: -0.09<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.8","b: -0.08<br />value: 0.0640<br />Function: Penalty<br />Lambda: 0.8","b: -0.07<br />value: 0.0560<br />Function: Penalty<br />Lambda: 0.8","b: -0.06<br />value: 0.0480<br />Function: Penalty<br />Lambda: 0.8","b: -0.05<br />value: 0.0400<br />Function: Penalty<br />Lambda: 0.8","b: -0.04<br />value: 0.0320<br />Function: Penalty<br />Lambda: 0.8","b: -0.03<br />value: 0.0240<br />Function: Penalty<br />Lambda: 0.8","b: -0.02<br />value: 0.0160<br />Function: Penalty<br />Lambda: 0.8","b: -0.01<br />value: 0.0080<br />Function: Penalty<br />Lambda: 0.8","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.8","b:  0.01<br />value: 0.0080<br />Function: Penalty<br />Lambda: 0.8","b:  0.02<br />value: 0.0160<br />Function: Penalty<br />Lambda: 0.8","b:  0.03<br />value: 0.0240<br />Function: Penalty<br />Lambda: 0.8","b:  0.04<br />value: 0.0320<br />Function: Penalty<br />Lambda: 0.8","b:  0.05<br />value: 0.0400<br />Function: Penalty<br />Lambda: 0.8","b:  0.06<br />value: 0.0480<br />Function: Penalty<br />Lambda: 0.8","b:  0.07<br />value: 0.0560<br />Function: Penalty<br />Lambda: 0.8","b:  0.08<br />value: 0.0640<br />Function: Penalty<br />Lambda: 0.8","b:  0.09<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.8","b:  0.10<br />value: 0.0800<br />Function: Penalty<br />Lambda: 0.8","b:  0.11<br />value: 0.0880<br />Function: Penalty<br />Lambda: 0.8","b:  0.12<br />value: 0.0960<br />Function: Penalty<br />Lambda: 0.8","b:  0.13<br />value: 0.1040<br />Function: Penalty<br />Lambda: 0.8","b:  0.14<br />value: 0.1120<br />Function: Penalty<br />Lambda: 0.8","b:  0.15<br />value: 0.1200<br />Function: Penalty<br />Lambda: 0.8","b:  0.16<br />value: 0.1280<br />Function: Penalty<br />Lambda: 0.8","b:  0.17<br />value: 0.1360<br />Function: Penalty<br />Lambda: 0.8","b:  0.18<br />value: 0.1440<br />Function: Penalty<br />Lambda: 0.8","b:  0.19<br />value: 0.1520<br />Function: Penalty<br />Lambda: 0.8","b:  0.20<br />value: 0.1600<br />Function: Penalty<br />Lambda: 0.8","b:  0.21<br />value: 0.1680<br />Function: Penalty<br />Lambda: 0.8","b:  0.22<br />value: 0.1760<br />Function: Penalty<br />Lambda: 0.8","b:  0.23<br />value: 0.1840<br />Function: Penalty<br />Lambda: 0.8","b:  0.24<br />value: 0.1920<br />Function: Penalty<br />Lambda: 0.8","b:  0.25<br />value: 0.2000<br />Function: Penalty<br />Lambda: 0.8","b:  0.26<br />value: 0.2080<br />Function: Penalty<br />Lambda: 0.8","b:  0.27<br />value: 0.2160<br />Function: Penalty<br />Lambda: 0.8","b:  0.28<br />value: 0.2240<br />Function: Penalty<br />Lambda: 0.8","b:  0.29<br />value: 0.2320<br />Function: Penalty<br />Lambda: 0.8","b:  0.30<br />value: 0.2400<br />Function: Penalty<br />Lambda: 0.8","b:  0.31<br />value: 0.2480<br />Function: Penalty<br />Lambda: 0.8","b:  0.32<br />value: 0.2560<br />Function: Penalty<br />Lambda: 0.8","b:  0.33<br />value: 0.2640<br />Function: Penalty<br />Lambda: 0.8","b:  0.34<br />value: 0.2720<br />Function: Penalty<br />Lambda: 0.8","b:  0.35<br />value: 0.2800<br />Function: Penalty<br />Lambda: 0.8","b:  0.36<br />value: 0.2880<br />Function: Penalty<br />Lambda: 0.8","b:  0.37<br />value: 0.2960<br />Function: Penalty<br />Lambda: 0.8","b:  0.38<br />value: 0.3040<br />Function: Penalty<br />Lambda: 0.8","b:  0.39<br />value: 0.3120<br />Function: Penalty<br />Lambda: 0.8","b:  0.40<br />value: 0.3200<br />Function: Penalty<br />Lambda: 0.8","b:  0.41<br />value: 0.3280<br />Function: Penalty<br />Lambda: 0.8","b:  0.42<br />value: 0.3360<br />Function: Penalty<br />Lambda: 0.8","b:  0.43<br />value: 0.3440<br />Function: Penalty<br />Lambda: 0.8","b:  0.44<br />value: 0.3520<br />Function: Penalty<br />Lambda: 0.8","b:  0.45<br />value: 0.3600<br />Function: Penalty<br />Lambda: 0.8","b:  0.46<br />value: 0.3680<br />Function: Penalty<br />Lambda: 0.8","b:  0.47<br />value: 0.3760<br />Function: Penalty<br />Lambda: 0.8","b:  0.48<br />value: 0.3840<br />Function: Penalty<br />Lambda: 0.8","b:  0.49<br />value: 0.3920<br />Function: Penalty<br />Lambda: 0.8","b:  0.50<br />value: 0.4000<br />Function: Penalty<br />Lambda: 0.8","b:  0.51<br />value: 0.4080<br />Function: Penalty<br />Lambda: 0.8","b:  0.52<br />value: 0.4160<br />Function: Penalty<br />Lambda: 0.8","b:  0.53<br />value: 0.4240<br />Function: Penalty<br />Lambda: 0.8","b:  0.54<br />value: 0.4320<br />Function: Penalty<br />Lambda: 0.8","b:  0.55<br />value: 0.4400<br />Function: Penalty<br />Lambda: 0.8","b:  0.56<br />value: 0.4480<br />Function: Penalty<br />Lambda: 0.8","b:  0.57<br />value: 0.4560<br />Function: Penalty<br />Lambda: 0.8","b:  0.58<br />value: 0.4640<br />Function: Penalty<br />Lambda: 0.8","b:  0.59<br />value: 0.4720<br />Function: Penalty<br />Lambda: 0.8","b:  0.60<br />value: 0.4800<br />Function: Penalty<br />Lambda: 0.8","b:  0.61<br />value: 0.4880<br />Function: Penalty<br />Lambda: 0.8","b:  0.62<br />value: 0.4960<br />Function: Penalty<br />Lambda: 0.8","b:  0.63<br />value: 0.5040<br />Function: Penalty<br />Lambda: 0.8","b:  0.64<br />value: 0.5120<br />Function: Penalty<br />Lambda: 0.8","b:  0.65<br />value: 0.5200<br />Function: Penalty<br />Lambda: 0.8","b:  0.66<br />value: 0.5280<br />Function: Penalty<br />Lambda: 0.8","b:  0.67<br />value: 0.5360<br />Function: Penalty<br />Lambda: 0.8","b:  0.68<br />value: 0.5440<br />Function: Penalty<br />Lambda: 0.8","b:  0.69<br />value: 0.5520<br />Function: Penalty<br />Lambda: 0.8","b:  0.70<br />value: 0.5600<br />Function: Penalty<br />Lambda: 0.8","b:  0.71<br />value: 0.5680<br />Function: Penalty<br />Lambda: 0.8","b:  0.72<br />value: 0.5760<br />Function: Penalty<br />Lambda: 0.8","b:  0.73<br />value: 0.5840<br />Function: Penalty<br />Lambda: 0.8","b:  0.74<br />value: 0.5920<br />Function: Penalty<br />Lambda: 0.8","b:  0.75<br />value: 0.6000<br />Function: Penalty<br />Lambda: 0.8","b:  0.76<br />value: 0.6080<br />Function: Penalty<br />Lambda: 0.8","b:  0.77<br />value: 0.6160<br />Function: Penalty<br />Lambda: 0.8","b:  0.78<br />value: 0.6240<br />Function: Penalty<br />Lambda: 0.8","b:  0.79<br />value: 0.6320<br />Function: Penalty<br />Lambda: 0.8","b:  0.80<br />value: 0.6400<br />Function: Penalty<br />Lambda: 0.8","b:  0.81<br />value: 0.6480<br />Function: Penalty<br />Lambda: 0.8","b:  0.82<br />value: 0.6560<br />Function: Penalty<br />Lambda: 0.8","b:  0.83<br />value: 0.6640<br />Function: Penalty<br />Lambda: 0.8","b:  0.84<br />value: 0.6720<br />Function: Penalty<br />Lambda: 0.8","b:  0.85<br />value: 0.6800<br />Function: Penalty<br />Lambda: 0.8","b:  0.86<br />value: 0.6880<br />Function: Penalty<br />Lambda: 0.8","b:  0.87<br />value: 0.6960<br />Function: Penalty<br />Lambda: 0.8","b:  0.88<br />value: 0.7040<br />Function: Penalty<br />Lambda: 0.8","b:  0.89<br />value: 0.7120<br />Function: Penalty<br />Lambda: 0.8","b:  0.90<br />value: 0.7200<br />Function: Penalty<br />Lambda: 0.8","b:  0.91<br />value: 0.7280<br />Function: Penalty<br />Lambda: 0.8","b:  0.92<br />value: 0.7360<br />Function: Penalty<br />Lambda: 0.8","b:  0.93<br />value: 0.7440<br />Function: Penalty<br />Lambda: 0.8","b:  0.94<br />value: 0.7520<br />Function: Penalty<br />Lambda: 0.8","b:  0.95<br />value: 0.7600<br />Function: Penalty<br />Lambda: 0.8","b:  0.96<br />value: 0.7680<br />Function: Penalty<br />Lambda: 0.8","b:  0.97<br />value: 0.7760<br />Function: Penalty<br />Lambda: 0.8","b:  0.98<br />value: 0.7840<br />Function: Penalty<br />Lambda: 0.8","b:  0.99<br />value: 0.7920<br />Function: Penalty<br />Lambda: 0.8","b:  1.00<br />value: 0.8000<br />Function: Penalty<br />Lambda: 0.8","b:  1.01<br />value: 0.8080<br />Function: Penalty<br />Lambda: 0.8","b:  1.02<br />value: 0.8160<br />Function: Penalty<br />Lambda: 0.8","b:  1.03<br />value: 0.8240<br />Function: Penalty<br />Lambda: 0.8","b:  1.04<br />value: 0.8320<br />Function: Penalty<br />Lambda: 0.8","b:  1.05<br />value: 0.8400<br />Function: Penalty<br />Lambda: 0.8","b:  1.06<br />value: 0.8480<br />Function: Penalty<br />Lambda: 0.8","b:  1.07<br />value: 0.8560<br />Function: Penalty<br />Lambda: 0.8","b:  1.08<br />value: 0.8640<br />Function: Penalty<br />Lambda: 0.8","b:  1.09<br />value: 0.8720<br />Function: Penalty<br />Lambda: 0.8","b:  1.10<br />value: 0.8800<br />Function: Penalty<br />Lambda: 0.8","b:  1.11<br />value: 0.8880<br />Function: Penalty<br />Lambda: 0.8","b:  1.12<br />value: 0.8960<br />Function: Penalty<br />Lambda: 0.8","b:  1.13<br />value: 0.9040<br />Function: Penalty<br />Lambda: 0.8","b:  1.14<br />value: 0.9120<br />Function: Penalty<br />Lambda: 0.8","b:  1.15<br />value: 0.9200<br />Function: Penalty<br />Lambda: 0.8","b:  1.16<br />value: 0.9280<br />Function: Penalty<br />Lambda: 0.8","b:  1.17<br />value: 0.9360<br />Function: Penalty<br />Lambda: 0.8","b:  1.18<br />value: 0.9440<br />Function: Penalty<br />Lambda: 0.8","b:  1.19<br />value: 0.9520<br />Function: Penalty<br />Lambda: 0.8","b:  1.20<br />value: 0.9600<br />Function: Penalty<br />Lambda: 0.8","b:  1.21<br />value: 0.9680<br />Function: Penalty<br />Lambda: 0.8","b:  1.22<br />value: 0.9760<br />Function: Penalty<br />Lambda: 0.8","b:  1.23<br />value: 0.9840<br />Function: Penalty<br />Lambda: 0.8","b:  1.24<br />value: 0.9920<br />Function: Penalty<br />Lambda: 0.8","b:  1.25<br />value: 1.0000<br />Function: Penalty<br />Lambda: 0.8","b:  1.26<br />value: 1.0080<br />Function: Penalty<br />Lambda: 0.8","b:  1.27<br />value: 1.0160<br />Function: Penalty<br />Lambda: 0.8","b:  1.28<br />value: 1.0240<br />Function: Penalty<br />Lambda: 0.8","b:  1.29<br />value: 1.0320<br />Function: Penalty<br />Lambda: 0.8","b:  1.30<br />value: 1.0400<br />Function: Penalty<br />Lambda: 0.8","b:  1.31<br />value: 1.0480<br />Function: Penalty<br />Lambda: 0.8","b:  1.32<br />value: 1.0560<br />Function: Penalty<br />Lambda: 0.8","b:  1.33<br />value: 1.0640<br />Function: Penalty<br />Lambda: 0.8","b:  1.34<br />value: 1.0720<br />Function: Penalty<br />Lambda: 0.8","b:  1.35<br />value: 1.0800<br />Function: Penalty<br />Lambda: 0.8","b:  1.36<br />value: 1.0880<br />Function: Penalty<br />Lambda: 0.8","b:  1.37<br />value: 1.0960<br />Function: Penalty<br />Lambda: 0.8","b:  1.38<br />value: 1.1040<br />Function: Penalty<br />Lambda: 0.8","b:  1.39<br />value: 1.1120<br />Function: Penalty<br />Lambda: 0.8","b:  1.40<br />value: 1.1200<br />Function: Penalty<br />Lambda: 0.8","b:  1.41<br />value: 1.1280<br />Function: Penalty<br />Lambda: 0.8","b:  1.42<br />value: 1.1360<br />Function: Penalty<br />Lambda: 0.8","b:  1.43<br />value: 1.1440<br />Function: Penalty<br />Lambda: 0.8","b:  1.44<br />value: 1.1520<br />Function: Penalty<br />Lambda: 0.8","b:  1.45<br />value: 1.1600<br />Function: Penalty<br />Lambda: 0.8","b:  1.46<br />value: 1.1680<br />Function: Penalty<br />Lambda: 0.8","b:  1.47<br />value: 1.1760<br />Function: Penalty<br />Lambda: 0.8","b:  1.48<br />value: 1.1840<br />Function: Penalty<br />Lambda: 0.8","b:  1.49<br />value: 1.1920<br />Function: Penalty<br />Lambda: 0.8","b:  1.50<br />value: 1.2000<br />Function: Penalty<br />Lambda: 0.8","b:  1.51<br />value: 1.2080<br />Function: Penalty<br />Lambda: 0.8","b:  1.52<br />value: 1.2160<br />Function: Penalty<br />Lambda: 0.8","b:  1.53<br />value: 1.2240<br />Function: Penalty<br />Lambda: 0.8","b:  1.54<br />value: 1.2320<br />Function: Penalty<br />Lambda: 0.8","b:  1.55<br />value: 1.2400<br />Function: Penalty<br />Lambda: 0.8","b:  1.56<br />value: 1.2480<br />Function: Penalty<br />Lambda: 0.8","b:  1.57<br />value: 1.2560<br />Function: Penalty<br />Lambda: 0.8","b:  1.58<br />value: 1.2640<br />Function: Penalty<br />Lambda: 0.8","b:  1.59<br />value: 1.2720<br />Function: Penalty<br />Lambda: 0.8","b:  1.60<br />value: 1.2800<br />Function: Penalty<br />Lambda: 0.8","b:  1.61<br />value: 1.2880<br />Function: Penalty<br />Lambda: 0.8","b:  1.62<br />value: 1.2960<br />Function: Penalty<br />Lambda: 0.8","b:  1.63<br />value: 1.3040<br />Function: Penalty<br />Lambda: 0.8","b:  1.64<br />value: 1.3120<br />Function: Penalty<br />Lambda: 0.8","b:  1.65<br />value: 1.3200<br />Function: Penalty<br />Lambda: 0.8","b:  1.66<br />value: 1.3280<br />Function: Penalty<br />Lambda: 0.8","b:  1.67<br />value: 1.3360<br />Function: Penalty<br />Lambda: 0.8","b:  1.68<br />value: 1.3440<br />Function: Penalty<br />Lambda: 0.8","b:  1.69<br />value: 1.3520<br />Function: Penalty<br />Lambda: 0.8","b:  1.70<br />value: 1.3600<br />Function: Penalty<br />Lambda: 0.8","b:  1.71<br />value: 1.3680<br />Function: Penalty<br />Lambda: 0.8","b:  1.72<br />value: 1.3760<br />Function: Penalty<br />Lambda: 0.8","b:  1.73<br />value: 1.3840<br />Function: Penalty<br />Lambda: 0.8","b:  1.74<br />value: 1.3920<br />Function: Penalty<br />Lambda: 0.8","b:  1.75<br />value: 1.4000<br />Function: Penalty<br />Lambda: 0.8","b:  1.76<br />value: 1.4080<br />Function: Penalty<br />Lambda: 0.8","b:  1.77<br />value: 1.4160<br />Function: Penalty<br />Lambda: 0.8","b:  1.78<br />value: 1.4240<br />Function: Penalty<br />Lambda: 0.8","b:  1.79<br />value: 1.4320<br />Function: Penalty<br />Lambda: 0.8","b:  1.80<br />value: 1.4400<br />Function: Penalty<br />Lambda: 0.8","b:  1.81<br />value: 1.4480<br />Function: Penalty<br />Lambda: 0.8","b:  1.82<br />value: 1.4560<br />Function: Penalty<br />Lambda: 0.8","b:  1.83<br />value: 1.4640<br />Function: Penalty<br />Lambda: 0.8","b:  1.84<br />value: 1.4720<br />Function: Penalty<br />Lambda: 0.8","b:  1.85<br />value: 1.4800<br />Function: Penalty<br />Lambda: 0.8","b:  1.86<br />value: 1.4880<br />Function: Penalty<br />Lambda: 0.8","b:  1.87<br />value: 1.4960<br />Function: Penalty<br />Lambda: 0.8","b:  1.88<br />value: 1.5040<br />Function: Penalty<br />Lambda: 0.8","b:  1.89<br />value: 1.5120<br />Function: Penalty<br />Lambda: 0.8","b:  1.90<br />value: 1.5200<br />Function: Penalty<br />Lambda: 0.8","b:  1.91<br />value: 1.5280<br />Function: Penalty<br />Lambda: 0.8","b:  1.92<br />value: 1.5360<br />Function: Penalty<br />Lambda: 0.8","b:  1.93<br />value: 1.5440<br />Function: Penalty<br />Lambda: 0.8","b:  1.94<br />value: 1.5520<br />Function: Penalty<br />Lambda: 0.8","b:  1.95<br />value: 1.5600<br />Function: Penalty<br />Lambda: 0.8","b:  1.96<br />value: 1.5680<br />Function: Penalty<br />Lambda: 0.8","b:  1.97<br />value: 1.5760<br />Function: Penalty<br />Lambda: 0.8","b:  1.98<br />value: 1.5840<br />Function: Penalty<br />Lambda: 0.8","b:  1.99<br />value: 1.5920<br />Function: Penalty<br />Lambda: 0.8","b:  2.00<br />value: 1.6000<br />Function: Penalty<br />Lambda: 0.8"],"frame":"0.8","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"0.9","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 0.9","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 0.9","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 0.9","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 0.9","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 0.9","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 0.9","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 0.9","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 0.9","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 0.9","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 0.9","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 0.9","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 0.9","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 0.9","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 0.9","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 0.9","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 0.9","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 0.9","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 0.9","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 0.9","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 0.9","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 0.9","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 0.9","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 0.9","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 0.9","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 0.9","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 0.9","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 0.9","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 0.9","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 0.9","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 0.9","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 0.9","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 0.9","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 0.9","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 0.9","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 0.9","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 0.9","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 0.9","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 0.9","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 0.9","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 0.9","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 0.9","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 0.9","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 0.9","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 0.9","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 0.9","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 0.9","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 0.9","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 0.9","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 0.9","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 0.9","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.9","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.9","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.9","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.9","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.9","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.9","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.9","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.9","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.9","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.9","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.9","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.9","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.9","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.9","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.9","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.9","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.9","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.9","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.9","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.9","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.9","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.9","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.9","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.9","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.9","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.9","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.9","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.9","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.9","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.9","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.9","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.9","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.9","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.9","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.9","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.9","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.9","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.9","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.9","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.9","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.9","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.9","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.9","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.9","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.9","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.9","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.9","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.9","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.9","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.9","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.9","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.9","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.9","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.9","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.9","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.9","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.9","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.9","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.9","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.9","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.9","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.9","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.9","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.9","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.9","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.9","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.9","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.9","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.9","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.9","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.9","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.9","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.9","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.9","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.9","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.9","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.9","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.9","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.9","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.9","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.9","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.9","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.9","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.9","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.9","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.9","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.9","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.9","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.9","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.9","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.9","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.9","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.9","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.9","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.9","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.9","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.9","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.9","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.9","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.9","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 0.9","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 0.9","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 0.9","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 0.9","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 0.9","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 0.9","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 0.9","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 0.9","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 0.9","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 0.9","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 0.9","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 0.9","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 0.9","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 0.9","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 0.9","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 0.9","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 0.9","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 0.9","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 0.9","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 0.9","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 0.9","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 0.9","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 0.9","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 0.9","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 0.9","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 0.9","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 0.9","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 0.9","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 0.9","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 0.9","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 0.9","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 0.9","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 0.9","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 0.9","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 0.9","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 0.9","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 0.9","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 0.9","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 0.9","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 0.9","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 0.9","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 0.9","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 0.9","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 0.9","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 0.9","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 0.9","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 0.9","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 0.9","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 0.9","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 0.9","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 0.9","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 0.9","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 0.9","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 0.9","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 0.9","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 0.9","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 0.9","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 0.9","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 0.9","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 0.9","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 0.9","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 0.9","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 0.9","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 0.9","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 0.9","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 0.9","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 0.9","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 0.9","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 0.9","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 0.9","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 0.9","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 0.9","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 0.9","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 0.9","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 0.9","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 0.9","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 0.9","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 0.9","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 0.9","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 0.9","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 0.9","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 0.9","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 0.9","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 0.9","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 0.9","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 0.9","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 0.9","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 0.9","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 0.9","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 0.9","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 0.9","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 0.9","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 0.9","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 0.9","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 0.9","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 0.9","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 0.9","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 0.9","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 0.9","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 0.9","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 0.9"],"frame":"0.9","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.2,3.1611,3.1224,3.0839,3.0456,3.0075,2.9696,2.9319,2.8944,2.8571,2.82,2.7831,2.7464,2.7099,2.6736,2.6375,2.6016,2.5659,2.5304,2.4951,2.46,2.4251,2.3904,2.3559,2.3216,2.2875,2.2536,2.2199,2.1864,2.1531,2.12,2.0871,2.0544,2.0219,1.9896,1.9575,1.9256,1.8939,1.8624,1.8311,1.8,1.7691,1.7384,1.7079,1.6776,1.6475,1.6176,1.5879,1.5584,1.5291,1.5,1.4891,1.4784,1.4679,1.4576,1.4475,1.4376,1.4279,1.4184,1.4091,1.4,1.3911,1.3824,1.3739,1.3656,1.3575,1.3496,1.3419,1.3344,1.3271,1.32,1.3131,1.3064,1.2999,1.2936,1.2875,1.2816,1.2759,1.2704,1.2651,1.26,1.2551,1.2504,1.2459,1.2416,1.2375,1.2336,1.2299,1.2264,1.2231,1.22,1.2171,1.2144,1.2119,1.2096,1.2075,1.2056,1.2039,1.2024,1.2011,1.2,1.1991,1.1984,1.1979,1.1976,1.1975,1.1976,1.1979,1.1984,1.1991,1.2,1.2011,1.2024,1.2039,1.2056,1.2075,1.2096,1.2119,1.2144,1.2171,1.22,1.2231,1.2264,1.2299,1.2336,1.2375,1.2416,1.2459,1.2504,1.2551,1.26,1.2651,1.2704,1.2759,1.2816,1.2875,1.2936,1.2999,1.3064,1.3131,1.32,1.3271,1.3344,1.3419,1.3496,1.3575,1.3656,1.3739,1.3824,1.3911,1.4,1.4091,1.4184,1.4279,1.4376,1.4475,1.4576,1.4679,1.4784,1.4891,1.5,1.5111,1.5224,1.5339,1.5456,1.5575,1.5696,1.5819,1.5944,1.6071,1.62,1.6331,1.6464,1.6599,1.6736,1.6875,1.7016,1.7159,1.7304,1.7451,1.76,1.7751,1.7904,1.8059,1.8216,1.8375,1.8536,1.8699,1.8864,1.9031,1.92,1.9371,1.9544,1.9719,1.9896,2.0075,2.0256,2.0439,2.0624,2.0811,2.1,2.1191,2.1384,2.1579,2.1776,2.1975,2.2176,2.2379,2.2584,2.2791,2.3,2.3211,2.3424,2.3639,2.3856,2.4075,2.4296,2.4519,2.4744,2.4971,2.52,2.5431,2.5664,2.5899,2.6136,2.6375,2.6616,2.6859,2.7104,2.7351,2.76,2.7851,2.8104,2.8359,2.8616,2.8875,2.9136,2.9399,2.9664,2.9931,3.02,3.0471,3.0744,3.1019,3.1296,3.1575,3.1856,3.2139,3.2424,3.2711,3.3],"text":["b: -0.50<br />value: 3.2000<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.49<br />value: 3.1611<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.48<br />value: 3.1224<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.47<br />value: 3.0839<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.46<br />value: 3.0456<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.45<br />value: 3.0075<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.44<br />value: 2.9696<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.43<br />value: 2.9319<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.42<br />value: 2.8944<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.41<br />value: 2.8571<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.40<br />value: 2.8200<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.39<br />value: 2.7831<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.38<br />value: 2.7464<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.37<br />value: 2.7099<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.36<br />value: 2.6736<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.35<br />value: 2.6375<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.34<br />value: 2.6016<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.33<br />value: 2.5659<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.32<br />value: 2.5304<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.31<br />value: 2.4951<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.30<br />value: 2.4600<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.29<br />value: 2.4251<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.28<br />value: 2.3904<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.27<br />value: 2.3559<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.26<br />value: 2.3216<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.25<br />value: 2.2875<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.24<br />value: 2.2536<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.23<br />value: 2.2199<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.22<br />value: 2.1864<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.21<br />value: 2.1531<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.20<br />value: 2.1200<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.19<br />value: 2.0871<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.18<br />value: 2.0544<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.17<br />value: 2.0219<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.16<br />value: 1.9896<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.15<br />value: 1.9575<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.14<br />value: 1.9256<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.13<br />value: 1.8939<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.12<br />value: 1.8624<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.11<br />value: 1.8311<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.10<br />value: 1.8000<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.09<br />value: 1.7691<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.08<br />value: 1.7384<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.07<br />value: 1.7079<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.06<br />value: 1.6776<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.05<br />value: 1.6475<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.04<br />value: 1.6176<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.03<br />value: 1.5879<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.02<br />value: 1.5584<br />Function: Loss + Penalty<br />Lambda: 0.9","b: -0.01<br />value: 1.5291<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.01<br />value: 1.4891<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.02<br />value: 1.4784<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.03<br />value: 1.4679<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.04<br />value: 1.4576<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.05<br />value: 1.4475<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.06<br />value: 1.4376<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.07<br />value: 1.4279<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.08<br />value: 1.4184<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.09<br />value: 1.4091<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.10<br />value: 1.4000<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.11<br />value: 1.3911<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.12<br />value: 1.3824<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.13<br />value: 1.3739<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.14<br />value: 1.3656<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.15<br />value: 1.3575<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.16<br />value: 1.3496<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.17<br />value: 1.3419<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.18<br />value: 1.3344<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.19<br />value: 1.3271<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.20<br />value: 1.3200<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.21<br />value: 1.3131<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.22<br />value: 1.3064<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.23<br />value: 1.2999<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.24<br />value: 1.2936<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.25<br />value: 1.2875<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.26<br />value: 1.2816<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.27<br />value: 1.2759<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.28<br />value: 1.2704<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.29<br />value: 1.2651<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.30<br />value: 1.2600<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.31<br />value: 1.2551<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.32<br />value: 1.2504<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.33<br />value: 1.2459<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.34<br />value: 1.2416<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.35<br />value: 1.2375<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.36<br />value: 1.2336<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.37<br />value: 1.2299<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.38<br />value: 1.2264<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.39<br />value: 1.2231<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.40<br />value: 1.2200<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.41<br />value: 1.2171<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.42<br />value: 1.2144<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.43<br />value: 1.2119<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.44<br />value: 1.2096<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.45<br />value: 1.2075<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.46<br />value: 1.2056<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.47<br />value: 1.2039<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.48<br />value: 1.2024<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.49<br />value: 1.2011<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.50<br />value: 1.2000<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.51<br />value: 1.1991<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.52<br />value: 1.1984<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.53<br />value: 1.1979<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.54<br />value: 1.1976<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.55<br />value: 1.1975<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.56<br />value: 1.1976<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.57<br />value: 1.1979<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.58<br />value: 1.1984<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.59<br />value: 1.1991<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.60<br />value: 1.2000<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.61<br />value: 1.2011<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.62<br />value: 1.2024<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.63<br />value: 1.2039<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.64<br />value: 1.2056<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.65<br />value: 1.2075<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.66<br />value: 1.2096<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.67<br />value: 1.2119<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.68<br />value: 1.2144<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.69<br />value: 1.2171<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.70<br />value: 1.2200<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.71<br />value: 1.2231<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.72<br />value: 1.2264<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.73<br />value: 1.2299<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.74<br />value: 1.2336<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.75<br />value: 1.2375<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.76<br />value: 1.2416<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.77<br />value: 1.2459<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.78<br />value: 1.2504<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.79<br />value: 1.2551<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.80<br />value: 1.2600<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.81<br />value: 1.2651<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.82<br />value: 1.2704<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.83<br />value: 1.2759<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.84<br />value: 1.2816<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.85<br />value: 1.2875<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.86<br />value: 1.2936<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.87<br />value: 1.2999<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.88<br />value: 1.3064<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.89<br />value: 1.3131<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.90<br />value: 1.3200<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.91<br />value: 1.3271<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.92<br />value: 1.3344<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.93<br />value: 1.3419<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.94<br />value: 1.3496<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.95<br />value: 1.3575<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.96<br />value: 1.3656<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.97<br />value: 1.3739<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.98<br />value: 1.3824<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  0.99<br />value: 1.3911<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.00<br />value: 1.4000<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.01<br />value: 1.4091<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.02<br />value: 1.4184<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.03<br />value: 1.4279<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.04<br />value: 1.4376<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.05<br />value: 1.4475<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.06<br />value: 1.4576<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.07<br />value: 1.4679<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.08<br />value: 1.4784<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.09<br />value: 1.4891<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.10<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.11<br />value: 1.5111<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.12<br />value: 1.5224<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.13<br />value: 1.5339<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.14<br />value: 1.5456<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.15<br />value: 1.5575<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.16<br />value: 1.5696<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.17<br />value: 1.5819<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.18<br />value: 1.5944<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.19<br />value: 1.6071<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.20<br />value: 1.6200<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.21<br />value: 1.6331<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.22<br />value: 1.6464<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.23<br />value: 1.6599<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.24<br />value: 1.6736<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.25<br />value: 1.6875<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.26<br />value: 1.7016<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.27<br />value: 1.7159<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.28<br />value: 1.7304<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.29<br />value: 1.7451<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.30<br />value: 1.7600<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.31<br />value: 1.7751<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.32<br />value: 1.7904<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.33<br />value: 1.8059<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.34<br />value: 1.8216<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.35<br />value: 1.8375<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.36<br />value: 1.8536<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.37<br />value: 1.8699<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.38<br />value: 1.8864<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.39<br />value: 1.9031<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.40<br />value: 1.9200<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.41<br />value: 1.9371<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.42<br />value: 1.9544<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.43<br />value: 1.9719<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.44<br />value: 1.9896<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.45<br />value: 2.0075<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.46<br />value: 2.0256<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.47<br />value: 2.0439<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.48<br />value: 2.0624<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.49<br />value: 2.0811<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.50<br />value: 2.1000<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.51<br />value: 2.1191<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.52<br />value: 2.1384<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.53<br />value: 2.1579<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.54<br />value: 2.1776<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.55<br />value: 2.1975<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.56<br />value: 2.2176<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.57<br />value: 2.2379<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.58<br />value: 2.2584<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.59<br />value: 2.2791<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.60<br />value: 2.3000<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.61<br />value: 2.3211<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.62<br />value: 2.3424<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.63<br />value: 2.3639<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.64<br />value: 2.3856<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.65<br />value: 2.4075<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.66<br />value: 2.4296<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.67<br />value: 2.4519<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.68<br />value: 2.4744<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.69<br />value: 2.4971<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.70<br />value: 2.5200<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.71<br />value: 2.5431<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.72<br />value: 2.5664<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.73<br />value: 2.5899<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.74<br />value: 2.6136<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.75<br />value: 2.6375<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.76<br />value: 2.6616<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.77<br />value: 2.6859<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.78<br />value: 2.7104<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.79<br />value: 2.7351<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.80<br />value: 2.7600<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.81<br />value: 2.7851<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.82<br />value: 2.8104<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.83<br />value: 2.8359<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.84<br />value: 2.8616<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.85<br />value: 2.8875<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.86<br />value: 2.9136<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.87<br />value: 2.9399<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.88<br />value: 2.9664<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.89<br />value: 2.9931<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.90<br />value: 3.0200<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.91<br />value: 3.0471<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.92<br />value: 3.0744<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.93<br />value: 3.1019<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.94<br />value: 3.1296<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.95<br />value: 3.1575<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.96<br />value: 3.1856<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.97<br />value: 3.2139<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.98<br />value: 3.2424<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  1.99<br />value: 3.2711<br />Function: Loss + Penalty<br />Lambda: 0.9","b:  2.00<br />value: 3.3000<br />Function: Loss + Penalty<br />Lambda: 0.9"],"frame":"0.9","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.45,0.441,0.432,0.423,0.414,0.405,0.396,0.387,0.378,0.369,0.36,0.351,0.342,0.333,0.324,0.315,0.306,0.297,0.288,0.279,0.27,0.261,0.252,0.243,0.234,0.225,0.216,0.207,0.198,0.189,0.18,0.171,0.162,0.153,0.144,0.135,0.126,0.117,0.108,0.099,0.09,0.081,0.072,0.063,0.054,0.045,0.036,0.027,0.018,0.00900000000000001,0,0.00900000000000001,0.018,0.027,0.036,0.045,0.054,0.0630000000000001,0.072,0.081,0.09,0.099,0.108,0.117,0.126,0.135,0.144,0.153,0.162,0.171,0.18,0.189,0.198,0.207,0.216,0.225,0.234,0.243,0.252,0.261,0.27,0.279,0.288,0.297,0.306,0.315,0.324,0.333,0.342,0.351,0.36,0.369,0.378,0.387,0.396,0.405,0.414,0.423,0.432,0.441,0.45,0.459,0.468,0.477,0.486,0.495,0.504,0.513,0.522,0.531,0.54,0.549,0.558,0.567,0.576,0.585,0.594,0.603,0.612,0.621,0.63,0.639,0.648,0.657,0.666,0.675,0.684,0.693,0.702,0.711,0.72,0.729,0.738,0.747,0.756,0.765,0.774,0.783,0.792,0.801,0.81,0.819,0.828,0.837,0.846,0.855,0.864,0.873,0.882,0.891,0.9,0.909,0.918,0.927,0.936,0.945,0.954,0.963,0.972,0.981,0.99,0.999,1.008,1.017,1.026,1.035,1.044,1.053,1.062,1.071,1.08,1.089,1.098,1.107,1.116,1.125,1.134,1.143,1.152,1.161,1.17,1.179,1.188,1.197,1.206,1.215,1.224,1.233,1.242,1.251,1.26,1.269,1.278,1.287,1.296,1.305,1.314,1.323,1.332,1.341,1.35,1.359,1.368,1.377,1.386,1.395,1.404,1.413,1.422,1.431,1.44,1.449,1.458,1.467,1.476,1.485,1.494,1.503,1.512,1.521,1.53,1.539,1.548,1.557,1.566,1.575,1.584,1.593,1.602,1.611,1.62,1.629,1.638,1.647,1.656,1.665,1.674,1.683,1.692,1.701,1.71,1.719,1.728,1.737,1.746,1.755,1.764,1.773,1.782,1.791,1.8],"text":["b: -0.50<br />value: 0.4500<br />Function: Penalty<br />Lambda: 0.9","b: -0.49<br />value: 0.4410<br />Function: Penalty<br />Lambda: 0.9","b: -0.48<br />value: 0.4320<br />Function: Penalty<br />Lambda: 0.9","b: -0.47<br />value: 0.4230<br />Function: Penalty<br />Lambda: 0.9","b: -0.46<br />value: 0.4140<br />Function: Penalty<br />Lambda: 0.9","b: -0.45<br />value: 0.4050<br />Function: Penalty<br />Lambda: 0.9","b: -0.44<br />value: 0.3960<br />Function: Penalty<br />Lambda: 0.9","b: -0.43<br />value: 0.3870<br />Function: Penalty<br />Lambda: 0.9","b: -0.42<br />value: 0.3780<br />Function: Penalty<br />Lambda: 0.9","b: -0.41<br />value: 0.3690<br />Function: Penalty<br />Lambda: 0.9","b: -0.40<br />value: 0.3600<br />Function: Penalty<br />Lambda: 0.9","b: -0.39<br />value: 0.3510<br />Function: Penalty<br />Lambda: 0.9","b: -0.38<br />value: 0.3420<br />Function: Penalty<br />Lambda: 0.9","b: -0.37<br />value: 0.3330<br />Function: Penalty<br />Lambda: 0.9","b: -0.36<br />value: 0.3240<br />Function: Penalty<br />Lambda: 0.9","b: -0.35<br />value: 0.3150<br />Function: Penalty<br />Lambda: 0.9","b: -0.34<br />value: 0.3060<br />Function: Penalty<br />Lambda: 0.9","b: -0.33<br />value: 0.2970<br />Function: Penalty<br />Lambda: 0.9","b: -0.32<br />value: 0.2880<br />Function: Penalty<br />Lambda: 0.9","b: -0.31<br />value: 0.2790<br />Function: Penalty<br />Lambda: 0.9","b: -0.30<br />value: 0.2700<br />Function: Penalty<br />Lambda: 0.9","b: -0.29<br />value: 0.2610<br />Function: Penalty<br />Lambda: 0.9","b: -0.28<br />value: 0.2520<br />Function: Penalty<br />Lambda: 0.9","b: -0.27<br />value: 0.2430<br />Function: Penalty<br />Lambda: 0.9","b: -0.26<br />value: 0.2340<br />Function: Penalty<br />Lambda: 0.9","b: -0.25<br />value: 0.2250<br />Function: Penalty<br />Lambda: 0.9","b: -0.24<br />value: 0.2160<br />Function: Penalty<br />Lambda: 0.9","b: -0.23<br />value: 0.2070<br />Function: Penalty<br />Lambda: 0.9","b: -0.22<br />value: 0.1980<br />Function: Penalty<br />Lambda: 0.9","b: -0.21<br />value: 0.1890<br />Function: Penalty<br />Lambda: 0.9","b: -0.20<br />value: 0.1800<br />Function: Penalty<br />Lambda: 0.9","b: -0.19<br />value: 0.1710<br />Function: Penalty<br />Lambda: 0.9","b: -0.18<br />value: 0.1620<br />Function: Penalty<br />Lambda: 0.9","b: -0.17<br />value: 0.1530<br />Function: Penalty<br />Lambda: 0.9","b: -0.16<br />value: 0.1440<br />Function: Penalty<br />Lambda: 0.9","b: -0.15<br />value: 0.1350<br />Function: Penalty<br />Lambda: 0.9","b: -0.14<br />value: 0.1260<br />Function: Penalty<br />Lambda: 0.9","b: -0.13<br />value: 0.1170<br />Function: Penalty<br />Lambda: 0.9","b: -0.12<br />value: 0.1080<br />Function: Penalty<br />Lambda: 0.9","b: -0.11<br />value: 0.0990<br />Function: Penalty<br />Lambda: 0.9","b: -0.10<br />value: 0.0900<br />Function: Penalty<br />Lambda: 0.9","b: -0.09<br />value: 0.0810<br />Function: Penalty<br />Lambda: 0.9","b: -0.08<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.9","b: -0.07<br />value: 0.0630<br />Function: Penalty<br />Lambda: 0.9","b: -0.06<br />value: 0.0540<br />Function: Penalty<br />Lambda: 0.9","b: -0.05<br />value: 0.0450<br />Function: Penalty<br />Lambda: 0.9","b: -0.04<br />value: 0.0360<br />Function: Penalty<br />Lambda: 0.9","b: -0.03<br />value: 0.0270<br />Function: Penalty<br />Lambda: 0.9","b: -0.02<br />value: 0.0180<br />Function: Penalty<br />Lambda: 0.9","b: -0.01<br />value: 0.0090<br />Function: Penalty<br />Lambda: 0.9","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 0.9","b:  0.01<br />value: 0.0090<br />Function: Penalty<br />Lambda: 0.9","b:  0.02<br />value: 0.0180<br />Function: Penalty<br />Lambda: 0.9","b:  0.03<br />value: 0.0270<br />Function: Penalty<br />Lambda: 0.9","b:  0.04<br />value: 0.0360<br />Function: Penalty<br />Lambda: 0.9","b:  0.05<br />value: 0.0450<br />Function: Penalty<br />Lambda: 0.9","b:  0.06<br />value: 0.0540<br />Function: Penalty<br />Lambda: 0.9","b:  0.07<br />value: 0.0630<br />Function: Penalty<br />Lambda: 0.9","b:  0.08<br />value: 0.0720<br />Function: Penalty<br />Lambda: 0.9","b:  0.09<br />value: 0.0810<br />Function: Penalty<br />Lambda: 0.9","b:  0.10<br />value: 0.0900<br />Function: Penalty<br />Lambda: 0.9","b:  0.11<br />value: 0.0990<br />Function: Penalty<br />Lambda: 0.9","b:  0.12<br />value: 0.1080<br />Function: Penalty<br />Lambda: 0.9","b:  0.13<br />value: 0.1170<br />Function: Penalty<br />Lambda: 0.9","b:  0.14<br />value: 0.1260<br />Function: Penalty<br />Lambda: 0.9","b:  0.15<br />value: 0.1350<br />Function: Penalty<br />Lambda: 0.9","b:  0.16<br />value: 0.1440<br />Function: Penalty<br />Lambda: 0.9","b:  0.17<br />value: 0.1530<br />Function: Penalty<br />Lambda: 0.9","b:  0.18<br />value: 0.1620<br />Function: Penalty<br />Lambda: 0.9","b:  0.19<br />value: 0.1710<br />Function: Penalty<br />Lambda: 0.9","b:  0.20<br />value: 0.1800<br />Function: Penalty<br />Lambda: 0.9","b:  0.21<br />value: 0.1890<br />Function: Penalty<br />Lambda: 0.9","b:  0.22<br />value: 0.1980<br />Function: Penalty<br />Lambda: 0.9","b:  0.23<br />value: 0.2070<br />Function: Penalty<br />Lambda: 0.9","b:  0.24<br />value: 0.2160<br />Function: Penalty<br />Lambda: 0.9","b:  0.25<br />value: 0.2250<br />Function: Penalty<br />Lambda: 0.9","b:  0.26<br />value: 0.2340<br />Function: Penalty<br />Lambda: 0.9","b:  0.27<br />value: 0.2430<br />Function: Penalty<br />Lambda: 0.9","b:  0.28<br />value: 0.2520<br />Function: Penalty<br />Lambda: 0.9","b:  0.29<br />value: 0.2610<br />Function: Penalty<br />Lambda: 0.9","b:  0.30<br />value: 0.2700<br />Function: Penalty<br />Lambda: 0.9","b:  0.31<br />value: 0.2790<br />Function: Penalty<br />Lambda: 0.9","b:  0.32<br />value: 0.2880<br />Function: Penalty<br />Lambda: 0.9","b:  0.33<br />value: 0.2970<br />Function: Penalty<br />Lambda: 0.9","b:  0.34<br />value: 0.3060<br />Function: Penalty<br />Lambda: 0.9","b:  0.35<br />value: 0.3150<br />Function: Penalty<br />Lambda: 0.9","b:  0.36<br />value: 0.3240<br />Function: Penalty<br />Lambda: 0.9","b:  0.37<br />value: 0.3330<br />Function: Penalty<br />Lambda: 0.9","b:  0.38<br />value: 0.3420<br />Function: Penalty<br />Lambda: 0.9","b:  0.39<br />value: 0.3510<br />Function: Penalty<br />Lambda: 0.9","b:  0.40<br />value: 0.3600<br />Function: Penalty<br />Lambda: 0.9","b:  0.41<br />value: 0.3690<br />Function: Penalty<br />Lambda: 0.9","b:  0.42<br />value: 0.3780<br />Function: Penalty<br />Lambda: 0.9","b:  0.43<br />value: 0.3870<br />Function: Penalty<br />Lambda: 0.9","b:  0.44<br />value: 0.3960<br />Function: Penalty<br />Lambda: 0.9","b:  0.45<br />value: 0.4050<br />Function: Penalty<br />Lambda: 0.9","b:  0.46<br />value: 0.4140<br />Function: Penalty<br />Lambda: 0.9","b:  0.47<br />value: 0.4230<br />Function: Penalty<br />Lambda: 0.9","b:  0.48<br />value: 0.4320<br />Function: Penalty<br />Lambda: 0.9","b:  0.49<br />value: 0.4410<br />Function: Penalty<br />Lambda: 0.9","b:  0.50<br />value: 0.4500<br />Function: Penalty<br />Lambda: 0.9","b:  0.51<br />value: 0.4590<br />Function: Penalty<br />Lambda: 0.9","b:  0.52<br />value: 0.4680<br />Function: Penalty<br />Lambda: 0.9","b:  0.53<br />value: 0.4770<br />Function: Penalty<br />Lambda: 0.9","b:  0.54<br />value: 0.4860<br />Function: Penalty<br />Lambda: 0.9","b:  0.55<br />value: 0.4950<br />Function: Penalty<br />Lambda: 0.9","b:  0.56<br />value: 0.5040<br />Function: Penalty<br />Lambda: 0.9","b:  0.57<br />value: 0.5130<br />Function: Penalty<br />Lambda: 0.9","b:  0.58<br />value: 0.5220<br />Function: Penalty<br />Lambda: 0.9","b:  0.59<br />value: 0.5310<br />Function: Penalty<br />Lambda: 0.9","b:  0.60<br />value: 0.5400<br />Function: Penalty<br />Lambda: 0.9","b:  0.61<br />value: 0.5490<br />Function: Penalty<br />Lambda: 0.9","b:  0.62<br />value: 0.5580<br />Function: Penalty<br />Lambda: 0.9","b:  0.63<br />value: 0.5670<br />Function: Penalty<br />Lambda: 0.9","b:  0.64<br />value: 0.5760<br />Function: Penalty<br />Lambda: 0.9","b:  0.65<br />value: 0.5850<br />Function: Penalty<br />Lambda: 0.9","b:  0.66<br />value: 0.5940<br />Function: Penalty<br />Lambda: 0.9","b:  0.67<br />value: 0.6030<br />Function: Penalty<br />Lambda: 0.9","b:  0.68<br />value: 0.6120<br />Function: Penalty<br />Lambda: 0.9","b:  0.69<br />value: 0.6210<br />Function: Penalty<br />Lambda: 0.9","b:  0.70<br />value: 0.6300<br />Function: Penalty<br />Lambda: 0.9","b:  0.71<br />value: 0.6390<br />Function: Penalty<br />Lambda: 0.9","b:  0.72<br />value: 0.6480<br />Function: Penalty<br />Lambda: 0.9","b:  0.73<br />value: 0.6570<br />Function: Penalty<br />Lambda: 0.9","b:  0.74<br />value: 0.6660<br />Function: Penalty<br />Lambda: 0.9","b:  0.75<br />value: 0.6750<br />Function: Penalty<br />Lambda: 0.9","b:  0.76<br />value: 0.6840<br />Function: Penalty<br />Lambda: 0.9","b:  0.77<br />value: 0.6930<br />Function: Penalty<br />Lambda: 0.9","b:  0.78<br />value: 0.7020<br />Function: Penalty<br />Lambda: 0.9","b:  0.79<br />value: 0.7110<br />Function: Penalty<br />Lambda: 0.9","b:  0.80<br />value: 0.7200<br />Function: Penalty<br />Lambda: 0.9","b:  0.81<br />value: 0.7290<br />Function: Penalty<br />Lambda: 0.9","b:  0.82<br />value: 0.7380<br />Function: Penalty<br />Lambda: 0.9","b:  0.83<br />value: 0.7470<br />Function: Penalty<br />Lambda: 0.9","b:  0.84<br />value: 0.7560<br />Function: Penalty<br />Lambda: 0.9","b:  0.85<br />value: 0.7650<br />Function: Penalty<br />Lambda: 0.9","b:  0.86<br />value: 0.7740<br />Function: Penalty<br />Lambda: 0.9","b:  0.87<br />value: 0.7830<br />Function: Penalty<br />Lambda: 0.9","b:  0.88<br />value: 0.7920<br />Function: Penalty<br />Lambda: 0.9","b:  0.89<br />value: 0.8010<br />Function: Penalty<br />Lambda: 0.9","b:  0.90<br />value: 0.8100<br />Function: Penalty<br />Lambda: 0.9","b:  0.91<br />value: 0.8190<br />Function: Penalty<br />Lambda: 0.9","b:  0.92<br />value: 0.8280<br />Function: Penalty<br />Lambda: 0.9","b:  0.93<br />value: 0.8370<br />Function: Penalty<br />Lambda: 0.9","b:  0.94<br />value: 0.8460<br />Function: Penalty<br />Lambda: 0.9","b:  0.95<br />value: 0.8550<br />Function: Penalty<br />Lambda: 0.9","b:  0.96<br />value: 0.8640<br />Function: Penalty<br />Lambda: 0.9","b:  0.97<br />value: 0.8730<br />Function: Penalty<br />Lambda: 0.9","b:  0.98<br />value: 0.8820<br />Function: Penalty<br />Lambda: 0.9","b:  0.99<br />value: 0.8910<br />Function: Penalty<br />Lambda: 0.9","b:  1.00<br />value: 0.9000<br />Function: Penalty<br />Lambda: 0.9","b:  1.01<br />value: 0.9090<br />Function: Penalty<br />Lambda: 0.9","b:  1.02<br />value: 0.9180<br />Function: Penalty<br />Lambda: 0.9","b:  1.03<br />value: 0.9270<br />Function: Penalty<br />Lambda: 0.9","b:  1.04<br />value: 0.9360<br />Function: Penalty<br />Lambda: 0.9","b:  1.05<br />value: 0.9450<br />Function: Penalty<br />Lambda: 0.9","b:  1.06<br />value: 0.9540<br />Function: Penalty<br />Lambda: 0.9","b:  1.07<br />value: 0.9630<br />Function: Penalty<br />Lambda: 0.9","b:  1.08<br />value: 0.9720<br />Function: Penalty<br />Lambda: 0.9","b:  1.09<br />value: 0.9810<br />Function: Penalty<br />Lambda: 0.9","b:  1.10<br />value: 0.9900<br />Function: Penalty<br />Lambda: 0.9","b:  1.11<br />value: 0.9990<br />Function: Penalty<br />Lambda: 0.9","b:  1.12<br />value: 1.0080<br />Function: Penalty<br />Lambda: 0.9","b:  1.13<br />value: 1.0170<br />Function: Penalty<br />Lambda: 0.9","b:  1.14<br />value: 1.0260<br />Function: Penalty<br />Lambda: 0.9","b:  1.15<br />value: 1.0350<br />Function: Penalty<br />Lambda: 0.9","b:  1.16<br />value: 1.0440<br />Function: Penalty<br />Lambda: 0.9","b:  1.17<br />value: 1.0530<br />Function: Penalty<br />Lambda: 0.9","b:  1.18<br />value: 1.0620<br />Function: Penalty<br />Lambda: 0.9","b:  1.19<br />value: 1.0710<br />Function: Penalty<br />Lambda: 0.9","b:  1.20<br />value: 1.0800<br />Function: Penalty<br />Lambda: 0.9","b:  1.21<br />value: 1.0890<br />Function: Penalty<br />Lambda: 0.9","b:  1.22<br />value: 1.0980<br />Function: Penalty<br />Lambda: 0.9","b:  1.23<br />value: 1.1070<br />Function: Penalty<br />Lambda: 0.9","b:  1.24<br />value: 1.1160<br />Function: Penalty<br />Lambda: 0.9","b:  1.25<br />value: 1.1250<br />Function: Penalty<br />Lambda: 0.9","b:  1.26<br />value: 1.1340<br />Function: Penalty<br />Lambda: 0.9","b:  1.27<br />value: 1.1430<br />Function: Penalty<br />Lambda: 0.9","b:  1.28<br />value: 1.1520<br />Function: Penalty<br />Lambda: 0.9","b:  1.29<br />value: 1.1610<br />Function: Penalty<br />Lambda: 0.9","b:  1.30<br />value: 1.1700<br />Function: Penalty<br />Lambda: 0.9","b:  1.31<br />value: 1.1790<br />Function: Penalty<br />Lambda: 0.9","b:  1.32<br />value: 1.1880<br />Function: Penalty<br />Lambda: 0.9","b:  1.33<br />value: 1.1970<br />Function: Penalty<br />Lambda: 0.9","b:  1.34<br />value: 1.2060<br />Function: Penalty<br />Lambda: 0.9","b:  1.35<br />value: 1.2150<br />Function: Penalty<br />Lambda: 0.9","b:  1.36<br />value: 1.2240<br />Function: Penalty<br />Lambda: 0.9","b:  1.37<br />value: 1.2330<br />Function: Penalty<br />Lambda: 0.9","b:  1.38<br />value: 1.2420<br />Function: Penalty<br />Lambda: 0.9","b:  1.39<br />value: 1.2510<br />Function: Penalty<br />Lambda: 0.9","b:  1.40<br />value: 1.2600<br />Function: Penalty<br />Lambda: 0.9","b:  1.41<br />value: 1.2690<br />Function: Penalty<br />Lambda: 0.9","b:  1.42<br />value: 1.2780<br />Function: Penalty<br />Lambda: 0.9","b:  1.43<br />value: 1.2870<br />Function: Penalty<br />Lambda: 0.9","b:  1.44<br />value: 1.2960<br />Function: Penalty<br />Lambda: 0.9","b:  1.45<br />value: 1.3050<br />Function: Penalty<br />Lambda: 0.9","b:  1.46<br />value: 1.3140<br />Function: Penalty<br />Lambda: 0.9","b:  1.47<br />value: 1.3230<br />Function: Penalty<br />Lambda: 0.9","b:  1.48<br />value: 1.3320<br />Function: Penalty<br />Lambda: 0.9","b:  1.49<br />value: 1.3410<br />Function: Penalty<br />Lambda: 0.9","b:  1.50<br />value: 1.3500<br />Function: Penalty<br />Lambda: 0.9","b:  1.51<br />value: 1.3590<br />Function: Penalty<br />Lambda: 0.9","b:  1.52<br />value: 1.3680<br />Function: Penalty<br />Lambda: 0.9","b:  1.53<br />value: 1.3770<br />Function: Penalty<br />Lambda: 0.9","b:  1.54<br />value: 1.3860<br />Function: Penalty<br />Lambda: 0.9","b:  1.55<br />value: 1.3950<br />Function: Penalty<br />Lambda: 0.9","b:  1.56<br />value: 1.4040<br />Function: Penalty<br />Lambda: 0.9","b:  1.57<br />value: 1.4130<br />Function: Penalty<br />Lambda: 0.9","b:  1.58<br />value: 1.4220<br />Function: Penalty<br />Lambda: 0.9","b:  1.59<br />value: 1.4310<br />Function: Penalty<br />Lambda: 0.9","b:  1.60<br />value: 1.4400<br />Function: Penalty<br />Lambda: 0.9","b:  1.61<br />value: 1.4490<br />Function: Penalty<br />Lambda: 0.9","b:  1.62<br />value: 1.4580<br />Function: Penalty<br />Lambda: 0.9","b:  1.63<br />value: 1.4670<br />Function: Penalty<br />Lambda: 0.9","b:  1.64<br />value: 1.4760<br />Function: Penalty<br />Lambda: 0.9","b:  1.65<br />value: 1.4850<br />Function: Penalty<br />Lambda: 0.9","b:  1.66<br />value: 1.4940<br />Function: Penalty<br />Lambda: 0.9","b:  1.67<br />value: 1.5030<br />Function: Penalty<br />Lambda: 0.9","b:  1.68<br />value: 1.5120<br />Function: Penalty<br />Lambda: 0.9","b:  1.69<br />value: 1.5210<br />Function: Penalty<br />Lambda: 0.9","b:  1.70<br />value: 1.5300<br />Function: Penalty<br />Lambda: 0.9","b:  1.71<br />value: 1.5390<br />Function: Penalty<br />Lambda: 0.9","b:  1.72<br />value: 1.5480<br />Function: Penalty<br />Lambda: 0.9","b:  1.73<br />value: 1.5570<br />Function: Penalty<br />Lambda: 0.9","b:  1.74<br />value: 1.5660<br />Function: Penalty<br />Lambda: 0.9","b:  1.75<br />value: 1.5750<br />Function: Penalty<br />Lambda: 0.9","b:  1.76<br />value: 1.5840<br />Function: Penalty<br />Lambda: 0.9","b:  1.77<br />value: 1.5930<br />Function: Penalty<br />Lambda: 0.9","b:  1.78<br />value: 1.6020<br />Function: Penalty<br />Lambda: 0.9","b:  1.79<br />value: 1.6110<br />Function: Penalty<br />Lambda: 0.9","b:  1.80<br />value: 1.6200<br />Function: Penalty<br />Lambda: 0.9","b:  1.81<br />value: 1.6290<br />Function: Penalty<br />Lambda: 0.9","b:  1.82<br />value: 1.6380<br />Function: Penalty<br />Lambda: 0.9","b:  1.83<br />value: 1.6470<br />Function: Penalty<br />Lambda: 0.9","b:  1.84<br />value: 1.6560<br />Function: Penalty<br />Lambda: 0.9","b:  1.85<br />value: 1.6650<br />Function: Penalty<br />Lambda: 0.9","b:  1.86<br />value: 1.6740<br />Function: Penalty<br />Lambda: 0.9","b:  1.87<br />value: 1.6830<br />Function: Penalty<br />Lambda: 0.9","b:  1.88<br />value: 1.6920<br />Function: Penalty<br />Lambda: 0.9","b:  1.89<br />value: 1.7010<br />Function: Penalty<br />Lambda: 0.9","b:  1.90<br />value: 1.7100<br />Function: Penalty<br />Lambda: 0.9","b:  1.91<br />value: 1.7190<br />Function: Penalty<br />Lambda: 0.9","b:  1.92<br />value: 1.7280<br />Function: Penalty<br />Lambda: 0.9","b:  1.93<br />value: 1.7370<br />Function: Penalty<br />Lambda: 0.9","b:  1.94<br />value: 1.7460<br />Function: Penalty<br />Lambda: 0.9","b:  1.95<br />value: 1.7550<br />Function: Penalty<br />Lambda: 0.9","b:  1.96<br />value: 1.7640<br />Function: Penalty<br />Lambda: 0.9","b:  1.97<br />value: 1.7730<br />Function: Penalty<br />Lambda: 0.9","b:  1.98<br />value: 1.7820<br />Function: Penalty<br />Lambda: 0.9","b:  1.99<br />value: 1.7910<br />Function: Penalty<br />Lambda: 0.9","b:  2.00<br />value: 1.8000<br />Function: Penalty<br />Lambda: 0.9"],"frame":"0.9","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"1","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 1.0","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 1.0","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 1.0","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 1.0","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 1.0","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 1.0","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 1.0","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 1.0","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 1.0","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 1.0","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 1.0","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 1.0","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 1.0","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 1.0","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 1.0","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 1.0","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 1.0","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 1.0","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 1.0","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 1.0","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 1.0","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 1.0","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 1.0","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 1.0","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 1.0","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 1.0","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 1.0","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 1.0","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 1.0","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 1.0","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 1.0","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 1.0","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 1.0","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 1.0","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 1.0","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 1.0","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 1.0","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 1.0","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 1.0","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 1.0","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 1.0","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 1.0","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 1.0","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 1.0","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 1.0","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 1.0","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 1.0","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 1.0","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 1.0","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 1.0","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.0","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.0","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.0","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.0","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.0","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.0","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.0","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.0","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.0","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.0","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.0","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.0","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.0","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.0","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.0","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.0","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.0","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.0","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.0","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.0","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.0","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.0","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.0","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.0","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.0","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.0","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.0","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.0","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.0","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.0","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.0","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.0","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.0","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.0","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.0","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.0","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.0","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.0","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.0","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.0","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.0","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.0","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.0","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.0","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.0","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.0","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.0","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.0","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.0","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.0","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.0","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.0","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.0","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.0","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.0","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.0","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.0","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.0","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.0","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.0","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.0","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.0","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.0","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.0","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.0","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.0","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.0","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.0","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.0","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.0","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.0","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.0","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.0","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.0","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.0","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.0","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.0","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.0","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.0","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.0","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.0","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.0","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.0","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.0","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.0","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.0","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.0","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.0","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.0","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.0","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.0","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.0","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.0","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.0","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.0","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.0","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.0","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.0","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.0","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.0","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 1.0","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.0","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.0","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.0","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.0","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.0","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.0","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.0","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.0","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.0","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.0","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.0","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.0","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.0","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.0","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.0","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.0","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.0","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.0","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.0","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.0","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.0","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.0","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.0","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.0","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.0","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.0","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.0","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.0","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.0","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.0","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.0","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.0","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.0","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.0","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.0","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.0","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.0","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.0","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.0","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.0","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.0","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.0","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.0","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.0","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.0","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.0","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.0","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.0","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.0","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.0","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.0","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.0","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.0","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.0","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.0","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.0","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.0","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.0","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.0","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.0","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.0","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.0","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.0","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.0","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.0","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.0","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.0","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.0","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.0","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.0","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.0","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.0","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.0","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.0","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.0","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.0","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.0","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.0","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.0","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.0","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.0","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.0","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.0","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.0","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.0","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.0","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.0","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.0","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.0","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.0","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.0","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.0","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.0","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.0","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.0","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.0","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.0","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.0","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.0","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.0"],"frame":"1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.25,3.2101,3.1704,3.1309,3.0916,3.0525,3.0136,2.9749,2.9364,2.8981,2.86,2.8221,2.7844,2.7469,2.7096,2.6725,2.6356,2.5989,2.5624,2.5261,2.49,2.4541,2.4184,2.3829,2.3476,2.3125,2.2776,2.2429,2.2084,2.1741,2.14,2.1061,2.0724,2.0389,2.0056,1.9725,1.9396,1.9069,1.8744,1.8421,1.81,1.7781,1.7464,1.7149,1.6836,1.6525,1.6216,1.5909,1.5604,1.5301,1.5,1.4901,1.4804,1.4709,1.4616,1.4525,1.4436,1.4349,1.4264,1.4181,1.41,1.4021,1.3944,1.3869,1.3796,1.3725,1.3656,1.3589,1.3524,1.3461,1.34,1.3341,1.3284,1.3229,1.3176,1.3125,1.3076,1.3029,1.2984,1.2941,1.29,1.2861,1.2824,1.2789,1.2756,1.2725,1.2696,1.2669,1.2644,1.2621,1.26,1.2581,1.2564,1.2549,1.2536,1.2525,1.2516,1.2509,1.2504,1.2501,1.25,1.2501,1.2504,1.2509,1.2516,1.2525,1.2536,1.2549,1.2564,1.2581,1.26,1.2621,1.2644,1.2669,1.2696,1.2725,1.2756,1.2789,1.2824,1.2861,1.29,1.2941,1.2984,1.3029,1.3076,1.3125,1.3176,1.3229,1.3284,1.3341,1.34,1.3461,1.3524,1.3589,1.3656,1.3725,1.3796,1.3869,1.3944,1.4021,1.41,1.4181,1.4264,1.4349,1.4436,1.4525,1.4616,1.4709,1.4804,1.4901,1.5,1.5101,1.5204,1.5309,1.5416,1.5525,1.5636,1.5749,1.5864,1.5981,1.61,1.6221,1.6344,1.6469,1.6596,1.6725,1.6856,1.6989,1.7124,1.7261,1.74,1.7541,1.7684,1.7829,1.7976,1.8125,1.8276,1.8429,1.8584,1.8741,1.89,1.9061,1.9224,1.9389,1.9556,1.9725,1.9896,2.0069,2.0244,2.0421,2.06,2.0781,2.0964,2.1149,2.1336,2.1525,2.1716,2.1909,2.2104,2.2301,2.25,2.2701,2.2904,2.3109,2.3316,2.3525,2.3736,2.3949,2.4164,2.4381,2.46,2.4821,2.5044,2.5269,2.5496,2.5725,2.5956,2.6189,2.6424,2.6661,2.69,2.7141,2.7384,2.7629,2.7876,2.8125,2.8376,2.8629,2.8884,2.9141,2.94,2.9661,2.9924,3.0189,3.0456,3.0725,3.0996,3.1269,3.1544,3.1821,3.21,3.2381,3.2664,3.2949,3.3236,3.3525,3.3816,3.4109,3.4404,3.4701,3.5],"text":["b: -0.50<br />value: 3.2500<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.49<br />value: 3.2101<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.48<br />value: 3.1704<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.47<br />value: 3.1309<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.46<br />value: 3.0916<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.45<br />value: 3.0525<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.44<br />value: 3.0136<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.43<br />value: 2.9749<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.42<br />value: 2.9364<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.41<br />value: 2.8981<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.40<br />value: 2.8600<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.39<br />value: 2.8221<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.38<br />value: 2.7844<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.37<br />value: 2.7469<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.36<br />value: 2.7096<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.35<br />value: 2.6725<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.34<br />value: 2.6356<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.33<br />value: 2.5989<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.32<br />value: 2.5624<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.31<br />value: 2.5261<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.30<br />value: 2.4900<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.29<br />value: 2.4541<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.28<br />value: 2.4184<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.27<br />value: 2.3829<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.26<br />value: 2.3476<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.25<br />value: 2.3125<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.24<br />value: 2.2776<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.23<br />value: 2.2429<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.22<br />value: 2.2084<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.21<br />value: 2.1741<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.20<br />value: 2.1400<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.19<br />value: 2.1061<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.18<br />value: 2.0724<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.17<br />value: 2.0389<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.16<br />value: 2.0056<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.15<br />value: 1.9725<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.14<br />value: 1.9396<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.13<br />value: 1.9069<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.12<br />value: 1.8744<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.11<br />value: 1.8421<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.10<br />value: 1.8100<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.09<br />value: 1.7781<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.08<br />value: 1.7464<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.07<br />value: 1.7149<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.06<br />value: 1.6836<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.05<br />value: 1.6525<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.04<br />value: 1.6216<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.03<br />value: 1.5909<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.02<br />value: 1.5604<br />Function: Loss + Penalty<br />Lambda: 1.0","b: -0.01<br />value: 1.5301<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.01<br />value: 1.4901<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.02<br />value: 1.4804<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.03<br />value: 1.4709<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.04<br />value: 1.4616<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.05<br />value: 1.4525<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.06<br />value: 1.4436<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.07<br />value: 1.4349<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.08<br />value: 1.4264<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.09<br />value: 1.4181<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.10<br />value: 1.4100<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.11<br />value: 1.4021<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.12<br />value: 1.3944<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.13<br />value: 1.3869<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.14<br />value: 1.3796<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.15<br />value: 1.3725<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.16<br />value: 1.3656<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.17<br />value: 1.3589<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.18<br />value: 1.3524<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.19<br />value: 1.3461<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.20<br />value: 1.3400<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.21<br />value: 1.3341<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.22<br />value: 1.3284<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.23<br />value: 1.3229<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.24<br />value: 1.3176<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.25<br />value: 1.3125<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.26<br />value: 1.3076<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.27<br />value: 1.3029<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.28<br />value: 1.2984<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.29<br />value: 1.2941<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.30<br />value: 1.2900<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.31<br />value: 1.2861<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.32<br />value: 1.2824<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.33<br />value: 1.2789<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.34<br />value: 1.2756<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.35<br />value: 1.2725<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.36<br />value: 1.2696<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.37<br />value: 1.2669<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.38<br />value: 1.2644<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.39<br />value: 1.2621<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.40<br />value: 1.2600<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.41<br />value: 1.2581<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.42<br />value: 1.2564<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.43<br />value: 1.2549<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.44<br />value: 1.2536<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.45<br />value: 1.2525<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.46<br />value: 1.2516<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.47<br />value: 1.2509<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.48<br />value: 1.2504<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.49<br />value: 1.2501<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.50<br />value: 1.2500<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.51<br />value: 1.2501<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.52<br />value: 1.2504<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.53<br />value: 1.2509<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.54<br />value: 1.2516<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.55<br />value: 1.2525<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.56<br />value: 1.2536<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.57<br />value: 1.2549<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.58<br />value: 1.2564<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.59<br />value: 1.2581<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.60<br />value: 1.2600<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.61<br />value: 1.2621<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.62<br />value: 1.2644<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.63<br />value: 1.2669<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.64<br />value: 1.2696<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.65<br />value: 1.2725<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.66<br />value: 1.2756<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.67<br />value: 1.2789<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.68<br />value: 1.2824<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.69<br />value: 1.2861<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.70<br />value: 1.2900<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.71<br />value: 1.2941<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.72<br />value: 1.2984<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.73<br />value: 1.3029<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.74<br />value: 1.3076<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.75<br />value: 1.3125<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.76<br />value: 1.3176<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.77<br />value: 1.3229<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.78<br />value: 1.3284<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.79<br />value: 1.3341<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.80<br />value: 1.3400<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.81<br />value: 1.3461<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.82<br />value: 1.3524<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.83<br />value: 1.3589<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.84<br />value: 1.3656<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.85<br />value: 1.3725<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.86<br />value: 1.3796<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.87<br />value: 1.3869<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.88<br />value: 1.3944<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.89<br />value: 1.4021<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.90<br />value: 1.4100<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.91<br />value: 1.4181<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.92<br />value: 1.4264<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.93<br />value: 1.4349<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.94<br />value: 1.4436<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.95<br />value: 1.4525<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.96<br />value: 1.4616<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.97<br />value: 1.4709<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.98<br />value: 1.4804<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  0.99<br />value: 1.4901<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.01<br />value: 1.5101<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.02<br />value: 1.5204<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.03<br />value: 1.5309<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.04<br />value: 1.5416<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.05<br />value: 1.5525<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.06<br />value: 1.5636<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.07<br />value: 1.5749<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.08<br />value: 1.5864<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.09<br />value: 1.5981<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.10<br />value: 1.6100<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.11<br />value: 1.6221<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.12<br />value: 1.6344<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.13<br />value: 1.6469<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.14<br />value: 1.6596<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.15<br />value: 1.6725<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.16<br />value: 1.6856<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.17<br />value: 1.6989<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.18<br />value: 1.7124<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.19<br />value: 1.7261<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.20<br />value: 1.7400<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.21<br />value: 1.7541<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.22<br />value: 1.7684<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.23<br />value: 1.7829<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.24<br />value: 1.7976<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.25<br />value: 1.8125<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.26<br />value: 1.8276<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.27<br />value: 1.8429<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.28<br />value: 1.8584<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.29<br />value: 1.8741<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.30<br />value: 1.8900<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.31<br />value: 1.9061<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.32<br />value: 1.9224<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.33<br />value: 1.9389<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.34<br />value: 1.9556<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.35<br />value: 1.9725<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.36<br />value: 1.9896<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.37<br />value: 2.0069<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.38<br />value: 2.0244<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.39<br />value: 2.0421<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.40<br />value: 2.0600<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.41<br />value: 2.0781<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.42<br />value: 2.0964<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.43<br />value: 2.1149<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.44<br />value: 2.1336<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.45<br />value: 2.1525<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.46<br />value: 2.1716<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.47<br />value: 2.1909<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.48<br />value: 2.2104<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.49<br />value: 2.2301<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.50<br />value: 2.2500<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.51<br />value: 2.2701<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.52<br />value: 2.2904<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.53<br />value: 2.3109<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.54<br />value: 2.3316<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.55<br />value: 2.3525<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.56<br />value: 2.3736<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.57<br />value: 2.3949<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.58<br />value: 2.4164<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.59<br />value: 2.4381<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.60<br />value: 2.4600<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.61<br />value: 2.4821<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.62<br />value: 2.5044<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.63<br />value: 2.5269<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.64<br />value: 2.5496<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.65<br />value: 2.5725<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.66<br />value: 2.5956<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.67<br />value: 2.6189<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.68<br />value: 2.6424<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.69<br />value: 2.6661<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.70<br />value: 2.6900<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.71<br />value: 2.7141<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.72<br />value: 2.7384<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.73<br />value: 2.7629<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.74<br />value: 2.7876<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.75<br />value: 2.8125<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.76<br />value: 2.8376<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.77<br />value: 2.8629<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.78<br />value: 2.8884<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.79<br />value: 2.9141<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.80<br />value: 2.9400<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.81<br />value: 2.9661<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.82<br />value: 2.9924<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.83<br />value: 3.0189<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.84<br />value: 3.0456<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.85<br />value: 3.0725<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.86<br />value: 3.0996<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.87<br />value: 3.1269<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.88<br />value: 3.1544<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.89<br />value: 3.1821<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.90<br />value: 3.2100<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.91<br />value: 3.2381<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.92<br />value: 3.2664<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.93<br />value: 3.2949<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.94<br />value: 3.3236<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.95<br />value: 3.3525<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.96<br />value: 3.3816<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.97<br />value: 3.4109<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.98<br />value: 3.4404<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  1.99<br />value: 3.4701<br />Function: Loss + Penalty<br />Lambda: 1.0","b:  2.00<br />value: 3.5000<br />Function: Loss + Penalty<br />Lambda: 1.0"],"frame":"1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.5,0.49,0.48,0.47,0.46,0.45,0.44,0.43,0.42,0.41,0.4,0.39,0.38,0.37,0.36,0.35,0.34,0.33,0.32,0.31,0.3,0.29,0.28,0.27,0.26,0.25,0.24,0.23,0.22,0.21,0.2,0.19,0.18,0.17,0.16,0.15,0.14,0.13,0.12,0.11,0.1,0.09,0.08,0.07,0.06,0.05,0.04,0.03,0.02,0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"text":["b: -0.50<br />value: 0.5000<br />Function: Penalty<br />Lambda: 1.0","b: -0.49<br />value: 0.4900<br />Function: Penalty<br />Lambda: 1.0","b: -0.48<br />value: 0.4800<br />Function: Penalty<br />Lambda: 1.0","b: -0.47<br />value: 0.4700<br />Function: Penalty<br />Lambda: 1.0","b: -0.46<br />value: 0.4600<br />Function: Penalty<br />Lambda: 1.0","b: -0.45<br />value: 0.4500<br />Function: Penalty<br />Lambda: 1.0","b: -0.44<br />value: 0.4400<br />Function: Penalty<br />Lambda: 1.0","b: -0.43<br />value: 0.4300<br />Function: Penalty<br />Lambda: 1.0","b: -0.42<br />value: 0.4200<br />Function: Penalty<br />Lambda: 1.0","b: -0.41<br />value: 0.4100<br />Function: Penalty<br />Lambda: 1.0","b: -0.40<br />value: 0.4000<br />Function: Penalty<br />Lambda: 1.0","b: -0.39<br />value: 0.3900<br />Function: Penalty<br />Lambda: 1.0","b: -0.38<br />value: 0.3800<br />Function: Penalty<br />Lambda: 1.0","b: -0.37<br />value: 0.3700<br />Function: Penalty<br />Lambda: 1.0","b: -0.36<br />value: 0.3600<br />Function: Penalty<br />Lambda: 1.0","b: -0.35<br />value: 0.3500<br />Function: Penalty<br />Lambda: 1.0","b: -0.34<br />value: 0.3400<br />Function: Penalty<br />Lambda: 1.0","b: -0.33<br />value: 0.3300<br />Function: Penalty<br />Lambda: 1.0","b: -0.32<br />value: 0.3200<br />Function: Penalty<br />Lambda: 1.0","b: -0.31<br />value: 0.3100<br />Function: Penalty<br />Lambda: 1.0","b: -0.30<br />value: 0.3000<br />Function: Penalty<br />Lambda: 1.0","b: -0.29<br />value: 0.2900<br />Function: Penalty<br />Lambda: 1.0","b: -0.28<br />value: 0.2800<br />Function: Penalty<br />Lambda: 1.0","b: -0.27<br />value: 0.2700<br />Function: Penalty<br />Lambda: 1.0","b: -0.26<br />value: 0.2600<br />Function: Penalty<br />Lambda: 1.0","b: -0.25<br />value: 0.2500<br />Function: Penalty<br />Lambda: 1.0","b: -0.24<br />value: 0.2400<br />Function: Penalty<br />Lambda: 1.0","b: -0.23<br />value: 0.2300<br />Function: Penalty<br />Lambda: 1.0","b: -0.22<br />value: 0.2200<br />Function: Penalty<br />Lambda: 1.0","b: -0.21<br />value: 0.2100<br />Function: Penalty<br />Lambda: 1.0","b: -0.20<br />value: 0.2000<br />Function: Penalty<br />Lambda: 1.0","b: -0.19<br />value: 0.1900<br />Function: Penalty<br />Lambda: 1.0","b: -0.18<br />value: 0.1800<br />Function: Penalty<br />Lambda: 1.0","b: -0.17<br />value: 0.1700<br />Function: Penalty<br />Lambda: 1.0","b: -0.16<br />value: 0.1600<br />Function: Penalty<br />Lambda: 1.0","b: -0.15<br />value: 0.1500<br />Function: Penalty<br />Lambda: 1.0","b: -0.14<br />value: 0.1400<br />Function: Penalty<br />Lambda: 1.0","b: -0.13<br />value: 0.1300<br />Function: Penalty<br />Lambda: 1.0","b: -0.12<br />value: 0.1200<br />Function: Penalty<br />Lambda: 1.0","b: -0.11<br />value: 0.1100<br />Function: Penalty<br />Lambda: 1.0","b: -0.10<br />value: 0.1000<br />Function: Penalty<br />Lambda: 1.0","b: -0.09<br />value: 0.0900<br />Function: Penalty<br />Lambda: 1.0","b: -0.08<br />value: 0.0800<br />Function: Penalty<br />Lambda: 1.0","b: -0.07<br />value: 0.0700<br />Function: Penalty<br />Lambda: 1.0","b: -0.06<br />value: 0.0600<br />Function: Penalty<br />Lambda: 1.0","b: -0.05<br />value: 0.0500<br />Function: Penalty<br />Lambda: 1.0","b: -0.04<br />value: 0.0400<br />Function: Penalty<br />Lambda: 1.0","b: -0.03<br />value: 0.0300<br />Function: Penalty<br />Lambda: 1.0","b: -0.02<br />value: 0.0200<br />Function: Penalty<br />Lambda: 1.0","b: -0.01<br />value: 0.0100<br />Function: Penalty<br />Lambda: 1.0","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 1.0","b:  0.01<br />value: 0.0100<br />Function: Penalty<br />Lambda: 1.0","b:  0.02<br />value: 0.0200<br />Function: Penalty<br />Lambda: 1.0","b:  0.03<br />value: 0.0300<br />Function: Penalty<br />Lambda: 1.0","b:  0.04<br />value: 0.0400<br />Function: Penalty<br />Lambda: 1.0","b:  0.05<br />value: 0.0500<br />Function: Penalty<br />Lambda: 1.0","b:  0.06<br />value: 0.0600<br />Function: Penalty<br />Lambda: 1.0","b:  0.07<br />value: 0.0700<br />Function: Penalty<br />Lambda: 1.0","b:  0.08<br />value: 0.0800<br />Function: Penalty<br />Lambda: 1.0","b:  0.09<br />value: 0.0900<br />Function: Penalty<br />Lambda: 1.0","b:  0.10<br />value: 0.1000<br />Function: Penalty<br />Lambda: 1.0","b:  0.11<br />value: 0.1100<br />Function: Penalty<br />Lambda: 1.0","b:  0.12<br />value: 0.1200<br />Function: Penalty<br />Lambda: 1.0","b:  0.13<br />value: 0.1300<br />Function: Penalty<br />Lambda: 1.0","b:  0.14<br />value: 0.1400<br />Function: Penalty<br />Lambda: 1.0","b:  0.15<br />value: 0.1500<br />Function: Penalty<br />Lambda: 1.0","b:  0.16<br />value: 0.1600<br />Function: Penalty<br />Lambda: 1.0","b:  0.17<br />value: 0.1700<br />Function: Penalty<br />Lambda: 1.0","b:  0.18<br />value: 0.1800<br />Function: Penalty<br />Lambda: 1.0","b:  0.19<br />value: 0.1900<br />Function: Penalty<br />Lambda: 1.0","b:  0.20<br />value: 0.2000<br />Function: Penalty<br />Lambda: 1.0","b:  0.21<br />value: 0.2100<br />Function: Penalty<br />Lambda: 1.0","b:  0.22<br />value: 0.2200<br />Function: Penalty<br />Lambda: 1.0","b:  0.23<br />value: 0.2300<br />Function: Penalty<br />Lambda: 1.0","b:  0.24<br />value: 0.2400<br />Function: Penalty<br />Lambda: 1.0","b:  0.25<br />value: 0.2500<br />Function: Penalty<br />Lambda: 1.0","b:  0.26<br />value: 0.2600<br />Function: Penalty<br />Lambda: 1.0","b:  0.27<br />value: 0.2700<br />Function: Penalty<br />Lambda: 1.0","b:  0.28<br />value: 0.2800<br />Function: Penalty<br />Lambda: 1.0","b:  0.29<br />value: 0.2900<br />Function: Penalty<br />Lambda: 1.0","b:  0.30<br />value: 0.3000<br />Function: Penalty<br />Lambda: 1.0","b:  0.31<br />value: 0.3100<br />Function: Penalty<br />Lambda: 1.0","b:  0.32<br />value: 0.3200<br />Function: Penalty<br />Lambda: 1.0","b:  0.33<br />value: 0.3300<br />Function: Penalty<br />Lambda: 1.0","b:  0.34<br />value: 0.3400<br />Function: Penalty<br />Lambda: 1.0","b:  0.35<br />value: 0.3500<br />Function: Penalty<br />Lambda: 1.0","b:  0.36<br />value: 0.3600<br />Function: Penalty<br />Lambda: 1.0","b:  0.37<br />value: 0.3700<br />Function: Penalty<br />Lambda: 1.0","b:  0.38<br />value: 0.3800<br />Function: Penalty<br />Lambda: 1.0","b:  0.39<br />value: 0.3900<br />Function: Penalty<br />Lambda: 1.0","b:  0.40<br />value: 0.4000<br />Function: Penalty<br />Lambda: 1.0","b:  0.41<br />value: 0.4100<br />Function: Penalty<br />Lambda: 1.0","b:  0.42<br />value: 0.4200<br />Function: Penalty<br />Lambda: 1.0","b:  0.43<br />value: 0.4300<br />Function: Penalty<br />Lambda: 1.0","b:  0.44<br />value: 0.4400<br />Function: Penalty<br />Lambda: 1.0","b:  0.45<br />value: 0.4500<br />Function: Penalty<br />Lambda: 1.0","b:  0.46<br />value: 0.4600<br />Function: Penalty<br />Lambda: 1.0","b:  0.47<br />value: 0.4700<br />Function: Penalty<br />Lambda: 1.0","b:  0.48<br />value: 0.4800<br />Function: Penalty<br />Lambda: 1.0","b:  0.49<br />value: 0.4900<br />Function: Penalty<br />Lambda: 1.0","b:  0.50<br />value: 0.5000<br />Function: Penalty<br />Lambda: 1.0","b:  0.51<br />value: 0.5100<br />Function: Penalty<br />Lambda: 1.0","b:  0.52<br />value: 0.5200<br />Function: Penalty<br />Lambda: 1.0","b:  0.53<br />value: 0.5300<br />Function: Penalty<br />Lambda: 1.0","b:  0.54<br />value: 0.5400<br />Function: Penalty<br />Lambda: 1.0","b:  0.55<br />value: 0.5500<br />Function: Penalty<br />Lambda: 1.0","b:  0.56<br />value: 0.5600<br />Function: Penalty<br />Lambda: 1.0","b:  0.57<br />value: 0.5700<br />Function: Penalty<br />Lambda: 1.0","b:  0.58<br />value: 0.5800<br />Function: Penalty<br />Lambda: 1.0","b:  0.59<br />value: 0.5900<br />Function: Penalty<br />Lambda: 1.0","b:  0.60<br />value: 0.6000<br />Function: Penalty<br />Lambda: 1.0","b:  0.61<br />value: 0.6100<br />Function: Penalty<br />Lambda: 1.0","b:  0.62<br />value: 0.6200<br />Function: Penalty<br />Lambda: 1.0","b:  0.63<br />value: 0.6300<br />Function: Penalty<br />Lambda: 1.0","b:  0.64<br />value: 0.6400<br />Function: Penalty<br />Lambda: 1.0","b:  0.65<br />value: 0.6500<br />Function: Penalty<br />Lambda: 1.0","b:  0.66<br />value: 0.6600<br />Function: Penalty<br />Lambda: 1.0","b:  0.67<br />value: 0.6700<br />Function: Penalty<br />Lambda: 1.0","b:  0.68<br />value: 0.6800<br />Function: Penalty<br />Lambda: 1.0","b:  0.69<br />value: 0.6900<br />Function: Penalty<br />Lambda: 1.0","b:  0.70<br />value: 0.7000<br />Function: Penalty<br />Lambda: 1.0","b:  0.71<br />value: 0.7100<br />Function: Penalty<br />Lambda: 1.0","b:  0.72<br />value: 0.7200<br />Function: Penalty<br />Lambda: 1.0","b:  0.73<br />value: 0.7300<br />Function: Penalty<br />Lambda: 1.0","b:  0.74<br />value: 0.7400<br />Function: Penalty<br />Lambda: 1.0","b:  0.75<br />value: 0.7500<br />Function: Penalty<br />Lambda: 1.0","b:  0.76<br />value: 0.7600<br />Function: Penalty<br />Lambda: 1.0","b:  0.77<br />value: 0.7700<br />Function: Penalty<br />Lambda: 1.0","b:  0.78<br />value: 0.7800<br />Function: Penalty<br />Lambda: 1.0","b:  0.79<br />value: 0.7900<br />Function: Penalty<br />Lambda: 1.0","b:  0.80<br />value: 0.8000<br />Function: Penalty<br />Lambda: 1.0","b:  0.81<br />value: 0.8100<br />Function: Penalty<br />Lambda: 1.0","b:  0.82<br />value: 0.8200<br />Function: Penalty<br />Lambda: 1.0","b:  0.83<br />value: 0.8300<br />Function: Penalty<br />Lambda: 1.0","b:  0.84<br />value: 0.8400<br />Function: Penalty<br />Lambda: 1.0","b:  0.85<br />value: 0.8500<br />Function: Penalty<br />Lambda: 1.0","b:  0.86<br />value: 0.8600<br />Function: Penalty<br />Lambda: 1.0","b:  0.87<br />value: 0.8700<br />Function: Penalty<br />Lambda: 1.0","b:  0.88<br />value: 0.8800<br />Function: Penalty<br />Lambda: 1.0","b:  0.89<br />value: 0.8900<br />Function: Penalty<br />Lambda: 1.0","b:  0.90<br />value: 0.9000<br />Function: Penalty<br />Lambda: 1.0","b:  0.91<br />value: 0.9100<br />Function: Penalty<br />Lambda: 1.0","b:  0.92<br />value: 0.9200<br />Function: Penalty<br />Lambda: 1.0","b:  0.93<br />value: 0.9300<br />Function: Penalty<br />Lambda: 1.0","b:  0.94<br />value: 0.9400<br />Function: Penalty<br />Lambda: 1.0","b:  0.95<br />value: 0.9500<br />Function: Penalty<br />Lambda: 1.0","b:  0.96<br />value: 0.9600<br />Function: Penalty<br />Lambda: 1.0","b:  0.97<br />value: 0.9700<br />Function: Penalty<br />Lambda: 1.0","b:  0.98<br />value: 0.9800<br />Function: Penalty<br />Lambda: 1.0","b:  0.99<br />value: 0.9900<br />Function: Penalty<br />Lambda: 1.0","b:  1.00<br />value: 1.0000<br />Function: Penalty<br />Lambda: 1.0","b:  1.01<br />value: 1.0100<br />Function: Penalty<br />Lambda: 1.0","b:  1.02<br />value: 1.0200<br />Function: Penalty<br />Lambda: 1.0","b:  1.03<br />value: 1.0300<br />Function: Penalty<br />Lambda: 1.0","b:  1.04<br />value: 1.0400<br />Function: Penalty<br />Lambda: 1.0","b:  1.05<br />value: 1.0500<br />Function: Penalty<br />Lambda: 1.0","b:  1.06<br />value: 1.0600<br />Function: Penalty<br />Lambda: 1.0","b:  1.07<br />value: 1.0700<br />Function: Penalty<br />Lambda: 1.0","b:  1.08<br />value: 1.0800<br />Function: Penalty<br />Lambda: 1.0","b:  1.09<br />value: 1.0900<br />Function: Penalty<br />Lambda: 1.0","b:  1.10<br />value: 1.1000<br />Function: Penalty<br />Lambda: 1.0","b:  1.11<br />value: 1.1100<br />Function: Penalty<br />Lambda: 1.0","b:  1.12<br />value: 1.1200<br />Function: Penalty<br />Lambda: 1.0","b:  1.13<br />value: 1.1300<br />Function: Penalty<br />Lambda: 1.0","b:  1.14<br />value: 1.1400<br />Function: Penalty<br />Lambda: 1.0","b:  1.15<br />value: 1.1500<br />Function: Penalty<br />Lambda: 1.0","b:  1.16<br />value: 1.1600<br />Function: Penalty<br />Lambda: 1.0","b:  1.17<br />value: 1.1700<br />Function: Penalty<br />Lambda: 1.0","b:  1.18<br />value: 1.1800<br />Function: Penalty<br />Lambda: 1.0","b:  1.19<br />value: 1.1900<br />Function: Penalty<br />Lambda: 1.0","b:  1.20<br />value: 1.2000<br />Function: Penalty<br />Lambda: 1.0","b:  1.21<br />value: 1.2100<br />Function: Penalty<br />Lambda: 1.0","b:  1.22<br />value: 1.2200<br />Function: Penalty<br />Lambda: 1.0","b:  1.23<br />value: 1.2300<br />Function: Penalty<br />Lambda: 1.0","b:  1.24<br />value: 1.2400<br />Function: Penalty<br />Lambda: 1.0","b:  1.25<br />value: 1.2500<br />Function: Penalty<br />Lambda: 1.0","b:  1.26<br />value: 1.2600<br />Function: Penalty<br />Lambda: 1.0","b:  1.27<br />value: 1.2700<br />Function: Penalty<br />Lambda: 1.0","b:  1.28<br />value: 1.2800<br />Function: Penalty<br />Lambda: 1.0","b:  1.29<br />value: 1.2900<br />Function: Penalty<br />Lambda: 1.0","b:  1.30<br />value: 1.3000<br />Function: Penalty<br />Lambda: 1.0","b:  1.31<br />value: 1.3100<br />Function: Penalty<br />Lambda: 1.0","b:  1.32<br />value: 1.3200<br />Function: Penalty<br />Lambda: 1.0","b:  1.33<br />value: 1.3300<br />Function: Penalty<br />Lambda: 1.0","b:  1.34<br />value: 1.3400<br />Function: Penalty<br />Lambda: 1.0","b:  1.35<br />value: 1.3500<br />Function: Penalty<br />Lambda: 1.0","b:  1.36<br />value: 1.3600<br />Function: Penalty<br />Lambda: 1.0","b:  1.37<br />value: 1.3700<br />Function: Penalty<br />Lambda: 1.0","b:  1.38<br />value: 1.3800<br />Function: Penalty<br />Lambda: 1.0","b:  1.39<br />value: 1.3900<br />Function: Penalty<br />Lambda: 1.0","b:  1.40<br />value: 1.4000<br />Function: Penalty<br />Lambda: 1.0","b:  1.41<br />value: 1.4100<br />Function: Penalty<br />Lambda: 1.0","b:  1.42<br />value: 1.4200<br />Function: Penalty<br />Lambda: 1.0","b:  1.43<br />value: 1.4300<br />Function: Penalty<br />Lambda: 1.0","b:  1.44<br />value: 1.4400<br />Function: Penalty<br />Lambda: 1.0","b:  1.45<br />value: 1.4500<br />Function: Penalty<br />Lambda: 1.0","b:  1.46<br />value: 1.4600<br />Function: Penalty<br />Lambda: 1.0","b:  1.47<br />value: 1.4700<br />Function: Penalty<br />Lambda: 1.0","b:  1.48<br />value: 1.4800<br />Function: Penalty<br />Lambda: 1.0","b:  1.49<br />value: 1.4900<br />Function: Penalty<br />Lambda: 1.0","b:  1.50<br />value: 1.5000<br />Function: Penalty<br />Lambda: 1.0","b:  1.51<br />value: 1.5100<br />Function: Penalty<br />Lambda: 1.0","b:  1.52<br />value: 1.5200<br />Function: Penalty<br />Lambda: 1.0","b:  1.53<br />value: 1.5300<br />Function: Penalty<br />Lambda: 1.0","b:  1.54<br />value: 1.5400<br />Function: Penalty<br />Lambda: 1.0","b:  1.55<br />value: 1.5500<br />Function: Penalty<br />Lambda: 1.0","b:  1.56<br />value: 1.5600<br />Function: Penalty<br />Lambda: 1.0","b:  1.57<br />value: 1.5700<br />Function: Penalty<br />Lambda: 1.0","b:  1.58<br />value: 1.5800<br />Function: Penalty<br />Lambda: 1.0","b:  1.59<br />value: 1.5900<br />Function: Penalty<br />Lambda: 1.0","b:  1.60<br />value: 1.6000<br />Function: Penalty<br />Lambda: 1.0","b:  1.61<br />value: 1.6100<br />Function: Penalty<br />Lambda: 1.0","b:  1.62<br />value: 1.6200<br />Function: Penalty<br />Lambda: 1.0","b:  1.63<br />value: 1.6300<br />Function: Penalty<br />Lambda: 1.0","b:  1.64<br />value: 1.6400<br />Function: Penalty<br />Lambda: 1.0","b:  1.65<br />value: 1.6500<br />Function: Penalty<br />Lambda: 1.0","b:  1.66<br />value: 1.6600<br />Function: Penalty<br />Lambda: 1.0","b:  1.67<br />value: 1.6700<br />Function: Penalty<br />Lambda: 1.0","b:  1.68<br />value: 1.6800<br />Function: Penalty<br />Lambda: 1.0","b:  1.69<br />value: 1.6900<br />Function: Penalty<br />Lambda: 1.0","b:  1.70<br />value: 1.7000<br />Function: Penalty<br />Lambda: 1.0","b:  1.71<br />value: 1.7100<br />Function: Penalty<br />Lambda: 1.0","b:  1.72<br />value: 1.7200<br />Function: Penalty<br />Lambda: 1.0","b:  1.73<br />value: 1.7300<br />Function: Penalty<br />Lambda: 1.0","b:  1.74<br />value: 1.7400<br />Function: Penalty<br />Lambda: 1.0","b:  1.75<br />value: 1.7500<br />Function: Penalty<br />Lambda: 1.0","b:  1.76<br />value: 1.7600<br />Function: Penalty<br />Lambda: 1.0","b:  1.77<br />value: 1.7700<br />Function: Penalty<br />Lambda: 1.0","b:  1.78<br />value: 1.7800<br />Function: Penalty<br />Lambda: 1.0","b:  1.79<br />value: 1.7900<br />Function: Penalty<br />Lambda: 1.0","b:  1.80<br />value: 1.8000<br />Function: Penalty<br />Lambda: 1.0","b:  1.81<br />value: 1.8100<br />Function: Penalty<br />Lambda: 1.0","b:  1.82<br />value: 1.8200<br />Function: Penalty<br />Lambda: 1.0","b:  1.83<br />value: 1.8300<br />Function: Penalty<br />Lambda: 1.0","b:  1.84<br />value: 1.8400<br />Function: Penalty<br />Lambda: 1.0","b:  1.85<br />value: 1.8500<br />Function: Penalty<br />Lambda: 1.0","b:  1.86<br />value: 1.8600<br />Function: Penalty<br />Lambda: 1.0","b:  1.87<br />value: 1.8700<br />Function: Penalty<br />Lambda: 1.0","b:  1.88<br />value: 1.8800<br />Function: Penalty<br />Lambda: 1.0","b:  1.89<br />value: 1.8900<br />Function: Penalty<br />Lambda: 1.0","b:  1.90<br />value: 1.9000<br />Function: Penalty<br />Lambda: 1.0","b:  1.91<br />value: 1.9100<br />Function: Penalty<br />Lambda: 1.0","b:  1.92<br />value: 1.9200<br />Function: Penalty<br />Lambda: 1.0","b:  1.93<br />value: 1.9300<br />Function: Penalty<br />Lambda: 1.0","b:  1.94<br />value: 1.9400<br />Function: Penalty<br />Lambda: 1.0","b:  1.95<br />value: 1.9500<br />Function: Penalty<br />Lambda: 1.0","b:  1.96<br />value: 1.9600<br />Function: Penalty<br />Lambda: 1.0","b:  1.97<br />value: 1.9700<br />Function: Penalty<br />Lambda: 1.0","b:  1.98<br />value: 1.9800<br />Function: Penalty<br />Lambda: 1.0","b:  1.99<br />value: 1.9900<br />Function: Penalty<br />Lambda: 1.0","b:  2.00<br />value: 2.0000<br />Function: Penalty<br />Lambda: 1.0"],"frame":"1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"1.1","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 1.1","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 1.1","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 1.1","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 1.1","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 1.1","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 1.1","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 1.1","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 1.1","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 1.1","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 1.1","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 1.1","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 1.1","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 1.1","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 1.1","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 1.1","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 1.1","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 1.1","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 1.1","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 1.1","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 1.1","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 1.1","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 1.1","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 1.1","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 1.1","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 1.1","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 1.1","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 1.1","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 1.1","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 1.1","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 1.1","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 1.1","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 1.1","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 1.1","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 1.1","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 1.1","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 1.1","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 1.1","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 1.1","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 1.1","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 1.1","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 1.1","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 1.1","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 1.1","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 1.1","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 1.1","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 1.1","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 1.1","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 1.1","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 1.1","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 1.1","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.1","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.1","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.1","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.1","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.1","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.1","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.1","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.1","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.1","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.1","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.1","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.1","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.1","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.1","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.1","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.1","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.1","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.1","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.1","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.1","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.1","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.1","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.1","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.1","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.1","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.1","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.1","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.1","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.1","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.1","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.1","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.1","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.1","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.1","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.1","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.1","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.1","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.1","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.1","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.1","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.1","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.1","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.1","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.1","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.1","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.1","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.1","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.1","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.1","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.1","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.1","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.1","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.1","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.1","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.1","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.1","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.1","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.1","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.1","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.1","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.1","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.1","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.1","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.1","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.1","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.1","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.1","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.1","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.1","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.1","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.1","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.1","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.1","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.1","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.1","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.1","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.1","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.1","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.1","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.1","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.1","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.1","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.1","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.1","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.1","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.1","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.1","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.1","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.1","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.1","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.1","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.1","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.1","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.1","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.1","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.1","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.1","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.1","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.1","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.1","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 1.1","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.1","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.1","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.1","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.1","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.1","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.1","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.1","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.1","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.1","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.1","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.1","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.1","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.1","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.1","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.1","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.1","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.1","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.1","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.1","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.1","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.1","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.1","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.1","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.1","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.1","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.1","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.1","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.1","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.1","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.1","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.1","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.1","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.1","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.1","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.1","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.1","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.1","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.1","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.1","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.1","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.1","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.1","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.1","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.1","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.1","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.1","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.1","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.1","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.1","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.1","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.1","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.1","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.1","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.1","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.1","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.1","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.1","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.1","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.1","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.1","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.1","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.1","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.1","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.1","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.1","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.1","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.1","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.1","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.1","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.1","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.1","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.1","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.1","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.1","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.1","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.1","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.1","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.1","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.1","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.1","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.1","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.1","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.1","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.1","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.1","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.1","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.1","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.1","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.1","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.1","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.1","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.1","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.1","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.1","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.1","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.1","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.1","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.1","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.1","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.1"],"frame":"1.1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.3,3.2591,3.2184,3.1779,3.1376,3.0975,3.0576,3.0179,2.9784,2.9391,2.9,2.8611,2.8224,2.7839,2.7456,2.7075,2.6696,2.6319,2.5944,2.5571,2.52,2.4831,2.4464,2.4099,2.3736,2.3375,2.3016,2.2659,2.2304,2.1951,2.16,2.1251,2.0904,2.0559,2.0216,1.9875,1.9536,1.9199,1.8864,1.8531,1.82,1.7871,1.7544,1.7219,1.6896,1.6575,1.6256,1.5939,1.5624,1.5311,1.5,1.4911,1.4824,1.4739,1.4656,1.4575,1.4496,1.4419,1.4344,1.4271,1.42,1.4131,1.4064,1.3999,1.3936,1.3875,1.3816,1.3759,1.3704,1.3651,1.36,1.3551,1.3504,1.3459,1.3416,1.3375,1.3336,1.3299,1.3264,1.3231,1.32,1.3171,1.3144,1.3119,1.3096,1.3075,1.3056,1.3039,1.3024,1.3011,1.3,1.2991,1.2984,1.2979,1.2976,1.2975,1.2976,1.2979,1.2984,1.2991,1.3,1.3011,1.3024,1.3039,1.3056,1.3075,1.3096,1.3119,1.3144,1.3171,1.32,1.3231,1.3264,1.3299,1.3336,1.3375,1.3416,1.3459,1.3504,1.3551,1.36,1.3651,1.3704,1.3759,1.3816,1.3875,1.3936,1.3999,1.4064,1.4131,1.42,1.4271,1.4344,1.4419,1.4496,1.4575,1.4656,1.4739,1.4824,1.4911,1.5,1.5091,1.5184,1.5279,1.5376,1.5475,1.5576,1.5679,1.5784,1.5891,1.6,1.6111,1.6224,1.6339,1.6456,1.6575,1.6696,1.6819,1.6944,1.7071,1.72,1.7331,1.7464,1.7599,1.7736,1.7875,1.8016,1.8159,1.8304,1.8451,1.86,1.8751,1.8904,1.9059,1.9216,1.9375,1.9536,1.9699,1.9864,2.0031,2.02,2.0371,2.0544,2.0719,2.0896,2.1075,2.1256,2.1439,2.1624,2.1811,2.2,2.2191,2.2384,2.2579,2.2776,2.2975,2.3176,2.3379,2.3584,2.3791,2.4,2.4211,2.4424,2.4639,2.4856,2.5075,2.5296,2.5519,2.5744,2.5971,2.62,2.6431,2.6664,2.6899,2.7136,2.7375,2.7616,2.7859,2.8104,2.8351,2.86,2.8851,2.9104,2.9359,2.9616,2.9875,3.0136,3.0399,3.0664,3.0931,3.12,3.1471,3.1744,3.2019,3.2296,3.2575,3.2856,3.3139,3.3424,3.3711,3.4,3.4291,3.4584,3.4879,3.5176,3.5475,3.5776,3.6079,3.6384,3.6691,3.7],"text":["b: -0.50<br />value: 3.3000<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.49<br />value: 3.2591<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.48<br />value: 3.2184<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.47<br />value: 3.1779<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.46<br />value: 3.1376<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.45<br />value: 3.0975<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.44<br />value: 3.0576<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.43<br />value: 3.0179<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.42<br />value: 2.9784<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.41<br />value: 2.9391<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.40<br />value: 2.9000<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.39<br />value: 2.8611<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.38<br />value: 2.8224<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.37<br />value: 2.7839<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.36<br />value: 2.7456<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.35<br />value: 2.7075<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.34<br />value: 2.6696<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.33<br />value: 2.6319<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.32<br />value: 2.5944<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.31<br />value: 2.5571<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.30<br />value: 2.5200<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.29<br />value: 2.4831<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.28<br />value: 2.4464<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.27<br />value: 2.4099<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.26<br />value: 2.3736<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.25<br />value: 2.3375<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.24<br />value: 2.3016<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.23<br />value: 2.2659<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.22<br />value: 2.2304<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.21<br />value: 2.1951<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.20<br />value: 2.1600<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.19<br />value: 2.1251<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.18<br />value: 2.0904<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.17<br />value: 2.0559<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.16<br />value: 2.0216<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.15<br />value: 1.9875<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.14<br />value: 1.9536<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.13<br />value: 1.9199<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.12<br />value: 1.8864<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.11<br />value: 1.8531<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.10<br />value: 1.8200<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.09<br />value: 1.7871<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.08<br />value: 1.7544<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.07<br />value: 1.7219<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.06<br />value: 1.6896<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.05<br />value: 1.6575<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.04<br />value: 1.6256<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.03<br />value: 1.5939<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.02<br />value: 1.5624<br />Function: Loss + Penalty<br />Lambda: 1.1","b: -0.01<br />value: 1.5311<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.01<br />value: 1.4911<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.02<br />value: 1.4824<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.03<br />value: 1.4739<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.04<br />value: 1.4656<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.05<br />value: 1.4575<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.06<br />value: 1.4496<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.07<br />value: 1.4419<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.08<br />value: 1.4344<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.09<br />value: 1.4271<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.10<br />value: 1.4200<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.11<br />value: 1.4131<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.12<br />value: 1.4064<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.13<br />value: 1.3999<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.14<br />value: 1.3936<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.15<br />value: 1.3875<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.16<br />value: 1.3816<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.17<br />value: 1.3759<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.18<br />value: 1.3704<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.19<br />value: 1.3651<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.20<br />value: 1.3600<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.21<br />value: 1.3551<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.22<br />value: 1.3504<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.23<br />value: 1.3459<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.24<br />value: 1.3416<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.25<br />value: 1.3375<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.26<br />value: 1.3336<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.27<br />value: 1.3299<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.28<br />value: 1.3264<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.29<br />value: 1.3231<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.30<br />value: 1.3200<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.31<br />value: 1.3171<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.32<br />value: 1.3144<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.33<br />value: 1.3119<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.34<br />value: 1.3096<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.35<br />value: 1.3075<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.36<br />value: 1.3056<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.37<br />value: 1.3039<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.38<br />value: 1.3024<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.39<br />value: 1.3011<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.40<br />value: 1.3000<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.41<br />value: 1.2991<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.42<br />value: 1.2984<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.43<br />value: 1.2979<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.44<br />value: 1.2976<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.45<br />value: 1.2975<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.46<br />value: 1.2976<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.47<br />value: 1.2979<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.48<br />value: 1.2984<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.49<br />value: 1.2991<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.50<br />value: 1.3000<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.51<br />value: 1.3011<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.52<br />value: 1.3024<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.53<br />value: 1.3039<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.54<br />value: 1.3056<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.55<br />value: 1.3075<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.56<br />value: 1.3096<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.57<br />value: 1.3119<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.58<br />value: 1.3144<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.59<br />value: 1.3171<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.60<br />value: 1.3200<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.61<br />value: 1.3231<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.62<br />value: 1.3264<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.63<br />value: 1.3299<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.64<br />value: 1.3336<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.65<br />value: 1.3375<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.66<br />value: 1.3416<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.67<br />value: 1.3459<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.68<br />value: 1.3504<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.69<br />value: 1.3551<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.70<br />value: 1.3600<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.71<br />value: 1.3651<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.72<br />value: 1.3704<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.73<br />value: 1.3759<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.74<br />value: 1.3816<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.75<br />value: 1.3875<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.76<br />value: 1.3936<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.77<br />value: 1.3999<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.78<br />value: 1.4064<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.79<br />value: 1.4131<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.80<br />value: 1.4200<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.81<br />value: 1.4271<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.82<br />value: 1.4344<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.83<br />value: 1.4419<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.84<br />value: 1.4496<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.85<br />value: 1.4575<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.86<br />value: 1.4656<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.87<br />value: 1.4739<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.88<br />value: 1.4824<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.89<br />value: 1.4911<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.90<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.91<br />value: 1.5091<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.92<br />value: 1.5184<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.93<br />value: 1.5279<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.94<br />value: 1.5376<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.95<br />value: 1.5475<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.96<br />value: 1.5576<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.97<br />value: 1.5679<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.98<br />value: 1.5784<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  0.99<br />value: 1.5891<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.00<br />value: 1.6000<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.01<br />value: 1.6111<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.02<br />value: 1.6224<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.03<br />value: 1.6339<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.04<br />value: 1.6456<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.05<br />value: 1.6575<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.06<br />value: 1.6696<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.07<br />value: 1.6819<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.08<br />value: 1.6944<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.09<br />value: 1.7071<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.10<br />value: 1.7200<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.11<br />value: 1.7331<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.12<br />value: 1.7464<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.13<br />value: 1.7599<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.14<br />value: 1.7736<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.15<br />value: 1.7875<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.16<br />value: 1.8016<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.17<br />value: 1.8159<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.18<br />value: 1.8304<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.19<br />value: 1.8451<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.20<br />value: 1.8600<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.21<br />value: 1.8751<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.22<br />value: 1.8904<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.23<br />value: 1.9059<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.24<br />value: 1.9216<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.25<br />value: 1.9375<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.26<br />value: 1.9536<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.27<br />value: 1.9699<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.28<br />value: 1.9864<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.29<br />value: 2.0031<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.30<br />value: 2.0200<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.31<br />value: 2.0371<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.32<br />value: 2.0544<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.33<br />value: 2.0719<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.34<br />value: 2.0896<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.35<br />value: 2.1075<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.36<br />value: 2.1256<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.37<br />value: 2.1439<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.38<br />value: 2.1624<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.39<br />value: 2.1811<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.40<br />value: 2.2000<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.41<br />value: 2.2191<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.42<br />value: 2.2384<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.43<br />value: 2.2579<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.44<br />value: 2.2776<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.45<br />value: 2.2975<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.46<br />value: 2.3176<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.47<br />value: 2.3379<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.48<br />value: 2.3584<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.49<br />value: 2.3791<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.50<br />value: 2.4000<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.51<br />value: 2.4211<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.52<br />value: 2.4424<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.53<br />value: 2.4639<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.54<br />value: 2.4856<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.55<br />value: 2.5075<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.56<br />value: 2.5296<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.57<br />value: 2.5519<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.58<br />value: 2.5744<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.59<br />value: 2.5971<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.60<br />value: 2.6200<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.61<br />value: 2.6431<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.62<br />value: 2.6664<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.63<br />value: 2.6899<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.64<br />value: 2.7136<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.65<br />value: 2.7375<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.66<br />value: 2.7616<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.67<br />value: 2.7859<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.68<br />value: 2.8104<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.69<br />value: 2.8351<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.70<br />value: 2.8600<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.71<br />value: 2.8851<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.72<br />value: 2.9104<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.73<br />value: 2.9359<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.74<br />value: 2.9616<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.75<br />value: 2.9875<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.76<br />value: 3.0136<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.77<br />value: 3.0399<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.78<br />value: 3.0664<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.79<br />value: 3.0931<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.80<br />value: 3.1200<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.81<br />value: 3.1471<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.82<br />value: 3.1744<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.83<br />value: 3.2019<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.84<br />value: 3.2296<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.85<br />value: 3.2575<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.86<br />value: 3.2856<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.87<br />value: 3.3139<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.88<br />value: 3.3424<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.89<br />value: 3.3711<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.90<br />value: 3.4000<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.91<br />value: 3.4291<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.92<br />value: 3.4584<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.93<br />value: 3.4879<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.94<br />value: 3.5176<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.95<br />value: 3.5475<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.96<br />value: 3.5776<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.97<br />value: 3.6079<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.98<br />value: 3.6384<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  1.99<br />value: 3.6691<br />Function: Loss + Penalty<br />Lambda: 1.1","b:  2.00<br />value: 3.7000<br />Function: Loss + Penalty<br />Lambda: 1.1"],"frame":"1.1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.55,0.539,0.528,0.517,0.506,0.495,0.484,0.473,0.462,0.451,0.44,0.429,0.418,0.407,0.396,0.385,0.374,0.363,0.352,0.341,0.33,0.319,0.308,0.297,0.286,0.275,0.264,0.253,0.242,0.231,0.22,0.209,0.198,0.187,0.176,0.165,0.154,0.143,0.132,0.121,0.11,0.099,0.088,0.077,0.066,0.055,0.044,0.033,0.022,0.011,0,0.011,0.022,0.033,0.044,0.0550000000000001,0.0660000000000001,0.0770000000000001,0.088,0.099,0.11,0.121,0.132,0.143,0.154,0.165,0.176,0.187,0.198,0.209,0.22,0.231,0.242,0.253,0.264,0.275,0.286,0.297,0.308,0.319,0.33,0.341,0.352,0.363,0.374,0.385,0.396,0.407,0.418,0.429,0.44,0.451,0.462,0.473,0.484,0.495,0.506,0.517,0.528,0.539,0.55,0.561,0.572,0.583,0.594,0.605,0.616,0.627,0.638,0.649,0.66,0.671,0.682,0.693,0.704,0.715,0.726,0.737,0.748,0.759,0.77,0.781,0.792,0.803,0.814,0.825,0.836,0.847,0.858,0.869,0.88,0.891,0.902,0.913,0.924,0.935,0.946,0.957,0.968,0.979,0.99,1.001,1.012,1.023,1.034,1.045,1.056,1.067,1.078,1.089,1.1,1.111,1.122,1.133,1.144,1.155,1.166,1.177,1.188,1.199,1.21,1.221,1.232,1.243,1.254,1.265,1.276,1.287,1.298,1.309,1.32,1.331,1.342,1.353,1.364,1.375,1.386,1.397,1.408,1.419,1.43,1.441,1.452,1.463,1.474,1.485,1.496,1.507,1.518,1.529,1.54,1.551,1.562,1.573,1.584,1.595,1.606,1.617,1.628,1.639,1.65,1.661,1.672,1.683,1.694,1.705,1.716,1.727,1.738,1.749,1.76,1.771,1.782,1.793,1.804,1.815,1.826,1.837,1.848,1.859,1.87,1.881,1.892,1.903,1.914,1.925,1.936,1.947,1.958,1.969,1.98,1.991,2.002,2.013,2.024,2.035,2.046,2.057,2.068,2.079,2.09,2.101,2.112,2.123,2.134,2.145,2.156,2.167,2.178,2.189,2.2],"text":["b: -0.50<br />value: 0.5500<br />Function: Penalty<br />Lambda: 1.1","b: -0.49<br />value: 0.5390<br />Function: Penalty<br />Lambda: 1.1","b: -0.48<br />value: 0.5280<br />Function: Penalty<br />Lambda: 1.1","b: -0.47<br />value: 0.5170<br />Function: Penalty<br />Lambda: 1.1","b: -0.46<br />value: 0.5060<br />Function: Penalty<br />Lambda: 1.1","b: -0.45<br />value: 0.4950<br />Function: Penalty<br />Lambda: 1.1","b: -0.44<br />value: 0.4840<br />Function: Penalty<br />Lambda: 1.1","b: -0.43<br />value: 0.4730<br />Function: Penalty<br />Lambda: 1.1","b: -0.42<br />value: 0.4620<br />Function: Penalty<br />Lambda: 1.1","b: -0.41<br />value: 0.4510<br />Function: Penalty<br />Lambda: 1.1","b: -0.40<br />value: 0.4400<br />Function: Penalty<br />Lambda: 1.1","b: -0.39<br />value: 0.4290<br />Function: Penalty<br />Lambda: 1.1","b: -0.38<br />value: 0.4180<br />Function: Penalty<br />Lambda: 1.1","b: -0.37<br />value: 0.4070<br />Function: Penalty<br />Lambda: 1.1","b: -0.36<br />value: 0.3960<br />Function: Penalty<br />Lambda: 1.1","b: -0.35<br />value: 0.3850<br />Function: Penalty<br />Lambda: 1.1","b: -0.34<br />value: 0.3740<br />Function: Penalty<br />Lambda: 1.1","b: -0.33<br />value: 0.3630<br />Function: Penalty<br />Lambda: 1.1","b: -0.32<br />value: 0.3520<br />Function: Penalty<br />Lambda: 1.1","b: -0.31<br />value: 0.3410<br />Function: Penalty<br />Lambda: 1.1","b: -0.30<br />value: 0.3300<br />Function: Penalty<br />Lambda: 1.1","b: -0.29<br />value: 0.3190<br />Function: Penalty<br />Lambda: 1.1","b: -0.28<br />value: 0.3080<br />Function: Penalty<br />Lambda: 1.1","b: -0.27<br />value: 0.2970<br />Function: Penalty<br />Lambda: 1.1","b: -0.26<br />value: 0.2860<br />Function: Penalty<br />Lambda: 1.1","b: -0.25<br />value: 0.2750<br />Function: Penalty<br />Lambda: 1.1","b: -0.24<br />value: 0.2640<br />Function: Penalty<br />Lambda: 1.1","b: -0.23<br />value: 0.2530<br />Function: Penalty<br />Lambda: 1.1","b: -0.22<br />value: 0.2420<br />Function: Penalty<br />Lambda: 1.1","b: -0.21<br />value: 0.2310<br />Function: Penalty<br />Lambda: 1.1","b: -0.20<br />value: 0.2200<br />Function: Penalty<br />Lambda: 1.1","b: -0.19<br />value: 0.2090<br />Function: Penalty<br />Lambda: 1.1","b: -0.18<br />value: 0.1980<br />Function: Penalty<br />Lambda: 1.1","b: -0.17<br />value: 0.1870<br />Function: Penalty<br />Lambda: 1.1","b: -0.16<br />value: 0.1760<br />Function: Penalty<br />Lambda: 1.1","b: -0.15<br />value: 0.1650<br />Function: Penalty<br />Lambda: 1.1","b: -0.14<br />value: 0.1540<br />Function: Penalty<br />Lambda: 1.1","b: -0.13<br />value: 0.1430<br />Function: Penalty<br />Lambda: 1.1","b: -0.12<br />value: 0.1320<br />Function: Penalty<br />Lambda: 1.1","b: -0.11<br />value: 0.1210<br />Function: Penalty<br />Lambda: 1.1","b: -0.10<br />value: 0.1100<br />Function: Penalty<br />Lambda: 1.1","b: -0.09<br />value: 0.0990<br />Function: Penalty<br />Lambda: 1.1","b: -0.08<br />value: 0.0880<br />Function: Penalty<br />Lambda: 1.1","b: -0.07<br />value: 0.0770<br />Function: Penalty<br />Lambda: 1.1","b: -0.06<br />value: 0.0660<br />Function: Penalty<br />Lambda: 1.1","b: -0.05<br />value: 0.0550<br />Function: Penalty<br />Lambda: 1.1","b: -0.04<br />value: 0.0440<br />Function: Penalty<br />Lambda: 1.1","b: -0.03<br />value: 0.0330<br />Function: Penalty<br />Lambda: 1.1","b: -0.02<br />value: 0.0220<br />Function: Penalty<br />Lambda: 1.1","b: -0.01<br />value: 0.0110<br />Function: Penalty<br />Lambda: 1.1","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 1.1","b:  0.01<br />value: 0.0110<br />Function: Penalty<br />Lambda: 1.1","b:  0.02<br />value: 0.0220<br />Function: Penalty<br />Lambda: 1.1","b:  0.03<br />value: 0.0330<br />Function: Penalty<br />Lambda: 1.1","b:  0.04<br />value: 0.0440<br />Function: Penalty<br />Lambda: 1.1","b:  0.05<br />value: 0.0550<br />Function: Penalty<br />Lambda: 1.1","b:  0.06<br />value: 0.0660<br />Function: Penalty<br />Lambda: 1.1","b:  0.07<br />value: 0.0770<br />Function: Penalty<br />Lambda: 1.1","b:  0.08<br />value: 0.0880<br />Function: Penalty<br />Lambda: 1.1","b:  0.09<br />value: 0.0990<br />Function: Penalty<br />Lambda: 1.1","b:  0.10<br />value: 0.1100<br />Function: Penalty<br />Lambda: 1.1","b:  0.11<br />value: 0.1210<br />Function: Penalty<br />Lambda: 1.1","b:  0.12<br />value: 0.1320<br />Function: Penalty<br />Lambda: 1.1","b:  0.13<br />value: 0.1430<br />Function: Penalty<br />Lambda: 1.1","b:  0.14<br />value: 0.1540<br />Function: Penalty<br />Lambda: 1.1","b:  0.15<br />value: 0.1650<br />Function: Penalty<br />Lambda: 1.1","b:  0.16<br />value: 0.1760<br />Function: Penalty<br />Lambda: 1.1","b:  0.17<br />value: 0.1870<br />Function: Penalty<br />Lambda: 1.1","b:  0.18<br />value: 0.1980<br />Function: Penalty<br />Lambda: 1.1","b:  0.19<br />value: 0.2090<br />Function: Penalty<br />Lambda: 1.1","b:  0.20<br />value: 0.2200<br />Function: Penalty<br />Lambda: 1.1","b:  0.21<br />value: 0.2310<br />Function: Penalty<br />Lambda: 1.1","b:  0.22<br />value: 0.2420<br />Function: Penalty<br />Lambda: 1.1","b:  0.23<br />value: 0.2530<br />Function: Penalty<br />Lambda: 1.1","b:  0.24<br />value: 0.2640<br />Function: Penalty<br />Lambda: 1.1","b:  0.25<br />value: 0.2750<br />Function: Penalty<br />Lambda: 1.1","b:  0.26<br />value: 0.2860<br />Function: Penalty<br />Lambda: 1.1","b:  0.27<br />value: 0.2970<br />Function: Penalty<br />Lambda: 1.1","b:  0.28<br />value: 0.3080<br />Function: Penalty<br />Lambda: 1.1","b:  0.29<br />value: 0.3190<br />Function: Penalty<br />Lambda: 1.1","b:  0.30<br />value: 0.3300<br />Function: Penalty<br />Lambda: 1.1","b:  0.31<br />value: 0.3410<br />Function: Penalty<br />Lambda: 1.1","b:  0.32<br />value: 0.3520<br />Function: Penalty<br />Lambda: 1.1","b:  0.33<br />value: 0.3630<br />Function: Penalty<br />Lambda: 1.1","b:  0.34<br />value: 0.3740<br />Function: Penalty<br />Lambda: 1.1","b:  0.35<br />value: 0.3850<br />Function: Penalty<br />Lambda: 1.1","b:  0.36<br />value: 0.3960<br />Function: Penalty<br />Lambda: 1.1","b:  0.37<br />value: 0.4070<br />Function: Penalty<br />Lambda: 1.1","b:  0.38<br />value: 0.4180<br />Function: Penalty<br />Lambda: 1.1","b:  0.39<br />value: 0.4290<br />Function: Penalty<br />Lambda: 1.1","b:  0.40<br />value: 0.4400<br />Function: Penalty<br />Lambda: 1.1","b:  0.41<br />value: 0.4510<br />Function: Penalty<br />Lambda: 1.1","b:  0.42<br />value: 0.4620<br />Function: Penalty<br />Lambda: 1.1","b:  0.43<br />value: 0.4730<br />Function: Penalty<br />Lambda: 1.1","b:  0.44<br />value: 0.4840<br />Function: Penalty<br />Lambda: 1.1","b:  0.45<br />value: 0.4950<br />Function: Penalty<br />Lambda: 1.1","b:  0.46<br />value: 0.5060<br />Function: Penalty<br />Lambda: 1.1","b:  0.47<br />value: 0.5170<br />Function: Penalty<br />Lambda: 1.1","b:  0.48<br />value: 0.5280<br />Function: Penalty<br />Lambda: 1.1","b:  0.49<br />value: 0.5390<br />Function: Penalty<br />Lambda: 1.1","b:  0.50<br />value: 0.5500<br />Function: Penalty<br />Lambda: 1.1","b:  0.51<br />value: 0.5610<br />Function: Penalty<br />Lambda: 1.1","b:  0.52<br />value: 0.5720<br />Function: Penalty<br />Lambda: 1.1","b:  0.53<br />value: 0.5830<br />Function: Penalty<br />Lambda: 1.1","b:  0.54<br />value: 0.5940<br />Function: Penalty<br />Lambda: 1.1","b:  0.55<br />value: 0.6050<br />Function: Penalty<br />Lambda: 1.1","b:  0.56<br />value: 0.6160<br />Function: Penalty<br />Lambda: 1.1","b:  0.57<br />value: 0.6270<br />Function: Penalty<br />Lambda: 1.1","b:  0.58<br />value: 0.6380<br />Function: Penalty<br />Lambda: 1.1","b:  0.59<br />value: 0.6490<br />Function: Penalty<br />Lambda: 1.1","b:  0.60<br />value: 0.6600<br />Function: Penalty<br />Lambda: 1.1","b:  0.61<br />value: 0.6710<br />Function: Penalty<br />Lambda: 1.1","b:  0.62<br />value: 0.6820<br />Function: Penalty<br />Lambda: 1.1","b:  0.63<br />value: 0.6930<br />Function: Penalty<br />Lambda: 1.1","b:  0.64<br />value: 0.7040<br />Function: Penalty<br />Lambda: 1.1","b:  0.65<br />value: 0.7150<br />Function: Penalty<br />Lambda: 1.1","b:  0.66<br />value: 0.7260<br />Function: Penalty<br />Lambda: 1.1","b:  0.67<br />value: 0.7370<br />Function: Penalty<br />Lambda: 1.1","b:  0.68<br />value: 0.7480<br />Function: Penalty<br />Lambda: 1.1","b:  0.69<br />value: 0.7590<br />Function: Penalty<br />Lambda: 1.1","b:  0.70<br />value: 0.7700<br />Function: Penalty<br />Lambda: 1.1","b:  0.71<br />value: 0.7810<br />Function: Penalty<br />Lambda: 1.1","b:  0.72<br />value: 0.7920<br />Function: Penalty<br />Lambda: 1.1","b:  0.73<br />value: 0.8030<br />Function: Penalty<br />Lambda: 1.1","b:  0.74<br />value: 0.8140<br />Function: Penalty<br />Lambda: 1.1","b:  0.75<br />value: 0.8250<br />Function: Penalty<br />Lambda: 1.1","b:  0.76<br />value: 0.8360<br />Function: Penalty<br />Lambda: 1.1","b:  0.77<br />value: 0.8470<br />Function: Penalty<br />Lambda: 1.1","b:  0.78<br />value: 0.8580<br />Function: Penalty<br />Lambda: 1.1","b:  0.79<br />value: 0.8690<br />Function: Penalty<br />Lambda: 1.1","b:  0.80<br />value: 0.8800<br />Function: Penalty<br />Lambda: 1.1","b:  0.81<br />value: 0.8910<br />Function: Penalty<br />Lambda: 1.1","b:  0.82<br />value: 0.9020<br />Function: Penalty<br />Lambda: 1.1","b:  0.83<br />value: 0.9130<br />Function: Penalty<br />Lambda: 1.1","b:  0.84<br />value: 0.9240<br />Function: Penalty<br />Lambda: 1.1","b:  0.85<br />value: 0.9350<br />Function: Penalty<br />Lambda: 1.1","b:  0.86<br />value: 0.9460<br />Function: Penalty<br />Lambda: 1.1","b:  0.87<br />value: 0.9570<br />Function: Penalty<br />Lambda: 1.1","b:  0.88<br />value: 0.9680<br />Function: Penalty<br />Lambda: 1.1","b:  0.89<br />value: 0.9790<br />Function: Penalty<br />Lambda: 1.1","b:  0.90<br />value: 0.9900<br />Function: Penalty<br />Lambda: 1.1","b:  0.91<br />value: 1.0010<br />Function: Penalty<br />Lambda: 1.1","b:  0.92<br />value: 1.0120<br />Function: Penalty<br />Lambda: 1.1","b:  0.93<br />value: 1.0230<br />Function: Penalty<br />Lambda: 1.1","b:  0.94<br />value: 1.0340<br />Function: Penalty<br />Lambda: 1.1","b:  0.95<br />value: 1.0450<br />Function: Penalty<br />Lambda: 1.1","b:  0.96<br />value: 1.0560<br />Function: Penalty<br />Lambda: 1.1","b:  0.97<br />value: 1.0670<br />Function: Penalty<br />Lambda: 1.1","b:  0.98<br />value: 1.0780<br />Function: Penalty<br />Lambda: 1.1","b:  0.99<br />value: 1.0890<br />Function: Penalty<br />Lambda: 1.1","b:  1.00<br />value: 1.1000<br />Function: Penalty<br />Lambda: 1.1","b:  1.01<br />value: 1.1110<br />Function: Penalty<br />Lambda: 1.1","b:  1.02<br />value: 1.1220<br />Function: Penalty<br />Lambda: 1.1","b:  1.03<br />value: 1.1330<br />Function: Penalty<br />Lambda: 1.1","b:  1.04<br />value: 1.1440<br />Function: Penalty<br />Lambda: 1.1","b:  1.05<br />value: 1.1550<br />Function: Penalty<br />Lambda: 1.1","b:  1.06<br />value: 1.1660<br />Function: Penalty<br />Lambda: 1.1","b:  1.07<br />value: 1.1770<br />Function: Penalty<br />Lambda: 1.1","b:  1.08<br />value: 1.1880<br />Function: Penalty<br />Lambda: 1.1","b:  1.09<br />value: 1.1990<br />Function: Penalty<br />Lambda: 1.1","b:  1.10<br />value: 1.2100<br />Function: Penalty<br />Lambda: 1.1","b:  1.11<br />value: 1.2210<br />Function: Penalty<br />Lambda: 1.1","b:  1.12<br />value: 1.2320<br />Function: Penalty<br />Lambda: 1.1","b:  1.13<br />value: 1.2430<br />Function: Penalty<br />Lambda: 1.1","b:  1.14<br />value: 1.2540<br />Function: Penalty<br />Lambda: 1.1","b:  1.15<br />value: 1.2650<br />Function: Penalty<br />Lambda: 1.1","b:  1.16<br />value: 1.2760<br />Function: Penalty<br />Lambda: 1.1","b:  1.17<br />value: 1.2870<br />Function: Penalty<br />Lambda: 1.1","b:  1.18<br />value: 1.2980<br />Function: Penalty<br />Lambda: 1.1","b:  1.19<br />value: 1.3090<br />Function: Penalty<br />Lambda: 1.1","b:  1.20<br />value: 1.3200<br />Function: Penalty<br />Lambda: 1.1","b:  1.21<br />value: 1.3310<br />Function: Penalty<br />Lambda: 1.1","b:  1.22<br />value: 1.3420<br />Function: Penalty<br />Lambda: 1.1","b:  1.23<br />value: 1.3530<br />Function: Penalty<br />Lambda: 1.1","b:  1.24<br />value: 1.3640<br />Function: Penalty<br />Lambda: 1.1","b:  1.25<br />value: 1.3750<br />Function: Penalty<br />Lambda: 1.1","b:  1.26<br />value: 1.3860<br />Function: Penalty<br />Lambda: 1.1","b:  1.27<br />value: 1.3970<br />Function: Penalty<br />Lambda: 1.1","b:  1.28<br />value: 1.4080<br />Function: Penalty<br />Lambda: 1.1","b:  1.29<br />value: 1.4190<br />Function: Penalty<br />Lambda: 1.1","b:  1.30<br />value: 1.4300<br />Function: Penalty<br />Lambda: 1.1","b:  1.31<br />value: 1.4410<br />Function: Penalty<br />Lambda: 1.1","b:  1.32<br />value: 1.4520<br />Function: Penalty<br />Lambda: 1.1","b:  1.33<br />value: 1.4630<br />Function: Penalty<br />Lambda: 1.1","b:  1.34<br />value: 1.4740<br />Function: Penalty<br />Lambda: 1.1","b:  1.35<br />value: 1.4850<br />Function: Penalty<br />Lambda: 1.1","b:  1.36<br />value: 1.4960<br />Function: Penalty<br />Lambda: 1.1","b:  1.37<br />value: 1.5070<br />Function: Penalty<br />Lambda: 1.1","b:  1.38<br />value: 1.5180<br />Function: Penalty<br />Lambda: 1.1","b:  1.39<br />value: 1.5290<br />Function: Penalty<br />Lambda: 1.1","b:  1.40<br />value: 1.5400<br />Function: Penalty<br />Lambda: 1.1","b:  1.41<br />value: 1.5510<br />Function: Penalty<br />Lambda: 1.1","b:  1.42<br />value: 1.5620<br />Function: Penalty<br />Lambda: 1.1","b:  1.43<br />value: 1.5730<br />Function: Penalty<br />Lambda: 1.1","b:  1.44<br />value: 1.5840<br />Function: Penalty<br />Lambda: 1.1","b:  1.45<br />value: 1.5950<br />Function: Penalty<br />Lambda: 1.1","b:  1.46<br />value: 1.6060<br />Function: Penalty<br />Lambda: 1.1","b:  1.47<br />value: 1.6170<br />Function: Penalty<br />Lambda: 1.1","b:  1.48<br />value: 1.6280<br />Function: Penalty<br />Lambda: 1.1","b:  1.49<br />value: 1.6390<br />Function: Penalty<br />Lambda: 1.1","b:  1.50<br />value: 1.6500<br />Function: Penalty<br />Lambda: 1.1","b:  1.51<br />value: 1.6610<br />Function: Penalty<br />Lambda: 1.1","b:  1.52<br />value: 1.6720<br />Function: Penalty<br />Lambda: 1.1","b:  1.53<br />value: 1.6830<br />Function: Penalty<br />Lambda: 1.1","b:  1.54<br />value: 1.6940<br />Function: Penalty<br />Lambda: 1.1","b:  1.55<br />value: 1.7050<br />Function: Penalty<br />Lambda: 1.1","b:  1.56<br />value: 1.7160<br />Function: Penalty<br />Lambda: 1.1","b:  1.57<br />value: 1.7270<br />Function: Penalty<br />Lambda: 1.1","b:  1.58<br />value: 1.7380<br />Function: Penalty<br />Lambda: 1.1","b:  1.59<br />value: 1.7490<br />Function: Penalty<br />Lambda: 1.1","b:  1.60<br />value: 1.7600<br />Function: Penalty<br />Lambda: 1.1","b:  1.61<br />value: 1.7710<br />Function: Penalty<br />Lambda: 1.1","b:  1.62<br />value: 1.7820<br />Function: Penalty<br />Lambda: 1.1","b:  1.63<br />value: 1.7930<br />Function: Penalty<br />Lambda: 1.1","b:  1.64<br />value: 1.8040<br />Function: Penalty<br />Lambda: 1.1","b:  1.65<br />value: 1.8150<br />Function: Penalty<br />Lambda: 1.1","b:  1.66<br />value: 1.8260<br />Function: Penalty<br />Lambda: 1.1","b:  1.67<br />value: 1.8370<br />Function: Penalty<br />Lambda: 1.1","b:  1.68<br />value: 1.8480<br />Function: Penalty<br />Lambda: 1.1","b:  1.69<br />value: 1.8590<br />Function: Penalty<br />Lambda: 1.1","b:  1.70<br />value: 1.8700<br />Function: Penalty<br />Lambda: 1.1","b:  1.71<br />value: 1.8810<br />Function: Penalty<br />Lambda: 1.1","b:  1.72<br />value: 1.8920<br />Function: Penalty<br />Lambda: 1.1","b:  1.73<br />value: 1.9030<br />Function: Penalty<br />Lambda: 1.1","b:  1.74<br />value: 1.9140<br />Function: Penalty<br />Lambda: 1.1","b:  1.75<br />value: 1.9250<br />Function: Penalty<br />Lambda: 1.1","b:  1.76<br />value: 1.9360<br />Function: Penalty<br />Lambda: 1.1","b:  1.77<br />value: 1.9470<br />Function: Penalty<br />Lambda: 1.1","b:  1.78<br />value: 1.9580<br />Function: Penalty<br />Lambda: 1.1","b:  1.79<br />value: 1.9690<br />Function: Penalty<br />Lambda: 1.1","b:  1.80<br />value: 1.9800<br />Function: Penalty<br />Lambda: 1.1","b:  1.81<br />value: 1.9910<br />Function: Penalty<br />Lambda: 1.1","b:  1.82<br />value: 2.0020<br />Function: Penalty<br />Lambda: 1.1","b:  1.83<br />value: 2.0130<br />Function: Penalty<br />Lambda: 1.1","b:  1.84<br />value: 2.0240<br />Function: Penalty<br />Lambda: 1.1","b:  1.85<br />value: 2.0350<br />Function: Penalty<br />Lambda: 1.1","b:  1.86<br />value: 2.0460<br />Function: Penalty<br />Lambda: 1.1","b:  1.87<br />value: 2.0570<br />Function: Penalty<br />Lambda: 1.1","b:  1.88<br />value: 2.0680<br />Function: Penalty<br />Lambda: 1.1","b:  1.89<br />value: 2.0790<br />Function: Penalty<br />Lambda: 1.1","b:  1.90<br />value: 2.0900<br />Function: Penalty<br />Lambda: 1.1","b:  1.91<br />value: 2.1010<br />Function: Penalty<br />Lambda: 1.1","b:  1.92<br />value: 2.1120<br />Function: Penalty<br />Lambda: 1.1","b:  1.93<br />value: 2.1230<br />Function: Penalty<br />Lambda: 1.1","b:  1.94<br />value: 2.1340<br />Function: Penalty<br />Lambda: 1.1","b:  1.95<br />value: 2.1450<br />Function: Penalty<br />Lambda: 1.1","b:  1.96<br />value: 2.1560<br />Function: Penalty<br />Lambda: 1.1","b:  1.97<br />value: 2.1670<br />Function: Penalty<br />Lambda: 1.1","b:  1.98<br />value: 2.1780<br />Function: Penalty<br />Lambda: 1.1","b:  1.99<br />value: 2.1890<br />Function: Penalty<br />Lambda: 1.1","b:  2.00<br />value: 2.2000<br />Function: Penalty<br />Lambda: 1.1"],"frame":"1.1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"1.2","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 1.2","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 1.2","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 1.2","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 1.2","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 1.2","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 1.2","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 1.2","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 1.2","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 1.2","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 1.2","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 1.2","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 1.2","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 1.2","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 1.2","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 1.2","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 1.2","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 1.2","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 1.2","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 1.2","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 1.2","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 1.2","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 1.2","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 1.2","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 1.2","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 1.2","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 1.2","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 1.2","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 1.2","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 1.2","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 1.2","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 1.2","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 1.2","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 1.2","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 1.2","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 1.2","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 1.2","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 1.2","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 1.2","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 1.2","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 1.2","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 1.2","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 1.2","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 1.2","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 1.2","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 1.2","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 1.2","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 1.2","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 1.2","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 1.2","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 1.2","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.2","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.2","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.2","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.2","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.2","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.2","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.2","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.2","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.2","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.2","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.2","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.2","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.2","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.2","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.2","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.2","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.2","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.2","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.2","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.2","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.2","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.2","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.2","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.2","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.2","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.2","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.2","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.2","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.2","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.2","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.2","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.2","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.2","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.2","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.2","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.2","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.2","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.2","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.2","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.2","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.2","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.2","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.2","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.2","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.2","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.2","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.2","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.2","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.2","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.2","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.2","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.2","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.2","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.2","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.2","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.2","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.2","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.2","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.2","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.2","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.2","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.2","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.2","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.2","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.2","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.2","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.2","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.2","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.2","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.2","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.2","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.2","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.2","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.2","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.2","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.2","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.2","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.2","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.2","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.2","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.2","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.2","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.2","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.2","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.2","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.2","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.2","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.2","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.2","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.2","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.2","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.2","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.2","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.2","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.2","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.2","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.2","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.2","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.2","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.2","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 1.2","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.2","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.2","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.2","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.2","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.2","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.2","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.2","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.2","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.2","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.2","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.2","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.2","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.2","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.2","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.2","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.2","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.2","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.2","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.2","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.2","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.2","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.2","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.2","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.2","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.2","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.2","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.2","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.2","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.2","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.2","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.2","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.2","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.2","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.2","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.2","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.2","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.2","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.2","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.2","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.2","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.2","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.2","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.2","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.2","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.2","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.2","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.2","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.2","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.2","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.2","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.2","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.2","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.2","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.2","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.2","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.2","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.2","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.2","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.2","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.2","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.2","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.2","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.2","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.2","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.2","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.2","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.2","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.2","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.2","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.2","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.2","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.2","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.2","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.2","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.2","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.2","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.2","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.2","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.2","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.2","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.2","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.2","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.2","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.2","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.2","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.2","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.2","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.2","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.2","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.2","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.2","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.2","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.2","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.2","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.2","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.2","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.2","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.2","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.2","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.2"],"frame":"1.2","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.35,3.3081,3.2664,3.2249,3.1836,3.1425,3.1016,3.0609,3.0204,2.9801,2.94,2.9001,2.8604,2.8209,2.7816,2.7425,2.7036,2.6649,2.6264,2.5881,2.55,2.5121,2.4744,2.4369,2.3996,2.3625,2.3256,2.2889,2.2524,2.2161,2.18,2.1441,2.1084,2.0729,2.0376,2.0025,1.9676,1.9329,1.8984,1.8641,1.83,1.7961,1.7624,1.7289,1.6956,1.6625,1.6296,1.5969,1.5644,1.5321,1.5,1.4921,1.4844,1.4769,1.4696,1.4625,1.4556,1.4489,1.4424,1.4361,1.43,1.4241,1.4184,1.4129,1.4076,1.4025,1.3976,1.3929,1.3884,1.3841,1.38,1.3761,1.3724,1.3689,1.3656,1.3625,1.3596,1.3569,1.3544,1.3521,1.35,1.3481,1.3464,1.3449,1.3436,1.3425,1.3416,1.3409,1.3404,1.3401,1.34,1.3401,1.3404,1.3409,1.3416,1.3425,1.3436,1.3449,1.3464,1.3481,1.35,1.3521,1.3544,1.3569,1.3596,1.3625,1.3656,1.3689,1.3724,1.3761,1.38,1.3841,1.3884,1.3929,1.3976,1.4025,1.4076,1.4129,1.4184,1.4241,1.43,1.4361,1.4424,1.4489,1.4556,1.4625,1.4696,1.4769,1.4844,1.4921,1.5,1.5081,1.5164,1.5249,1.5336,1.5425,1.5516,1.5609,1.5704,1.5801,1.59,1.6001,1.6104,1.6209,1.6316,1.6425,1.6536,1.6649,1.6764,1.6881,1.7,1.7121,1.7244,1.7369,1.7496,1.7625,1.7756,1.7889,1.8024,1.8161,1.83,1.8441,1.8584,1.8729,1.8876,1.9025,1.9176,1.9329,1.9484,1.9641,1.98,1.9961,2.0124,2.0289,2.0456,2.0625,2.0796,2.0969,2.1144,2.1321,2.15,2.1681,2.1864,2.2049,2.2236,2.2425,2.2616,2.2809,2.3004,2.3201,2.34,2.3601,2.3804,2.4009,2.4216,2.4425,2.4636,2.4849,2.5064,2.5281,2.55,2.5721,2.5944,2.6169,2.6396,2.6625,2.6856,2.7089,2.7324,2.7561,2.78,2.8041,2.8284,2.8529,2.8776,2.9025,2.9276,2.9529,2.9784,3.0041,3.03,3.0561,3.0824,3.1089,3.1356,3.1625,3.1896,3.2169,3.2444,3.2721,3.3,3.3281,3.3564,3.3849,3.4136,3.4425,3.4716,3.5009,3.5304,3.5601,3.59,3.6201,3.6504,3.6809,3.7116,3.7425,3.7736,3.8049,3.8364,3.8681,3.9],"text":["b: -0.50<br />value: 3.3500<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.49<br />value: 3.3081<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.48<br />value: 3.2664<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.47<br />value: 3.2249<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.46<br />value: 3.1836<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.45<br />value: 3.1425<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.44<br />value: 3.1016<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.43<br />value: 3.0609<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.42<br />value: 3.0204<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.41<br />value: 2.9801<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.40<br />value: 2.9400<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.39<br />value: 2.9001<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.38<br />value: 2.8604<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.37<br />value: 2.8209<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.36<br />value: 2.7816<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.35<br />value: 2.7425<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.34<br />value: 2.7036<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.33<br />value: 2.6649<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.32<br />value: 2.6264<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.31<br />value: 2.5881<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.30<br />value: 2.5500<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.29<br />value: 2.5121<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.28<br />value: 2.4744<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.27<br />value: 2.4369<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.26<br />value: 2.3996<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.25<br />value: 2.3625<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.24<br />value: 2.3256<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.23<br />value: 2.2889<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.22<br />value: 2.2524<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.21<br />value: 2.2161<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.20<br />value: 2.1800<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.19<br />value: 2.1441<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.18<br />value: 2.1084<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.17<br />value: 2.0729<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.16<br />value: 2.0376<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.15<br />value: 2.0025<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.14<br />value: 1.9676<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.13<br />value: 1.9329<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.12<br />value: 1.8984<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.11<br />value: 1.8641<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.10<br />value: 1.8300<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.09<br />value: 1.7961<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.08<br />value: 1.7624<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.07<br />value: 1.7289<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.06<br />value: 1.6956<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.05<br />value: 1.6625<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.04<br />value: 1.6296<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.03<br />value: 1.5969<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.02<br />value: 1.5644<br />Function: Loss + Penalty<br />Lambda: 1.2","b: -0.01<br />value: 1.5321<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.01<br />value: 1.4921<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.02<br />value: 1.4844<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.03<br />value: 1.4769<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.04<br />value: 1.4696<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.05<br />value: 1.4625<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.06<br />value: 1.4556<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.07<br />value: 1.4489<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.08<br />value: 1.4424<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.09<br />value: 1.4361<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.10<br />value: 1.4300<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.11<br />value: 1.4241<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.12<br />value: 1.4184<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.13<br />value: 1.4129<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.14<br />value: 1.4076<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.15<br />value: 1.4025<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.16<br />value: 1.3976<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.17<br />value: 1.3929<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.18<br />value: 1.3884<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.19<br />value: 1.3841<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.20<br />value: 1.3800<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.21<br />value: 1.3761<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.22<br />value: 1.3724<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.23<br />value: 1.3689<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.24<br />value: 1.3656<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.25<br />value: 1.3625<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.26<br />value: 1.3596<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.27<br />value: 1.3569<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.28<br />value: 1.3544<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.29<br />value: 1.3521<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.30<br />value: 1.3500<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.31<br />value: 1.3481<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.32<br />value: 1.3464<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.33<br />value: 1.3449<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.34<br />value: 1.3436<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.35<br />value: 1.3425<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.36<br />value: 1.3416<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.37<br />value: 1.3409<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.38<br />value: 1.3404<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.39<br />value: 1.3401<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.40<br />value: 1.3400<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.41<br />value: 1.3401<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.42<br />value: 1.3404<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.43<br />value: 1.3409<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.44<br />value: 1.3416<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.45<br />value: 1.3425<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.46<br />value: 1.3436<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.47<br />value: 1.3449<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.48<br />value: 1.3464<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.49<br />value: 1.3481<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.50<br />value: 1.3500<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.51<br />value: 1.3521<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.52<br />value: 1.3544<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.53<br />value: 1.3569<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.54<br />value: 1.3596<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.55<br />value: 1.3625<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.56<br />value: 1.3656<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.57<br />value: 1.3689<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.58<br />value: 1.3724<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.59<br />value: 1.3761<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.60<br />value: 1.3800<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.61<br />value: 1.3841<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.62<br />value: 1.3884<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.63<br />value: 1.3929<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.64<br />value: 1.3976<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.65<br />value: 1.4025<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.66<br />value: 1.4076<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.67<br />value: 1.4129<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.68<br />value: 1.4184<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.69<br />value: 1.4241<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.70<br />value: 1.4300<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.71<br />value: 1.4361<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.72<br />value: 1.4424<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.73<br />value: 1.4489<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.74<br />value: 1.4556<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.75<br />value: 1.4625<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.76<br />value: 1.4696<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.77<br />value: 1.4769<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.78<br />value: 1.4844<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.79<br />value: 1.4921<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.80<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.81<br />value: 1.5081<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.82<br />value: 1.5164<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.83<br />value: 1.5249<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.84<br />value: 1.5336<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.85<br />value: 1.5425<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.86<br />value: 1.5516<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.87<br />value: 1.5609<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.88<br />value: 1.5704<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.89<br />value: 1.5801<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.90<br />value: 1.5900<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.91<br />value: 1.6001<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.92<br />value: 1.6104<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.93<br />value: 1.6209<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.94<br />value: 1.6316<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.95<br />value: 1.6425<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.96<br />value: 1.6536<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.97<br />value: 1.6649<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.98<br />value: 1.6764<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  0.99<br />value: 1.6881<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.00<br />value: 1.7000<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.01<br />value: 1.7121<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.02<br />value: 1.7244<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.03<br />value: 1.7369<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.04<br />value: 1.7496<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.05<br />value: 1.7625<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.06<br />value: 1.7756<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.07<br />value: 1.7889<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.08<br />value: 1.8024<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.09<br />value: 1.8161<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.10<br />value: 1.8300<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.11<br />value: 1.8441<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.12<br />value: 1.8584<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.13<br />value: 1.8729<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.14<br />value: 1.8876<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.15<br />value: 1.9025<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.16<br />value: 1.9176<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.17<br />value: 1.9329<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.18<br />value: 1.9484<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.19<br />value: 1.9641<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.20<br />value: 1.9800<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.21<br />value: 1.9961<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.22<br />value: 2.0124<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.23<br />value: 2.0289<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.24<br />value: 2.0456<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.25<br />value: 2.0625<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.26<br />value: 2.0796<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.27<br />value: 2.0969<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.28<br />value: 2.1144<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.29<br />value: 2.1321<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.30<br />value: 2.1500<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.31<br />value: 2.1681<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.32<br />value: 2.1864<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.33<br />value: 2.2049<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.34<br />value: 2.2236<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.35<br />value: 2.2425<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.36<br />value: 2.2616<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.37<br />value: 2.2809<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.38<br />value: 2.3004<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.39<br />value: 2.3201<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.40<br />value: 2.3400<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.41<br />value: 2.3601<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.42<br />value: 2.3804<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.43<br />value: 2.4009<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.44<br />value: 2.4216<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.45<br />value: 2.4425<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.46<br />value: 2.4636<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.47<br />value: 2.4849<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.48<br />value: 2.5064<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.49<br />value: 2.5281<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.50<br />value: 2.5500<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.51<br />value: 2.5721<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.52<br />value: 2.5944<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.53<br />value: 2.6169<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.54<br />value: 2.6396<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.55<br />value: 2.6625<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.56<br />value: 2.6856<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.57<br />value: 2.7089<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.58<br />value: 2.7324<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.59<br />value: 2.7561<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.60<br />value: 2.7800<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.61<br />value: 2.8041<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.62<br />value: 2.8284<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.63<br />value: 2.8529<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.64<br />value: 2.8776<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.65<br />value: 2.9025<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.66<br />value: 2.9276<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.67<br />value: 2.9529<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.68<br />value: 2.9784<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.69<br />value: 3.0041<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.70<br />value: 3.0300<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.71<br />value: 3.0561<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.72<br />value: 3.0824<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.73<br />value: 3.1089<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.74<br />value: 3.1356<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.75<br />value: 3.1625<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.76<br />value: 3.1896<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.77<br />value: 3.2169<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.78<br />value: 3.2444<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.79<br />value: 3.2721<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.80<br />value: 3.3000<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.81<br />value: 3.3281<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.82<br />value: 3.3564<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.83<br />value: 3.3849<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.84<br />value: 3.4136<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.85<br />value: 3.4425<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.86<br />value: 3.4716<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.87<br />value: 3.5009<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.88<br />value: 3.5304<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.89<br />value: 3.5601<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.90<br />value: 3.5900<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.91<br />value: 3.6201<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.92<br />value: 3.6504<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.93<br />value: 3.6809<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.94<br />value: 3.7116<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.95<br />value: 3.7425<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.96<br />value: 3.7736<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.97<br />value: 3.8049<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.98<br />value: 3.8364<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  1.99<br />value: 3.8681<br />Function: Loss + Penalty<br />Lambda: 1.2","b:  2.00<br />value: 3.9000<br />Function: Loss + Penalty<br />Lambda: 1.2"],"frame":"1.2","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.6,0.588,0.576,0.564,0.552,0.54,0.528,0.516,0.504,0.492,0.48,0.468,0.456,0.444,0.432,0.42,0.408,0.396,0.384,0.372,0.36,0.348,0.336,0.324,0.312,0.3,0.288,0.276,0.264,0.252,0.24,0.228,0.216,0.204,0.192,0.18,0.168,0.156,0.144,0.132,0.12,0.108,0.096,0.084,0.072,0.06,0.048,0.036,0.024,0.012,0,0.012,0.024,0.036,0.048,0.0600000000000001,0.0720000000000001,0.0840000000000001,0.096,0.108,0.12,0.132,0.144,0.156,0.168,0.18,0.192,0.204,0.216,0.228,0.24,0.252,0.264,0.276,0.288,0.3,0.312,0.324,0.336,0.348,0.36,0.372,0.384,0.396,0.408,0.42,0.432,0.444,0.456,0.468,0.48,0.492,0.504,0.516,0.528,0.54,0.552,0.564,0.576,0.588,0.6,0.612,0.624,0.636,0.648,0.66,0.672,0.684,0.696,0.708,0.72,0.732,0.744,0.756,0.768,0.78,0.792,0.804,0.816,0.828,0.84,0.852,0.864,0.876,0.888,0.9,0.912,0.924,0.936,0.948,0.96,0.972,0.984,0.996,1.008,1.02,1.032,1.044,1.056,1.068,1.08,1.092,1.104,1.116,1.128,1.14,1.152,1.164,1.176,1.188,1.2,1.212,1.224,1.236,1.248,1.26,1.272,1.284,1.296,1.308,1.32,1.332,1.344,1.356,1.368,1.38,1.392,1.404,1.416,1.428,1.44,1.452,1.464,1.476,1.488,1.5,1.512,1.524,1.536,1.548,1.56,1.572,1.584,1.596,1.608,1.62,1.632,1.644,1.656,1.668,1.68,1.692,1.704,1.716,1.728,1.74,1.752,1.764,1.776,1.788,1.8,1.812,1.824,1.836,1.848,1.86,1.872,1.884,1.896,1.908,1.92,1.932,1.944,1.956,1.968,1.98,1.992,2.004,2.016,2.028,2.04,2.052,2.064,2.076,2.088,2.1,2.112,2.124,2.136,2.148,2.16,2.172,2.184,2.196,2.208,2.22,2.232,2.244,2.256,2.268,2.28,2.292,2.304,2.316,2.328,2.34,2.352,2.364,2.376,2.388,2.4],"text":["b: -0.50<br />value: 0.6000<br />Function: Penalty<br />Lambda: 1.2","b: -0.49<br />value: 0.5880<br />Function: Penalty<br />Lambda: 1.2","b: -0.48<br />value: 0.5760<br />Function: Penalty<br />Lambda: 1.2","b: -0.47<br />value: 0.5640<br />Function: Penalty<br />Lambda: 1.2","b: -0.46<br />value: 0.5520<br />Function: Penalty<br />Lambda: 1.2","b: -0.45<br />value: 0.5400<br />Function: Penalty<br />Lambda: 1.2","b: -0.44<br />value: 0.5280<br />Function: Penalty<br />Lambda: 1.2","b: -0.43<br />value: 0.5160<br />Function: Penalty<br />Lambda: 1.2","b: -0.42<br />value: 0.5040<br />Function: Penalty<br />Lambda: 1.2","b: -0.41<br />value: 0.4920<br />Function: Penalty<br />Lambda: 1.2","b: -0.40<br />value: 0.4800<br />Function: Penalty<br />Lambda: 1.2","b: -0.39<br />value: 0.4680<br />Function: Penalty<br />Lambda: 1.2","b: -0.38<br />value: 0.4560<br />Function: Penalty<br />Lambda: 1.2","b: -0.37<br />value: 0.4440<br />Function: Penalty<br />Lambda: 1.2","b: -0.36<br />value: 0.4320<br />Function: Penalty<br />Lambda: 1.2","b: -0.35<br />value: 0.4200<br />Function: Penalty<br />Lambda: 1.2","b: -0.34<br />value: 0.4080<br />Function: Penalty<br />Lambda: 1.2","b: -0.33<br />value: 0.3960<br />Function: Penalty<br />Lambda: 1.2","b: -0.32<br />value: 0.3840<br />Function: Penalty<br />Lambda: 1.2","b: -0.31<br />value: 0.3720<br />Function: Penalty<br />Lambda: 1.2","b: -0.30<br />value: 0.3600<br />Function: Penalty<br />Lambda: 1.2","b: -0.29<br />value: 0.3480<br />Function: Penalty<br />Lambda: 1.2","b: -0.28<br />value: 0.3360<br />Function: Penalty<br />Lambda: 1.2","b: -0.27<br />value: 0.3240<br />Function: Penalty<br />Lambda: 1.2","b: -0.26<br />value: 0.3120<br />Function: Penalty<br />Lambda: 1.2","b: -0.25<br />value: 0.3000<br />Function: Penalty<br />Lambda: 1.2","b: -0.24<br />value: 0.2880<br />Function: Penalty<br />Lambda: 1.2","b: -0.23<br />value: 0.2760<br />Function: Penalty<br />Lambda: 1.2","b: -0.22<br />value: 0.2640<br />Function: Penalty<br />Lambda: 1.2","b: -0.21<br />value: 0.2520<br />Function: Penalty<br />Lambda: 1.2","b: -0.20<br />value: 0.2400<br />Function: Penalty<br />Lambda: 1.2","b: -0.19<br />value: 0.2280<br />Function: Penalty<br />Lambda: 1.2","b: -0.18<br />value: 0.2160<br />Function: Penalty<br />Lambda: 1.2","b: -0.17<br />value: 0.2040<br />Function: Penalty<br />Lambda: 1.2","b: -0.16<br />value: 0.1920<br />Function: Penalty<br />Lambda: 1.2","b: -0.15<br />value: 0.1800<br />Function: Penalty<br />Lambda: 1.2","b: -0.14<br />value: 0.1680<br />Function: Penalty<br />Lambda: 1.2","b: -0.13<br />value: 0.1560<br />Function: Penalty<br />Lambda: 1.2","b: -0.12<br />value: 0.1440<br />Function: Penalty<br />Lambda: 1.2","b: -0.11<br />value: 0.1320<br />Function: Penalty<br />Lambda: 1.2","b: -0.10<br />value: 0.1200<br />Function: Penalty<br />Lambda: 1.2","b: -0.09<br />value: 0.1080<br />Function: Penalty<br />Lambda: 1.2","b: -0.08<br />value: 0.0960<br />Function: Penalty<br />Lambda: 1.2","b: -0.07<br />value: 0.0840<br />Function: Penalty<br />Lambda: 1.2","b: -0.06<br />value: 0.0720<br />Function: Penalty<br />Lambda: 1.2","b: -0.05<br />value: 0.0600<br />Function: Penalty<br />Lambda: 1.2","b: -0.04<br />value: 0.0480<br />Function: Penalty<br />Lambda: 1.2","b: -0.03<br />value: 0.0360<br />Function: Penalty<br />Lambda: 1.2","b: -0.02<br />value: 0.0240<br />Function: Penalty<br />Lambda: 1.2","b: -0.01<br />value: 0.0120<br />Function: Penalty<br />Lambda: 1.2","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 1.2","b:  0.01<br />value: 0.0120<br />Function: Penalty<br />Lambda: 1.2","b:  0.02<br />value: 0.0240<br />Function: Penalty<br />Lambda: 1.2","b:  0.03<br />value: 0.0360<br />Function: Penalty<br />Lambda: 1.2","b:  0.04<br />value: 0.0480<br />Function: Penalty<br />Lambda: 1.2","b:  0.05<br />value: 0.0600<br />Function: Penalty<br />Lambda: 1.2","b:  0.06<br />value: 0.0720<br />Function: Penalty<br />Lambda: 1.2","b:  0.07<br />value: 0.0840<br />Function: Penalty<br />Lambda: 1.2","b:  0.08<br />value: 0.0960<br />Function: Penalty<br />Lambda: 1.2","b:  0.09<br />value: 0.1080<br />Function: Penalty<br />Lambda: 1.2","b:  0.10<br />value: 0.1200<br />Function: Penalty<br />Lambda: 1.2","b:  0.11<br />value: 0.1320<br />Function: Penalty<br />Lambda: 1.2","b:  0.12<br />value: 0.1440<br />Function: Penalty<br />Lambda: 1.2","b:  0.13<br />value: 0.1560<br />Function: Penalty<br />Lambda: 1.2","b:  0.14<br />value: 0.1680<br />Function: Penalty<br />Lambda: 1.2","b:  0.15<br />value: 0.1800<br />Function: Penalty<br />Lambda: 1.2","b:  0.16<br />value: 0.1920<br />Function: Penalty<br />Lambda: 1.2","b:  0.17<br />value: 0.2040<br />Function: Penalty<br />Lambda: 1.2","b:  0.18<br />value: 0.2160<br />Function: Penalty<br />Lambda: 1.2","b:  0.19<br />value: 0.2280<br />Function: Penalty<br />Lambda: 1.2","b:  0.20<br />value: 0.2400<br />Function: Penalty<br />Lambda: 1.2","b:  0.21<br />value: 0.2520<br />Function: Penalty<br />Lambda: 1.2","b:  0.22<br />value: 0.2640<br />Function: Penalty<br />Lambda: 1.2","b:  0.23<br />value: 0.2760<br />Function: Penalty<br />Lambda: 1.2","b:  0.24<br />value: 0.2880<br />Function: Penalty<br />Lambda: 1.2","b:  0.25<br />value: 0.3000<br />Function: Penalty<br />Lambda: 1.2","b:  0.26<br />value: 0.3120<br />Function: Penalty<br />Lambda: 1.2","b:  0.27<br />value: 0.3240<br />Function: Penalty<br />Lambda: 1.2","b:  0.28<br />value: 0.3360<br />Function: Penalty<br />Lambda: 1.2","b:  0.29<br />value: 0.3480<br />Function: Penalty<br />Lambda: 1.2","b:  0.30<br />value: 0.3600<br />Function: Penalty<br />Lambda: 1.2","b:  0.31<br />value: 0.3720<br />Function: Penalty<br />Lambda: 1.2","b:  0.32<br />value: 0.3840<br />Function: Penalty<br />Lambda: 1.2","b:  0.33<br />value: 0.3960<br />Function: Penalty<br />Lambda: 1.2","b:  0.34<br />value: 0.4080<br />Function: Penalty<br />Lambda: 1.2","b:  0.35<br />value: 0.4200<br />Function: Penalty<br />Lambda: 1.2","b:  0.36<br />value: 0.4320<br />Function: Penalty<br />Lambda: 1.2","b:  0.37<br />value: 0.4440<br />Function: Penalty<br />Lambda: 1.2","b:  0.38<br />value: 0.4560<br />Function: Penalty<br />Lambda: 1.2","b:  0.39<br />value: 0.4680<br />Function: Penalty<br />Lambda: 1.2","b:  0.40<br />value: 0.4800<br />Function: Penalty<br />Lambda: 1.2","b:  0.41<br />value: 0.4920<br />Function: Penalty<br />Lambda: 1.2","b:  0.42<br />value: 0.5040<br />Function: Penalty<br />Lambda: 1.2","b:  0.43<br />value: 0.5160<br />Function: Penalty<br />Lambda: 1.2","b:  0.44<br />value: 0.5280<br />Function: Penalty<br />Lambda: 1.2","b:  0.45<br />value: 0.5400<br />Function: Penalty<br />Lambda: 1.2","b:  0.46<br />value: 0.5520<br />Function: Penalty<br />Lambda: 1.2","b:  0.47<br />value: 0.5640<br />Function: Penalty<br />Lambda: 1.2","b:  0.48<br />value: 0.5760<br />Function: Penalty<br />Lambda: 1.2","b:  0.49<br />value: 0.5880<br />Function: Penalty<br />Lambda: 1.2","b:  0.50<br />value: 0.6000<br />Function: Penalty<br />Lambda: 1.2","b:  0.51<br />value: 0.6120<br />Function: Penalty<br />Lambda: 1.2","b:  0.52<br />value: 0.6240<br />Function: Penalty<br />Lambda: 1.2","b:  0.53<br />value: 0.6360<br />Function: Penalty<br />Lambda: 1.2","b:  0.54<br />value: 0.6480<br />Function: Penalty<br />Lambda: 1.2","b:  0.55<br />value: 0.6600<br />Function: Penalty<br />Lambda: 1.2","b:  0.56<br />value: 0.6720<br />Function: Penalty<br />Lambda: 1.2","b:  0.57<br />value: 0.6840<br />Function: Penalty<br />Lambda: 1.2","b:  0.58<br />value: 0.6960<br />Function: Penalty<br />Lambda: 1.2","b:  0.59<br />value: 0.7080<br />Function: Penalty<br />Lambda: 1.2","b:  0.60<br />value: 0.7200<br />Function: Penalty<br />Lambda: 1.2","b:  0.61<br />value: 0.7320<br />Function: Penalty<br />Lambda: 1.2","b:  0.62<br />value: 0.7440<br />Function: Penalty<br />Lambda: 1.2","b:  0.63<br />value: 0.7560<br />Function: Penalty<br />Lambda: 1.2","b:  0.64<br />value: 0.7680<br />Function: Penalty<br />Lambda: 1.2","b:  0.65<br />value: 0.7800<br />Function: Penalty<br />Lambda: 1.2","b:  0.66<br />value: 0.7920<br />Function: Penalty<br />Lambda: 1.2","b:  0.67<br />value: 0.8040<br />Function: Penalty<br />Lambda: 1.2","b:  0.68<br />value: 0.8160<br />Function: Penalty<br />Lambda: 1.2","b:  0.69<br />value: 0.8280<br />Function: Penalty<br />Lambda: 1.2","b:  0.70<br />value: 0.8400<br />Function: Penalty<br />Lambda: 1.2","b:  0.71<br />value: 0.8520<br />Function: Penalty<br />Lambda: 1.2","b:  0.72<br />value: 0.8640<br />Function: Penalty<br />Lambda: 1.2","b:  0.73<br />value: 0.8760<br />Function: Penalty<br />Lambda: 1.2","b:  0.74<br />value: 0.8880<br />Function: Penalty<br />Lambda: 1.2","b:  0.75<br />value: 0.9000<br />Function: Penalty<br />Lambda: 1.2","b:  0.76<br />value: 0.9120<br />Function: Penalty<br />Lambda: 1.2","b:  0.77<br />value: 0.9240<br />Function: Penalty<br />Lambda: 1.2","b:  0.78<br />value: 0.9360<br />Function: Penalty<br />Lambda: 1.2","b:  0.79<br />value: 0.9480<br />Function: Penalty<br />Lambda: 1.2","b:  0.80<br />value: 0.9600<br />Function: Penalty<br />Lambda: 1.2","b:  0.81<br />value: 0.9720<br />Function: Penalty<br />Lambda: 1.2","b:  0.82<br />value: 0.9840<br />Function: Penalty<br />Lambda: 1.2","b:  0.83<br />value: 0.9960<br />Function: Penalty<br />Lambda: 1.2","b:  0.84<br />value: 1.0080<br />Function: Penalty<br />Lambda: 1.2","b:  0.85<br />value: 1.0200<br />Function: Penalty<br />Lambda: 1.2","b:  0.86<br />value: 1.0320<br />Function: Penalty<br />Lambda: 1.2","b:  0.87<br />value: 1.0440<br />Function: Penalty<br />Lambda: 1.2","b:  0.88<br />value: 1.0560<br />Function: Penalty<br />Lambda: 1.2","b:  0.89<br />value: 1.0680<br />Function: Penalty<br />Lambda: 1.2","b:  0.90<br />value: 1.0800<br />Function: Penalty<br />Lambda: 1.2","b:  0.91<br />value: 1.0920<br />Function: Penalty<br />Lambda: 1.2","b:  0.92<br />value: 1.1040<br />Function: Penalty<br />Lambda: 1.2","b:  0.93<br />value: 1.1160<br />Function: Penalty<br />Lambda: 1.2","b:  0.94<br />value: 1.1280<br />Function: Penalty<br />Lambda: 1.2","b:  0.95<br />value: 1.1400<br />Function: Penalty<br />Lambda: 1.2","b:  0.96<br />value: 1.1520<br />Function: Penalty<br />Lambda: 1.2","b:  0.97<br />value: 1.1640<br />Function: Penalty<br />Lambda: 1.2","b:  0.98<br />value: 1.1760<br />Function: Penalty<br />Lambda: 1.2","b:  0.99<br />value: 1.1880<br />Function: Penalty<br />Lambda: 1.2","b:  1.00<br />value: 1.2000<br />Function: Penalty<br />Lambda: 1.2","b:  1.01<br />value: 1.2120<br />Function: Penalty<br />Lambda: 1.2","b:  1.02<br />value: 1.2240<br />Function: Penalty<br />Lambda: 1.2","b:  1.03<br />value: 1.2360<br />Function: Penalty<br />Lambda: 1.2","b:  1.04<br />value: 1.2480<br />Function: Penalty<br />Lambda: 1.2","b:  1.05<br />value: 1.2600<br />Function: Penalty<br />Lambda: 1.2","b:  1.06<br />value: 1.2720<br />Function: Penalty<br />Lambda: 1.2","b:  1.07<br />value: 1.2840<br />Function: Penalty<br />Lambda: 1.2","b:  1.08<br />value: 1.2960<br />Function: Penalty<br />Lambda: 1.2","b:  1.09<br />value: 1.3080<br />Function: Penalty<br />Lambda: 1.2","b:  1.10<br />value: 1.3200<br />Function: Penalty<br />Lambda: 1.2","b:  1.11<br />value: 1.3320<br />Function: Penalty<br />Lambda: 1.2","b:  1.12<br />value: 1.3440<br />Function: Penalty<br />Lambda: 1.2","b:  1.13<br />value: 1.3560<br />Function: Penalty<br />Lambda: 1.2","b:  1.14<br />value: 1.3680<br />Function: Penalty<br />Lambda: 1.2","b:  1.15<br />value: 1.3800<br />Function: Penalty<br />Lambda: 1.2","b:  1.16<br />value: 1.3920<br />Function: Penalty<br />Lambda: 1.2","b:  1.17<br />value: 1.4040<br />Function: Penalty<br />Lambda: 1.2","b:  1.18<br />value: 1.4160<br />Function: Penalty<br />Lambda: 1.2","b:  1.19<br />value: 1.4280<br />Function: Penalty<br />Lambda: 1.2","b:  1.20<br />value: 1.4400<br />Function: Penalty<br />Lambda: 1.2","b:  1.21<br />value: 1.4520<br />Function: Penalty<br />Lambda: 1.2","b:  1.22<br />value: 1.4640<br />Function: Penalty<br />Lambda: 1.2","b:  1.23<br />value: 1.4760<br />Function: Penalty<br />Lambda: 1.2","b:  1.24<br />value: 1.4880<br />Function: Penalty<br />Lambda: 1.2","b:  1.25<br />value: 1.5000<br />Function: Penalty<br />Lambda: 1.2","b:  1.26<br />value: 1.5120<br />Function: Penalty<br />Lambda: 1.2","b:  1.27<br />value: 1.5240<br />Function: Penalty<br />Lambda: 1.2","b:  1.28<br />value: 1.5360<br />Function: Penalty<br />Lambda: 1.2","b:  1.29<br />value: 1.5480<br />Function: Penalty<br />Lambda: 1.2","b:  1.30<br />value: 1.5600<br />Function: Penalty<br />Lambda: 1.2","b:  1.31<br />value: 1.5720<br />Function: Penalty<br />Lambda: 1.2","b:  1.32<br />value: 1.5840<br />Function: Penalty<br />Lambda: 1.2","b:  1.33<br />value: 1.5960<br />Function: Penalty<br />Lambda: 1.2","b:  1.34<br />value: 1.6080<br />Function: Penalty<br />Lambda: 1.2","b:  1.35<br />value: 1.6200<br />Function: Penalty<br />Lambda: 1.2","b:  1.36<br />value: 1.6320<br />Function: Penalty<br />Lambda: 1.2","b:  1.37<br />value: 1.6440<br />Function: Penalty<br />Lambda: 1.2","b:  1.38<br />value: 1.6560<br />Function: Penalty<br />Lambda: 1.2","b:  1.39<br />value: 1.6680<br />Function: Penalty<br />Lambda: 1.2","b:  1.40<br />value: 1.6800<br />Function: Penalty<br />Lambda: 1.2","b:  1.41<br />value: 1.6920<br />Function: Penalty<br />Lambda: 1.2","b:  1.42<br />value: 1.7040<br />Function: Penalty<br />Lambda: 1.2","b:  1.43<br />value: 1.7160<br />Function: Penalty<br />Lambda: 1.2","b:  1.44<br />value: 1.7280<br />Function: Penalty<br />Lambda: 1.2","b:  1.45<br />value: 1.7400<br />Function: Penalty<br />Lambda: 1.2","b:  1.46<br />value: 1.7520<br />Function: Penalty<br />Lambda: 1.2","b:  1.47<br />value: 1.7640<br />Function: Penalty<br />Lambda: 1.2","b:  1.48<br />value: 1.7760<br />Function: Penalty<br />Lambda: 1.2","b:  1.49<br />value: 1.7880<br />Function: Penalty<br />Lambda: 1.2","b:  1.50<br />value: 1.8000<br />Function: Penalty<br />Lambda: 1.2","b:  1.51<br />value: 1.8120<br />Function: Penalty<br />Lambda: 1.2","b:  1.52<br />value: 1.8240<br />Function: Penalty<br />Lambda: 1.2","b:  1.53<br />value: 1.8360<br />Function: Penalty<br />Lambda: 1.2","b:  1.54<br />value: 1.8480<br />Function: Penalty<br />Lambda: 1.2","b:  1.55<br />value: 1.8600<br />Function: Penalty<br />Lambda: 1.2","b:  1.56<br />value: 1.8720<br />Function: Penalty<br />Lambda: 1.2","b:  1.57<br />value: 1.8840<br />Function: Penalty<br />Lambda: 1.2","b:  1.58<br />value: 1.8960<br />Function: Penalty<br />Lambda: 1.2","b:  1.59<br />value: 1.9080<br />Function: Penalty<br />Lambda: 1.2","b:  1.60<br />value: 1.9200<br />Function: Penalty<br />Lambda: 1.2","b:  1.61<br />value: 1.9320<br />Function: Penalty<br />Lambda: 1.2","b:  1.62<br />value: 1.9440<br />Function: Penalty<br />Lambda: 1.2","b:  1.63<br />value: 1.9560<br />Function: Penalty<br />Lambda: 1.2","b:  1.64<br />value: 1.9680<br />Function: Penalty<br />Lambda: 1.2","b:  1.65<br />value: 1.9800<br />Function: Penalty<br />Lambda: 1.2","b:  1.66<br />value: 1.9920<br />Function: Penalty<br />Lambda: 1.2","b:  1.67<br />value: 2.0040<br />Function: Penalty<br />Lambda: 1.2","b:  1.68<br />value: 2.0160<br />Function: Penalty<br />Lambda: 1.2","b:  1.69<br />value: 2.0280<br />Function: Penalty<br />Lambda: 1.2","b:  1.70<br />value: 2.0400<br />Function: Penalty<br />Lambda: 1.2","b:  1.71<br />value: 2.0520<br />Function: Penalty<br />Lambda: 1.2","b:  1.72<br />value: 2.0640<br />Function: Penalty<br />Lambda: 1.2","b:  1.73<br />value: 2.0760<br />Function: Penalty<br />Lambda: 1.2","b:  1.74<br />value: 2.0880<br />Function: Penalty<br />Lambda: 1.2","b:  1.75<br />value: 2.1000<br />Function: Penalty<br />Lambda: 1.2","b:  1.76<br />value: 2.1120<br />Function: Penalty<br />Lambda: 1.2","b:  1.77<br />value: 2.1240<br />Function: Penalty<br />Lambda: 1.2","b:  1.78<br />value: 2.1360<br />Function: Penalty<br />Lambda: 1.2","b:  1.79<br />value: 2.1480<br />Function: Penalty<br />Lambda: 1.2","b:  1.80<br />value: 2.1600<br />Function: Penalty<br />Lambda: 1.2","b:  1.81<br />value: 2.1720<br />Function: Penalty<br />Lambda: 1.2","b:  1.82<br />value: 2.1840<br />Function: Penalty<br />Lambda: 1.2","b:  1.83<br />value: 2.1960<br />Function: Penalty<br />Lambda: 1.2","b:  1.84<br />value: 2.2080<br />Function: Penalty<br />Lambda: 1.2","b:  1.85<br />value: 2.2200<br />Function: Penalty<br />Lambda: 1.2","b:  1.86<br />value: 2.2320<br />Function: Penalty<br />Lambda: 1.2","b:  1.87<br />value: 2.2440<br />Function: Penalty<br />Lambda: 1.2","b:  1.88<br />value: 2.2560<br />Function: Penalty<br />Lambda: 1.2","b:  1.89<br />value: 2.2680<br />Function: Penalty<br />Lambda: 1.2","b:  1.90<br />value: 2.2800<br />Function: Penalty<br />Lambda: 1.2","b:  1.91<br />value: 2.2920<br />Function: Penalty<br />Lambda: 1.2","b:  1.92<br />value: 2.3040<br />Function: Penalty<br />Lambda: 1.2","b:  1.93<br />value: 2.3160<br />Function: Penalty<br />Lambda: 1.2","b:  1.94<br />value: 2.3280<br />Function: Penalty<br />Lambda: 1.2","b:  1.95<br />value: 2.3400<br />Function: Penalty<br />Lambda: 1.2","b:  1.96<br />value: 2.3520<br />Function: Penalty<br />Lambda: 1.2","b:  1.97<br />value: 2.3640<br />Function: Penalty<br />Lambda: 1.2","b:  1.98<br />value: 2.3760<br />Function: Penalty<br />Lambda: 1.2","b:  1.99<br />value: 2.3880<br />Function: Penalty<br />Lambda: 1.2","b:  2.00<br />value: 2.4000<br />Function: Penalty<br />Lambda: 1.2"],"frame":"1.2","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"1.3","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 1.3","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 1.3","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 1.3","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 1.3","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 1.3","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 1.3","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 1.3","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 1.3","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 1.3","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 1.3","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 1.3","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 1.3","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 1.3","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 1.3","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 1.3","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 1.3","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 1.3","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 1.3","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 1.3","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 1.3","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 1.3","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 1.3","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 1.3","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 1.3","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 1.3","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 1.3","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 1.3","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 1.3","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 1.3","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 1.3","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 1.3","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 1.3","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 1.3","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 1.3","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 1.3","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 1.3","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 1.3","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 1.3","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 1.3","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 1.3","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 1.3","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 1.3","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 1.3","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 1.3","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 1.3","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 1.3","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 1.3","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 1.3","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 1.3","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 1.3","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.3","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.3","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.3","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.3","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.3","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.3","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.3","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.3","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.3","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.3","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.3","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.3","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.3","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.3","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.3","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.3","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.3","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.3","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.3","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.3","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.3","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.3","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.3","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.3","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.3","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.3","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.3","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.3","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.3","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.3","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.3","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.3","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.3","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.3","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.3","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.3","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.3","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.3","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.3","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.3","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.3","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.3","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.3","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.3","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.3","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.3","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.3","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.3","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.3","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.3","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.3","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.3","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.3","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.3","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.3","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.3","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.3","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.3","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.3","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.3","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.3","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.3","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.3","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.3","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.3","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.3","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.3","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.3","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.3","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.3","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.3","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.3","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.3","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.3","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.3","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.3","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.3","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.3","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.3","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.3","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.3","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.3","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.3","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.3","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.3","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.3","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.3","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.3","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.3","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.3","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.3","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.3","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.3","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.3","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.3","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.3","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.3","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.3","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.3","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.3","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 1.3","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.3","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.3","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.3","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.3","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.3","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.3","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.3","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.3","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.3","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.3","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.3","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.3","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.3","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.3","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.3","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.3","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.3","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.3","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.3","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.3","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.3","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.3","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.3","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.3","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.3","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.3","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.3","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.3","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.3","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.3","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.3","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.3","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.3","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.3","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.3","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.3","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.3","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.3","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.3","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.3","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.3","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.3","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.3","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.3","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.3","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.3","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.3","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.3","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.3","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.3","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.3","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.3","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.3","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.3","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.3","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.3","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.3","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.3","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.3","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.3","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.3","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.3","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.3","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.3","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.3","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.3","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.3","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.3","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.3","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.3","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.3","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.3","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.3","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.3","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.3","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.3","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.3","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.3","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.3","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.3","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.3","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.3","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.3","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.3","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.3","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.3","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.3","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.3","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.3","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.3","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.3","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.3","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.3","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.3","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.3","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.3","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.3","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.3","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.3","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.3"],"frame":"1.3","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.4,3.3571,3.3144,3.2719,3.2296,3.1875,3.1456,3.1039,3.0624,3.0211,2.98,2.9391,2.8984,2.8579,2.8176,2.7775,2.7376,2.6979,2.6584,2.6191,2.58,2.5411,2.5024,2.4639,2.4256,2.3875,2.3496,2.3119,2.2744,2.2371,2.2,2.1631,2.1264,2.0899,2.0536,2.0175,1.9816,1.9459,1.9104,1.8751,1.84,1.8051,1.7704,1.7359,1.7016,1.6675,1.6336,1.5999,1.5664,1.5331,1.5,1.4931,1.4864,1.4799,1.4736,1.4675,1.4616,1.4559,1.4504,1.4451,1.44,1.4351,1.4304,1.4259,1.4216,1.4175,1.4136,1.4099,1.4064,1.4031,1.4,1.3971,1.3944,1.3919,1.3896,1.3875,1.3856,1.3839,1.3824,1.3811,1.38,1.3791,1.3784,1.3779,1.3776,1.3775,1.3776,1.3779,1.3784,1.3791,1.38,1.3811,1.3824,1.3839,1.3856,1.3875,1.3896,1.3919,1.3944,1.3971,1.4,1.4031,1.4064,1.4099,1.4136,1.4175,1.4216,1.4259,1.4304,1.4351,1.44,1.4451,1.4504,1.4559,1.4616,1.4675,1.4736,1.4799,1.4864,1.4931,1.5,1.5071,1.5144,1.5219,1.5296,1.5375,1.5456,1.5539,1.5624,1.5711,1.58,1.5891,1.5984,1.6079,1.6176,1.6275,1.6376,1.6479,1.6584,1.6691,1.68,1.6911,1.7024,1.7139,1.7256,1.7375,1.7496,1.7619,1.7744,1.7871,1.8,1.8131,1.8264,1.8399,1.8536,1.8675,1.8816,1.8959,1.9104,1.9251,1.94,1.9551,1.9704,1.9859,2.0016,2.0175,2.0336,2.0499,2.0664,2.0831,2.1,2.1171,2.1344,2.1519,2.1696,2.1875,2.2056,2.2239,2.2424,2.2611,2.28,2.2991,2.3184,2.3379,2.3576,2.3775,2.3976,2.4179,2.4384,2.4591,2.48,2.5011,2.5224,2.5439,2.5656,2.5875,2.6096,2.6319,2.6544,2.6771,2.7,2.7231,2.7464,2.7699,2.7936,2.8175,2.8416,2.8659,2.8904,2.9151,2.94,2.9651,2.9904,3.0159,3.0416,3.0675,3.0936,3.1199,3.1464,3.1731,3.2,3.2271,3.2544,3.2819,3.3096,3.3375,3.3656,3.3939,3.4224,3.4511,3.48,3.5091,3.5384,3.5679,3.5976,3.6275,3.6576,3.6879,3.7184,3.7491,3.78,3.8111,3.8424,3.8739,3.9056,3.9375,3.9696,null,null,null,null],"text":["b: -0.50<br />value: 3.4000<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.49<br />value: 3.3571<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.48<br />value: 3.3144<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.47<br />value: 3.2719<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.46<br />value: 3.2296<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.45<br />value: 3.1875<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.44<br />value: 3.1456<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.43<br />value: 3.1039<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.42<br />value: 3.0624<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.41<br />value: 3.0211<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.40<br />value: 2.9800<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.39<br />value: 2.9391<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.38<br />value: 2.8984<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.37<br />value: 2.8579<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.36<br />value: 2.8176<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.35<br />value: 2.7775<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.34<br />value: 2.7376<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.33<br />value: 2.6979<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.32<br />value: 2.6584<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.31<br />value: 2.6191<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.30<br />value: 2.5800<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.29<br />value: 2.5411<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.28<br />value: 2.5024<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.27<br />value: 2.4639<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.26<br />value: 2.4256<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.25<br />value: 2.3875<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.24<br />value: 2.3496<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.23<br />value: 2.3119<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.22<br />value: 2.2744<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.21<br />value: 2.2371<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.20<br />value: 2.2000<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.19<br />value: 2.1631<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.18<br />value: 2.1264<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.17<br />value: 2.0899<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.16<br />value: 2.0536<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.15<br />value: 2.0175<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.14<br />value: 1.9816<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.13<br />value: 1.9459<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.12<br />value: 1.9104<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.11<br />value: 1.8751<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.10<br />value: 1.8400<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.09<br />value: 1.8051<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.08<br />value: 1.7704<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.07<br />value: 1.7359<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.06<br />value: 1.7016<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.05<br />value: 1.6675<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.04<br />value: 1.6336<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.03<br />value: 1.5999<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.02<br />value: 1.5664<br />Function: Loss + Penalty<br />Lambda: 1.3","b: -0.01<br />value: 1.5331<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.01<br />value: 1.4931<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.02<br />value: 1.4864<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.03<br />value: 1.4799<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.04<br />value: 1.4736<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.05<br />value: 1.4675<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.06<br />value: 1.4616<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.07<br />value: 1.4559<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.08<br />value: 1.4504<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.09<br />value: 1.4451<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.10<br />value: 1.4400<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.11<br />value: 1.4351<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.12<br />value: 1.4304<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.13<br />value: 1.4259<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.14<br />value: 1.4216<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.15<br />value: 1.4175<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.16<br />value: 1.4136<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.17<br />value: 1.4099<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.18<br />value: 1.4064<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.19<br />value: 1.4031<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.20<br />value: 1.4000<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.21<br />value: 1.3971<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.22<br />value: 1.3944<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.23<br />value: 1.3919<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.24<br />value: 1.3896<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.25<br />value: 1.3875<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.26<br />value: 1.3856<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.27<br />value: 1.3839<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.28<br />value: 1.3824<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.29<br />value: 1.3811<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.30<br />value: 1.3800<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.31<br />value: 1.3791<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.32<br />value: 1.3784<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.33<br />value: 1.3779<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.34<br />value: 1.3776<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.35<br />value: 1.3775<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.36<br />value: 1.3776<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.37<br />value: 1.3779<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.38<br />value: 1.3784<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.39<br />value: 1.3791<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.40<br />value: 1.3800<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.41<br />value: 1.3811<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.42<br />value: 1.3824<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.43<br />value: 1.3839<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.44<br />value: 1.3856<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.45<br />value: 1.3875<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.46<br />value: 1.3896<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.47<br />value: 1.3919<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.48<br />value: 1.3944<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.49<br />value: 1.3971<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.50<br />value: 1.4000<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.51<br />value: 1.4031<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.52<br />value: 1.4064<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.53<br />value: 1.4099<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.54<br />value: 1.4136<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.55<br />value: 1.4175<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.56<br />value: 1.4216<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.57<br />value: 1.4259<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.58<br />value: 1.4304<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.59<br />value: 1.4351<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.60<br />value: 1.4400<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.61<br />value: 1.4451<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.62<br />value: 1.4504<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.63<br />value: 1.4559<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.64<br />value: 1.4616<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.65<br />value: 1.4675<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.66<br />value: 1.4736<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.67<br />value: 1.4799<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.68<br />value: 1.4864<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.69<br />value: 1.4931<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.70<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.71<br />value: 1.5071<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.72<br />value: 1.5144<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.73<br />value: 1.5219<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.74<br />value: 1.5296<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.75<br />value: 1.5375<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.76<br />value: 1.5456<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.77<br />value: 1.5539<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.78<br />value: 1.5624<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.79<br />value: 1.5711<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.80<br />value: 1.5800<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.81<br />value: 1.5891<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.82<br />value: 1.5984<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.83<br />value: 1.6079<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.84<br />value: 1.6176<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.85<br />value: 1.6275<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.86<br />value: 1.6376<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.87<br />value: 1.6479<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.88<br />value: 1.6584<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.89<br />value: 1.6691<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.90<br />value: 1.6800<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.91<br />value: 1.6911<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.92<br />value: 1.7024<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.93<br />value: 1.7139<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.94<br />value: 1.7256<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.95<br />value: 1.7375<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.96<br />value: 1.7496<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.97<br />value: 1.7619<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.98<br />value: 1.7744<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  0.99<br />value: 1.7871<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.00<br />value: 1.8000<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.01<br />value: 1.8131<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.02<br />value: 1.8264<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.03<br />value: 1.8399<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.04<br />value: 1.8536<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.05<br />value: 1.8675<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.06<br />value: 1.8816<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.07<br />value: 1.8959<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.08<br />value: 1.9104<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.09<br />value: 1.9251<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.10<br />value: 1.9400<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.11<br />value: 1.9551<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.12<br />value: 1.9704<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.13<br />value: 1.9859<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.14<br />value: 2.0016<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.15<br />value: 2.0175<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.16<br />value: 2.0336<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.17<br />value: 2.0499<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.18<br />value: 2.0664<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.19<br />value: 2.0831<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.20<br />value: 2.1000<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.21<br />value: 2.1171<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.22<br />value: 2.1344<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.23<br />value: 2.1519<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.24<br />value: 2.1696<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.25<br />value: 2.1875<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.26<br />value: 2.2056<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.27<br />value: 2.2239<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.28<br />value: 2.2424<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.29<br />value: 2.2611<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.30<br />value: 2.2800<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.31<br />value: 2.2991<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.32<br />value: 2.3184<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.33<br />value: 2.3379<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.34<br />value: 2.3576<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.35<br />value: 2.3775<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.36<br />value: 2.3976<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.37<br />value: 2.4179<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.38<br />value: 2.4384<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.39<br />value: 2.4591<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.40<br />value: 2.4800<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.41<br />value: 2.5011<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.42<br />value: 2.5224<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.43<br />value: 2.5439<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.44<br />value: 2.5656<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.45<br />value: 2.5875<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.46<br />value: 2.6096<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.47<br />value: 2.6319<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.48<br />value: 2.6544<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.49<br />value: 2.6771<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.50<br />value: 2.7000<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.51<br />value: 2.7231<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.52<br />value: 2.7464<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.53<br />value: 2.7699<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.54<br />value: 2.7936<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.55<br />value: 2.8175<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.56<br />value: 2.8416<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.57<br />value: 2.8659<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.58<br />value: 2.8904<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.59<br />value: 2.9151<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.60<br />value: 2.9400<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.61<br />value: 2.9651<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.62<br />value: 2.9904<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.63<br />value: 3.0159<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.64<br />value: 3.0416<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.65<br />value: 3.0675<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.66<br />value: 3.0936<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.67<br />value: 3.1199<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.68<br />value: 3.1464<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.69<br />value: 3.1731<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.70<br />value: 3.2000<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.71<br />value: 3.2271<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.72<br />value: 3.2544<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.73<br />value: 3.2819<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.74<br />value: 3.3096<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.75<br />value: 3.3375<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.76<br />value: 3.3656<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.77<br />value: 3.3939<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.78<br />value: 3.4224<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.79<br />value: 3.4511<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.80<br />value: 3.4800<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.81<br />value: 3.5091<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.82<br />value: 3.5384<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.83<br />value: 3.5679<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.84<br />value: 3.5976<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.85<br />value: 3.6275<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.86<br />value: 3.6576<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.87<br />value: 3.6879<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.88<br />value: 3.7184<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.89<br />value: 3.7491<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.90<br />value: 3.7800<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.91<br />value: 3.8111<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.92<br />value: 3.8424<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.93<br />value: 3.8739<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.94<br />value: 3.9056<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.95<br />value: 3.9375<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.96<br />value: 3.9696<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.97<br />value: 4.0019<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.98<br />value: 4.0344<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  1.99<br />value: 4.0671<br />Function: Loss + Penalty<br />Lambda: 1.3","b:  2.00<br />value: 4.1000<br />Function: Loss + Penalty<br />Lambda: 1.3"],"frame":"1.3","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.65,0.637,0.624,0.611,0.598,0.585,0.572,0.559,0.546,0.533,0.52,0.507,0.494,0.481,0.468,0.455,0.442,0.429,0.416,0.403,0.39,0.377,0.364,0.351,0.338,0.325,0.312,0.299,0.286,0.273,0.26,0.247,0.234,0.221,0.208,0.195,0.182,0.169,0.156,0.143,0.13,0.117,0.104,0.091,0.078,0.065,0.052,0.039,0.026,0.013,0,0.013,0.026,0.039,0.052,0.0650000000000001,0.0780000000000001,0.0910000000000001,0.104,0.117,0.13,0.143,0.156,0.169,0.182,0.195,0.208,0.221,0.234,0.247,0.26,0.273,0.286,0.299,0.312,0.325,0.338,0.351,0.364,0.377,0.39,0.403,0.416,0.429,0.442,0.455,0.468,0.481,0.494,0.507,0.52,0.533,0.546,0.559,0.572,0.585,0.598,0.611,0.624,0.637,0.65,0.663,0.676,0.689,0.702,0.715,0.728,0.741,0.754,0.767,0.78,0.793,0.806,0.819,0.832,0.845,0.858,0.871,0.884,0.897,0.91,0.923,0.936,0.949,0.962,0.975,0.988,1.001,1.014,1.027,1.04,1.053,1.066,1.079,1.092,1.105,1.118,1.131,1.144,1.157,1.17,1.183,1.196,1.209,1.222,1.235,1.248,1.261,1.274,1.287,1.3,1.313,1.326,1.339,1.352,1.365,1.378,1.391,1.404,1.417,1.43,1.443,1.456,1.469,1.482,1.495,1.508,1.521,1.534,1.547,1.56,1.573,1.586,1.599,1.612,1.625,1.638,1.651,1.664,1.677,1.69,1.703,1.716,1.729,1.742,1.755,1.768,1.781,1.794,1.807,1.82,1.833,1.846,1.859,1.872,1.885,1.898,1.911,1.924,1.937,1.95,1.963,1.976,1.989,2.002,2.015,2.028,2.041,2.054,2.067,2.08,2.093,2.106,2.119,2.132,2.145,2.158,2.171,2.184,2.197,2.21,2.223,2.236,2.249,2.262,2.275,2.288,2.301,2.314,2.327,2.34,2.353,2.366,2.379,2.392,2.405,2.418,2.431,2.444,2.457,2.47,2.483,2.496,2.509,2.522,2.535,2.548,2.561,2.574,2.587,2.6],"text":["b: -0.50<br />value: 0.6500<br />Function: Penalty<br />Lambda: 1.3","b: -0.49<br />value: 0.6370<br />Function: Penalty<br />Lambda: 1.3","b: -0.48<br />value: 0.6240<br />Function: Penalty<br />Lambda: 1.3","b: -0.47<br />value: 0.6110<br />Function: Penalty<br />Lambda: 1.3","b: -0.46<br />value: 0.5980<br />Function: Penalty<br />Lambda: 1.3","b: -0.45<br />value: 0.5850<br />Function: Penalty<br />Lambda: 1.3","b: -0.44<br />value: 0.5720<br />Function: Penalty<br />Lambda: 1.3","b: -0.43<br />value: 0.5590<br />Function: Penalty<br />Lambda: 1.3","b: -0.42<br />value: 0.5460<br />Function: Penalty<br />Lambda: 1.3","b: -0.41<br />value: 0.5330<br />Function: Penalty<br />Lambda: 1.3","b: -0.40<br />value: 0.5200<br />Function: Penalty<br />Lambda: 1.3","b: -0.39<br />value: 0.5070<br />Function: Penalty<br />Lambda: 1.3","b: -0.38<br />value: 0.4940<br />Function: Penalty<br />Lambda: 1.3","b: -0.37<br />value: 0.4810<br />Function: Penalty<br />Lambda: 1.3","b: -0.36<br />value: 0.4680<br />Function: Penalty<br />Lambda: 1.3","b: -0.35<br />value: 0.4550<br />Function: Penalty<br />Lambda: 1.3","b: -0.34<br />value: 0.4420<br />Function: Penalty<br />Lambda: 1.3","b: -0.33<br />value: 0.4290<br />Function: Penalty<br />Lambda: 1.3","b: -0.32<br />value: 0.4160<br />Function: Penalty<br />Lambda: 1.3","b: -0.31<br />value: 0.4030<br />Function: Penalty<br />Lambda: 1.3","b: -0.30<br />value: 0.3900<br />Function: Penalty<br />Lambda: 1.3","b: -0.29<br />value: 0.3770<br />Function: Penalty<br />Lambda: 1.3","b: -0.28<br />value: 0.3640<br />Function: Penalty<br />Lambda: 1.3","b: -0.27<br />value: 0.3510<br />Function: Penalty<br />Lambda: 1.3","b: -0.26<br />value: 0.3380<br />Function: Penalty<br />Lambda: 1.3","b: -0.25<br />value: 0.3250<br />Function: Penalty<br />Lambda: 1.3","b: -0.24<br />value: 0.3120<br />Function: Penalty<br />Lambda: 1.3","b: -0.23<br />value: 0.2990<br />Function: Penalty<br />Lambda: 1.3","b: -0.22<br />value: 0.2860<br />Function: Penalty<br />Lambda: 1.3","b: -0.21<br />value: 0.2730<br />Function: Penalty<br />Lambda: 1.3","b: -0.20<br />value: 0.2600<br />Function: Penalty<br />Lambda: 1.3","b: -0.19<br />value: 0.2470<br />Function: Penalty<br />Lambda: 1.3","b: -0.18<br />value: 0.2340<br />Function: Penalty<br />Lambda: 1.3","b: -0.17<br />value: 0.2210<br />Function: Penalty<br />Lambda: 1.3","b: -0.16<br />value: 0.2080<br />Function: Penalty<br />Lambda: 1.3","b: -0.15<br />value: 0.1950<br />Function: Penalty<br />Lambda: 1.3","b: -0.14<br />value: 0.1820<br />Function: Penalty<br />Lambda: 1.3","b: -0.13<br />value: 0.1690<br />Function: Penalty<br />Lambda: 1.3","b: -0.12<br />value: 0.1560<br />Function: Penalty<br />Lambda: 1.3","b: -0.11<br />value: 0.1430<br />Function: Penalty<br />Lambda: 1.3","b: -0.10<br />value: 0.1300<br />Function: Penalty<br />Lambda: 1.3","b: -0.09<br />value: 0.1170<br />Function: Penalty<br />Lambda: 1.3","b: -0.08<br />value: 0.1040<br />Function: Penalty<br />Lambda: 1.3","b: -0.07<br />value: 0.0910<br />Function: Penalty<br />Lambda: 1.3","b: -0.06<br />value: 0.0780<br />Function: Penalty<br />Lambda: 1.3","b: -0.05<br />value: 0.0650<br />Function: Penalty<br />Lambda: 1.3","b: -0.04<br />value: 0.0520<br />Function: Penalty<br />Lambda: 1.3","b: -0.03<br />value: 0.0390<br />Function: Penalty<br />Lambda: 1.3","b: -0.02<br />value: 0.0260<br />Function: Penalty<br />Lambda: 1.3","b: -0.01<br />value: 0.0130<br />Function: Penalty<br />Lambda: 1.3","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 1.3","b:  0.01<br />value: 0.0130<br />Function: Penalty<br />Lambda: 1.3","b:  0.02<br />value: 0.0260<br />Function: Penalty<br />Lambda: 1.3","b:  0.03<br />value: 0.0390<br />Function: Penalty<br />Lambda: 1.3","b:  0.04<br />value: 0.0520<br />Function: Penalty<br />Lambda: 1.3","b:  0.05<br />value: 0.0650<br />Function: Penalty<br />Lambda: 1.3","b:  0.06<br />value: 0.0780<br />Function: Penalty<br />Lambda: 1.3","b:  0.07<br />value: 0.0910<br />Function: Penalty<br />Lambda: 1.3","b:  0.08<br />value: 0.1040<br />Function: Penalty<br />Lambda: 1.3","b:  0.09<br />value: 0.1170<br />Function: Penalty<br />Lambda: 1.3","b:  0.10<br />value: 0.1300<br />Function: Penalty<br />Lambda: 1.3","b:  0.11<br />value: 0.1430<br />Function: Penalty<br />Lambda: 1.3","b:  0.12<br />value: 0.1560<br />Function: Penalty<br />Lambda: 1.3","b:  0.13<br />value: 0.1690<br />Function: Penalty<br />Lambda: 1.3","b:  0.14<br />value: 0.1820<br />Function: Penalty<br />Lambda: 1.3","b:  0.15<br />value: 0.1950<br />Function: Penalty<br />Lambda: 1.3","b:  0.16<br />value: 0.2080<br />Function: Penalty<br />Lambda: 1.3","b:  0.17<br />value: 0.2210<br />Function: Penalty<br />Lambda: 1.3","b:  0.18<br />value: 0.2340<br />Function: Penalty<br />Lambda: 1.3","b:  0.19<br />value: 0.2470<br />Function: Penalty<br />Lambda: 1.3","b:  0.20<br />value: 0.2600<br />Function: Penalty<br />Lambda: 1.3","b:  0.21<br />value: 0.2730<br />Function: Penalty<br />Lambda: 1.3","b:  0.22<br />value: 0.2860<br />Function: Penalty<br />Lambda: 1.3","b:  0.23<br />value: 0.2990<br />Function: Penalty<br />Lambda: 1.3","b:  0.24<br />value: 0.3120<br />Function: Penalty<br />Lambda: 1.3","b:  0.25<br />value: 0.3250<br />Function: Penalty<br />Lambda: 1.3","b:  0.26<br />value: 0.3380<br />Function: Penalty<br />Lambda: 1.3","b:  0.27<br />value: 0.3510<br />Function: Penalty<br />Lambda: 1.3","b:  0.28<br />value: 0.3640<br />Function: Penalty<br />Lambda: 1.3","b:  0.29<br />value: 0.3770<br />Function: Penalty<br />Lambda: 1.3","b:  0.30<br />value: 0.3900<br />Function: Penalty<br />Lambda: 1.3","b:  0.31<br />value: 0.4030<br />Function: Penalty<br />Lambda: 1.3","b:  0.32<br />value: 0.4160<br />Function: Penalty<br />Lambda: 1.3","b:  0.33<br />value: 0.4290<br />Function: Penalty<br />Lambda: 1.3","b:  0.34<br />value: 0.4420<br />Function: Penalty<br />Lambda: 1.3","b:  0.35<br />value: 0.4550<br />Function: Penalty<br />Lambda: 1.3","b:  0.36<br />value: 0.4680<br />Function: Penalty<br />Lambda: 1.3","b:  0.37<br />value: 0.4810<br />Function: Penalty<br />Lambda: 1.3","b:  0.38<br />value: 0.4940<br />Function: Penalty<br />Lambda: 1.3","b:  0.39<br />value: 0.5070<br />Function: Penalty<br />Lambda: 1.3","b:  0.40<br />value: 0.5200<br />Function: Penalty<br />Lambda: 1.3","b:  0.41<br />value: 0.5330<br />Function: Penalty<br />Lambda: 1.3","b:  0.42<br />value: 0.5460<br />Function: Penalty<br />Lambda: 1.3","b:  0.43<br />value: 0.5590<br />Function: Penalty<br />Lambda: 1.3","b:  0.44<br />value: 0.5720<br />Function: Penalty<br />Lambda: 1.3","b:  0.45<br />value: 0.5850<br />Function: Penalty<br />Lambda: 1.3","b:  0.46<br />value: 0.5980<br />Function: Penalty<br />Lambda: 1.3","b:  0.47<br />value: 0.6110<br />Function: Penalty<br />Lambda: 1.3","b:  0.48<br />value: 0.6240<br />Function: Penalty<br />Lambda: 1.3","b:  0.49<br />value: 0.6370<br />Function: Penalty<br />Lambda: 1.3","b:  0.50<br />value: 0.6500<br />Function: Penalty<br />Lambda: 1.3","b:  0.51<br />value: 0.6630<br />Function: Penalty<br />Lambda: 1.3","b:  0.52<br />value: 0.6760<br />Function: Penalty<br />Lambda: 1.3","b:  0.53<br />value: 0.6890<br />Function: Penalty<br />Lambda: 1.3","b:  0.54<br />value: 0.7020<br />Function: Penalty<br />Lambda: 1.3","b:  0.55<br />value: 0.7150<br />Function: Penalty<br />Lambda: 1.3","b:  0.56<br />value: 0.7280<br />Function: Penalty<br />Lambda: 1.3","b:  0.57<br />value: 0.7410<br />Function: Penalty<br />Lambda: 1.3","b:  0.58<br />value: 0.7540<br />Function: Penalty<br />Lambda: 1.3","b:  0.59<br />value: 0.7670<br />Function: Penalty<br />Lambda: 1.3","b:  0.60<br />value: 0.7800<br />Function: Penalty<br />Lambda: 1.3","b:  0.61<br />value: 0.7930<br />Function: Penalty<br />Lambda: 1.3","b:  0.62<br />value: 0.8060<br />Function: Penalty<br />Lambda: 1.3","b:  0.63<br />value: 0.8190<br />Function: Penalty<br />Lambda: 1.3","b:  0.64<br />value: 0.8320<br />Function: Penalty<br />Lambda: 1.3","b:  0.65<br />value: 0.8450<br />Function: Penalty<br />Lambda: 1.3","b:  0.66<br />value: 0.8580<br />Function: Penalty<br />Lambda: 1.3","b:  0.67<br />value: 0.8710<br />Function: Penalty<br />Lambda: 1.3","b:  0.68<br />value: 0.8840<br />Function: Penalty<br />Lambda: 1.3","b:  0.69<br />value: 0.8970<br />Function: Penalty<br />Lambda: 1.3","b:  0.70<br />value: 0.9100<br />Function: Penalty<br />Lambda: 1.3","b:  0.71<br />value: 0.9230<br />Function: Penalty<br />Lambda: 1.3","b:  0.72<br />value: 0.9360<br />Function: Penalty<br />Lambda: 1.3","b:  0.73<br />value: 0.9490<br />Function: Penalty<br />Lambda: 1.3","b:  0.74<br />value: 0.9620<br />Function: Penalty<br />Lambda: 1.3","b:  0.75<br />value: 0.9750<br />Function: Penalty<br />Lambda: 1.3","b:  0.76<br />value: 0.9880<br />Function: Penalty<br />Lambda: 1.3","b:  0.77<br />value: 1.0010<br />Function: Penalty<br />Lambda: 1.3","b:  0.78<br />value: 1.0140<br />Function: Penalty<br />Lambda: 1.3","b:  0.79<br />value: 1.0270<br />Function: Penalty<br />Lambda: 1.3","b:  0.80<br />value: 1.0400<br />Function: Penalty<br />Lambda: 1.3","b:  0.81<br />value: 1.0530<br />Function: Penalty<br />Lambda: 1.3","b:  0.82<br />value: 1.0660<br />Function: Penalty<br />Lambda: 1.3","b:  0.83<br />value: 1.0790<br />Function: Penalty<br />Lambda: 1.3","b:  0.84<br />value: 1.0920<br />Function: Penalty<br />Lambda: 1.3","b:  0.85<br />value: 1.1050<br />Function: Penalty<br />Lambda: 1.3","b:  0.86<br />value: 1.1180<br />Function: Penalty<br />Lambda: 1.3","b:  0.87<br />value: 1.1310<br />Function: Penalty<br />Lambda: 1.3","b:  0.88<br />value: 1.1440<br />Function: Penalty<br />Lambda: 1.3","b:  0.89<br />value: 1.1570<br />Function: Penalty<br />Lambda: 1.3","b:  0.90<br />value: 1.1700<br />Function: Penalty<br />Lambda: 1.3","b:  0.91<br />value: 1.1830<br />Function: Penalty<br />Lambda: 1.3","b:  0.92<br />value: 1.1960<br />Function: Penalty<br />Lambda: 1.3","b:  0.93<br />value: 1.2090<br />Function: Penalty<br />Lambda: 1.3","b:  0.94<br />value: 1.2220<br />Function: Penalty<br />Lambda: 1.3","b:  0.95<br />value: 1.2350<br />Function: Penalty<br />Lambda: 1.3","b:  0.96<br />value: 1.2480<br />Function: Penalty<br />Lambda: 1.3","b:  0.97<br />value: 1.2610<br />Function: Penalty<br />Lambda: 1.3","b:  0.98<br />value: 1.2740<br />Function: Penalty<br />Lambda: 1.3","b:  0.99<br />value: 1.2870<br />Function: Penalty<br />Lambda: 1.3","b:  1.00<br />value: 1.3000<br />Function: Penalty<br />Lambda: 1.3","b:  1.01<br />value: 1.3130<br />Function: Penalty<br />Lambda: 1.3","b:  1.02<br />value: 1.3260<br />Function: Penalty<br />Lambda: 1.3","b:  1.03<br />value: 1.3390<br />Function: Penalty<br />Lambda: 1.3","b:  1.04<br />value: 1.3520<br />Function: Penalty<br />Lambda: 1.3","b:  1.05<br />value: 1.3650<br />Function: Penalty<br />Lambda: 1.3","b:  1.06<br />value: 1.3780<br />Function: Penalty<br />Lambda: 1.3","b:  1.07<br />value: 1.3910<br />Function: Penalty<br />Lambda: 1.3","b:  1.08<br />value: 1.4040<br />Function: Penalty<br />Lambda: 1.3","b:  1.09<br />value: 1.4170<br />Function: Penalty<br />Lambda: 1.3","b:  1.10<br />value: 1.4300<br />Function: Penalty<br />Lambda: 1.3","b:  1.11<br />value: 1.4430<br />Function: Penalty<br />Lambda: 1.3","b:  1.12<br />value: 1.4560<br />Function: Penalty<br />Lambda: 1.3","b:  1.13<br />value: 1.4690<br />Function: Penalty<br />Lambda: 1.3","b:  1.14<br />value: 1.4820<br />Function: Penalty<br />Lambda: 1.3","b:  1.15<br />value: 1.4950<br />Function: Penalty<br />Lambda: 1.3","b:  1.16<br />value: 1.5080<br />Function: Penalty<br />Lambda: 1.3","b:  1.17<br />value: 1.5210<br />Function: Penalty<br />Lambda: 1.3","b:  1.18<br />value: 1.5340<br />Function: Penalty<br />Lambda: 1.3","b:  1.19<br />value: 1.5470<br />Function: Penalty<br />Lambda: 1.3","b:  1.20<br />value: 1.5600<br />Function: Penalty<br />Lambda: 1.3","b:  1.21<br />value: 1.5730<br />Function: Penalty<br />Lambda: 1.3","b:  1.22<br />value: 1.5860<br />Function: Penalty<br />Lambda: 1.3","b:  1.23<br />value: 1.5990<br />Function: Penalty<br />Lambda: 1.3","b:  1.24<br />value: 1.6120<br />Function: Penalty<br />Lambda: 1.3","b:  1.25<br />value: 1.6250<br />Function: Penalty<br />Lambda: 1.3","b:  1.26<br />value: 1.6380<br />Function: Penalty<br />Lambda: 1.3","b:  1.27<br />value: 1.6510<br />Function: Penalty<br />Lambda: 1.3","b:  1.28<br />value: 1.6640<br />Function: Penalty<br />Lambda: 1.3","b:  1.29<br />value: 1.6770<br />Function: Penalty<br />Lambda: 1.3","b:  1.30<br />value: 1.6900<br />Function: Penalty<br />Lambda: 1.3","b:  1.31<br />value: 1.7030<br />Function: Penalty<br />Lambda: 1.3","b:  1.32<br />value: 1.7160<br />Function: Penalty<br />Lambda: 1.3","b:  1.33<br />value: 1.7290<br />Function: Penalty<br />Lambda: 1.3","b:  1.34<br />value: 1.7420<br />Function: Penalty<br />Lambda: 1.3","b:  1.35<br />value: 1.7550<br />Function: Penalty<br />Lambda: 1.3","b:  1.36<br />value: 1.7680<br />Function: Penalty<br />Lambda: 1.3","b:  1.37<br />value: 1.7810<br />Function: Penalty<br />Lambda: 1.3","b:  1.38<br />value: 1.7940<br />Function: Penalty<br />Lambda: 1.3","b:  1.39<br />value: 1.8070<br />Function: Penalty<br />Lambda: 1.3","b:  1.40<br />value: 1.8200<br />Function: Penalty<br />Lambda: 1.3","b:  1.41<br />value: 1.8330<br />Function: Penalty<br />Lambda: 1.3","b:  1.42<br />value: 1.8460<br />Function: Penalty<br />Lambda: 1.3","b:  1.43<br />value: 1.8590<br />Function: Penalty<br />Lambda: 1.3","b:  1.44<br />value: 1.8720<br />Function: Penalty<br />Lambda: 1.3","b:  1.45<br />value: 1.8850<br />Function: Penalty<br />Lambda: 1.3","b:  1.46<br />value: 1.8980<br />Function: Penalty<br />Lambda: 1.3","b:  1.47<br />value: 1.9110<br />Function: Penalty<br />Lambda: 1.3","b:  1.48<br />value: 1.9240<br />Function: Penalty<br />Lambda: 1.3","b:  1.49<br />value: 1.9370<br />Function: Penalty<br />Lambda: 1.3","b:  1.50<br />value: 1.9500<br />Function: Penalty<br />Lambda: 1.3","b:  1.51<br />value: 1.9630<br />Function: Penalty<br />Lambda: 1.3","b:  1.52<br />value: 1.9760<br />Function: Penalty<br />Lambda: 1.3","b:  1.53<br />value: 1.9890<br />Function: Penalty<br />Lambda: 1.3","b:  1.54<br />value: 2.0020<br />Function: Penalty<br />Lambda: 1.3","b:  1.55<br />value: 2.0150<br />Function: Penalty<br />Lambda: 1.3","b:  1.56<br />value: 2.0280<br />Function: Penalty<br />Lambda: 1.3","b:  1.57<br />value: 2.0410<br />Function: Penalty<br />Lambda: 1.3","b:  1.58<br />value: 2.0540<br />Function: Penalty<br />Lambda: 1.3","b:  1.59<br />value: 2.0670<br />Function: Penalty<br />Lambda: 1.3","b:  1.60<br />value: 2.0800<br />Function: Penalty<br />Lambda: 1.3","b:  1.61<br />value: 2.0930<br />Function: Penalty<br />Lambda: 1.3","b:  1.62<br />value: 2.1060<br />Function: Penalty<br />Lambda: 1.3","b:  1.63<br />value: 2.1190<br />Function: Penalty<br />Lambda: 1.3","b:  1.64<br />value: 2.1320<br />Function: Penalty<br />Lambda: 1.3","b:  1.65<br />value: 2.1450<br />Function: Penalty<br />Lambda: 1.3","b:  1.66<br />value: 2.1580<br />Function: Penalty<br />Lambda: 1.3","b:  1.67<br />value: 2.1710<br />Function: Penalty<br />Lambda: 1.3","b:  1.68<br />value: 2.1840<br />Function: Penalty<br />Lambda: 1.3","b:  1.69<br />value: 2.1970<br />Function: Penalty<br />Lambda: 1.3","b:  1.70<br />value: 2.2100<br />Function: Penalty<br />Lambda: 1.3","b:  1.71<br />value: 2.2230<br />Function: Penalty<br />Lambda: 1.3","b:  1.72<br />value: 2.2360<br />Function: Penalty<br />Lambda: 1.3","b:  1.73<br />value: 2.2490<br />Function: Penalty<br />Lambda: 1.3","b:  1.74<br />value: 2.2620<br />Function: Penalty<br />Lambda: 1.3","b:  1.75<br />value: 2.2750<br />Function: Penalty<br />Lambda: 1.3","b:  1.76<br />value: 2.2880<br />Function: Penalty<br />Lambda: 1.3","b:  1.77<br />value: 2.3010<br />Function: Penalty<br />Lambda: 1.3","b:  1.78<br />value: 2.3140<br />Function: Penalty<br />Lambda: 1.3","b:  1.79<br />value: 2.3270<br />Function: Penalty<br />Lambda: 1.3","b:  1.80<br />value: 2.3400<br />Function: Penalty<br />Lambda: 1.3","b:  1.81<br />value: 2.3530<br />Function: Penalty<br />Lambda: 1.3","b:  1.82<br />value: 2.3660<br />Function: Penalty<br />Lambda: 1.3","b:  1.83<br />value: 2.3790<br />Function: Penalty<br />Lambda: 1.3","b:  1.84<br />value: 2.3920<br />Function: Penalty<br />Lambda: 1.3","b:  1.85<br />value: 2.4050<br />Function: Penalty<br />Lambda: 1.3","b:  1.86<br />value: 2.4180<br />Function: Penalty<br />Lambda: 1.3","b:  1.87<br />value: 2.4310<br />Function: Penalty<br />Lambda: 1.3","b:  1.88<br />value: 2.4440<br />Function: Penalty<br />Lambda: 1.3","b:  1.89<br />value: 2.4570<br />Function: Penalty<br />Lambda: 1.3","b:  1.90<br />value: 2.4700<br />Function: Penalty<br />Lambda: 1.3","b:  1.91<br />value: 2.4830<br />Function: Penalty<br />Lambda: 1.3","b:  1.92<br />value: 2.4960<br />Function: Penalty<br />Lambda: 1.3","b:  1.93<br />value: 2.5090<br />Function: Penalty<br />Lambda: 1.3","b:  1.94<br />value: 2.5220<br />Function: Penalty<br />Lambda: 1.3","b:  1.95<br />value: 2.5350<br />Function: Penalty<br />Lambda: 1.3","b:  1.96<br />value: 2.5480<br />Function: Penalty<br />Lambda: 1.3","b:  1.97<br />value: 2.5610<br />Function: Penalty<br />Lambda: 1.3","b:  1.98<br />value: 2.5740<br />Function: Penalty<br />Lambda: 1.3","b:  1.99<br />value: 2.5870<br />Function: Penalty<br />Lambda: 1.3","b:  2.00<br />value: 2.6000<br />Function: Penalty<br />Lambda: 1.3"],"frame":"1.3","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"1.4","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 1.4","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 1.4","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 1.4","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 1.4","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 1.4","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 1.4","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 1.4","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 1.4","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 1.4","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 1.4","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 1.4","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 1.4","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 1.4","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 1.4","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 1.4","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 1.4","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 1.4","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 1.4","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 1.4","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 1.4","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 1.4","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 1.4","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 1.4","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 1.4","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 1.4","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 1.4","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 1.4","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 1.4","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 1.4","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 1.4","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 1.4","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 1.4","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 1.4","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 1.4","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 1.4","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 1.4","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 1.4","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 1.4","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 1.4","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 1.4","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 1.4","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 1.4","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 1.4","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 1.4","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 1.4","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 1.4","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 1.4","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 1.4","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 1.4","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 1.4","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.4","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.4","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.4","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.4","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.4","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.4","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.4","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.4","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.4","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.4","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.4","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.4","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.4","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.4","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.4","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.4","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.4","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.4","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.4","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.4","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.4","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.4","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.4","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.4","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.4","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.4","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.4","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.4","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.4","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.4","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.4","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.4","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.4","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.4","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.4","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.4","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.4","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.4","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.4","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.4","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.4","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.4","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.4","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.4","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.4","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.4","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.4","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.4","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.4","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.4","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.4","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.4","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.4","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.4","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.4","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.4","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.4","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.4","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.4","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.4","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.4","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.4","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.4","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.4","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.4","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.4","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.4","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.4","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.4","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.4","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.4","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.4","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.4","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.4","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.4","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.4","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.4","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.4","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.4","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.4","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.4","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.4","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.4","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.4","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.4","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.4","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.4","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.4","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.4","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.4","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.4","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.4","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.4","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.4","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.4","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.4","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.4","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.4","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.4","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.4","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 1.4","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.4","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.4","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.4","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.4","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.4","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.4","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.4","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.4","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.4","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.4","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.4","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.4","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.4","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.4","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.4","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.4","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.4","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.4","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.4","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.4","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.4","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.4","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.4","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.4","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.4","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.4","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.4","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.4","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.4","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.4","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.4","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.4","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.4","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.4","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.4","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.4","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.4","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.4","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.4","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.4","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.4","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.4","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.4","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.4","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.4","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.4","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.4","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.4","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.4","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.4","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.4","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.4","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.4","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.4","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.4","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.4","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.4","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.4","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.4","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.4","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.4","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.4","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.4","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.4","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.4","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.4","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.4","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.4","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.4","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.4","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.4","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.4","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.4","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.4","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.4","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.4","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.4","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.4","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.4","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.4","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.4","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.4","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.4","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.4","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.4","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.4","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.4","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.4","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.4","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.4","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.4","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.4","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.4","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.4","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.4","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.4","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.4","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.4","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.4","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.4"],"frame":"1.4","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.45,3.4061,3.3624,3.3189,3.2756,3.2325,3.1896,3.1469,3.1044,3.0621,3.02,2.9781,2.9364,2.8949,2.8536,2.8125,2.7716,2.7309,2.6904,2.6501,2.61,2.5701,2.5304,2.4909,2.4516,2.4125,2.3736,2.3349,2.2964,2.2581,2.22,2.1821,2.1444,2.1069,2.0696,2.0325,1.9956,1.9589,1.9224,1.8861,1.85,1.8141,1.7784,1.7429,1.7076,1.6725,1.6376,1.6029,1.5684,1.5341,1.5,1.4941,1.4884,1.4829,1.4776,1.4725,1.4676,1.4629,1.4584,1.4541,1.45,1.4461,1.4424,1.4389,1.4356,1.4325,1.4296,1.4269,1.4244,1.4221,1.42,1.4181,1.4164,1.4149,1.4136,1.4125,1.4116,1.4109,1.4104,1.4101,1.41,1.4101,1.4104,1.4109,1.4116,1.4125,1.4136,1.4149,1.4164,1.4181,1.42,1.4221,1.4244,1.4269,1.4296,1.4325,1.4356,1.4389,1.4424,1.4461,1.45,1.4541,1.4584,1.4629,1.4676,1.4725,1.4776,1.4829,1.4884,1.4941,1.5,1.5061,1.5124,1.5189,1.5256,1.5325,1.5396,1.5469,1.5544,1.5621,1.57,1.5781,1.5864,1.5949,1.6036,1.6125,1.6216,1.6309,1.6404,1.6501,1.66,1.6701,1.6804,1.6909,1.7016,1.7125,1.7236,1.7349,1.7464,1.7581,1.77,1.7821,1.7944,1.8069,1.8196,1.8325,1.8456,1.8589,1.8724,1.8861,1.9,1.9141,1.9284,1.9429,1.9576,1.9725,1.9876,2.0029,2.0184,2.0341,2.05,2.0661,2.0824,2.0989,2.1156,2.1325,2.1496,2.1669,2.1844,2.2021,2.22,2.2381,2.2564,2.2749,2.2936,2.3125,2.3316,2.3509,2.3704,2.3901,2.41,2.4301,2.4504,2.4709,2.4916,2.5125,2.5336,2.5549,2.5764,2.5981,2.62,2.6421,2.6644,2.6869,2.7096,2.7325,2.7556,2.7789,2.8024,2.8261,2.85,2.8741,2.8984,2.9229,2.9476,2.9725,2.9976,3.0229,3.0484,3.0741,3.1,3.1261,3.1524,3.1789,3.2056,3.2325,3.2596,3.2869,3.3144,3.3421,3.37,3.3981,3.4264,3.4549,3.4836,3.5125,3.5416,3.5709,3.6004,3.6301,3.66,3.6901,3.7204,3.7509,3.7816,3.8125,3.8436,3.8749,3.9064,3.9381,3.97,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 3.4500<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.49<br />value: 3.4061<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.48<br />value: 3.3624<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.47<br />value: 3.3189<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.46<br />value: 3.2756<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.45<br />value: 3.2325<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.44<br />value: 3.1896<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.43<br />value: 3.1469<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.42<br />value: 3.1044<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.41<br />value: 3.0621<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.40<br />value: 3.0200<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.39<br />value: 2.9781<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.38<br />value: 2.9364<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.37<br />value: 2.8949<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.36<br />value: 2.8536<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.35<br />value: 2.8125<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.34<br />value: 2.7716<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.33<br />value: 2.7309<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.32<br />value: 2.6904<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.31<br />value: 2.6501<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.30<br />value: 2.6100<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.29<br />value: 2.5701<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.28<br />value: 2.5304<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.27<br />value: 2.4909<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.26<br />value: 2.4516<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.25<br />value: 2.4125<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.24<br />value: 2.3736<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.23<br />value: 2.3349<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.22<br />value: 2.2964<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.21<br />value: 2.2581<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.20<br />value: 2.2200<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.19<br />value: 2.1821<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.18<br />value: 2.1444<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.17<br />value: 2.1069<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.16<br />value: 2.0696<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.15<br />value: 2.0325<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.14<br />value: 1.9956<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.13<br />value: 1.9589<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.12<br />value: 1.9224<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.11<br />value: 1.8861<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.10<br />value: 1.8500<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.09<br />value: 1.8141<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.08<br />value: 1.7784<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.07<br />value: 1.7429<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.06<br />value: 1.7076<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.05<br />value: 1.6725<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.04<br />value: 1.6376<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.03<br />value: 1.6029<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.02<br />value: 1.5684<br />Function: Loss + Penalty<br />Lambda: 1.4","b: -0.01<br />value: 1.5341<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.01<br />value: 1.4941<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.02<br />value: 1.4884<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.03<br />value: 1.4829<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.04<br />value: 1.4776<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.05<br />value: 1.4725<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.06<br />value: 1.4676<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.07<br />value: 1.4629<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.08<br />value: 1.4584<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.09<br />value: 1.4541<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.10<br />value: 1.4500<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.11<br />value: 1.4461<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.12<br />value: 1.4424<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.13<br />value: 1.4389<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.14<br />value: 1.4356<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.15<br />value: 1.4325<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.16<br />value: 1.4296<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.17<br />value: 1.4269<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.18<br />value: 1.4244<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.19<br />value: 1.4221<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.20<br />value: 1.4200<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.21<br />value: 1.4181<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.22<br />value: 1.4164<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.23<br />value: 1.4149<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.24<br />value: 1.4136<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.25<br />value: 1.4125<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.26<br />value: 1.4116<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.27<br />value: 1.4109<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.28<br />value: 1.4104<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.29<br />value: 1.4101<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.30<br />value: 1.4100<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.31<br />value: 1.4101<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.32<br />value: 1.4104<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.33<br />value: 1.4109<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.34<br />value: 1.4116<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.35<br />value: 1.4125<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.36<br />value: 1.4136<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.37<br />value: 1.4149<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.38<br />value: 1.4164<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.39<br />value: 1.4181<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.40<br />value: 1.4200<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.41<br />value: 1.4221<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.42<br />value: 1.4244<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.43<br />value: 1.4269<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.44<br />value: 1.4296<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.45<br />value: 1.4325<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.46<br />value: 1.4356<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.47<br />value: 1.4389<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.48<br />value: 1.4424<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.49<br />value: 1.4461<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.50<br />value: 1.4500<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.51<br />value: 1.4541<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.52<br />value: 1.4584<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.53<br />value: 1.4629<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.54<br />value: 1.4676<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.55<br />value: 1.4725<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.56<br />value: 1.4776<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.57<br />value: 1.4829<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.58<br />value: 1.4884<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.59<br />value: 1.4941<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.60<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.61<br />value: 1.5061<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.62<br />value: 1.5124<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.63<br />value: 1.5189<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.64<br />value: 1.5256<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.65<br />value: 1.5325<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.66<br />value: 1.5396<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.67<br />value: 1.5469<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.68<br />value: 1.5544<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.69<br />value: 1.5621<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.70<br />value: 1.5700<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.71<br />value: 1.5781<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.72<br />value: 1.5864<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.73<br />value: 1.5949<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.74<br />value: 1.6036<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.75<br />value: 1.6125<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.76<br />value: 1.6216<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.77<br />value: 1.6309<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.78<br />value: 1.6404<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.79<br />value: 1.6501<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.80<br />value: 1.6600<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.81<br />value: 1.6701<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.82<br />value: 1.6804<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.83<br />value: 1.6909<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.84<br />value: 1.7016<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.85<br />value: 1.7125<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.86<br />value: 1.7236<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.87<br />value: 1.7349<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.88<br />value: 1.7464<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.89<br />value: 1.7581<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.90<br />value: 1.7700<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.91<br />value: 1.7821<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.92<br />value: 1.7944<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.93<br />value: 1.8069<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.94<br />value: 1.8196<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.95<br />value: 1.8325<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.96<br />value: 1.8456<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.97<br />value: 1.8589<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.98<br />value: 1.8724<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  0.99<br />value: 1.8861<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.00<br />value: 1.9000<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.01<br />value: 1.9141<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.02<br />value: 1.9284<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.03<br />value: 1.9429<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.04<br />value: 1.9576<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.05<br />value: 1.9725<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.06<br />value: 1.9876<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.07<br />value: 2.0029<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.08<br />value: 2.0184<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.09<br />value: 2.0341<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.10<br />value: 2.0500<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.11<br />value: 2.0661<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.12<br />value: 2.0824<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.13<br />value: 2.0989<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.14<br />value: 2.1156<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.15<br />value: 2.1325<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.16<br />value: 2.1496<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.17<br />value: 2.1669<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.18<br />value: 2.1844<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.19<br />value: 2.2021<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.20<br />value: 2.2200<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.21<br />value: 2.2381<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.22<br />value: 2.2564<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.23<br />value: 2.2749<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.24<br />value: 2.2936<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.25<br />value: 2.3125<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.26<br />value: 2.3316<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.27<br />value: 2.3509<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.28<br />value: 2.3704<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.29<br />value: 2.3901<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.30<br />value: 2.4100<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.31<br />value: 2.4301<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.32<br />value: 2.4504<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.33<br />value: 2.4709<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.34<br />value: 2.4916<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.35<br />value: 2.5125<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.36<br />value: 2.5336<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.37<br />value: 2.5549<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.38<br />value: 2.5764<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.39<br />value: 2.5981<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.40<br />value: 2.6200<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.41<br />value: 2.6421<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.42<br />value: 2.6644<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.43<br />value: 2.6869<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.44<br />value: 2.7096<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.45<br />value: 2.7325<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.46<br />value: 2.7556<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.47<br />value: 2.7789<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.48<br />value: 2.8024<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.49<br />value: 2.8261<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.50<br />value: 2.8500<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.51<br />value: 2.8741<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.52<br />value: 2.8984<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.53<br />value: 2.9229<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.54<br />value: 2.9476<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.55<br />value: 2.9725<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.56<br />value: 2.9976<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.57<br />value: 3.0229<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.58<br />value: 3.0484<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.59<br />value: 3.0741<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.60<br />value: 3.1000<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.61<br />value: 3.1261<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.62<br />value: 3.1524<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.63<br />value: 3.1789<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.64<br />value: 3.2056<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.65<br />value: 3.2325<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.66<br />value: 3.2596<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.67<br />value: 3.2869<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.68<br />value: 3.3144<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.69<br />value: 3.3421<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.70<br />value: 3.3700<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.71<br />value: 3.3981<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.72<br />value: 3.4264<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.73<br />value: 3.4549<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.74<br />value: 3.4836<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.75<br />value: 3.5125<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.76<br />value: 3.5416<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.77<br />value: 3.5709<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.78<br />value: 3.6004<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.79<br />value: 3.6301<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.80<br />value: 3.6600<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.81<br />value: 3.6901<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.82<br />value: 3.7204<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.83<br />value: 3.7509<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.84<br />value: 3.7816<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.85<br />value: 3.8125<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.86<br />value: 3.8436<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.87<br />value: 3.8749<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.88<br />value: 3.9064<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.89<br />value: 3.9381<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.90<br />value: 3.9700<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.91<br />value: 4.0021<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.92<br />value: 4.0344<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.93<br />value: 4.0669<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.94<br />value: 4.0996<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.95<br />value: 4.1325<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.96<br />value: 4.1656<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.97<br />value: 4.1989<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.98<br />value: 4.2324<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  1.99<br />value: 4.2661<br />Function: Loss + Penalty<br />Lambda: 1.4","b:  2.00<br />value: 4.3000<br />Function: Loss + Penalty<br />Lambda: 1.4"],"frame":"1.4","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.7,0.686,0.672,0.658,0.644,0.63,0.616,0.602,0.588,0.574,0.56,0.546,0.532,0.518,0.504,0.49,0.476,0.462,0.448,0.434,0.42,0.406,0.392,0.378,0.364,0.35,0.336,0.322,0.308,0.294,0.28,0.266,0.252,0.238,0.224,0.21,0.196,0.182,0.168,0.154,0.14,0.126,0.112,0.098,0.084,0.07,0.056,0.042,0.028,0.014,0,0.014,0.028,0.042,0.0560000000000001,0.0700000000000001,0.0840000000000001,0.0980000000000001,0.112,0.126,0.14,0.154,0.168,0.182,0.196,0.21,0.224,0.238,0.252,0.266,0.28,0.294,0.308,0.322,0.336,0.35,0.364,0.378,0.392,0.406,0.42,0.434,0.448,0.462,0.476,0.49,0.504,0.518,0.532,0.546,0.56,0.574,0.588,0.602,0.616,0.63,0.644,0.658,0.672,0.686,0.7,0.714,0.728,0.742,0.756,0.77,0.784,0.798,0.812,0.826,0.84,0.854,0.868,0.882,0.896,0.91,0.924,0.938,0.952,0.966,0.98,0.994,1.008,1.022,1.036,1.05,1.064,1.078,1.092,1.106,1.12,1.134,1.148,1.162,1.176,1.19,1.204,1.218,1.232,1.246,1.26,1.274,1.288,1.302,1.316,1.33,1.344,1.358,1.372,1.386,1.4,1.414,1.428,1.442,1.456,1.47,1.484,1.498,1.512,1.526,1.54,1.554,1.568,1.582,1.596,1.61,1.624,1.638,1.652,1.666,1.68,1.694,1.708,1.722,1.736,1.75,1.764,1.778,1.792,1.806,1.82,1.834,1.848,1.862,1.876,1.89,1.904,1.918,1.932,1.946,1.96,1.974,1.988,2.002,2.016,2.03,2.044,2.058,2.072,2.086,2.1,2.114,2.128,2.142,2.156,2.17,2.184,2.198,2.212,2.226,2.24,2.254,2.268,2.282,2.296,2.31,2.324,2.338,2.352,2.366,2.38,2.394,2.408,2.422,2.436,2.45,2.464,2.478,2.492,2.506,2.52,2.534,2.548,2.562,2.576,2.59,2.604,2.618,2.632,2.646,2.66,2.674,2.688,2.702,2.716,2.73,2.744,2.758,2.772,2.786,2.8],"text":["b: -0.50<br />value: 0.7000<br />Function: Penalty<br />Lambda: 1.4","b: -0.49<br />value: 0.6860<br />Function: Penalty<br />Lambda: 1.4","b: -0.48<br />value: 0.6720<br />Function: Penalty<br />Lambda: 1.4","b: -0.47<br />value: 0.6580<br />Function: Penalty<br />Lambda: 1.4","b: -0.46<br />value: 0.6440<br />Function: Penalty<br />Lambda: 1.4","b: -0.45<br />value: 0.6300<br />Function: Penalty<br />Lambda: 1.4","b: -0.44<br />value: 0.6160<br />Function: Penalty<br />Lambda: 1.4","b: -0.43<br />value: 0.6020<br />Function: Penalty<br />Lambda: 1.4","b: -0.42<br />value: 0.5880<br />Function: Penalty<br />Lambda: 1.4","b: -0.41<br />value: 0.5740<br />Function: Penalty<br />Lambda: 1.4","b: -0.40<br />value: 0.5600<br />Function: Penalty<br />Lambda: 1.4","b: -0.39<br />value: 0.5460<br />Function: Penalty<br />Lambda: 1.4","b: -0.38<br />value: 0.5320<br />Function: Penalty<br />Lambda: 1.4","b: -0.37<br />value: 0.5180<br />Function: Penalty<br />Lambda: 1.4","b: -0.36<br />value: 0.5040<br />Function: Penalty<br />Lambda: 1.4","b: -0.35<br />value: 0.4900<br />Function: Penalty<br />Lambda: 1.4","b: -0.34<br />value: 0.4760<br />Function: Penalty<br />Lambda: 1.4","b: -0.33<br />value: 0.4620<br />Function: Penalty<br />Lambda: 1.4","b: -0.32<br />value: 0.4480<br />Function: Penalty<br />Lambda: 1.4","b: -0.31<br />value: 0.4340<br />Function: Penalty<br />Lambda: 1.4","b: -0.30<br />value: 0.4200<br />Function: Penalty<br />Lambda: 1.4","b: -0.29<br />value: 0.4060<br />Function: Penalty<br />Lambda: 1.4","b: -0.28<br />value: 0.3920<br />Function: Penalty<br />Lambda: 1.4","b: -0.27<br />value: 0.3780<br />Function: Penalty<br />Lambda: 1.4","b: -0.26<br />value: 0.3640<br />Function: Penalty<br />Lambda: 1.4","b: -0.25<br />value: 0.3500<br />Function: Penalty<br />Lambda: 1.4","b: -0.24<br />value: 0.3360<br />Function: Penalty<br />Lambda: 1.4","b: -0.23<br />value: 0.3220<br />Function: Penalty<br />Lambda: 1.4","b: -0.22<br />value: 0.3080<br />Function: Penalty<br />Lambda: 1.4","b: -0.21<br />value: 0.2940<br />Function: Penalty<br />Lambda: 1.4","b: -0.20<br />value: 0.2800<br />Function: Penalty<br />Lambda: 1.4","b: -0.19<br />value: 0.2660<br />Function: Penalty<br />Lambda: 1.4","b: -0.18<br />value: 0.2520<br />Function: Penalty<br />Lambda: 1.4","b: -0.17<br />value: 0.2380<br />Function: Penalty<br />Lambda: 1.4","b: -0.16<br />value: 0.2240<br />Function: Penalty<br />Lambda: 1.4","b: -0.15<br />value: 0.2100<br />Function: Penalty<br />Lambda: 1.4","b: -0.14<br />value: 0.1960<br />Function: Penalty<br />Lambda: 1.4","b: -0.13<br />value: 0.1820<br />Function: Penalty<br />Lambda: 1.4","b: -0.12<br />value: 0.1680<br />Function: Penalty<br />Lambda: 1.4","b: -0.11<br />value: 0.1540<br />Function: Penalty<br />Lambda: 1.4","b: -0.10<br />value: 0.1400<br />Function: Penalty<br />Lambda: 1.4","b: -0.09<br />value: 0.1260<br />Function: Penalty<br />Lambda: 1.4","b: -0.08<br />value: 0.1120<br />Function: Penalty<br />Lambda: 1.4","b: -0.07<br />value: 0.0980<br />Function: Penalty<br />Lambda: 1.4","b: -0.06<br />value: 0.0840<br />Function: Penalty<br />Lambda: 1.4","b: -0.05<br />value: 0.0700<br />Function: Penalty<br />Lambda: 1.4","b: -0.04<br />value: 0.0560<br />Function: Penalty<br />Lambda: 1.4","b: -0.03<br />value: 0.0420<br />Function: Penalty<br />Lambda: 1.4","b: -0.02<br />value: 0.0280<br />Function: Penalty<br />Lambda: 1.4","b: -0.01<br />value: 0.0140<br />Function: Penalty<br />Lambda: 1.4","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 1.4","b:  0.01<br />value: 0.0140<br />Function: Penalty<br />Lambda: 1.4","b:  0.02<br />value: 0.0280<br />Function: Penalty<br />Lambda: 1.4","b:  0.03<br />value: 0.0420<br />Function: Penalty<br />Lambda: 1.4","b:  0.04<br />value: 0.0560<br />Function: Penalty<br />Lambda: 1.4","b:  0.05<br />value: 0.0700<br />Function: Penalty<br />Lambda: 1.4","b:  0.06<br />value: 0.0840<br />Function: Penalty<br />Lambda: 1.4","b:  0.07<br />value: 0.0980<br />Function: Penalty<br />Lambda: 1.4","b:  0.08<br />value: 0.1120<br />Function: Penalty<br />Lambda: 1.4","b:  0.09<br />value: 0.1260<br />Function: Penalty<br />Lambda: 1.4","b:  0.10<br />value: 0.1400<br />Function: Penalty<br />Lambda: 1.4","b:  0.11<br />value: 0.1540<br />Function: Penalty<br />Lambda: 1.4","b:  0.12<br />value: 0.1680<br />Function: Penalty<br />Lambda: 1.4","b:  0.13<br />value: 0.1820<br />Function: Penalty<br />Lambda: 1.4","b:  0.14<br />value: 0.1960<br />Function: Penalty<br />Lambda: 1.4","b:  0.15<br />value: 0.2100<br />Function: Penalty<br />Lambda: 1.4","b:  0.16<br />value: 0.2240<br />Function: Penalty<br />Lambda: 1.4","b:  0.17<br />value: 0.2380<br />Function: Penalty<br />Lambda: 1.4","b:  0.18<br />value: 0.2520<br />Function: Penalty<br />Lambda: 1.4","b:  0.19<br />value: 0.2660<br />Function: Penalty<br />Lambda: 1.4","b:  0.20<br />value: 0.2800<br />Function: Penalty<br />Lambda: 1.4","b:  0.21<br />value: 0.2940<br />Function: Penalty<br />Lambda: 1.4","b:  0.22<br />value: 0.3080<br />Function: Penalty<br />Lambda: 1.4","b:  0.23<br />value: 0.3220<br />Function: Penalty<br />Lambda: 1.4","b:  0.24<br />value: 0.3360<br />Function: Penalty<br />Lambda: 1.4","b:  0.25<br />value: 0.3500<br />Function: Penalty<br />Lambda: 1.4","b:  0.26<br />value: 0.3640<br />Function: Penalty<br />Lambda: 1.4","b:  0.27<br />value: 0.3780<br />Function: Penalty<br />Lambda: 1.4","b:  0.28<br />value: 0.3920<br />Function: Penalty<br />Lambda: 1.4","b:  0.29<br />value: 0.4060<br />Function: Penalty<br />Lambda: 1.4","b:  0.30<br />value: 0.4200<br />Function: Penalty<br />Lambda: 1.4","b:  0.31<br />value: 0.4340<br />Function: Penalty<br />Lambda: 1.4","b:  0.32<br />value: 0.4480<br />Function: Penalty<br />Lambda: 1.4","b:  0.33<br />value: 0.4620<br />Function: Penalty<br />Lambda: 1.4","b:  0.34<br />value: 0.4760<br />Function: Penalty<br />Lambda: 1.4","b:  0.35<br />value: 0.4900<br />Function: Penalty<br />Lambda: 1.4","b:  0.36<br />value: 0.5040<br />Function: Penalty<br />Lambda: 1.4","b:  0.37<br />value: 0.5180<br />Function: Penalty<br />Lambda: 1.4","b:  0.38<br />value: 0.5320<br />Function: Penalty<br />Lambda: 1.4","b:  0.39<br />value: 0.5460<br />Function: Penalty<br />Lambda: 1.4","b:  0.40<br />value: 0.5600<br />Function: Penalty<br />Lambda: 1.4","b:  0.41<br />value: 0.5740<br />Function: Penalty<br />Lambda: 1.4","b:  0.42<br />value: 0.5880<br />Function: Penalty<br />Lambda: 1.4","b:  0.43<br />value: 0.6020<br />Function: Penalty<br />Lambda: 1.4","b:  0.44<br />value: 0.6160<br />Function: Penalty<br />Lambda: 1.4","b:  0.45<br />value: 0.6300<br />Function: Penalty<br />Lambda: 1.4","b:  0.46<br />value: 0.6440<br />Function: Penalty<br />Lambda: 1.4","b:  0.47<br />value: 0.6580<br />Function: Penalty<br />Lambda: 1.4","b:  0.48<br />value: 0.6720<br />Function: Penalty<br />Lambda: 1.4","b:  0.49<br />value: 0.6860<br />Function: Penalty<br />Lambda: 1.4","b:  0.50<br />value: 0.7000<br />Function: Penalty<br />Lambda: 1.4","b:  0.51<br />value: 0.7140<br />Function: Penalty<br />Lambda: 1.4","b:  0.52<br />value: 0.7280<br />Function: Penalty<br />Lambda: 1.4","b:  0.53<br />value: 0.7420<br />Function: Penalty<br />Lambda: 1.4","b:  0.54<br />value: 0.7560<br />Function: Penalty<br />Lambda: 1.4","b:  0.55<br />value: 0.7700<br />Function: Penalty<br />Lambda: 1.4","b:  0.56<br />value: 0.7840<br />Function: Penalty<br />Lambda: 1.4","b:  0.57<br />value: 0.7980<br />Function: Penalty<br />Lambda: 1.4","b:  0.58<br />value: 0.8120<br />Function: Penalty<br />Lambda: 1.4","b:  0.59<br />value: 0.8260<br />Function: Penalty<br />Lambda: 1.4","b:  0.60<br />value: 0.8400<br />Function: Penalty<br />Lambda: 1.4","b:  0.61<br />value: 0.8540<br />Function: Penalty<br />Lambda: 1.4","b:  0.62<br />value: 0.8680<br />Function: Penalty<br />Lambda: 1.4","b:  0.63<br />value: 0.8820<br />Function: Penalty<br />Lambda: 1.4","b:  0.64<br />value: 0.8960<br />Function: Penalty<br />Lambda: 1.4","b:  0.65<br />value: 0.9100<br />Function: Penalty<br />Lambda: 1.4","b:  0.66<br />value: 0.9240<br />Function: Penalty<br />Lambda: 1.4","b:  0.67<br />value: 0.9380<br />Function: Penalty<br />Lambda: 1.4","b:  0.68<br />value: 0.9520<br />Function: Penalty<br />Lambda: 1.4","b:  0.69<br />value: 0.9660<br />Function: Penalty<br />Lambda: 1.4","b:  0.70<br />value: 0.9800<br />Function: Penalty<br />Lambda: 1.4","b:  0.71<br />value: 0.9940<br />Function: Penalty<br />Lambda: 1.4","b:  0.72<br />value: 1.0080<br />Function: Penalty<br />Lambda: 1.4","b:  0.73<br />value: 1.0220<br />Function: Penalty<br />Lambda: 1.4","b:  0.74<br />value: 1.0360<br />Function: Penalty<br />Lambda: 1.4","b:  0.75<br />value: 1.0500<br />Function: Penalty<br />Lambda: 1.4","b:  0.76<br />value: 1.0640<br />Function: Penalty<br />Lambda: 1.4","b:  0.77<br />value: 1.0780<br />Function: Penalty<br />Lambda: 1.4","b:  0.78<br />value: 1.0920<br />Function: Penalty<br />Lambda: 1.4","b:  0.79<br />value: 1.1060<br />Function: Penalty<br />Lambda: 1.4","b:  0.80<br />value: 1.1200<br />Function: Penalty<br />Lambda: 1.4","b:  0.81<br />value: 1.1340<br />Function: Penalty<br />Lambda: 1.4","b:  0.82<br />value: 1.1480<br />Function: Penalty<br />Lambda: 1.4","b:  0.83<br />value: 1.1620<br />Function: Penalty<br />Lambda: 1.4","b:  0.84<br />value: 1.1760<br />Function: Penalty<br />Lambda: 1.4","b:  0.85<br />value: 1.1900<br />Function: Penalty<br />Lambda: 1.4","b:  0.86<br />value: 1.2040<br />Function: Penalty<br />Lambda: 1.4","b:  0.87<br />value: 1.2180<br />Function: Penalty<br />Lambda: 1.4","b:  0.88<br />value: 1.2320<br />Function: Penalty<br />Lambda: 1.4","b:  0.89<br />value: 1.2460<br />Function: Penalty<br />Lambda: 1.4","b:  0.90<br />value: 1.2600<br />Function: Penalty<br />Lambda: 1.4","b:  0.91<br />value: 1.2740<br />Function: Penalty<br />Lambda: 1.4","b:  0.92<br />value: 1.2880<br />Function: Penalty<br />Lambda: 1.4","b:  0.93<br />value: 1.3020<br />Function: Penalty<br />Lambda: 1.4","b:  0.94<br />value: 1.3160<br />Function: Penalty<br />Lambda: 1.4","b:  0.95<br />value: 1.3300<br />Function: Penalty<br />Lambda: 1.4","b:  0.96<br />value: 1.3440<br />Function: Penalty<br />Lambda: 1.4","b:  0.97<br />value: 1.3580<br />Function: Penalty<br />Lambda: 1.4","b:  0.98<br />value: 1.3720<br />Function: Penalty<br />Lambda: 1.4","b:  0.99<br />value: 1.3860<br />Function: Penalty<br />Lambda: 1.4","b:  1.00<br />value: 1.4000<br />Function: Penalty<br />Lambda: 1.4","b:  1.01<br />value: 1.4140<br />Function: Penalty<br />Lambda: 1.4","b:  1.02<br />value: 1.4280<br />Function: Penalty<br />Lambda: 1.4","b:  1.03<br />value: 1.4420<br />Function: Penalty<br />Lambda: 1.4","b:  1.04<br />value: 1.4560<br />Function: Penalty<br />Lambda: 1.4","b:  1.05<br />value: 1.4700<br />Function: Penalty<br />Lambda: 1.4","b:  1.06<br />value: 1.4840<br />Function: Penalty<br />Lambda: 1.4","b:  1.07<br />value: 1.4980<br />Function: Penalty<br />Lambda: 1.4","b:  1.08<br />value: 1.5120<br />Function: Penalty<br />Lambda: 1.4","b:  1.09<br />value: 1.5260<br />Function: Penalty<br />Lambda: 1.4","b:  1.10<br />value: 1.5400<br />Function: Penalty<br />Lambda: 1.4","b:  1.11<br />value: 1.5540<br />Function: Penalty<br />Lambda: 1.4","b:  1.12<br />value: 1.5680<br />Function: Penalty<br />Lambda: 1.4","b:  1.13<br />value: 1.5820<br />Function: Penalty<br />Lambda: 1.4","b:  1.14<br />value: 1.5960<br />Function: Penalty<br />Lambda: 1.4","b:  1.15<br />value: 1.6100<br />Function: Penalty<br />Lambda: 1.4","b:  1.16<br />value: 1.6240<br />Function: Penalty<br />Lambda: 1.4","b:  1.17<br />value: 1.6380<br />Function: Penalty<br />Lambda: 1.4","b:  1.18<br />value: 1.6520<br />Function: Penalty<br />Lambda: 1.4","b:  1.19<br />value: 1.6660<br />Function: Penalty<br />Lambda: 1.4","b:  1.20<br />value: 1.6800<br />Function: Penalty<br />Lambda: 1.4","b:  1.21<br />value: 1.6940<br />Function: Penalty<br />Lambda: 1.4","b:  1.22<br />value: 1.7080<br />Function: Penalty<br />Lambda: 1.4","b:  1.23<br />value: 1.7220<br />Function: Penalty<br />Lambda: 1.4","b:  1.24<br />value: 1.7360<br />Function: Penalty<br />Lambda: 1.4","b:  1.25<br />value: 1.7500<br />Function: Penalty<br />Lambda: 1.4","b:  1.26<br />value: 1.7640<br />Function: Penalty<br />Lambda: 1.4","b:  1.27<br />value: 1.7780<br />Function: Penalty<br />Lambda: 1.4","b:  1.28<br />value: 1.7920<br />Function: Penalty<br />Lambda: 1.4","b:  1.29<br />value: 1.8060<br />Function: Penalty<br />Lambda: 1.4","b:  1.30<br />value: 1.8200<br />Function: Penalty<br />Lambda: 1.4","b:  1.31<br />value: 1.8340<br />Function: Penalty<br />Lambda: 1.4","b:  1.32<br />value: 1.8480<br />Function: Penalty<br />Lambda: 1.4","b:  1.33<br />value: 1.8620<br />Function: Penalty<br />Lambda: 1.4","b:  1.34<br />value: 1.8760<br />Function: Penalty<br />Lambda: 1.4","b:  1.35<br />value: 1.8900<br />Function: Penalty<br />Lambda: 1.4","b:  1.36<br />value: 1.9040<br />Function: Penalty<br />Lambda: 1.4","b:  1.37<br />value: 1.9180<br />Function: Penalty<br />Lambda: 1.4","b:  1.38<br />value: 1.9320<br />Function: Penalty<br />Lambda: 1.4","b:  1.39<br />value: 1.9460<br />Function: Penalty<br />Lambda: 1.4","b:  1.40<br />value: 1.9600<br />Function: Penalty<br />Lambda: 1.4","b:  1.41<br />value: 1.9740<br />Function: Penalty<br />Lambda: 1.4","b:  1.42<br />value: 1.9880<br />Function: Penalty<br />Lambda: 1.4","b:  1.43<br />value: 2.0020<br />Function: Penalty<br />Lambda: 1.4","b:  1.44<br />value: 2.0160<br />Function: Penalty<br />Lambda: 1.4","b:  1.45<br />value: 2.0300<br />Function: Penalty<br />Lambda: 1.4","b:  1.46<br />value: 2.0440<br />Function: Penalty<br />Lambda: 1.4","b:  1.47<br />value: 2.0580<br />Function: Penalty<br />Lambda: 1.4","b:  1.48<br />value: 2.0720<br />Function: Penalty<br />Lambda: 1.4","b:  1.49<br />value: 2.0860<br />Function: Penalty<br />Lambda: 1.4","b:  1.50<br />value: 2.1000<br />Function: Penalty<br />Lambda: 1.4","b:  1.51<br />value: 2.1140<br />Function: Penalty<br />Lambda: 1.4","b:  1.52<br />value: 2.1280<br />Function: Penalty<br />Lambda: 1.4","b:  1.53<br />value: 2.1420<br />Function: Penalty<br />Lambda: 1.4","b:  1.54<br />value: 2.1560<br />Function: Penalty<br />Lambda: 1.4","b:  1.55<br />value: 2.1700<br />Function: Penalty<br />Lambda: 1.4","b:  1.56<br />value: 2.1840<br />Function: Penalty<br />Lambda: 1.4","b:  1.57<br />value: 2.1980<br />Function: Penalty<br />Lambda: 1.4","b:  1.58<br />value: 2.2120<br />Function: Penalty<br />Lambda: 1.4","b:  1.59<br />value: 2.2260<br />Function: Penalty<br />Lambda: 1.4","b:  1.60<br />value: 2.2400<br />Function: Penalty<br />Lambda: 1.4","b:  1.61<br />value: 2.2540<br />Function: Penalty<br />Lambda: 1.4","b:  1.62<br />value: 2.2680<br />Function: Penalty<br />Lambda: 1.4","b:  1.63<br />value: 2.2820<br />Function: Penalty<br />Lambda: 1.4","b:  1.64<br />value: 2.2960<br />Function: Penalty<br />Lambda: 1.4","b:  1.65<br />value: 2.3100<br />Function: Penalty<br />Lambda: 1.4","b:  1.66<br />value: 2.3240<br />Function: Penalty<br />Lambda: 1.4","b:  1.67<br />value: 2.3380<br />Function: Penalty<br />Lambda: 1.4","b:  1.68<br />value: 2.3520<br />Function: Penalty<br />Lambda: 1.4","b:  1.69<br />value: 2.3660<br />Function: Penalty<br />Lambda: 1.4","b:  1.70<br />value: 2.3800<br />Function: Penalty<br />Lambda: 1.4","b:  1.71<br />value: 2.3940<br />Function: Penalty<br />Lambda: 1.4","b:  1.72<br />value: 2.4080<br />Function: Penalty<br />Lambda: 1.4","b:  1.73<br />value: 2.4220<br />Function: Penalty<br />Lambda: 1.4","b:  1.74<br />value: 2.4360<br />Function: Penalty<br />Lambda: 1.4","b:  1.75<br />value: 2.4500<br />Function: Penalty<br />Lambda: 1.4","b:  1.76<br />value: 2.4640<br />Function: Penalty<br />Lambda: 1.4","b:  1.77<br />value: 2.4780<br />Function: Penalty<br />Lambda: 1.4","b:  1.78<br />value: 2.4920<br />Function: Penalty<br />Lambda: 1.4","b:  1.79<br />value: 2.5060<br />Function: Penalty<br />Lambda: 1.4","b:  1.80<br />value: 2.5200<br />Function: Penalty<br />Lambda: 1.4","b:  1.81<br />value: 2.5340<br />Function: Penalty<br />Lambda: 1.4","b:  1.82<br />value: 2.5480<br />Function: Penalty<br />Lambda: 1.4","b:  1.83<br />value: 2.5620<br />Function: Penalty<br />Lambda: 1.4","b:  1.84<br />value: 2.5760<br />Function: Penalty<br />Lambda: 1.4","b:  1.85<br />value: 2.5900<br />Function: Penalty<br />Lambda: 1.4","b:  1.86<br />value: 2.6040<br />Function: Penalty<br />Lambda: 1.4","b:  1.87<br />value: 2.6180<br />Function: Penalty<br />Lambda: 1.4","b:  1.88<br />value: 2.6320<br />Function: Penalty<br />Lambda: 1.4","b:  1.89<br />value: 2.6460<br />Function: Penalty<br />Lambda: 1.4","b:  1.90<br />value: 2.6600<br />Function: Penalty<br />Lambda: 1.4","b:  1.91<br />value: 2.6740<br />Function: Penalty<br />Lambda: 1.4","b:  1.92<br />value: 2.6880<br />Function: Penalty<br />Lambda: 1.4","b:  1.93<br />value: 2.7020<br />Function: Penalty<br />Lambda: 1.4","b:  1.94<br />value: 2.7160<br />Function: Penalty<br />Lambda: 1.4","b:  1.95<br />value: 2.7300<br />Function: Penalty<br />Lambda: 1.4","b:  1.96<br />value: 2.7440<br />Function: Penalty<br />Lambda: 1.4","b:  1.97<br />value: 2.7580<br />Function: Penalty<br />Lambda: 1.4","b:  1.98<br />value: 2.7720<br />Function: Penalty<br />Lambda: 1.4","b:  1.99<br />value: 2.7860<br />Function: Penalty<br />Lambda: 1.4","b:  2.00<br />value: 2.8000<br />Function: Penalty<br />Lambda: 1.4"],"frame":"1.4","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"1.5","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 1.5","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 1.5","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 1.5","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 1.5","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 1.5","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 1.5","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 1.5","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 1.5","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 1.5","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 1.5","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 1.5","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 1.5","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 1.5","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 1.5","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 1.5","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 1.5","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 1.5","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 1.5","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 1.5","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 1.5","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 1.5","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 1.5","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 1.5","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 1.5","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 1.5","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 1.5","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 1.5","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 1.5","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 1.5","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 1.5","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 1.5","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 1.5","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 1.5","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 1.5","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 1.5","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 1.5","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 1.5","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 1.5","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 1.5","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 1.5","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 1.5","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 1.5","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 1.5","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 1.5","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 1.5","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 1.5","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 1.5","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 1.5","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 1.5","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 1.5","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.5","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.5","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.5","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.5","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.5","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.5","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.5","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.5","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.5","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.5","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.5","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.5","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.5","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.5","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.5","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.5","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.5","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.5","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.5","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.5","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.5","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.5","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.5","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.5","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.5","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.5","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.5","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.5","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.5","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.5","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.5","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.5","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.5","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.5","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.5","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.5","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.5","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.5","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.5","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.5","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.5","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.5","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.5","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.5","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.5","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.5","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.5","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.5","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.5","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.5","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.5","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.5","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.5","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.5","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.5","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.5","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.5","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.5","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.5","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.5","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.5","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.5","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.5","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.5","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.5","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.5","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.5","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.5","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.5","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.5","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.5","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.5","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.5","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.5","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.5","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.5","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.5","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.5","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.5","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.5","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.5","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.5","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.5","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.5","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.5","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.5","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.5","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.5","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.5","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.5","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.5","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.5","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.5","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.5","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.5","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.5","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.5","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.5","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.5","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.5","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 1.5","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.5","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.5","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.5","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.5","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.5","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.5","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.5","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.5","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.5","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.5","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.5","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.5","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.5","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.5","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.5","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.5","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.5","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.5","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.5","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.5","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.5","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.5","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.5","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.5","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.5","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.5","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.5","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.5","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.5","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.5","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.5","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.5","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.5","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.5","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.5","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.5","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.5","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.5","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.5","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.5","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.5","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.5","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.5","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.5","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.5","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.5","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.5","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.5","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.5","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.5","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.5","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.5","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.5","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.5","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.5","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.5","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.5","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.5","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.5","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.5","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.5","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.5","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.5","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.5","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.5","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.5","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.5","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.5","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.5","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.5","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.5","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.5","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.5","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.5","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.5","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.5","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.5","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.5","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.5","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.5","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.5","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.5","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.5","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.5","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.5","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.5","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.5","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.5","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.5","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.5","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.5","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.5","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.5","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.5","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.5","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.5","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.5","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.5","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.5","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.5"],"frame":"1.5","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.5,3.4551,3.4104,3.3659,3.3216,3.2775,3.2336,3.1899,3.1464,3.1031,3.06,3.0171,2.9744,2.9319,2.8896,2.8475,2.8056,2.7639,2.7224,2.6811,2.64,2.5991,2.5584,2.5179,2.4776,2.4375,2.3976,2.3579,2.3184,2.2791,2.24,2.2011,2.1624,2.1239,2.0856,2.0475,2.0096,1.9719,1.9344,1.8971,1.86,1.8231,1.7864,1.7499,1.7136,1.6775,1.6416,1.6059,1.5704,1.5351,1.5,1.4951,1.4904,1.4859,1.4816,1.4775,1.4736,1.4699,1.4664,1.4631,1.46,1.4571,1.4544,1.4519,1.4496,1.4475,1.4456,1.4439,1.4424,1.4411,1.44,1.4391,1.4384,1.4379,1.4376,1.4375,1.4376,1.4379,1.4384,1.4391,1.44,1.4411,1.4424,1.4439,1.4456,1.4475,1.4496,1.4519,1.4544,1.4571,1.46,1.4631,1.4664,1.4699,1.4736,1.4775,1.4816,1.4859,1.4904,1.4951,1.5,1.5051,1.5104,1.5159,1.5216,1.5275,1.5336,1.5399,1.5464,1.5531,1.56,1.5671,1.5744,1.5819,1.5896,1.5975,1.6056,1.6139,1.6224,1.6311,1.64,1.6491,1.6584,1.6679,1.6776,1.6875,1.6976,1.7079,1.7184,1.7291,1.74,1.7511,1.7624,1.7739,1.7856,1.7975,1.8096,1.8219,1.8344,1.8471,1.86,1.8731,1.8864,1.8999,1.9136,1.9275,1.9416,1.9559,1.9704,1.9851,2,2.0151,2.0304,2.0459,2.0616,2.0775,2.0936,2.1099,2.1264,2.1431,2.16,2.1771,2.1944,2.2119,2.2296,2.2475,2.2656,2.2839,2.3024,2.3211,2.34,2.3591,2.3784,2.3979,2.4176,2.4375,2.4576,2.4779,2.4984,2.5191,2.54,2.5611,2.5824,2.6039,2.6256,2.6475,2.6696,2.6919,2.7144,2.7371,2.76,2.7831,2.8064,2.8299,2.8536,2.8775,2.9016,2.9259,2.9504,2.9751,3,3.0251,3.0504,3.0759,3.1016,3.1275,3.1536,3.1799,3.2064,3.2331,3.26,3.2871,3.3144,3.3419,3.3696,3.3975,3.4256,3.4539,3.4824,3.5111,3.54,3.5691,3.5984,3.6279,3.6576,3.6875,3.7176,3.7479,3.7784,3.8091,3.84,3.8711,3.9024,3.9339,3.9656,3.9975,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 3.5000<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.49<br />value: 3.4551<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.48<br />value: 3.4104<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.47<br />value: 3.3659<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.46<br />value: 3.3216<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.45<br />value: 3.2775<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.44<br />value: 3.2336<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.43<br />value: 3.1899<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.42<br />value: 3.1464<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.41<br />value: 3.1031<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.40<br />value: 3.0600<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.39<br />value: 3.0171<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.38<br />value: 2.9744<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.37<br />value: 2.9319<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.36<br />value: 2.8896<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.35<br />value: 2.8475<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.34<br />value: 2.8056<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.33<br />value: 2.7639<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.32<br />value: 2.7224<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.31<br />value: 2.6811<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.30<br />value: 2.6400<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.29<br />value: 2.5991<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.28<br />value: 2.5584<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.27<br />value: 2.5179<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.26<br />value: 2.4776<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.25<br />value: 2.4375<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.24<br />value: 2.3976<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.23<br />value: 2.3579<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.22<br />value: 2.3184<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.21<br />value: 2.2791<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.20<br />value: 2.2400<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.19<br />value: 2.2011<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.18<br />value: 2.1624<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.17<br />value: 2.1239<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.16<br />value: 2.0856<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.15<br />value: 2.0475<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.14<br />value: 2.0096<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.13<br />value: 1.9719<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.12<br />value: 1.9344<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.11<br />value: 1.8971<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.10<br />value: 1.8600<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.09<br />value: 1.8231<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.08<br />value: 1.7864<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.07<br />value: 1.7499<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.06<br />value: 1.7136<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.05<br />value: 1.6775<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.04<br />value: 1.6416<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.03<br />value: 1.6059<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.02<br />value: 1.5704<br />Function: Loss + Penalty<br />Lambda: 1.5","b: -0.01<br />value: 1.5351<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.01<br />value: 1.4951<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.02<br />value: 1.4904<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.03<br />value: 1.4859<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.04<br />value: 1.4816<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.05<br />value: 1.4775<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.06<br />value: 1.4736<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.07<br />value: 1.4699<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.08<br />value: 1.4664<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.09<br />value: 1.4631<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.10<br />value: 1.4600<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.11<br />value: 1.4571<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.12<br />value: 1.4544<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.13<br />value: 1.4519<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.14<br />value: 1.4496<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.15<br />value: 1.4475<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.16<br />value: 1.4456<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.17<br />value: 1.4439<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.18<br />value: 1.4424<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.19<br />value: 1.4411<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.20<br />value: 1.4400<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.21<br />value: 1.4391<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.22<br />value: 1.4384<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.23<br />value: 1.4379<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.24<br />value: 1.4376<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.25<br />value: 1.4375<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.26<br />value: 1.4376<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.27<br />value: 1.4379<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.28<br />value: 1.4384<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.29<br />value: 1.4391<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.30<br />value: 1.4400<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.31<br />value: 1.4411<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.32<br />value: 1.4424<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.33<br />value: 1.4439<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.34<br />value: 1.4456<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.35<br />value: 1.4475<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.36<br />value: 1.4496<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.37<br />value: 1.4519<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.38<br />value: 1.4544<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.39<br />value: 1.4571<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.40<br />value: 1.4600<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.41<br />value: 1.4631<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.42<br />value: 1.4664<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.43<br />value: 1.4699<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.44<br />value: 1.4736<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.45<br />value: 1.4775<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.46<br />value: 1.4816<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.47<br />value: 1.4859<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.48<br />value: 1.4904<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.49<br />value: 1.4951<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.50<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.51<br />value: 1.5051<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.52<br />value: 1.5104<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.53<br />value: 1.5159<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.54<br />value: 1.5216<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.55<br />value: 1.5275<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.56<br />value: 1.5336<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.57<br />value: 1.5399<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.58<br />value: 1.5464<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.59<br />value: 1.5531<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.60<br />value: 1.5600<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.61<br />value: 1.5671<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.62<br />value: 1.5744<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.63<br />value: 1.5819<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.64<br />value: 1.5896<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.65<br />value: 1.5975<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.66<br />value: 1.6056<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.67<br />value: 1.6139<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.68<br />value: 1.6224<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.69<br />value: 1.6311<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.70<br />value: 1.6400<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.71<br />value: 1.6491<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.72<br />value: 1.6584<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.73<br />value: 1.6679<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.74<br />value: 1.6776<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.75<br />value: 1.6875<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.76<br />value: 1.6976<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.77<br />value: 1.7079<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.78<br />value: 1.7184<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.79<br />value: 1.7291<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.80<br />value: 1.7400<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.81<br />value: 1.7511<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.82<br />value: 1.7624<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.83<br />value: 1.7739<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.84<br />value: 1.7856<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.85<br />value: 1.7975<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.86<br />value: 1.8096<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.87<br />value: 1.8219<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.88<br />value: 1.8344<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.89<br />value: 1.8471<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.90<br />value: 1.8600<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.91<br />value: 1.8731<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.92<br />value: 1.8864<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.93<br />value: 1.8999<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.94<br />value: 1.9136<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.95<br />value: 1.9275<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.96<br />value: 1.9416<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.97<br />value: 1.9559<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.98<br />value: 1.9704<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  0.99<br />value: 1.9851<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.00<br />value: 2.0000<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.01<br />value: 2.0151<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.02<br />value: 2.0304<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.03<br />value: 2.0459<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.04<br />value: 2.0616<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.05<br />value: 2.0775<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.06<br />value: 2.0936<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.07<br />value: 2.1099<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.08<br />value: 2.1264<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.09<br />value: 2.1431<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.10<br />value: 2.1600<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.11<br />value: 2.1771<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.12<br />value: 2.1944<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.13<br />value: 2.2119<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.14<br />value: 2.2296<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.15<br />value: 2.2475<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.16<br />value: 2.2656<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.17<br />value: 2.2839<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.18<br />value: 2.3024<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.19<br />value: 2.3211<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.20<br />value: 2.3400<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.21<br />value: 2.3591<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.22<br />value: 2.3784<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.23<br />value: 2.3979<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.24<br />value: 2.4176<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.25<br />value: 2.4375<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.26<br />value: 2.4576<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.27<br />value: 2.4779<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.28<br />value: 2.4984<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.29<br />value: 2.5191<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.30<br />value: 2.5400<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.31<br />value: 2.5611<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.32<br />value: 2.5824<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.33<br />value: 2.6039<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.34<br />value: 2.6256<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.35<br />value: 2.6475<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.36<br />value: 2.6696<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.37<br />value: 2.6919<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.38<br />value: 2.7144<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.39<br />value: 2.7371<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.40<br />value: 2.7600<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.41<br />value: 2.7831<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.42<br />value: 2.8064<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.43<br />value: 2.8299<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.44<br />value: 2.8536<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.45<br />value: 2.8775<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.46<br />value: 2.9016<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.47<br />value: 2.9259<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.48<br />value: 2.9504<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.49<br />value: 2.9751<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.50<br />value: 3.0000<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.51<br />value: 3.0251<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.52<br />value: 3.0504<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.53<br />value: 3.0759<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.54<br />value: 3.1016<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.55<br />value: 3.1275<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.56<br />value: 3.1536<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.57<br />value: 3.1799<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.58<br />value: 3.2064<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.59<br />value: 3.2331<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.60<br />value: 3.2600<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.61<br />value: 3.2871<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.62<br />value: 3.3144<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.63<br />value: 3.3419<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.64<br />value: 3.3696<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.65<br />value: 3.3975<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.66<br />value: 3.4256<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.67<br />value: 3.4539<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.68<br />value: 3.4824<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.69<br />value: 3.5111<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.70<br />value: 3.5400<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.71<br />value: 3.5691<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.72<br />value: 3.5984<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.73<br />value: 3.6279<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.74<br />value: 3.6576<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.75<br />value: 3.6875<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.76<br />value: 3.7176<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.77<br />value: 3.7479<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.78<br />value: 3.7784<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.79<br />value: 3.8091<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.80<br />value: 3.8400<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.81<br />value: 3.8711<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.82<br />value: 3.9024<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.83<br />value: 3.9339<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.84<br />value: 3.9656<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.85<br />value: 3.9975<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.86<br />value: 4.0296<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.87<br />value: 4.0619<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.88<br />value: 4.0944<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.89<br />value: 4.1271<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.90<br />value: 4.1600<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.91<br />value: 4.1931<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.92<br />value: 4.2264<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.93<br />value: 4.2599<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.94<br />value: 4.2936<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.95<br />value: 4.3275<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.96<br />value: 4.3616<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.97<br />value: 4.3959<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.98<br />value: 4.4304<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  1.99<br />value: 4.4651<br />Function: Loss + Penalty<br />Lambda: 1.5","b:  2.00<br />value: 4.5000<br />Function: Loss + Penalty<br />Lambda: 1.5"],"frame":"1.5","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.75,0.735,0.72,0.705,0.69,0.675,0.66,0.645,0.63,0.615,0.6,0.585,0.57,0.555,0.54,0.525,0.51,0.495,0.48,0.465,0.45,0.435,0.42,0.405,0.39,0.375,0.36,0.345,0.33,0.315,0.3,0.285,0.27,0.255,0.24,0.225,0.21,0.195,0.18,0.165,0.15,0.135,0.12,0.105,0.09,0.075,0.06,0.045,0.03,0.015,0,0.015,0.03,0.045,0.0600000000000001,0.0750000000000001,0.0900000000000001,0.105,0.12,0.135,0.15,0.165,0.18,0.195,0.21,0.225,0.24,0.255,0.27,0.285,0.3,0.315,0.33,0.345,0.36,0.375,0.39,0.405,0.42,0.435,0.45,0.465,0.48,0.495,0.51,0.525,0.54,0.555,0.57,0.585,0.6,0.615,0.63,0.645,0.66,0.675,0.69,0.705,0.72,0.735,0.75,0.765,0.78,0.795,0.81,0.825,0.84,0.855,0.87,0.885,0.9,0.915,0.93,0.945,0.96,0.975,0.99,1.005,1.02,1.035,1.05,1.065,1.08,1.095,1.11,1.125,1.14,1.155,1.17,1.185,1.2,1.215,1.23,1.245,1.26,1.275,1.29,1.305,1.32,1.335,1.35,1.365,1.38,1.395,1.41,1.425,1.44,1.455,1.47,1.485,1.5,1.515,1.53,1.545,1.56,1.575,1.59,1.605,1.62,1.635,1.65,1.665,1.68,1.695,1.71,1.725,1.74,1.755,1.77,1.785,1.8,1.815,1.83,1.845,1.86,1.875,1.89,1.905,1.92,1.935,1.95,1.965,1.98,1.995,2.01,2.025,2.04,2.055,2.07,2.085,2.1,2.115,2.13,2.145,2.16,2.175,2.19,2.205,2.22,2.235,2.25,2.265,2.28,2.295,2.31,2.325,2.34,2.355,2.37,2.385,2.4,2.415,2.43,2.445,2.46,2.475,2.49,2.505,2.52,2.535,2.55,2.565,2.58,2.595,2.61,2.625,2.64,2.655,2.67,2.685,2.7,2.715,2.73,2.745,2.76,2.775,2.79,2.805,2.82,2.835,2.85,2.865,2.88,2.895,2.91,2.925,2.94,2.955,2.97,2.985,3],"text":["b: -0.50<br />value: 0.7500<br />Function: Penalty<br />Lambda: 1.5","b: -0.49<br />value: 0.7350<br />Function: Penalty<br />Lambda: 1.5","b: -0.48<br />value: 0.7200<br />Function: Penalty<br />Lambda: 1.5","b: -0.47<br />value: 0.7050<br />Function: Penalty<br />Lambda: 1.5","b: -0.46<br />value: 0.6900<br />Function: Penalty<br />Lambda: 1.5","b: -0.45<br />value: 0.6750<br />Function: Penalty<br />Lambda: 1.5","b: -0.44<br />value: 0.6600<br />Function: Penalty<br />Lambda: 1.5","b: -0.43<br />value: 0.6450<br />Function: Penalty<br />Lambda: 1.5","b: -0.42<br />value: 0.6300<br />Function: Penalty<br />Lambda: 1.5","b: -0.41<br />value: 0.6150<br />Function: Penalty<br />Lambda: 1.5","b: -0.40<br />value: 0.6000<br />Function: Penalty<br />Lambda: 1.5","b: -0.39<br />value: 0.5850<br />Function: Penalty<br />Lambda: 1.5","b: -0.38<br />value: 0.5700<br />Function: Penalty<br />Lambda: 1.5","b: -0.37<br />value: 0.5550<br />Function: Penalty<br />Lambda: 1.5","b: -0.36<br />value: 0.5400<br />Function: Penalty<br />Lambda: 1.5","b: -0.35<br />value: 0.5250<br />Function: Penalty<br />Lambda: 1.5","b: -0.34<br />value: 0.5100<br />Function: Penalty<br />Lambda: 1.5","b: -0.33<br />value: 0.4950<br />Function: Penalty<br />Lambda: 1.5","b: -0.32<br />value: 0.4800<br />Function: Penalty<br />Lambda: 1.5","b: -0.31<br />value: 0.4650<br />Function: Penalty<br />Lambda: 1.5","b: -0.30<br />value: 0.4500<br />Function: Penalty<br />Lambda: 1.5","b: -0.29<br />value: 0.4350<br />Function: Penalty<br />Lambda: 1.5","b: -0.28<br />value: 0.4200<br />Function: Penalty<br />Lambda: 1.5","b: -0.27<br />value: 0.4050<br />Function: Penalty<br />Lambda: 1.5","b: -0.26<br />value: 0.3900<br />Function: Penalty<br />Lambda: 1.5","b: -0.25<br />value: 0.3750<br />Function: Penalty<br />Lambda: 1.5","b: -0.24<br />value: 0.3600<br />Function: Penalty<br />Lambda: 1.5","b: -0.23<br />value: 0.3450<br />Function: Penalty<br />Lambda: 1.5","b: -0.22<br />value: 0.3300<br />Function: Penalty<br />Lambda: 1.5","b: -0.21<br />value: 0.3150<br />Function: Penalty<br />Lambda: 1.5","b: -0.20<br />value: 0.3000<br />Function: Penalty<br />Lambda: 1.5","b: -0.19<br />value: 0.2850<br />Function: Penalty<br />Lambda: 1.5","b: -0.18<br />value: 0.2700<br />Function: Penalty<br />Lambda: 1.5","b: -0.17<br />value: 0.2550<br />Function: Penalty<br />Lambda: 1.5","b: -0.16<br />value: 0.2400<br />Function: Penalty<br />Lambda: 1.5","b: -0.15<br />value: 0.2250<br />Function: Penalty<br />Lambda: 1.5","b: -0.14<br />value: 0.2100<br />Function: Penalty<br />Lambda: 1.5","b: -0.13<br />value: 0.1950<br />Function: Penalty<br />Lambda: 1.5","b: -0.12<br />value: 0.1800<br />Function: Penalty<br />Lambda: 1.5","b: -0.11<br />value: 0.1650<br />Function: Penalty<br />Lambda: 1.5","b: -0.10<br />value: 0.1500<br />Function: Penalty<br />Lambda: 1.5","b: -0.09<br />value: 0.1350<br />Function: Penalty<br />Lambda: 1.5","b: -0.08<br />value: 0.1200<br />Function: Penalty<br />Lambda: 1.5","b: -0.07<br />value: 0.1050<br />Function: Penalty<br />Lambda: 1.5","b: -0.06<br />value: 0.0900<br />Function: Penalty<br />Lambda: 1.5","b: -0.05<br />value: 0.0750<br />Function: Penalty<br />Lambda: 1.5","b: -0.04<br />value: 0.0600<br />Function: Penalty<br />Lambda: 1.5","b: -0.03<br />value: 0.0450<br />Function: Penalty<br />Lambda: 1.5","b: -0.02<br />value: 0.0300<br />Function: Penalty<br />Lambda: 1.5","b: -0.01<br />value: 0.0150<br />Function: Penalty<br />Lambda: 1.5","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 1.5","b:  0.01<br />value: 0.0150<br />Function: Penalty<br />Lambda: 1.5","b:  0.02<br />value: 0.0300<br />Function: Penalty<br />Lambda: 1.5","b:  0.03<br />value: 0.0450<br />Function: Penalty<br />Lambda: 1.5","b:  0.04<br />value: 0.0600<br />Function: Penalty<br />Lambda: 1.5","b:  0.05<br />value: 0.0750<br />Function: Penalty<br />Lambda: 1.5","b:  0.06<br />value: 0.0900<br />Function: Penalty<br />Lambda: 1.5","b:  0.07<br />value: 0.1050<br />Function: Penalty<br />Lambda: 1.5","b:  0.08<br />value: 0.1200<br />Function: Penalty<br />Lambda: 1.5","b:  0.09<br />value: 0.1350<br />Function: Penalty<br />Lambda: 1.5","b:  0.10<br />value: 0.1500<br />Function: Penalty<br />Lambda: 1.5","b:  0.11<br />value: 0.1650<br />Function: Penalty<br />Lambda: 1.5","b:  0.12<br />value: 0.1800<br />Function: Penalty<br />Lambda: 1.5","b:  0.13<br />value: 0.1950<br />Function: Penalty<br />Lambda: 1.5","b:  0.14<br />value: 0.2100<br />Function: Penalty<br />Lambda: 1.5","b:  0.15<br />value: 0.2250<br />Function: Penalty<br />Lambda: 1.5","b:  0.16<br />value: 0.2400<br />Function: Penalty<br />Lambda: 1.5","b:  0.17<br />value: 0.2550<br />Function: Penalty<br />Lambda: 1.5","b:  0.18<br />value: 0.2700<br />Function: Penalty<br />Lambda: 1.5","b:  0.19<br />value: 0.2850<br />Function: Penalty<br />Lambda: 1.5","b:  0.20<br />value: 0.3000<br />Function: Penalty<br />Lambda: 1.5","b:  0.21<br />value: 0.3150<br />Function: Penalty<br />Lambda: 1.5","b:  0.22<br />value: 0.3300<br />Function: Penalty<br />Lambda: 1.5","b:  0.23<br />value: 0.3450<br />Function: Penalty<br />Lambda: 1.5","b:  0.24<br />value: 0.3600<br />Function: Penalty<br />Lambda: 1.5","b:  0.25<br />value: 0.3750<br />Function: Penalty<br />Lambda: 1.5","b:  0.26<br />value: 0.3900<br />Function: Penalty<br />Lambda: 1.5","b:  0.27<br />value: 0.4050<br />Function: Penalty<br />Lambda: 1.5","b:  0.28<br />value: 0.4200<br />Function: Penalty<br />Lambda: 1.5","b:  0.29<br />value: 0.4350<br />Function: Penalty<br />Lambda: 1.5","b:  0.30<br />value: 0.4500<br />Function: Penalty<br />Lambda: 1.5","b:  0.31<br />value: 0.4650<br />Function: Penalty<br />Lambda: 1.5","b:  0.32<br />value: 0.4800<br />Function: Penalty<br />Lambda: 1.5","b:  0.33<br />value: 0.4950<br />Function: Penalty<br />Lambda: 1.5","b:  0.34<br />value: 0.5100<br />Function: Penalty<br />Lambda: 1.5","b:  0.35<br />value: 0.5250<br />Function: Penalty<br />Lambda: 1.5","b:  0.36<br />value: 0.5400<br />Function: Penalty<br />Lambda: 1.5","b:  0.37<br />value: 0.5550<br />Function: Penalty<br />Lambda: 1.5","b:  0.38<br />value: 0.5700<br />Function: Penalty<br />Lambda: 1.5","b:  0.39<br />value: 0.5850<br />Function: Penalty<br />Lambda: 1.5","b:  0.40<br />value: 0.6000<br />Function: Penalty<br />Lambda: 1.5","b:  0.41<br />value: 0.6150<br />Function: Penalty<br />Lambda: 1.5","b:  0.42<br />value: 0.6300<br />Function: Penalty<br />Lambda: 1.5","b:  0.43<br />value: 0.6450<br />Function: Penalty<br />Lambda: 1.5","b:  0.44<br />value: 0.6600<br />Function: Penalty<br />Lambda: 1.5","b:  0.45<br />value: 0.6750<br />Function: Penalty<br />Lambda: 1.5","b:  0.46<br />value: 0.6900<br />Function: Penalty<br />Lambda: 1.5","b:  0.47<br />value: 0.7050<br />Function: Penalty<br />Lambda: 1.5","b:  0.48<br />value: 0.7200<br />Function: Penalty<br />Lambda: 1.5","b:  0.49<br />value: 0.7350<br />Function: Penalty<br />Lambda: 1.5","b:  0.50<br />value: 0.7500<br />Function: Penalty<br />Lambda: 1.5","b:  0.51<br />value: 0.7650<br />Function: Penalty<br />Lambda: 1.5","b:  0.52<br />value: 0.7800<br />Function: Penalty<br />Lambda: 1.5","b:  0.53<br />value: 0.7950<br />Function: Penalty<br />Lambda: 1.5","b:  0.54<br />value: 0.8100<br />Function: Penalty<br />Lambda: 1.5","b:  0.55<br />value: 0.8250<br />Function: Penalty<br />Lambda: 1.5","b:  0.56<br />value: 0.8400<br />Function: Penalty<br />Lambda: 1.5","b:  0.57<br />value: 0.8550<br />Function: Penalty<br />Lambda: 1.5","b:  0.58<br />value: 0.8700<br />Function: Penalty<br />Lambda: 1.5","b:  0.59<br />value: 0.8850<br />Function: Penalty<br />Lambda: 1.5","b:  0.60<br />value: 0.9000<br />Function: Penalty<br />Lambda: 1.5","b:  0.61<br />value: 0.9150<br />Function: Penalty<br />Lambda: 1.5","b:  0.62<br />value: 0.9300<br />Function: Penalty<br />Lambda: 1.5","b:  0.63<br />value: 0.9450<br />Function: Penalty<br />Lambda: 1.5","b:  0.64<br />value: 0.9600<br />Function: Penalty<br />Lambda: 1.5","b:  0.65<br />value: 0.9750<br />Function: Penalty<br />Lambda: 1.5","b:  0.66<br />value: 0.9900<br />Function: Penalty<br />Lambda: 1.5","b:  0.67<br />value: 1.0050<br />Function: Penalty<br />Lambda: 1.5","b:  0.68<br />value: 1.0200<br />Function: Penalty<br />Lambda: 1.5","b:  0.69<br />value: 1.0350<br />Function: Penalty<br />Lambda: 1.5","b:  0.70<br />value: 1.0500<br />Function: Penalty<br />Lambda: 1.5","b:  0.71<br />value: 1.0650<br />Function: Penalty<br />Lambda: 1.5","b:  0.72<br />value: 1.0800<br />Function: Penalty<br />Lambda: 1.5","b:  0.73<br />value: 1.0950<br />Function: Penalty<br />Lambda: 1.5","b:  0.74<br />value: 1.1100<br />Function: Penalty<br />Lambda: 1.5","b:  0.75<br />value: 1.1250<br />Function: Penalty<br />Lambda: 1.5","b:  0.76<br />value: 1.1400<br />Function: Penalty<br />Lambda: 1.5","b:  0.77<br />value: 1.1550<br />Function: Penalty<br />Lambda: 1.5","b:  0.78<br />value: 1.1700<br />Function: Penalty<br />Lambda: 1.5","b:  0.79<br />value: 1.1850<br />Function: Penalty<br />Lambda: 1.5","b:  0.80<br />value: 1.2000<br />Function: Penalty<br />Lambda: 1.5","b:  0.81<br />value: 1.2150<br />Function: Penalty<br />Lambda: 1.5","b:  0.82<br />value: 1.2300<br />Function: Penalty<br />Lambda: 1.5","b:  0.83<br />value: 1.2450<br />Function: Penalty<br />Lambda: 1.5","b:  0.84<br />value: 1.2600<br />Function: Penalty<br />Lambda: 1.5","b:  0.85<br />value: 1.2750<br />Function: Penalty<br />Lambda: 1.5","b:  0.86<br />value: 1.2900<br />Function: Penalty<br />Lambda: 1.5","b:  0.87<br />value: 1.3050<br />Function: Penalty<br />Lambda: 1.5","b:  0.88<br />value: 1.3200<br />Function: Penalty<br />Lambda: 1.5","b:  0.89<br />value: 1.3350<br />Function: Penalty<br />Lambda: 1.5","b:  0.90<br />value: 1.3500<br />Function: Penalty<br />Lambda: 1.5","b:  0.91<br />value: 1.3650<br />Function: Penalty<br />Lambda: 1.5","b:  0.92<br />value: 1.3800<br />Function: Penalty<br />Lambda: 1.5","b:  0.93<br />value: 1.3950<br />Function: Penalty<br />Lambda: 1.5","b:  0.94<br />value: 1.4100<br />Function: Penalty<br />Lambda: 1.5","b:  0.95<br />value: 1.4250<br />Function: Penalty<br />Lambda: 1.5","b:  0.96<br />value: 1.4400<br />Function: Penalty<br />Lambda: 1.5","b:  0.97<br />value: 1.4550<br />Function: Penalty<br />Lambda: 1.5","b:  0.98<br />value: 1.4700<br />Function: Penalty<br />Lambda: 1.5","b:  0.99<br />value: 1.4850<br />Function: Penalty<br />Lambda: 1.5","b:  1.00<br />value: 1.5000<br />Function: Penalty<br />Lambda: 1.5","b:  1.01<br />value: 1.5150<br />Function: Penalty<br />Lambda: 1.5","b:  1.02<br />value: 1.5300<br />Function: Penalty<br />Lambda: 1.5","b:  1.03<br />value: 1.5450<br />Function: Penalty<br />Lambda: 1.5","b:  1.04<br />value: 1.5600<br />Function: Penalty<br />Lambda: 1.5","b:  1.05<br />value: 1.5750<br />Function: Penalty<br />Lambda: 1.5","b:  1.06<br />value: 1.5900<br />Function: Penalty<br />Lambda: 1.5","b:  1.07<br />value: 1.6050<br />Function: Penalty<br />Lambda: 1.5","b:  1.08<br />value: 1.6200<br />Function: Penalty<br />Lambda: 1.5","b:  1.09<br />value: 1.6350<br />Function: Penalty<br />Lambda: 1.5","b:  1.10<br />value: 1.6500<br />Function: Penalty<br />Lambda: 1.5","b:  1.11<br />value: 1.6650<br />Function: Penalty<br />Lambda: 1.5","b:  1.12<br />value: 1.6800<br />Function: Penalty<br />Lambda: 1.5","b:  1.13<br />value: 1.6950<br />Function: Penalty<br />Lambda: 1.5","b:  1.14<br />value: 1.7100<br />Function: Penalty<br />Lambda: 1.5","b:  1.15<br />value: 1.7250<br />Function: Penalty<br />Lambda: 1.5","b:  1.16<br />value: 1.7400<br />Function: Penalty<br />Lambda: 1.5","b:  1.17<br />value: 1.7550<br />Function: Penalty<br />Lambda: 1.5","b:  1.18<br />value: 1.7700<br />Function: Penalty<br />Lambda: 1.5","b:  1.19<br />value: 1.7850<br />Function: Penalty<br />Lambda: 1.5","b:  1.20<br />value: 1.8000<br />Function: Penalty<br />Lambda: 1.5","b:  1.21<br />value: 1.8150<br />Function: Penalty<br />Lambda: 1.5","b:  1.22<br />value: 1.8300<br />Function: Penalty<br />Lambda: 1.5","b:  1.23<br />value: 1.8450<br />Function: Penalty<br />Lambda: 1.5","b:  1.24<br />value: 1.8600<br />Function: Penalty<br />Lambda: 1.5","b:  1.25<br />value: 1.8750<br />Function: Penalty<br />Lambda: 1.5","b:  1.26<br />value: 1.8900<br />Function: Penalty<br />Lambda: 1.5","b:  1.27<br />value: 1.9050<br />Function: Penalty<br />Lambda: 1.5","b:  1.28<br />value: 1.9200<br />Function: Penalty<br />Lambda: 1.5","b:  1.29<br />value: 1.9350<br />Function: Penalty<br />Lambda: 1.5","b:  1.30<br />value: 1.9500<br />Function: Penalty<br />Lambda: 1.5","b:  1.31<br />value: 1.9650<br />Function: Penalty<br />Lambda: 1.5","b:  1.32<br />value: 1.9800<br />Function: Penalty<br />Lambda: 1.5","b:  1.33<br />value: 1.9950<br />Function: Penalty<br />Lambda: 1.5","b:  1.34<br />value: 2.0100<br />Function: Penalty<br />Lambda: 1.5","b:  1.35<br />value: 2.0250<br />Function: Penalty<br />Lambda: 1.5","b:  1.36<br />value: 2.0400<br />Function: Penalty<br />Lambda: 1.5","b:  1.37<br />value: 2.0550<br />Function: Penalty<br />Lambda: 1.5","b:  1.38<br />value: 2.0700<br />Function: Penalty<br />Lambda: 1.5","b:  1.39<br />value: 2.0850<br />Function: Penalty<br />Lambda: 1.5","b:  1.40<br />value: 2.1000<br />Function: Penalty<br />Lambda: 1.5","b:  1.41<br />value: 2.1150<br />Function: Penalty<br />Lambda: 1.5","b:  1.42<br />value: 2.1300<br />Function: Penalty<br />Lambda: 1.5","b:  1.43<br />value: 2.1450<br />Function: Penalty<br />Lambda: 1.5","b:  1.44<br />value: 2.1600<br />Function: Penalty<br />Lambda: 1.5","b:  1.45<br />value: 2.1750<br />Function: Penalty<br />Lambda: 1.5","b:  1.46<br />value: 2.1900<br />Function: Penalty<br />Lambda: 1.5","b:  1.47<br />value: 2.2050<br />Function: Penalty<br />Lambda: 1.5","b:  1.48<br />value: 2.2200<br />Function: Penalty<br />Lambda: 1.5","b:  1.49<br />value: 2.2350<br />Function: Penalty<br />Lambda: 1.5","b:  1.50<br />value: 2.2500<br />Function: Penalty<br />Lambda: 1.5","b:  1.51<br />value: 2.2650<br />Function: Penalty<br />Lambda: 1.5","b:  1.52<br />value: 2.2800<br />Function: Penalty<br />Lambda: 1.5","b:  1.53<br />value: 2.2950<br />Function: Penalty<br />Lambda: 1.5","b:  1.54<br />value: 2.3100<br />Function: Penalty<br />Lambda: 1.5","b:  1.55<br />value: 2.3250<br />Function: Penalty<br />Lambda: 1.5","b:  1.56<br />value: 2.3400<br />Function: Penalty<br />Lambda: 1.5","b:  1.57<br />value: 2.3550<br />Function: Penalty<br />Lambda: 1.5","b:  1.58<br />value: 2.3700<br />Function: Penalty<br />Lambda: 1.5","b:  1.59<br />value: 2.3850<br />Function: Penalty<br />Lambda: 1.5","b:  1.60<br />value: 2.4000<br />Function: Penalty<br />Lambda: 1.5","b:  1.61<br />value: 2.4150<br />Function: Penalty<br />Lambda: 1.5","b:  1.62<br />value: 2.4300<br />Function: Penalty<br />Lambda: 1.5","b:  1.63<br />value: 2.4450<br />Function: Penalty<br />Lambda: 1.5","b:  1.64<br />value: 2.4600<br />Function: Penalty<br />Lambda: 1.5","b:  1.65<br />value: 2.4750<br />Function: Penalty<br />Lambda: 1.5","b:  1.66<br />value: 2.4900<br />Function: Penalty<br />Lambda: 1.5","b:  1.67<br />value: 2.5050<br />Function: Penalty<br />Lambda: 1.5","b:  1.68<br />value: 2.5200<br />Function: Penalty<br />Lambda: 1.5","b:  1.69<br />value: 2.5350<br />Function: Penalty<br />Lambda: 1.5","b:  1.70<br />value: 2.5500<br />Function: Penalty<br />Lambda: 1.5","b:  1.71<br />value: 2.5650<br />Function: Penalty<br />Lambda: 1.5","b:  1.72<br />value: 2.5800<br />Function: Penalty<br />Lambda: 1.5","b:  1.73<br />value: 2.5950<br />Function: Penalty<br />Lambda: 1.5","b:  1.74<br />value: 2.6100<br />Function: Penalty<br />Lambda: 1.5","b:  1.75<br />value: 2.6250<br />Function: Penalty<br />Lambda: 1.5","b:  1.76<br />value: 2.6400<br />Function: Penalty<br />Lambda: 1.5","b:  1.77<br />value: 2.6550<br />Function: Penalty<br />Lambda: 1.5","b:  1.78<br />value: 2.6700<br />Function: Penalty<br />Lambda: 1.5","b:  1.79<br />value: 2.6850<br />Function: Penalty<br />Lambda: 1.5","b:  1.80<br />value: 2.7000<br />Function: Penalty<br />Lambda: 1.5","b:  1.81<br />value: 2.7150<br />Function: Penalty<br />Lambda: 1.5","b:  1.82<br />value: 2.7300<br />Function: Penalty<br />Lambda: 1.5","b:  1.83<br />value: 2.7450<br />Function: Penalty<br />Lambda: 1.5","b:  1.84<br />value: 2.7600<br />Function: Penalty<br />Lambda: 1.5","b:  1.85<br />value: 2.7750<br />Function: Penalty<br />Lambda: 1.5","b:  1.86<br />value: 2.7900<br />Function: Penalty<br />Lambda: 1.5","b:  1.87<br />value: 2.8050<br />Function: Penalty<br />Lambda: 1.5","b:  1.88<br />value: 2.8200<br />Function: Penalty<br />Lambda: 1.5","b:  1.89<br />value: 2.8350<br />Function: Penalty<br />Lambda: 1.5","b:  1.90<br />value: 2.8500<br />Function: Penalty<br />Lambda: 1.5","b:  1.91<br />value: 2.8650<br />Function: Penalty<br />Lambda: 1.5","b:  1.92<br />value: 2.8800<br />Function: Penalty<br />Lambda: 1.5","b:  1.93<br />value: 2.8950<br />Function: Penalty<br />Lambda: 1.5","b:  1.94<br />value: 2.9100<br />Function: Penalty<br />Lambda: 1.5","b:  1.95<br />value: 2.9250<br />Function: Penalty<br />Lambda: 1.5","b:  1.96<br />value: 2.9400<br />Function: Penalty<br />Lambda: 1.5","b:  1.97<br />value: 2.9550<br />Function: Penalty<br />Lambda: 1.5","b:  1.98<br />value: 2.9700<br />Function: Penalty<br />Lambda: 1.5","b:  1.99<br />value: 2.9850<br />Function: Penalty<br />Lambda: 1.5","b:  2.00<br />value: 3.0000<br />Function: Penalty<br />Lambda: 1.5"],"frame":"1.5","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"1.6","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 1.6","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 1.6","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 1.6","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 1.6","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 1.6","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 1.6","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 1.6","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 1.6","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 1.6","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 1.6","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 1.6","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 1.6","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 1.6","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 1.6","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 1.6","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 1.6","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 1.6","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 1.6","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 1.6","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 1.6","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 1.6","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 1.6","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 1.6","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 1.6","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 1.6","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 1.6","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 1.6","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 1.6","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 1.6","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 1.6","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 1.6","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 1.6","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 1.6","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 1.6","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 1.6","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 1.6","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 1.6","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 1.6","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 1.6","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 1.6","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 1.6","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 1.6","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 1.6","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 1.6","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 1.6","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 1.6","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 1.6","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 1.6","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 1.6","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 1.6","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.6","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.6","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.6","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.6","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.6","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.6","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.6","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.6","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.6","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.6","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.6","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.6","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.6","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.6","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.6","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.6","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.6","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.6","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.6","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.6","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.6","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.6","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.6","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.6","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.6","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.6","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.6","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.6","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.6","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.6","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.6","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.6","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.6","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.6","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.6","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.6","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.6","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.6","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.6","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.6","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.6","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.6","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.6","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.6","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.6","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.6","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.6","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.6","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.6","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.6","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.6","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.6","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.6","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.6","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.6","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.6","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.6","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.6","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.6","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.6","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.6","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.6","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.6","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.6","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.6","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.6","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.6","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.6","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.6","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.6","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.6","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.6","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.6","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.6","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.6","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.6","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.6","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.6","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.6","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.6","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.6","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.6","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.6","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.6","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.6","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.6","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.6","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.6","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.6","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.6","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.6","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.6","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.6","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.6","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.6","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.6","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.6","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.6","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.6","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.6","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 1.6","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.6","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.6","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.6","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.6","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.6","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.6","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.6","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.6","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.6","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.6","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.6","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.6","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.6","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.6","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.6","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.6","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.6","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.6","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.6","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.6","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.6","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.6","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.6","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.6","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.6","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.6","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.6","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.6","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.6","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.6","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.6","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.6","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.6","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.6","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.6","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.6","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.6","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.6","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.6","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.6","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.6","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.6","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.6","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.6","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.6","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.6","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.6","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.6","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.6","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.6","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.6","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.6","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.6","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.6","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.6","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.6","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.6","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.6","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.6","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.6","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.6","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.6","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.6","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.6","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.6","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.6","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.6","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.6","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.6","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.6","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.6","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.6","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.6","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.6","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.6","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.6","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.6","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.6","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.6","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.6","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.6","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.6","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.6","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.6","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.6","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.6","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.6","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.6","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.6","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.6","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.6","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.6","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.6","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.6","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.6","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.6","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.6","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.6","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.6","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.6"],"frame":"1.6","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.55,3.5041,3.4584,3.4129,3.3676,3.3225,3.2776,3.2329,3.1884,3.1441,3.1,3.0561,3.0124,2.9689,2.9256,2.8825,2.8396,2.7969,2.7544,2.7121,2.67,2.6281,2.5864,2.5449,2.5036,2.4625,2.4216,2.3809,2.3404,2.3001,2.26,2.2201,2.1804,2.1409,2.1016,2.0625,2.0236,1.9849,1.9464,1.9081,1.87,1.8321,1.7944,1.7569,1.7196,1.6825,1.6456,1.6089,1.5724,1.5361,1.5,1.4961,1.4924,1.4889,1.4856,1.4825,1.4796,1.4769,1.4744,1.4721,1.47,1.4681,1.4664,1.4649,1.4636,1.4625,1.4616,1.4609,1.4604,1.4601,1.46,1.4601,1.4604,1.4609,1.4616,1.4625,1.4636,1.4649,1.4664,1.4681,1.47,1.4721,1.4744,1.4769,1.4796,1.4825,1.4856,1.4889,1.4924,1.4961,1.5,1.5041,1.5084,1.5129,1.5176,1.5225,1.5276,1.5329,1.5384,1.5441,1.55,1.5561,1.5624,1.5689,1.5756,1.5825,1.5896,1.5969,1.6044,1.6121,1.62,1.6281,1.6364,1.6449,1.6536,1.6625,1.6716,1.6809,1.6904,1.7001,1.71,1.7201,1.7304,1.7409,1.7516,1.7625,1.7736,1.7849,1.7964,1.8081,1.82,1.8321,1.8444,1.8569,1.8696,1.8825,1.8956,1.9089,1.9224,1.9361,1.95,1.9641,1.9784,1.9929,2.0076,2.0225,2.0376,2.0529,2.0684,2.0841,2.1,2.1161,2.1324,2.1489,2.1656,2.1825,2.1996,2.2169,2.2344,2.2521,2.27,2.2881,2.3064,2.3249,2.3436,2.3625,2.3816,2.4009,2.4204,2.4401,2.46,2.4801,2.5004,2.5209,2.5416,2.5625,2.5836,2.6049,2.6264,2.6481,2.67,2.6921,2.7144,2.7369,2.7596,2.7825,2.8056,2.8289,2.8524,2.8761,2.9,2.9241,2.9484,2.9729,2.9976,3.0225,3.0476,3.0729,3.0984,3.1241,3.15,3.1761,3.2024,3.2289,3.2556,3.2825,3.3096,3.3369,3.3644,3.3921,3.42,3.4481,3.4764,3.5049,3.5336,3.5625,3.5916,3.6209,3.6504,3.6801,3.71,3.7401,3.7704,3.8009,3.8316,3.8625,3.8936,3.9249,3.9564,3.9881,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 3.5500<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.49<br />value: 3.5041<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.48<br />value: 3.4584<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.47<br />value: 3.4129<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.46<br />value: 3.3676<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.45<br />value: 3.3225<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.44<br />value: 3.2776<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.43<br />value: 3.2329<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.42<br />value: 3.1884<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.41<br />value: 3.1441<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.40<br />value: 3.1000<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.39<br />value: 3.0561<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.38<br />value: 3.0124<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.37<br />value: 2.9689<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.36<br />value: 2.9256<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.35<br />value: 2.8825<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.34<br />value: 2.8396<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.33<br />value: 2.7969<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.32<br />value: 2.7544<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.31<br />value: 2.7121<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.30<br />value: 2.6700<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.29<br />value: 2.6281<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.28<br />value: 2.5864<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.27<br />value: 2.5449<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.26<br />value: 2.5036<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.25<br />value: 2.4625<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.24<br />value: 2.4216<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.23<br />value: 2.3809<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.22<br />value: 2.3404<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.21<br />value: 2.3001<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.20<br />value: 2.2600<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.19<br />value: 2.2201<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.18<br />value: 2.1804<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.17<br />value: 2.1409<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.16<br />value: 2.1016<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.15<br />value: 2.0625<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.14<br />value: 2.0236<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.13<br />value: 1.9849<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.12<br />value: 1.9464<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.11<br />value: 1.9081<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.10<br />value: 1.8700<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.09<br />value: 1.8321<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.08<br />value: 1.7944<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.07<br />value: 1.7569<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.06<br />value: 1.7196<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.05<br />value: 1.6825<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.04<br />value: 1.6456<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.03<br />value: 1.6089<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.02<br />value: 1.5724<br />Function: Loss + Penalty<br />Lambda: 1.6","b: -0.01<br />value: 1.5361<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.01<br />value: 1.4961<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.02<br />value: 1.4924<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.03<br />value: 1.4889<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.04<br />value: 1.4856<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.05<br />value: 1.4825<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.06<br />value: 1.4796<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.07<br />value: 1.4769<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.08<br />value: 1.4744<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.09<br />value: 1.4721<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.10<br />value: 1.4700<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.11<br />value: 1.4681<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.12<br />value: 1.4664<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.13<br />value: 1.4649<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.14<br />value: 1.4636<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.15<br />value: 1.4625<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.16<br />value: 1.4616<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.17<br />value: 1.4609<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.18<br />value: 1.4604<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.19<br />value: 1.4601<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.20<br />value: 1.4600<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.21<br />value: 1.4601<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.22<br />value: 1.4604<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.23<br />value: 1.4609<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.24<br />value: 1.4616<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.25<br />value: 1.4625<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.26<br />value: 1.4636<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.27<br />value: 1.4649<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.28<br />value: 1.4664<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.29<br />value: 1.4681<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.30<br />value: 1.4700<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.31<br />value: 1.4721<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.32<br />value: 1.4744<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.33<br />value: 1.4769<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.34<br />value: 1.4796<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.35<br />value: 1.4825<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.36<br />value: 1.4856<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.37<br />value: 1.4889<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.38<br />value: 1.4924<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.39<br />value: 1.4961<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.40<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.41<br />value: 1.5041<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.42<br />value: 1.5084<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.43<br />value: 1.5129<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.44<br />value: 1.5176<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.45<br />value: 1.5225<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.46<br />value: 1.5276<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.47<br />value: 1.5329<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.48<br />value: 1.5384<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.49<br />value: 1.5441<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.50<br />value: 1.5500<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.51<br />value: 1.5561<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.52<br />value: 1.5624<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.53<br />value: 1.5689<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.54<br />value: 1.5756<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.55<br />value: 1.5825<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.56<br />value: 1.5896<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.57<br />value: 1.5969<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.58<br />value: 1.6044<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.59<br />value: 1.6121<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.60<br />value: 1.6200<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.61<br />value: 1.6281<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.62<br />value: 1.6364<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.63<br />value: 1.6449<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.64<br />value: 1.6536<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.65<br />value: 1.6625<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.66<br />value: 1.6716<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.67<br />value: 1.6809<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.68<br />value: 1.6904<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.69<br />value: 1.7001<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.70<br />value: 1.7100<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.71<br />value: 1.7201<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.72<br />value: 1.7304<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.73<br />value: 1.7409<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.74<br />value: 1.7516<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.75<br />value: 1.7625<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.76<br />value: 1.7736<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.77<br />value: 1.7849<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.78<br />value: 1.7964<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.79<br />value: 1.8081<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.80<br />value: 1.8200<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.81<br />value: 1.8321<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.82<br />value: 1.8444<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.83<br />value: 1.8569<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.84<br />value: 1.8696<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.85<br />value: 1.8825<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.86<br />value: 1.8956<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.87<br />value: 1.9089<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.88<br />value: 1.9224<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.89<br />value: 1.9361<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.90<br />value: 1.9500<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.91<br />value: 1.9641<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.92<br />value: 1.9784<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.93<br />value: 1.9929<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.94<br />value: 2.0076<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.95<br />value: 2.0225<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.96<br />value: 2.0376<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.97<br />value: 2.0529<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.98<br />value: 2.0684<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  0.99<br />value: 2.0841<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.00<br />value: 2.1000<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.01<br />value: 2.1161<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.02<br />value: 2.1324<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.03<br />value: 2.1489<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.04<br />value: 2.1656<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.05<br />value: 2.1825<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.06<br />value: 2.1996<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.07<br />value: 2.2169<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.08<br />value: 2.2344<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.09<br />value: 2.2521<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.10<br />value: 2.2700<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.11<br />value: 2.2881<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.12<br />value: 2.3064<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.13<br />value: 2.3249<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.14<br />value: 2.3436<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.15<br />value: 2.3625<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.16<br />value: 2.3816<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.17<br />value: 2.4009<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.18<br />value: 2.4204<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.19<br />value: 2.4401<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.20<br />value: 2.4600<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.21<br />value: 2.4801<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.22<br />value: 2.5004<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.23<br />value: 2.5209<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.24<br />value: 2.5416<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.25<br />value: 2.5625<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.26<br />value: 2.5836<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.27<br />value: 2.6049<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.28<br />value: 2.6264<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.29<br />value: 2.6481<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.30<br />value: 2.6700<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.31<br />value: 2.6921<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.32<br />value: 2.7144<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.33<br />value: 2.7369<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.34<br />value: 2.7596<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.35<br />value: 2.7825<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.36<br />value: 2.8056<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.37<br />value: 2.8289<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.38<br />value: 2.8524<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.39<br />value: 2.8761<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.40<br />value: 2.9000<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.41<br />value: 2.9241<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.42<br />value: 2.9484<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.43<br />value: 2.9729<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.44<br />value: 2.9976<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.45<br />value: 3.0225<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.46<br />value: 3.0476<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.47<br />value: 3.0729<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.48<br />value: 3.0984<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.49<br />value: 3.1241<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.50<br />value: 3.1500<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.51<br />value: 3.1761<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.52<br />value: 3.2024<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.53<br />value: 3.2289<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.54<br />value: 3.2556<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.55<br />value: 3.2825<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.56<br />value: 3.3096<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.57<br />value: 3.3369<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.58<br />value: 3.3644<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.59<br />value: 3.3921<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.60<br />value: 3.4200<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.61<br />value: 3.4481<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.62<br />value: 3.4764<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.63<br />value: 3.5049<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.64<br />value: 3.5336<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.65<br />value: 3.5625<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.66<br />value: 3.5916<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.67<br />value: 3.6209<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.68<br />value: 3.6504<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.69<br />value: 3.6801<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.70<br />value: 3.7100<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.71<br />value: 3.7401<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.72<br />value: 3.7704<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.73<br />value: 3.8009<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.74<br />value: 3.8316<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.75<br />value: 3.8625<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.76<br />value: 3.8936<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.77<br />value: 3.9249<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.78<br />value: 3.9564<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.79<br />value: 3.9881<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.80<br />value: 4.0200<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.81<br />value: 4.0521<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.82<br />value: 4.0844<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.83<br />value: 4.1169<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.84<br />value: 4.1496<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.85<br />value: 4.1825<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.86<br />value: 4.2156<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.87<br />value: 4.2489<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.88<br />value: 4.2824<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.89<br />value: 4.3161<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.90<br />value: 4.3500<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.91<br />value: 4.3841<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.92<br />value: 4.4184<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.93<br />value: 4.4529<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.94<br />value: 4.4876<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.95<br />value: 4.5225<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.96<br />value: 4.5576<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.97<br />value: 4.5929<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.98<br />value: 4.6284<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  1.99<br />value: 4.6641<br />Function: Loss + Penalty<br />Lambda: 1.6","b:  2.00<br />value: 4.7000<br />Function: Loss + Penalty<br />Lambda: 1.6"],"frame":"1.6","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.8,0.784,0.768,0.752,0.736,0.72,0.704,0.688,0.672,0.656,0.64,0.624,0.608,0.592,0.576,0.56,0.544,0.528,0.512,0.496,0.48,0.464,0.448,0.432,0.416,0.4,0.384,0.368,0.352,0.336,0.32,0.304,0.288,0.272,0.256,0.24,0.224,0.208,0.192,0.176,0.16,0.144,0.128,0.112,0.096,0.08,0.064,0.048,0.032,0.016,0,0.016,0.032,0.048,0.0640000000000001,0.0800000000000001,0.0960000000000001,0.112,0.128,0.144,0.16,0.176,0.192,0.208,0.224,0.24,0.256,0.272,0.288,0.304,0.32,0.336,0.352,0.368,0.384,0.4,0.416,0.432,0.448,0.464,0.48,0.496,0.512,0.528,0.544,0.56,0.576,0.592,0.608,0.624,0.64,0.656,0.672,0.688,0.704,0.72,0.736,0.752,0.768,0.784,0.8,0.816,0.832,0.848,0.864,0.88,0.896,0.912,0.928,0.944,0.96,0.976,0.992,1.008,1.024,1.04,1.056,1.072,1.088,1.104,1.12,1.136,1.152,1.168,1.184,1.2,1.216,1.232,1.248,1.264,1.28,1.296,1.312,1.328,1.344,1.36,1.376,1.392,1.408,1.424,1.44,1.456,1.472,1.488,1.504,1.52,1.536,1.552,1.568,1.584,1.6,1.616,1.632,1.648,1.664,1.68,1.696,1.712,1.728,1.744,1.76,1.776,1.792,1.808,1.824,1.84,1.856,1.872,1.888,1.904,1.92,1.936,1.952,1.968,1.984,2,2.016,2.032,2.048,2.064,2.08,2.096,2.112,2.128,2.144,2.16,2.176,2.192,2.208,2.224,2.24,2.256,2.272,2.288,2.304,2.32,2.336,2.352,2.368,2.384,2.4,2.416,2.432,2.448,2.464,2.48,2.496,2.512,2.528,2.544,2.56,2.576,2.592,2.608,2.624,2.64,2.656,2.672,2.688,2.704,2.72,2.736,2.752,2.768,2.784,2.8,2.816,2.832,2.848,2.864,2.88,2.896,2.912,2.928,2.944,2.96,2.976,2.992,3.008,3.024,3.04,3.056,3.072,3.088,3.104,3.12,3.136,3.152,3.168,3.184,3.2],"text":["b: -0.50<br />value: 0.8000<br />Function: Penalty<br />Lambda: 1.6","b: -0.49<br />value: 0.7840<br />Function: Penalty<br />Lambda: 1.6","b: -0.48<br />value: 0.7680<br />Function: Penalty<br />Lambda: 1.6","b: -0.47<br />value: 0.7520<br />Function: Penalty<br />Lambda: 1.6","b: -0.46<br />value: 0.7360<br />Function: Penalty<br />Lambda: 1.6","b: -0.45<br />value: 0.7200<br />Function: Penalty<br />Lambda: 1.6","b: -0.44<br />value: 0.7040<br />Function: Penalty<br />Lambda: 1.6","b: -0.43<br />value: 0.6880<br />Function: Penalty<br />Lambda: 1.6","b: -0.42<br />value: 0.6720<br />Function: Penalty<br />Lambda: 1.6","b: -0.41<br />value: 0.6560<br />Function: Penalty<br />Lambda: 1.6","b: -0.40<br />value: 0.6400<br />Function: Penalty<br />Lambda: 1.6","b: -0.39<br />value: 0.6240<br />Function: Penalty<br />Lambda: 1.6","b: -0.38<br />value: 0.6080<br />Function: Penalty<br />Lambda: 1.6","b: -0.37<br />value: 0.5920<br />Function: Penalty<br />Lambda: 1.6","b: -0.36<br />value: 0.5760<br />Function: Penalty<br />Lambda: 1.6","b: -0.35<br />value: 0.5600<br />Function: Penalty<br />Lambda: 1.6","b: -0.34<br />value: 0.5440<br />Function: Penalty<br />Lambda: 1.6","b: -0.33<br />value: 0.5280<br />Function: Penalty<br />Lambda: 1.6","b: -0.32<br />value: 0.5120<br />Function: Penalty<br />Lambda: 1.6","b: -0.31<br />value: 0.4960<br />Function: Penalty<br />Lambda: 1.6","b: -0.30<br />value: 0.4800<br />Function: Penalty<br />Lambda: 1.6","b: -0.29<br />value: 0.4640<br />Function: Penalty<br />Lambda: 1.6","b: -0.28<br />value: 0.4480<br />Function: Penalty<br />Lambda: 1.6","b: -0.27<br />value: 0.4320<br />Function: Penalty<br />Lambda: 1.6","b: -0.26<br />value: 0.4160<br />Function: Penalty<br />Lambda: 1.6","b: -0.25<br />value: 0.4000<br />Function: Penalty<br />Lambda: 1.6","b: -0.24<br />value: 0.3840<br />Function: Penalty<br />Lambda: 1.6","b: -0.23<br />value: 0.3680<br />Function: Penalty<br />Lambda: 1.6","b: -0.22<br />value: 0.3520<br />Function: Penalty<br />Lambda: 1.6","b: -0.21<br />value: 0.3360<br />Function: Penalty<br />Lambda: 1.6","b: -0.20<br />value: 0.3200<br />Function: Penalty<br />Lambda: 1.6","b: -0.19<br />value: 0.3040<br />Function: Penalty<br />Lambda: 1.6","b: -0.18<br />value: 0.2880<br />Function: Penalty<br />Lambda: 1.6","b: -0.17<br />value: 0.2720<br />Function: Penalty<br />Lambda: 1.6","b: -0.16<br />value: 0.2560<br />Function: Penalty<br />Lambda: 1.6","b: -0.15<br />value: 0.2400<br />Function: Penalty<br />Lambda: 1.6","b: -0.14<br />value: 0.2240<br />Function: Penalty<br />Lambda: 1.6","b: -0.13<br />value: 0.2080<br />Function: Penalty<br />Lambda: 1.6","b: -0.12<br />value: 0.1920<br />Function: Penalty<br />Lambda: 1.6","b: -0.11<br />value: 0.1760<br />Function: Penalty<br />Lambda: 1.6","b: -0.10<br />value: 0.1600<br />Function: Penalty<br />Lambda: 1.6","b: -0.09<br />value: 0.1440<br />Function: Penalty<br />Lambda: 1.6","b: -0.08<br />value: 0.1280<br />Function: Penalty<br />Lambda: 1.6","b: -0.07<br />value: 0.1120<br />Function: Penalty<br />Lambda: 1.6","b: -0.06<br />value: 0.0960<br />Function: Penalty<br />Lambda: 1.6","b: -0.05<br />value: 0.0800<br />Function: Penalty<br />Lambda: 1.6","b: -0.04<br />value: 0.0640<br />Function: Penalty<br />Lambda: 1.6","b: -0.03<br />value: 0.0480<br />Function: Penalty<br />Lambda: 1.6","b: -0.02<br />value: 0.0320<br />Function: Penalty<br />Lambda: 1.6","b: -0.01<br />value: 0.0160<br />Function: Penalty<br />Lambda: 1.6","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 1.6","b:  0.01<br />value: 0.0160<br />Function: Penalty<br />Lambda: 1.6","b:  0.02<br />value: 0.0320<br />Function: Penalty<br />Lambda: 1.6","b:  0.03<br />value: 0.0480<br />Function: Penalty<br />Lambda: 1.6","b:  0.04<br />value: 0.0640<br />Function: Penalty<br />Lambda: 1.6","b:  0.05<br />value: 0.0800<br />Function: Penalty<br />Lambda: 1.6","b:  0.06<br />value: 0.0960<br />Function: Penalty<br />Lambda: 1.6","b:  0.07<br />value: 0.1120<br />Function: Penalty<br />Lambda: 1.6","b:  0.08<br />value: 0.1280<br />Function: Penalty<br />Lambda: 1.6","b:  0.09<br />value: 0.1440<br />Function: Penalty<br />Lambda: 1.6","b:  0.10<br />value: 0.1600<br />Function: Penalty<br />Lambda: 1.6","b:  0.11<br />value: 0.1760<br />Function: Penalty<br />Lambda: 1.6","b:  0.12<br />value: 0.1920<br />Function: Penalty<br />Lambda: 1.6","b:  0.13<br />value: 0.2080<br />Function: Penalty<br />Lambda: 1.6","b:  0.14<br />value: 0.2240<br />Function: Penalty<br />Lambda: 1.6","b:  0.15<br />value: 0.2400<br />Function: Penalty<br />Lambda: 1.6","b:  0.16<br />value: 0.2560<br />Function: Penalty<br />Lambda: 1.6","b:  0.17<br />value: 0.2720<br />Function: Penalty<br />Lambda: 1.6","b:  0.18<br />value: 0.2880<br />Function: Penalty<br />Lambda: 1.6","b:  0.19<br />value: 0.3040<br />Function: Penalty<br />Lambda: 1.6","b:  0.20<br />value: 0.3200<br />Function: Penalty<br />Lambda: 1.6","b:  0.21<br />value: 0.3360<br />Function: Penalty<br />Lambda: 1.6","b:  0.22<br />value: 0.3520<br />Function: Penalty<br />Lambda: 1.6","b:  0.23<br />value: 0.3680<br />Function: Penalty<br />Lambda: 1.6","b:  0.24<br />value: 0.3840<br />Function: Penalty<br />Lambda: 1.6","b:  0.25<br />value: 0.4000<br />Function: Penalty<br />Lambda: 1.6","b:  0.26<br />value: 0.4160<br />Function: Penalty<br />Lambda: 1.6","b:  0.27<br />value: 0.4320<br />Function: Penalty<br />Lambda: 1.6","b:  0.28<br />value: 0.4480<br />Function: Penalty<br />Lambda: 1.6","b:  0.29<br />value: 0.4640<br />Function: Penalty<br />Lambda: 1.6","b:  0.30<br />value: 0.4800<br />Function: Penalty<br />Lambda: 1.6","b:  0.31<br />value: 0.4960<br />Function: Penalty<br />Lambda: 1.6","b:  0.32<br />value: 0.5120<br />Function: Penalty<br />Lambda: 1.6","b:  0.33<br />value: 0.5280<br />Function: Penalty<br />Lambda: 1.6","b:  0.34<br />value: 0.5440<br />Function: Penalty<br />Lambda: 1.6","b:  0.35<br />value: 0.5600<br />Function: Penalty<br />Lambda: 1.6","b:  0.36<br />value: 0.5760<br />Function: Penalty<br />Lambda: 1.6","b:  0.37<br />value: 0.5920<br />Function: Penalty<br />Lambda: 1.6","b:  0.38<br />value: 0.6080<br />Function: Penalty<br />Lambda: 1.6","b:  0.39<br />value: 0.6240<br />Function: Penalty<br />Lambda: 1.6","b:  0.40<br />value: 0.6400<br />Function: Penalty<br />Lambda: 1.6","b:  0.41<br />value: 0.6560<br />Function: Penalty<br />Lambda: 1.6","b:  0.42<br />value: 0.6720<br />Function: Penalty<br />Lambda: 1.6","b:  0.43<br />value: 0.6880<br />Function: Penalty<br />Lambda: 1.6","b:  0.44<br />value: 0.7040<br />Function: Penalty<br />Lambda: 1.6","b:  0.45<br />value: 0.7200<br />Function: Penalty<br />Lambda: 1.6","b:  0.46<br />value: 0.7360<br />Function: Penalty<br />Lambda: 1.6","b:  0.47<br />value: 0.7520<br />Function: Penalty<br />Lambda: 1.6","b:  0.48<br />value: 0.7680<br />Function: Penalty<br />Lambda: 1.6","b:  0.49<br />value: 0.7840<br />Function: Penalty<br />Lambda: 1.6","b:  0.50<br />value: 0.8000<br />Function: Penalty<br />Lambda: 1.6","b:  0.51<br />value: 0.8160<br />Function: Penalty<br />Lambda: 1.6","b:  0.52<br />value: 0.8320<br />Function: Penalty<br />Lambda: 1.6","b:  0.53<br />value: 0.8480<br />Function: Penalty<br />Lambda: 1.6","b:  0.54<br />value: 0.8640<br />Function: Penalty<br />Lambda: 1.6","b:  0.55<br />value: 0.8800<br />Function: Penalty<br />Lambda: 1.6","b:  0.56<br />value: 0.8960<br />Function: Penalty<br />Lambda: 1.6","b:  0.57<br />value: 0.9120<br />Function: Penalty<br />Lambda: 1.6","b:  0.58<br />value: 0.9280<br />Function: Penalty<br />Lambda: 1.6","b:  0.59<br />value: 0.9440<br />Function: Penalty<br />Lambda: 1.6","b:  0.60<br />value: 0.9600<br />Function: Penalty<br />Lambda: 1.6","b:  0.61<br />value: 0.9760<br />Function: Penalty<br />Lambda: 1.6","b:  0.62<br />value: 0.9920<br />Function: Penalty<br />Lambda: 1.6","b:  0.63<br />value: 1.0080<br />Function: Penalty<br />Lambda: 1.6","b:  0.64<br />value: 1.0240<br />Function: Penalty<br />Lambda: 1.6","b:  0.65<br />value: 1.0400<br />Function: Penalty<br />Lambda: 1.6","b:  0.66<br />value: 1.0560<br />Function: Penalty<br />Lambda: 1.6","b:  0.67<br />value: 1.0720<br />Function: Penalty<br />Lambda: 1.6","b:  0.68<br />value: 1.0880<br />Function: Penalty<br />Lambda: 1.6","b:  0.69<br />value: 1.1040<br />Function: Penalty<br />Lambda: 1.6","b:  0.70<br />value: 1.1200<br />Function: Penalty<br />Lambda: 1.6","b:  0.71<br />value: 1.1360<br />Function: Penalty<br />Lambda: 1.6","b:  0.72<br />value: 1.1520<br />Function: Penalty<br />Lambda: 1.6","b:  0.73<br />value: 1.1680<br />Function: Penalty<br />Lambda: 1.6","b:  0.74<br />value: 1.1840<br />Function: Penalty<br />Lambda: 1.6","b:  0.75<br />value: 1.2000<br />Function: Penalty<br />Lambda: 1.6","b:  0.76<br />value: 1.2160<br />Function: Penalty<br />Lambda: 1.6","b:  0.77<br />value: 1.2320<br />Function: Penalty<br />Lambda: 1.6","b:  0.78<br />value: 1.2480<br />Function: Penalty<br />Lambda: 1.6","b:  0.79<br />value: 1.2640<br />Function: Penalty<br />Lambda: 1.6","b:  0.80<br />value: 1.2800<br />Function: Penalty<br />Lambda: 1.6","b:  0.81<br />value: 1.2960<br />Function: Penalty<br />Lambda: 1.6","b:  0.82<br />value: 1.3120<br />Function: Penalty<br />Lambda: 1.6","b:  0.83<br />value: 1.3280<br />Function: Penalty<br />Lambda: 1.6","b:  0.84<br />value: 1.3440<br />Function: Penalty<br />Lambda: 1.6","b:  0.85<br />value: 1.3600<br />Function: Penalty<br />Lambda: 1.6","b:  0.86<br />value: 1.3760<br />Function: Penalty<br />Lambda: 1.6","b:  0.87<br />value: 1.3920<br />Function: Penalty<br />Lambda: 1.6","b:  0.88<br />value: 1.4080<br />Function: Penalty<br />Lambda: 1.6","b:  0.89<br />value: 1.4240<br />Function: Penalty<br />Lambda: 1.6","b:  0.90<br />value: 1.4400<br />Function: Penalty<br />Lambda: 1.6","b:  0.91<br />value: 1.4560<br />Function: Penalty<br />Lambda: 1.6","b:  0.92<br />value: 1.4720<br />Function: Penalty<br />Lambda: 1.6","b:  0.93<br />value: 1.4880<br />Function: Penalty<br />Lambda: 1.6","b:  0.94<br />value: 1.5040<br />Function: Penalty<br />Lambda: 1.6","b:  0.95<br />value: 1.5200<br />Function: Penalty<br />Lambda: 1.6","b:  0.96<br />value: 1.5360<br />Function: Penalty<br />Lambda: 1.6","b:  0.97<br />value: 1.5520<br />Function: Penalty<br />Lambda: 1.6","b:  0.98<br />value: 1.5680<br />Function: Penalty<br />Lambda: 1.6","b:  0.99<br />value: 1.5840<br />Function: Penalty<br />Lambda: 1.6","b:  1.00<br />value: 1.6000<br />Function: Penalty<br />Lambda: 1.6","b:  1.01<br />value: 1.6160<br />Function: Penalty<br />Lambda: 1.6","b:  1.02<br />value: 1.6320<br />Function: Penalty<br />Lambda: 1.6","b:  1.03<br />value: 1.6480<br />Function: Penalty<br />Lambda: 1.6","b:  1.04<br />value: 1.6640<br />Function: Penalty<br />Lambda: 1.6","b:  1.05<br />value: 1.6800<br />Function: Penalty<br />Lambda: 1.6","b:  1.06<br />value: 1.6960<br />Function: Penalty<br />Lambda: 1.6","b:  1.07<br />value: 1.7120<br />Function: Penalty<br />Lambda: 1.6","b:  1.08<br />value: 1.7280<br />Function: Penalty<br />Lambda: 1.6","b:  1.09<br />value: 1.7440<br />Function: Penalty<br />Lambda: 1.6","b:  1.10<br />value: 1.7600<br />Function: Penalty<br />Lambda: 1.6","b:  1.11<br />value: 1.7760<br />Function: Penalty<br />Lambda: 1.6","b:  1.12<br />value: 1.7920<br />Function: Penalty<br />Lambda: 1.6","b:  1.13<br />value: 1.8080<br />Function: Penalty<br />Lambda: 1.6","b:  1.14<br />value: 1.8240<br />Function: Penalty<br />Lambda: 1.6","b:  1.15<br />value: 1.8400<br />Function: Penalty<br />Lambda: 1.6","b:  1.16<br />value: 1.8560<br />Function: Penalty<br />Lambda: 1.6","b:  1.17<br />value: 1.8720<br />Function: Penalty<br />Lambda: 1.6","b:  1.18<br />value: 1.8880<br />Function: Penalty<br />Lambda: 1.6","b:  1.19<br />value: 1.9040<br />Function: Penalty<br />Lambda: 1.6","b:  1.20<br />value: 1.9200<br />Function: Penalty<br />Lambda: 1.6","b:  1.21<br />value: 1.9360<br />Function: Penalty<br />Lambda: 1.6","b:  1.22<br />value: 1.9520<br />Function: Penalty<br />Lambda: 1.6","b:  1.23<br />value: 1.9680<br />Function: Penalty<br />Lambda: 1.6","b:  1.24<br />value: 1.9840<br />Function: Penalty<br />Lambda: 1.6","b:  1.25<br />value: 2.0000<br />Function: Penalty<br />Lambda: 1.6","b:  1.26<br />value: 2.0160<br />Function: Penalty<br />Lambda: 1.6","b:  1.27<br />value: 2.0320<br />Function: Penalty<br />Lambda: 1.6","b:  1.28<br />value: 2.0480<br />Function: Penalty<br />Lambda: 1.6","b:  1.29<br />value: 2.0640<br />Function: Penalty<br />Lambda: 1.6","b:  1.30<br />value: 2.0800<br />Function: Penalty<br />Lambda: 1.6","b:  1.31<br />value: 2.0960<br />Function: Penalty<br />Lambda: 1.6","b:  1.32<br />value: 2.1120<br />Function: Penalty<br />Lambda: 1.6","b:  1.33<br />value: 2.1280<br />Function: Penalty<br />Lambda: 1.6","b:  1.34<br />value: 2.1440<br />Function: Penalty<br />Lambda: 1.6","b:  1.35<br />value: 2.1600<br />Function: Penalty<br />Lambda: 1.6","b:  1.36<br />value: 2.1760<br />Function: Penalty<br />Lambda: 1.6","b:  1.37<br />value: 2.1920<br />Function: Penalty<br />Lambda: 1.6","b:  1.38<br />value: 2.2080<br />Function: Penalty<br />Lambda: 1.6","b:  1.39<br />value: 2.2240<br />Function: Penalty<br />Lambda: 1.6","b:  1.40<br />value: 2.2400<br />Function: Penalty<br />Lambda: 1.6","b:  1.41<br />value: 2.2560<br />Function: Penalty<br />Lambda: 1.6","b:  1.42<br />value: 2.2720<br />Function: Penalty<br />Lambda: 1.6","b:  1.43<br />value: 2.2880<br />Function: Penalty<br />Lambda: 1.6","b:  1.44<br />value: 2.3040<br />Function: Penalty<br />Lambda: 1.6","b:  1.45<br />value: 2.3200<br />Function: Penalty<br />Lambda: 1.6","b:  1.46<br />value: 2.3360<br />Function: Penalty<br />Lambda: 1.6","b:  1.47<br />value: 2.3520<br />Function: Penalty<br />Lambda: 1.6","b:  1.48<br />value: 2.3680<br />Function: Penalty<br />Lambda: 1.6","b:  1.49<br />value: 2.3840<br />Function: Penalty<br />Lambda: 1.6","b:  1.50<br />value: 2.4000<br />Function: Penalty<br />Lambda: 1.6","b:  1.51<br />value: 2.4160<br />Function: Penalty<br />Lambda: 1.6","b:  1.52<br />value: 2.4320<br />Function: Penalty<br />Lambda: 1.6","b:  1.53<br />value: 2.4480<br />Function: Penalty<br />Lambda: 1.6","b:  1.54<br />value: 2.4640<br />Function: Penalty<br />Lambda: 1.6","b:  1.55<br />value: 2.4800<br />Function: Penalty<br />Lambda: 1.6","b:  1.56<br />value: 2.4960<br />Function: Penalty<br />Lambda: 1.6","b:  1.57<br />value: 2.5120<br />Function: Penalty<br />Lambda: 1.6","b:  1.58<br />value: 2.5280<br />Function: Penalty<br />Lambda: 1.6","b:  1.59<br />value: 2.5440<br />Function: Penalty<br />Lambda: 1.6","b:  1.60<br />value: 2.5600<br />Function: Penalty<br />Lambda: 1.6","b:  1.61<br />value: 2.5760<br />Function: Penalty<br />Lambda: 1.6","b:  1.62<br />value: 2.5920<br />Function: Penalty<br />Lambda: 1.6","b:  1.63<br />value: 2.6080<br />Function: Penalty<br />Lambda: 1.6","b:  1.64<br />value: 2.6240<br />Function: Penalty<br />Lambda: 1.6","b:  1.65<br />value: 2.6400<br />Function: Penalty<br />Lambda: 1.6","b:  1.66<br />value: 2.6560<br />Function: Penalty<br />Lambda: 1.6","b:  1.67<br />value: 2.6720<br />Function: Penalty<br />Lambda: 1.6","b:  1.68<br />value: 2.6880<br />Function: Penalty<br />Lambda: 1.6","b:  1.69<br />value: 2.7040<br />Function: Penalty<br />Lambda: 1.6","b:  1.70<br />value: 2.7200<br />Function: Penalty<br />Lambda: 1.6","b:  1.71<br />value: 2.7360<br />Function: Penalty<br />Lambda: 1.6","b:  1.72<br />value: 2.7520<br />Function: Penalty<br />Lambda: 1.6","b:  1.73<br />value: 2.7680<br />Function: Penalty<br />Lambda: 1.6","b:  1.74<br />value: 2.7840<br />Function: Penalty<br />Lambda: 1.6","b:  1.75<br />value: 2.8000<br />Function: Penalty<br />Lambda: 1.6","b:  1.76<br />value: 2.8160<br />Function: Penalty<br />Lambda: 1.6","b:  1.77<br />value: 2.8320<br />Function: Penalty<br />Lambda: 1.6","b:  1.78<br />value: 2.8480<br />Function: Penalty<br />Lambda: 1.6","b:  1.79<br />value: 2.8640<br />Function: Penalty<br />Lambda: 1.6","b:  1.80<br />value: 2.8800<br />Function: Penalty<br />Lambda: 1.6","b:  1.81<br />value: 2.8960<br />Function: Penalty<br />Lambda: 1.6","b:  1.82<br />value: 2.9120<br />Function: Penalty<br />Lambda: 1.6","b:  1.83<br />value: 2.9280<br />Function: Penalty<br />Lambda: 1.6","b:  1.84<br />value: 2.9440<br />Function: Penalty<br />Lambda: 1.6","b:  1.85<br />value: 2.9600<br />Function: Penalty<br />Lambda: 1.6","b:  1.86<br />value: 2.9760<br />Function: Penalty<br />Lambda: 1.6","b:  1.87<br />value: 2.9920<br />Function: Penalty<br />Lambda: 1.6","b:  1.88<br />value: 3.0080<br />Function: Penalty<br />Lambda: 1.6","b:  1.89<br />value: 3.0240<br />Function: Penalty<br />Lambda: 1.6","b:  1.90<br />value: 3.0400<br />Function: Penalty<br />Lambda: 1.6","b:  1.91<br />value: 3.0560<br />Function: Penalty<br />Lambda: 1.6","b:  1.92<br />value: 3.0720<br />Function: Penalty<br />Lambda: 1.6","b:  1.93<br />value: 3.0880<br />Function: Penalty<br />Lambda: 1.6","b:  1.94<br />value: 3.1040<br />Function: Penalty<br />Lambda: 1.6","b:  1.95<br />value: 3.1200<br />Function: Penalty<br />Lambda: 1.6","b:  1.96<br />value: 3.1360<br />Function: Penalty<br />Lambda: 1.6","b:  1.97<br />value: 3.1520<br />Function: Penalty<br />Lambda: 1.6","b:  1.98<br />value: 3.1680<br />Function: Penalty<br />Lambda: 1.6","b:  1.99<br />value: 3.1840<br />Function: Penalty<br />Lambda: 1.6","b:  2.00<br />value: 3.2000<br />Function: Penalty<br />Lambda: 1.6"],"frame":"1.6","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"1.7","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 1.7","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 1.7","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 1.7","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 1.7","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 1.7","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 1.7","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 1.7","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 1.7","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 1.7","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 1.7","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 1.7","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 1.7","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 1.7","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 1.7","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 1.7","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 1.7","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 1.7","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 1.7","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 1.7","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 1.7","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 1.7","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 1.7","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 1.7","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 1.7","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 1.7","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 1.7","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 1.7","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 1.7","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 1.7","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 1.7","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 1.7","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 1.7","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 1.7","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 1.7","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 1.7","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 1.7","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 1.7","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 1.7","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 1.7","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 1.7","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 1.7","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 1.7","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 1.7","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 1.7","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 1.7","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 1.7","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 1.7","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 1.7","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 1.7","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 1.7","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.7","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.7","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.7","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.7","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.7","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.7","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.7","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.7","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.7","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.7","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.7","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.7","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.7","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.7","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.7","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.7","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.7","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.7","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.7","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.7","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.7","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.7","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.7","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.7","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.7","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.7","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.7","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.7","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.7","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.7","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.7","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.7","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.7","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.7","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.7","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.7","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.7","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.7","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.7","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.7","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.7","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.7","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.7","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.7","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.7","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.7","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.7","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.7","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.7","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.7","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.7","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.7","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.7","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.7","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.7","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.7","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.7","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.7","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.7","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.7","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.7","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.7","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.7","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.7","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.7","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.7","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.7","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.7","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.7","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.7","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.7","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.7","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.7","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.7","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.7","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.7","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.7","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.7","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.7","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.7","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.7","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.7","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.7","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.7","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.7","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.7","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.7","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.7","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.7","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.7","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.7","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.7","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.7","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.7","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.7","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.7","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.7","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.7","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.7","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.7","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 1.7","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.7","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.7","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.7","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.7","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.7","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.7","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.7","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.7","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.7","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.7","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.7","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.7","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.7","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.7","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.7","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.7","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.7","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.7","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.7","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.7","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.7","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.7","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.7","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.7","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.7","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.7","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.7","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.7","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.7","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.7","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.7","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.7","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.7","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.7","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.7","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.7","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.7","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.7","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.7","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.7","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.7","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.7","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.7","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.7","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.7","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.7","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.7","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.7","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.7","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.7","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.7","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.7","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.7","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.7","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.7","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.7","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.7","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.7","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.7","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.7","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.7","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.7","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.7","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.7","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.7","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.7","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.7","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.7","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.7","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.7","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.7","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.7","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.7","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.7","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.7","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.7","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.7","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.7","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.7","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.7","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.7","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.7","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.7","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.7","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.7","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.7","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.7","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.7","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.7","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.7","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.7","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.7","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.7","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.7","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.7","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.7","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.7","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.7","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.7","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.7"],"frame":"1.7","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.6,3.5531,3.5064,3.4599,3.4136,3.3675,3.3216,3.2759,3.2304,3.1851,3.14,3.0951,3.0504,3.0059,2.9616,2.9175,2.8736,2.8299,2.7864,2.7431,2.7,2.6571,2.6144,2.5719,2.5296,2.4875,2.4456,2.4039,2.3624,2.3211,2.28,2.2391,2.1984,2.1579,2.1176,2.0775,2.0376,1.9979,1.9584,1.9191,1.88,1.8411,1.8024,1.7639,1.7256,1.6875,1.6496,1.6119,1.5744,1.5371,1.5,1.4971,1.4944,1.4919,1.4896,1.4875,1.4856,1.4839,1.4824,1.4811,1.48,1.4791,1.4784,1.4779,1.4776,1.4775,1.4776,1.4779,1.4784,1.4791,1.48,1.4811,1.4824,1.4839,1.4856,1.4875,1.4896,1.4919,1.4944,1.4971,1.5,1.5031,1.5064,1.5099,1.5136,1.5175,1.5216,1.5259,1.5304,1.5351,1.54,1.5451,1.5504,1.5559,1.5616,1.5675,1.5736,1.5799,1.5864,1.5931,1.6,1.6071,1.6144,1.6219,1.6296,1.6375,1.6456,1.6539,1.6624,1.6711,1.68,1.6891,1.6984,1.7079,1.7176,1.7275,1.7376,1.7479,1.7584,1.7691,1.78,1.7911,1.8024,1.8139,1.8256,1.8375,1.8496,1.8619,1.8744,1.8871,1.9,1.9131,1.9264,1.9399,1.9536,1.9675,1.9816,1.9959,2.0104,2.0251,2.04,2.0551,2.0704,2.0859,2.1016,2.1175,2.1336,2.1499,2.1664,2.1831,2.2,2.2171,2.2344,2.2519,2.2696,2.2875,2.3056,2.3239,2.3424,2.3611,2.38,2.3991,2.4184,2.4379,2.4576,2.4775,2.4976,2.5179,2.5384,2.5591,2.58,2.6011,2.6224,2.6439,2.6656,2.6875,2.7096,2.7319,2.7544,2.7771,2.8,2.8231,2.8464,2.8699,2.8936,2.9175,2.9416,2.9659,2.9904,3.0151,3.04,3.0651,3.0904,3.1159,3.1416,3.1675,3.1936,3.2199,3.2464,3.2731,3.3,3.3271,3.3544,3.3819,3.4096,3.4375,3.4656,3.4939,3.5224,3.5511,3.58,3.6091,3.6384,3.6679,3.6976,3.7275,3.7576,3.7879,3.8184,3.8491,3.88,3.9111,3.9424,3.9739,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 3.6000<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.49<br />value: 3.5531<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.48<br />value: 3.5064<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.47<br />value: 3.4599<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.46<br />value: 3.4136<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.45<br />value: 3.3675<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.44<br />value: 3.3216<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.43<br />value: 3.2759<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.42<br />value: 3.2304<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.41<br />value: 3.1851<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.40<br />value: 3.1400<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.39<br />value: 3.0951<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.38<br />value: 3.0504<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.37<br />value: 3.0059<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.36<br />value: 2.9616<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.35<br />value: 2.9175<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.34<br />value: 2.8736<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.33<br />value: 2.8299<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.32<br />value: 2.7864<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.31<br />value: 2.7431<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.30<br />value: 2.7000<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.29<br />value: 2.6571<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.28<br />value: 2.6144<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.27<br />value: 2.5719<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.26<br />value: 2.5296<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.25<br />value: 2.4875<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.24<br />value: 2.4456<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.23<br />value: 2.4039<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.22<br />value: 2.3624<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.21<br />value: 2.3211<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.20<br />value: 2.2800<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.19<br />value: 2.2391<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.18<br />value: 2.1984<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.17<br />value: 2.1579<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.16<br />value: 2.1176<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.15<br />value: 2.0775<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.14<br />value: 2.0376<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.13<br />value: 1.9979<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.12<br />value: 1.9584<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.11<br />value: 1.9191<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.10<br />value: 1.8800<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.09<br />value: 1.8411<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.08<br />value: 1.8024<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.07<br />value: 1.7639<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.06<br />value: 1.7256<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.05<br />value: 1.6875<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.04<br />value: 1.6496<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.03<br />value: 1.6119<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.02<br />value: 1.5744<br />Function: Loss + Penalty<br />Lambda: 1.7","b: -0.01<br />value: 1.5371<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.01<br />value: 1.4971<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.02<br />value: 1.4944<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.03<br />value: 1.4919<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.04<br />value: 1.4896<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.05<br />value: 1.4875<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.06<br />value: 1.4856<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.07<br />value: 1.4839<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.08<br />value: 1.4824<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.09<br />value: 1.4811<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.10<br />value: 1.4800<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.11<br />value: 1.4791<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.12<br />value: 1.4784<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.13<br />value: 1.4779<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.14<br />value: 1.4776<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.15<br />value: 1.4775<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.16<br />value: 1.4776<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.17<br />value: 1.4779<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.18<br />value: 1.4784<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.19<br />value: 1.4791<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.20<br />value: 1.4800<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.21<br />value: 1.4811<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.22<br />value: 1.4824<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.23<br />value: 1.4839<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.24<br />value: 1.4856<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.25<br />value: 1.4875<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.26<br />value: 1.4896<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.27<br />value: 1.4919<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.28<br />value: 1.4944<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.29<br />value: 1.4971<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.30<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.31<br />value: 1.5031<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.32<br />value: 1.5064<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.33<br />value: 1.5099<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.34<br />value: 1.5136<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.35<br />value: 1.5175<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.36<br />value: 1.5216<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.37<br />value: 1.5259<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.38<br />value: 1.5304<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.39<br />value: 1.5351<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.40<br />value: 1.5400<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.41<br />value: 1.5451<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.42<br />value: 1.5504<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.43<br />value: 1.5559<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.44<br />value: 1.5616<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.45<br />value: 1.5675<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.46<br />value: 1.5736<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.47<br />value: 1.5799<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.48<br />value: 1.5864<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.49<br />value: 1.5931<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.50<br />value: 1.6000<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.51<br />value: 1.6071<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.52<br />value: 1.6144<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.53<br />value: 1.6219<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.54<br />value: 1.6296<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.55<br />value: 1.6375<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.56<br />value: 1.6456<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.57<br />value: 1.6539<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.58<br />value: 1.6624<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.59<br />value: 1.6711<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.60<br />value: 1.6800<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.61<br />value: 1.6891<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.62<br />value: 1.6984<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.63<br />value: 1.7079<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.64<br />value: 1.7176<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.65<br />value: 1.7275<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.66<br />value: 1.7376<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.67<br />value: 1.7479<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.68<br />value: 1.7584<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.69<br />value: 1.7691<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.70<br />value: 1.7800<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.71<br />value: 1.7911<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.72<br />value: 1.8024<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.73<br />value: 1.8139<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.74<br />value: 1.8256<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.75<br />value: 1.8375<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.76<br />value: 1.8496<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.77<br />value: 1.8619<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.78<br />value: 1.8744<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.79<br />value: 1.8871<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.80<br />value: 1.9000<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.81<br />value: 1.9131<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.82<br />value: 1.9264<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.83<br />value: 1.9399<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.84<br />value: 1.9536<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.85<br />value: 1.9675<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.86<br />value: 1.9816<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.87<br />value: 1.9959<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.88<br />value: 2.0104<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.89<br />value: 2.0251<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.90<br />value: 2.0400<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.91<br />value: 2.0551<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.92<br />value: 2.0704<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.93<br />value: 2.0859<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.94<br />value: 2.1016<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.95<br />value: 2.1175<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.96<br />value: 2.1336<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.97<br />value: 2.1499<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.98<br />value: 2.1664<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  0.99<br />value: 2.1831<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.00<br />value: 2.2000<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.01<br />value: 2.2171<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.02<br />value: 2.2344<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.03<br />value: 2.2519<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.04<br />value: 2.2696<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.05<br />value: 2.2875<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.06<br />value: 2.3056<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.07<br />value: 2.3239<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.08<br />value: 2.3424<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.09<br />value: 2.3611<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.10<br />value: 2.3800<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.11<br />value: 2.3991<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.12<br />value: 2.4184<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.13<br />value: 2.4379<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.14<br />value: 2.4576<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.15<br />value: 2.4775<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.16<br />value: 2.4976<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.17<br />value: 2.5179<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.18<br />value: 2.5384<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.19<br />value: 2.5591<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.20<br />value: 2.5800<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.21<br />value: 2.6011<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.22<br />value: 2.6224<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.23<br />value: 2.6439<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.24<br />value: 2.6656<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.25<br />value: 2.6875<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.26<br />value: 2.7096<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.27<br />value: 2.7319<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.28<br />value: 2.7544<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.29<br />value: 2.7771<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.30<br />value: 2.8000<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.31<br />value: 2.8231<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.32<br />value: 2.8464<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.33<br />value: 2.8699<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.34<br />value: 2.8936<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.35<br />value: 2.9175<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.36<br />value: 2.9416<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.37<br />value: 2.9659<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.38<br />value: 2.9904<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.39<br />value: 3.0151<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.40<br />value: 3.0400<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.41<br />value: 3.0651<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.42<br />value: 3.0904<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.43<br />value: 3.1159<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.44<br />value: 3.1416<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.45<br />value: 3.1675<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.46<br />value: 3.1936<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.47<br />value: 3.2199<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.48<br />value: 3.2464<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.49<br />value: 3.2731<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.50<br />value: 3.3000<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.51<br />value: 3.3271<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.52<br />value: 3.3544<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.53<br />value: 3.3819<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.54<br />value: 3.4096<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.55<br />value: 3.4375<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.56<br />value: 3.4656<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.57<br />value: 3.4939<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.58<br />value: 3.5224<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.59<br />value: 3.5511<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.60<br />value: 3.5800<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.61<br />value: 3.6091<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.62<br />value: 3.6384<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.63<br />value: 3.6679<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.64<br />value: 3.6976<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.65<br />value: 3.7275<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.66<br />value: 3.7576<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.67<br />value: 3.7879<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.68<br />value: 3.8184<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.69<br />value: 3.8491<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.70<br />value: 3.8800<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.71<br />value: 3.9111<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.72<br />value: 3.9424<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.73<br />value: 3.9739<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.74<br />value: 4.0056<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.75<br />value: 4.0375<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.76<br />value: 4.0696<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.77<br />value: 4.1019<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.78<br />value: 4.1344<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.79<br />value: 4.1671<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.80<br />value: 4.2000<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.81<br />value: 4.2331<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.82<br />value: 4.2664<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.83<br />value: 4.2999<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.84<br />value: 4.3336<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.85<br />value: 4.3675<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.86<br />value: 4.4016<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.87<br />value: 4.4359<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.88<br />value: 4.4704<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.89<br />value: 4.5051<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.90<br />value: 4.5400<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.91<br />value: 4.5751<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.92<br />value: 4.6104<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.93<br />value: 4.6459<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.94<br />value: 4.6816<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.95<br />value: 4.7175<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.96<br />value: 4.7536<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.97<br />value: 4.7899<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.98<br />value: 4.8264<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  1.99<br />value: 4.8631<br />Function: Loss + Penalty<br />Lambda: 1.7","b:  2.00<br />value: 4.9000<br />Function: Loss + Penalty<br />Lambda: 1.7"],"frame":"1.7","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.85,0.833,0.816,0.799,0.782,0.765,0.748,0.731,0.714,0.697,0.68,0.663,0.646,0.629,0.612,0.595,0.578,0.561,0.544,0.527,0.51,0.493,0.476,0.459,0.442,0.425,0.408,0.391,0.374,0.357,0.34,0.323,0.306,0.289,0.272,0.255,0.238,0.221,0.204,0.187,0.17,0.153,0.136,0.119,0.102,0.085,0.068,0.051,0.034,0.017,0,0.017,0.034,0.0510000000000001,0.0680000000000001,0.0850000000000001,0.102,0.119,0.136,0.153,0.17,0.187,0.204,0.221,0.238,0.255,0.272,0.289,0.306,0.323,0.34,0.357,0.374,0.391,0.408,0.425,0.442,0.459,0.476,0.493,0.51,0.527,0.544,0.561,0.578,0.595,0.612,0.629,0.646,0.663,0.68,0.697,0.714,0.731,0.748,0.765,0.782,0.799,0.816,0.833,0.85,0.867,0.884,0.901,0.918,0.935,0.952,0.969,0.986,1.003,1.02,1.037,1.054,1.071,1.088,1.105,1.122,1.139,1.156,1.173,1.19,1.207,1.224,1.241,1.258,1.275,1.292,1.309,1.326,1.343,1.36,1.377,1.394,1.411,1.428,1.445,1.462,1.479,1.496,1.513,1.53,1.547,1.564,1.581,1.598,1.615,1.632,1.649,1.666,1.683,1.7,1.717,1.734,1.751,1.768,1.785,1.802,1.819,1.836,1.853,1.87,1.887,1.904,1.921,1.938,1.955,1.972,1.989,2.006,2.023,2.04,2.057,2.074,2.091,2.108,2.125,2.142,2.159,2.176,2.193,2.21,2.227,2.244,2.261,2.278,2.295,2.312,2.329,2.346,2.363,2.38,2.397,2.414,2.431,2.448,2.465,2.482,2.499,2.516,2.533,2.55,2.567,2.584,2.601,2.618,2.635,2.652,2.669,2.686,2.703,2.72,2.737,2.754,2.771,2.788,2.805,2.822,2.839,2.856,2.873,2.89,2.907,2.924,2.941,2.958,2.975,2.992,3.009,3.026,3.043,3.06,3.077,3.094,3.111,3.128,3.145,3.162,3.179,3.196,3.213,3.23,3.247,3.264,3.281,3.298,3.315,3.332,3.349,3.366,3.383,3.4],"text":["b: -0.50<br />value: 0.8500<br />Function: Penalty<br />Lambda: 1.7","b: -0.49<br />value: 0.8330<br />Function: Penalty<br />Lambda: 1.7","b: -0.48<br />value: 0.8160<br />Function: Penalty<br />Lambda: 1.7","b: -0.47<br />value: 0.7990<br />Function: Penalty<br />Lambda: 1.7","b: -0.46<br />value: 0.7820<br />Function: Penalty<br />Lambda: 1.7","b: -0.45<br />value: 0.7650<br />Function: Penalty<br />Lambda: 1.7","b: -0.44<br />value: 0.7480<br />Function: Penalty<br />Lambda: 1.7","b: -0.43<br />value: 0.7310<br />Function: Penalty<br />Lambda: 1.7","b: -0.42<br />value: 0.7140<br />Function: Penalty<br />Lambda: 1.7","b: -0.41<br />value: 0.6970<br />Function: Penalty<br />Lambda: 1.7","b: -0.40<br />value: 0.6800<br />Function: Penalty<br />Lambda: 1.7","b: -0.39<br />value: 0.6630<br />Function: Penalty<br />Lambda: 1.7","b: -0.38<br />value: 0.6460<br />Function: Penalty<br />Lambda: 1.7","b: -0.37<br />value: 0.6290<br />Function: Penalty<br />Lambda: 1.7","b: -0.36<br />value: 0.6120<br />Function: Penalty<br />Lambda: 1.7","b: -0.35<br />value: 0.5950<br />Function: Penalty<br />Lambda: 1.7","b: -0.34<br />value: 0.5780<br />Function: Penalty<br />Lambda: 1.7","b: -0.33<br />value: 0.5610<br />Function: Penalty<br />Lambda: 1.7","b: -0.32<br />value: 0.5440<br />Function: Penalty<br />Lambda: 1.7","b: -0.31<br />value: 0.5270<br />Function: Penalty<br />Lambda: 1.7","b: -0.30<br />value: 0.5100<br />Function: Penalty<br />Lambda: 1.7","b: -0.29<br />value: 0.4930<br />Function: Penalty<br />Lambda: 1.7","b: -0.28<br />value: 0.4760<br />Function: Penalty<br />Lambda: 1.7","b: -0.27<br />value: 0.4590<br />Function: Penalty<br />Lambda: 1.7","b: -0.26<br />value: 0.4420<br />Function: Penalty<br />Lambda: 1.7","b: -0.25<br />value: 0.4250<br />Function: Penalty<br />Lambda: 1.7","b: -0.24<br />value: 0.4080<br />Function: Penalty<br />Lambda: 1.7","b: -0.23<br />value: 0.3910<br />Function: Penalty<br />Lambda: 1.7","b: -0.22<br />value: 0.3740<br />Function: Penalty<br />Lambda: 1.7","b: -0.21<br />value: 0.3570<br />Function: Penalty<br />Lambda: 1.7","b: -0.20<br />value: 0.3400<br />Function: Penalty<br />Lambda: 1.7","b: -0.19<br />value: 0.3230<br />Function: Penalty<br />Lambda: 1.7","b: -0.18<br />value: 0.3060<br />Function: Penalty<br />Lambda: 1.7","b: -0.17<br />value: 0.2890<br />Function: Penalty<br />Lambda: 1.7","b: -0.16<br />value: 0.2720<br />Function: Penalty<br />Lambda: 1.7","b: -0.15<br />value: 0.2550<br />Function: Penalty<br />Lambda: 1.7","b: -0.14<br />value: 0.2380<br />Function: Penalty<br />Lambda: 1.7","b: -0.13<br />value: 0.2210<br />Function: Penalty<br />Lambda: 1.7","b: -0.12<br />value: 0.2040<br />Function: Penalty<br />Lambda: 1.7","b: -0.11<br />value: 0.1870<br />Function: Penalty<br />Lambda: 1.7","b: -0.10<br />value: 0.1700<br />Function: Penalty<br />Lambda: 1.7","b: -0.09<br />value: 0.1530<br />Function: Penalty<br />Lambda: 1.7","b: -0.08<br />value: 0.1360<br />Function: Penalty<br />Lambda: 1.7","b: -0.07<br />value: 0.1190<br />Function: Penalty<br />Lambda: 1.7","b: -0.06<br />value: 0.1020<br />Function: Penalty<br />Lambda: 1.7","b: -0.05<br />value: 0.0850<br />Function: Penalty<br />Lambda: 1.7","b: -0.04<br />value: 0.0680<br />Function: Penalty<br />Lambda: 1.7","b: -0.03<br />value: 0.0510<br />Function: Penalty<br />Lambda: 1.7","b: -0.02<br />value: 0.0340<br />Function: Penalty<br />Lambda: 1.7","b: -0.01<br />value: 0.0170<br />Function: Penalty<br />Lambda: 1.7","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 1.7","b:  0.01<br />value: 0.0170<br />Function: Penalty<br />Lambda: 1.7","b:  0.02<br />value: 0.0340<br />Function: Penalty<br />Lambda: 1.7","b:  0.03<br />value: 0.0510<br />Function: Penalty<br />Lambda: 1.7","b:  0.04<br />value: 0.0680<br />Function: Penalty<br />Lambda: 1.7","b:  0.05<br />value: 0.0850<br />Function: Penalty<br />Lambda: 1.7","b:  0.06<br />value: 0.1020<br />Function: Penalty<br />Lambda: 1.7","b:  0.07<br />value: 0.1190<br />Function: Penalty<br />Lambda: 1.7","b:  0.08<br />value: 0.1360<br />Function: Penalty<br />Lambda: 1.7","b:  0.09<br />value: 0.1530<br />Function: Penalty<br />Lambda: 1.7","b:  0.10<br />value: 0.1700<br />Function: Penalty<br />Lambda: 1.7","b:  0.11<br />value: 0.1870<br />Function: Penalty<br />Lambda: 1.7","b:  0.12<br />value: 0.2040<br />Function: Penalty<br />Lambda: 1.7","b:  0.13<br />value: 0.2210<br />Function: Penalty<br />Lambda: 1.7","b:  0.14<br />value: 0.2380<br />Function: Penalty<br />Lambda: 1.7","b:  0.15<br />value: 0.2550<br />Function: Penalty<br />Lambda: 1.7","b:  0.16<br />value: 0.2720<br />Function: Penalty<br />Lambda: 1.7","b:  0.17<br />value: 0.2890<br />Function: Penalty<br />Lambda: 1.7","b:  0.18<br />value: 0.3060<br />Function: Penalty<br />Lambda: 1.7","b:  0.19<br />value: 0.3230<br />Function: Penalty<br />Lambda: 1.7","b:  0.20<br />value: 0.3400<br />Function: Penalty<br />Lambda: 1.7","b:  0.21<br />value: 0.3570<br />Function: Penalty<br />Lambda: 1.7","b:  0.22<br />value: 0.3740<br />Function: Penalty<br />Lambda: 1.7","b:  0.23<br />value: 0.3910<br />Function: Penalty<br />Lambda: 1.7","b:  0.24<br />value: 0.4080<br />Function: Penalty<br />Lambda: 1.7","b:  0.25<br />value: 0.4250<br />Function: Penalty<br />Lambda: 1.7","b:  0.26<br />value: 0.4420<br />Function: Penalty<br />Lambda: 1.7","b:  0.27<br />value: 0.4590<br />Function: Penalty<br />Lambda: 1.7","b:  0.28<br />value: 0.4760<br />Function: Penalty<br />Lambda: 1.7","b:  0.29<br />value: 0.4930<br />Function: Penalty<br />Lambda: 1.7","b:  0.30<br />value: 0.5100<br />Function: Penalty<br />Lambda: 1.7","b:  0.31<br />value: 0.5270<br />Function: Penalty<br />Lambda: 1.7","b:  0.32<br />value: 0.5440<br />Function: Penalty<br />Lambda: 1.7","b:  0.33<br />value: 0.5610<br />Function: Penalty<br />Lambda: 1.7","b:  0.34<br />value: 0.5780<br />Function: Penalty<br />Lambda: 1.7","b:  0.35<br />value: 0.5950<br />Function: Penalty<br />Lambda: 1.7","b:  0.36<br />value: 0.6120<br />Function: Penalty<br />Lambda: 1.7","b:  0.37<br />value: 0.6290<br />Function: Penalty<br />Lambda: 1.7","b:  0.38<br />value: 0.6460<br />Function: Penalty<br />Lambda: 1.7","b:  0.39<br />value: 0.6630<br />Function: Penalty<br />Lambda: 1.7","b:  0.40<br />value: 0.6800<br />Function: Penalty<br />Lambda: 1.7","b:  0.41<br />value: 0.6970<br />Function: Penalty<br />Lambda: 1.7","b:  0.42<br />value: 0.7140<br />Function: Penalty<br />Lambda: 1.7","b:  0.43<br />value: 0.7310<br />Function: Penalty<br />Lambda: 1.7","b:  0.44<br />value: 0.7480<br />Function: Penalty<br />Lambda: 1.7","b:  0.45<br />value: 0.7650<br />Function: Penalty<br />Lambda: 1.7","b:  0.46<br />value: 0.7820<br />Function: Penalty<br />Lambda: 1.7","b:  0.47<br />value: 0.7990<br />Function: Penalty<br />Lambda: 1.7","b:  0.48<br />value: 0.8160<br />Function: Penalty<br />Lambda: 1.7","b:  0.49<br />value: 0.8330<br />Function: Penalty<br />Lambda: 1.7","b:  0.50<br />value: 0.8500<br />Function: Penalty<br />Lambda: 1.7","b:  0.51<br />value: 0.8670<br />Function: Penalty<br />Lambda: 1.7","b:  0.52<br />value: 0.8840<br />Function: Penalty<br />Lambda: 1.7","b:  0.53<br />value: 0.9010<br />Function: Penalty<br />Lambda: 1.7","b:  0.54<br />value: 0.9180<br />Function: Penalty<br />Lambda: 1.7","b:  0.55<br />value: 0.9350<br />Function: Penalty<br />Lambda: 1.7","b:  0.56<br />value: 0.9520<br />Function: Penalty<br />Lambda: 1.7","b:  0.57<br />value: 0.9690<br />Function: Penalty<br />Lambda: 1.7","b:  0.58<br />value: 0.9860<br />Function: Penalty<br />Lambda: 1.7","b:  0.59<br />value: 1.0030<br />Function: Penalty<br />Lambda: 1.7","b:  0.60<br />value: 1.0200<br />Function: Penalty<br />Lambda: 1.7","b:  0.61<br />value: 1.0370<br />Function: Penalty<br />Lambda: 1.7","b:  0.62<br />value: 1.0540<br />Function: Penalty<br />Lambda: 1.7","b:  0.63<br />value: 1.0710<br />Function: Penalty<br />Lambda: 1.7","b:  0.64<br />value: 1.0880<br />Function: Penalty<br />Lambda: 1.7","b:  0.65<br />value: 1.1050<br />Function: Penalty<br />Lambda: 1.7","b:  0.66<br />value: 1.1220<br />Function: Penalty<br />Lambda: 1.7","b:  0.67<br />value: 1.1390<br />Function: Penalty<br />Lambda: 1.7","b:  0.68<br />value: 1.1560<br />Function: Penalty<br />Lambda: 1.7","b:  0.69<br />value: 1.1730<br />Function: Penalty<br />Lambda: 1.7","b:  0.70<br />value: 1.1900<br />Function: Penalty<br />Lambda: 1.7","b:  0.71<br />value: 1.2070<br />Function: Penalty<br />Lambda: 1.7","b:  0.72<br />value: 1.2240<br />Function: Penalty<br />Lambda: 1.7","b:  0.73<br />value: 1.2410<br />Function: Penalty<br />Lambda: 1.7","b:  0.74<br />value: 1.2580<br />Function: Penalty<br />Lambda: 1.7","b:  0.75<br />value: 1.2750<br />Function: Penalty<br />Lambda: 1.7","b:  0.76<br />value: 1.2920<br />Function: Penalty<br />Lambda: 1.7","b:  0.77<br />value: 1.3090<br />Function: Penalty<br />Lambda: 1.7","b:  0.78<br />value: 1.3260<br />Function: Penalty<br />Lambda: 1.7","b:  0.79<br />value: 1.3430<br />Function: Penalty<br />Lambda: 1.7","b:  0.80<br />value: 1.3600<br />Function: Penalty<br />Lambda: 1.7","b:  0.81<br />value: 1.3770<br />Function: Penalty<br />Lambda: 1.7","b:  0.82<br />value: 1.3940<br />Function: Penalty<br />Lambda: 1.7","b:  0.83<br />value: 1.4110<br />Function: Penalty<br />Lambda: 1.7","b:  0.84<br />value: 1.4280<br />Function: Penalty<br />Lambda: 1.7","b:  0.85<br />value: 1.4450<br />Function: Penalty<br />Lambda: 1.7","b:  0.86<br />value: 1.4620<br />Function: Penalty<br />Lambda: 1.7","b:  0.87<br />value: 1.4790<br />Function: Penalty<br />Lambda: 1.7","b:  0.88<br />value: 1.4960<br />Function: Penalty<br />Lambda: 1.7","b:  0.89<br />value: 1.5130<br />Function: Penalty<br />Lambda: 1.7","b:  0.90<br />value: 1.5300<br />Function: Penalty<br />Lambda: 1.7","b:  0.91<br />value: 1.5470<br />Function: Penalty<br />Lambda: 1.7","b:  0.92<br />value: 1.5640<br />Function: Penalty<br />Lambda: 1.7","b:  0.93<br />value: 1.5810<br />Function: Penalty<br />Lambda: 1.7","b:  0.94<br />value: 1.5980<br />Function: Penalty<br />Lambda: 1.7","b:  0.95<br />value: 1.6150<br />Function: Penalty<br />Lambda: 1.7","b:  0.96<br />value: 1.6320<br />Function: Penalty<br />Lambda: 1.7","b:  0.97<br />value: 1.6490<br />Function: Penalty<br />Lambda: 1.7","b:  0.98<br />value: 1.6660<br />Function: Penalty<br />Lambda: 1.7","b:  0.99<br />value: 1.6830<br />Function: Penalty<br />Lambda: 1.7","b:  1.00<br />value: 1.7000<br />Function: Penalty<br />Lambda: 1.7","b:  1.01<br />value: 1.7170<br />Function: Penalty<br />Lambda: 1.7","b:  1.02<br />value: 1.7340<br />Function: Penalty<br />Lambda: 1.7","b:  1.03<br />value: 1.7510<br />Function: Penalty<br />Lambda: 1.7","b:  1.04<br />value: 1.7680<br />Function: Penalty<br />Lambda: 1.7","b:  1.05<br />value: 1.7850<br />Function: Penalty<br />Lambda: 1.7","b:  1.06<br />value: 1.8020<br />Function: Penalty<br />Lambda: 1.7","b:  1.07<br />value: 1.8190<br />Function: Penalty<br />Lambda: 1.7","b:  1.08<br />value: 1.8360<br />Function: Penalty<br />Lambda: 1.7","b:  1.09<br />value: 1.8530<br />Function: Penalty<br />Lambda: 1.7","b:  1.10<br />value: 1.8700<br />Function: Penalty<br />Lambda: 1.7","b:  1.11<br />value: 1.8870<br />Function: Penalty<br />Lambda: 1.7","b:  1.12<br />value: 1.9040<br />Function: Penalty<br />Lambda: 1.7","b:  1.13<br />value: 1.9210<br />Function: Penalty<br />Lambda: 1.7","b:  1.14<br />value: 1.9380<br />Function: Penalty<br />Lambda: 1.7","b:  1.15<br />value: 1.9550<br />Function: Penalty<br />Lambda: 1.7","b:  1.16<br />value: 1.9720<br />Function: Penalty<br />Lambda: 1.7","b:  1.17<br />value: 1.9890<br />Function: Penalty<br />Lambda: 1.7","b:  1.18<br />value: 2.0060<br />Function: Penalty<br />Lambda: 1.7","b:  1.19<br />value: 2.0230<br />Function: Penalty<br />Lambda: 1.7","b:  1.20<br />value: 2.0400<br />Function: Penalty<br />Lambda: 1.7","b:  1.21<br />value: 2.0570<br />Function: Penalty<br />Lambda: 1.7","b:  1.22<br />value: 2.0740<br />Function: Penalty<br />Lambda: 1.7","b:  1.23<br />value: 2.0910<br />Function: Penalty<br />Lambda: 1.7","b:  1.24<br />value: 2.1080<br />Function: Penalty<br />Lambda: 1.7","b:  1.25<br />value: 2.1250<br />Function: Penalty<br />Lambda: 1.7","b:  1.26<br />value: 2.1420<br />Function: Penalty<br />Lambda: 1.7","b:  1.27<br />value: 2.1590<br />Function: Penalty<br />Lambda: 1.7","b:  1.28<br />value: 2.1760<br />Function: Penalty<br />Lambda: 1.7","b:  1.29<br />value: 2.1930<br />Function: Penalty<br />Lambda: 1.7","b:  1.30<br />value: 2.2100<br />Function: Penalty<br />Lambda: 1.7","b:  1.31<br />value: 2.2270<br />Function: Penalty<br />Lambda: 1.7","b:  1.32<br />value: 2.2440<br />Function: Penalty<br />Lambda: 1.7","b:  1.33<br />value: 2.2610<br />Function: Penalty<br />Lambda: 1.7","b:  1.34<br />value: 2.2780<br />Function: Penalty<br />Lambda: 1.7","b:  1.35<br />value: 2.2950<br />Function: Penalty<br />Lambda: 1.7","b:  1.36<br />value: 2.3120<br />Function: Penalty<br />Lambda: 1.7","b:  1.37<br />value: 2.3290<br />Function: Penalty<br />Lambda: 1.7","b:  1.38<br />value: 2.3460<br />Function: Penalty<br />Lambda: 1.7","b:  1.39<br />value: 2.3630<br />Function: Penalty<br />Lambda: 1.7","b:  1.40<br />value: 2.3800<br />Function: Penalty<br />Lambda: 1.7","b:  1.41<br />value: 2.3970<br />Function: Penalty<br />Lambda: 1.7","b:  1.42<br />value: 2.4140<br />Function: Penalty<br />Lambda: 1.7","b:  1.43<br />value: 2.4310<br />Function: Penalty<br />Lambda: 1.7","b:  1.44<br />value: 2.4480<br />Function: Penalty<br />Lambda: 1.7","b:  1.45<br />value: 2.4650<br />Function: Penalty<br />Lambda: 1.7","b:  1.46<br />value: 2.4820<br />Function: Penalty<br />Lambda: 1.7","b:  1.47<br />value: 2.4990<br />Function: Penalty<br />Lambda: 1.7","b:  1.48<br />value: 2.5160<br />Function: Penalty<br />Lambda: 1.7","b:  1.49<br />value: 2.5330<br />Function: Penalty<br />Lambda: 1.7","b:  1.50<br />value: 2.5500<br />Function: Penalty<br />Lambda: 1.7","b:  1.51<br />value: 2.5670<br />Function: Penalty<br />Lambda: 1.7","b:  1.52<br />value: 2.5840<br />Function: Penalty<br />Lambda: 1.7","b:  1.53<br />value: 2.6010<br />Function: Penalty<br />Lambda: 1.7","b:  1.54<br />value: 2.6180<br />Function: Penalty<br />Lambda: 1.7","b:  1.55<br />value: 2.6350<br />Function: Penalty<br />Lambda: 1.7","b:  1.56<br />value: 2.6520<br />Function: Penalty<br />Lambda: 1.7","b:  1.57<br />value: 2.6690<br />Function: Penalty<br />Lambda: 1.7","b:  1.58<br />value: 2.6860<br />Function: Penalty<br />Lambda: 1.7","b:  1.59<br />value: 2.7030<br />Function: Penalty<br />Lambda: 1.7","b:  1.60<br />value: 2.7200<br />Function: Penalty<br />Lambda: 1.7","b:  1.61<br />value: 2.7370<br />Function: Penalty<br />Lambda: 1.7","b:  1.62<br />value: 2.7540<br />Function: Penalty<br />Lambda: 1.7","b:  1.63<br />value: 2.7710<br />Function: Penalty<br />Lambda: 1.7","b:  1.64<br />value: 2.7880<br />Function: Penalty<br />Lambda: 1.7","b:  1.65<br />value: 2.8050<br />Function: Penalty<br />Lambda: 1.7","b:  1.66<br />value: 2.8220<br />Function: Penalty<br />Lambda: 1.7","b:  1.67<br />value: 2.8390<br />Function: Penalty<br />Lambda: 1.7","b:  1.68<br />value: 2.8560<br />Function: Penalty<br />Lambda: 1.7","b:  1.69<br />value: 2.8730<br />Function: Penalty<br />Lambda: 1.7","b:  1.70<br />value: 2.8900<br />Function: Penalty<br />Lambda: 1.7","b:  1.71<br />value: 2.9070<br />Function: Penalty<br />Lambda: 1.7","b:  1.72<br />value: 2.9240<br />Function: Penalty<br />Lambda: 1.7","b:  1.73<br />value: 2.9410<br />Function: Penalty<br />Lambda: 1.7","b:  1.74<br />value: 2.9580<br />Function: Penalty<br />Lambda: 1.7","b:  1.75<br />value: 2.9750<br />Function: Penalty<br />Lambda: 1.7","b:  1.76<br />value: 2.9920<br />Function: Penalty<br />Lambda: 1.7","b:  1.77<br />value: 3.0090<br />Function: Penalty<br />Lambda: 1.7","b:  1.78<br />value: 3.0260<br />Function: Penalty<br />Lambda: 1.7","b:  1.79<br />value: 3.0430<br />Function: Penalty<br />Lambda: 1.7","b:  1.80<br />value: 3.0600<br />Function: Penalty<br />Lambda: 1.7","b:  1.81<br />value: 3.0770<br />Function: Penalty<br />Lambda: 1.7","b:  1.82<br />value: 3.0940<br />Function: Penalty<br />Lambda: 1.7","b:  1.83<br />value: 3.1110<br />Function: Penalty<br />Lambda: 1.7","b:  1.84<br />value: 3.1280<br />Function: Penalty<br />Lambda: 1.7","b:  1.85<br />value: 3.1450<br />Function: Penalty<br />Lambda: 1.7","b:  1.86<br />value: 3.1620<br />Function: Penalty<br />Lambda: 1.7","b:  1.87<br />value: 3.1790<br />Function: Penalty<br />Lambda: 1.7","b:  1.88<br />value: 3.1960<br />Function: Penalty<br />Lambda: 1.7","b:  1.89<br />value: 3.2130<br />Function: Penalty<br />Lambda: 1.7","b:  1.90<br />value: 3.2300<br />Function: Penalty<br />Lambda: 1.7","b:  1.91<br />value: 3.2470<br />Function: Penalty<br />Lambda: 1.7","b:  1.92<br />value: 3.2640<br />Function: Penalty<br />Lambda: 1.7","b:  1.93<br />value: 3.2810<br />Function: Penalty<br />Lambda: 1.7","b:  1.94<br />value: 3.2980<br />Function: Penalty<br />Lambda: 1.7","b:  1.95<br />value: 3.3150<br />Function: Penalty<br />Lambda: 1.7","b:  1.96<br />value: 3.3320<br />Function: Penalty<br />Lambda: 1.7","b:  1.97<br />value: 3.3490<br />Function: Penalty<br />Lambda: 1.7","b:  1.98<br />value: 3.3660<br />Function: Penalty<br />Lambda: 1.7","b:  1.99<br />value: 3.3830<br />Function: Penalty<br />Lambda: 1.7","b:  2.00<br />value: 3.4000<br />Function: Penalty<br />Lambda: 1.7"],"frame":"1.7","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"1.8","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 1.8","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 1.8","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 1.8","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 1.8","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 1.8","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 1.8","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 1.8","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 1.8","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 1.8","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 1.8","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 1.8","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 1.8","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 1.8","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 1.8","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 1.8","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 1.8","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 1.8","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 1.8","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 1.8","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 1.8","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 1.8","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 1.8","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 1.8","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 1.8","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 1.8","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 1.8","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 1.8","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 1.8","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 1.8","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 1.8","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 1.8","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 1.8","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 1.8","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 1.8","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 1.8","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 1.8","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 1.8","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 1.8","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 1.8","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 1.8","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 1.8","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 1.8","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 1.8","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 1.8","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 1.8","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 1.8","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 1.8","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 1.8","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 1.8","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 1.8","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.8","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.8","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.8","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.8","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.8","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.8","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.8","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.8","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.8","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.8","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.8","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.8","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.8","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.8","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.8","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.8","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.8","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.8","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.8","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.8","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.8","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.8","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.8","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.8","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.8","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.8","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.8","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.8","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.8","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.8","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.8","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.8","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.8","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.8","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.8","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.8","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.8","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.8","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.8","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.8","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.8","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.8","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.8","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.8","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.8","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.8","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.8","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.8","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.8","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.8","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.8","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.8","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.8","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.8","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.8","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.8","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.8","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.8","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.8","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.8","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.8","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.8","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.8","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.8","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.8","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.8","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.8","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.8","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.8","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.8","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.8","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.8","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.8","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.8","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.8","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.8","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.8","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.8","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.8","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.8","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.8","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.8","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.8","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.8","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.8","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.8","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.8","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.8","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.8","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.8","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.8","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.8","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.8","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.8","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.8","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.8","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.8","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.8","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.8","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.8","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 1.8","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.8","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.8","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.8","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.8","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.8","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.8","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.8","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.8","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.8","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.8","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.8","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.8","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.8","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.8","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.8","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.8","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.8","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.8","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.8","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.8","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.8","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.8","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.8","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.8","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.8","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.8","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.8","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.8","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.8","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.8","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.8","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.8","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.8","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.8","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.8","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.8","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.8","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.8","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.8","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.8","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.8","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.8","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.8","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.8","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.8","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.8","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.8","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.8","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.8","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.8","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.8","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.8","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.8","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.8","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.8","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.8","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.8","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.8","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.8","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.8","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.8","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.8","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.8","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.8","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.8","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.8","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.8","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.8","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.8","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.8","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.8","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.8","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.8","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.8","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.8","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.8","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.8","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.8","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.8","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.8","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.8","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.8","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.8","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.8","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.8","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.8","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.8","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.8","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.8","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.8","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.8","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.8","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.8","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.8","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.8","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.8","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.8","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.8","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.8","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.8"],"frame":"1.8","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.65,3.6021,3.5544,3.5069,3.4596,3.4125,3.3656,3.3189,3.2724,3.2261,3.18,3.1341,3.0884,3.0429,2.9976,2.9525,2.9076,2.8629,2.8184,2.7741,2.73,2.6861,2.6424,2.5989,2.5556,2.5125,2.4696,2.4269,2.3844,2.3421,2.3,2.2581,2.2164,2.1749,2.1336,2.0925,2.0516,2.0109,1.9704,1.9301,1.89,1.8501,1.8104,1.7709,1.7316,1.6925,1.6536,1.6149,1.5764,1.5381,1.5,1.4981,1.4964,1.4949,1.4936,1.4925,1.4916,1.4909,1.4904,1.4901,1.49,1.4901,1.4904,1.4909,1.4916,1.4925,1.4936,1.4949,1.4964,1.4981,1.5,1.5021,1.5044,1.5069,1.5096,1.5125,1.5156,1.5189,1.5224,1.5261,1.53,1.5341,1.5384,1.5429,1.5476,1.5525,1.5576,1.5629,1.5684,1.5741,1.58,1.5861,1.5924,1.5989,1.6056,1.6125,1.6196,1.6269,1.6344,1.6421,1.65,1.6581,1.6664,1.6749,1.6836,1.6925,1.7016,1.7109,1.7204,1.7301,1.74,1.7501,1.7604,1.7709,1.7816,1.7925,1.8036,1.8149,1.8264,1.8381,1.85,1.8621,1.8744,1.8869,1.8996,1.9125,1.9256,1.9389,1.9524,1.9661,1.98,1.9941,2.0084,2.0229,2.0376,2.0525,2.0676,2.0829,2.0984,2.1141,2.13,2.1461,2.1624,2.1789,2.1956,2.2125,2.2296,2.2469,2.2644,2.2821,2.3,2.3181,2.3364,2.3549,2.3736,2.3925,2.4116,2.4309,2.4504,2.4701,2.49,2.5101,2.5304,2.5509,2.5716,2.5925,2.6136,2.6349,2.6564,2.6781,2.7,2.7221,2.7444,2.7669,2.7896,2.8125,2.8356,2.8589,2.8824,2.9061,2.93,2.9541,2.9784,3.0029,3.0276,3.0525,3.0776,3.1029,3.1284,3.1541,3.18,3.2061,3.2324,3.2589,3.2856,3.3125,3.3396,3.3669,3.3944,3.4221,3.45,3.4781,3.5064,3.5349,3.5636,3.5925,3.6216,3.6509,3.6804,3.7101,3.74,3.7701,3.8004,3.8309,3.8616,3.8925,3.9236,3.9549,3.9864,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 3.6500<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.49<br />value: 3.6021<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.48<br />value: 3.5544<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.47<br />value: 3.5069<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.46<br />value: 3.4596<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.45<br />value: 3.4125<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.44<br />value: 3.3656<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.43<br />value: 3.3189<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.42<br />value: 3.2724<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.41<br />value: 3.2261<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.40<br />value: 3.1800<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.39<br />value: 3.1341<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.38<br />value: 3.0884<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.37<br />value: 3.0429<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.36<br />value: 2.9976<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.35<br />value: 2.9525<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.34<br />value: 2.9076<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.33<br />value: 2.8629<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.32<br />value: 2.8184<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.31<br />value: 2.7741<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.30<br />value: 2.7300<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.29<br />value: 2.6861<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.28<br />value: 2.6424<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.27<br />value: 2.5989<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.26<br />value: 2.5556<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.25<br />value: 2.5125<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.24<br />value: 2.4696<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.23<br />value: 2.4269<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.22<br />value: 2.3844<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.21<br />value: 2.3421<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.20<br />value: 2.3000<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.19<br />value: 2.2581<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.18<br />value: 2.2164<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.17<br />value: 2.1749<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.16<br />value: 2.1336<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.15<br />value: 2.0925<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.14<br />value: 2.0516<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.13<br />value: 2.0109<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.12<br />value: 1.9704<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.11<br />value: 1.9301<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.10<br />value: 1.8900<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.09<br />value: 1.8501<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.08<br />value: 1.8104<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.07<br />value: 1.7709<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.06<br />value: 1.7316<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.05<br />value: 1.6925<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.04<br />value: 1.6536<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.03<br />value: 1.6149<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.02<br />value: 1.5764<br />Function: Loss + Penalty<br />Lambda: 1.8","b: -0.01<br />value: 1.5381<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.01<br />value: 1.4981<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.02<br />value: 1.4964<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.03<br />value: 1.4949<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.04<br />value: 1.4936<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.05<br />value: 1.4925<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.06<br />value: 1.4916<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.07<br />value: 1.4909<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.08<br />value: 1.4904<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.09<br />value: 1.4901<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.10<br />value: 1.4900<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.11<br />value: 1.4901<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.12<br />value: 1.4904<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.13<br />value: 1.4909<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.14<br />value: 1.4916<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.15<br />value: 1.4925<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.16<br />value: 1.4936<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.17<br />value: 1.4949<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.18<br />value: 1.4964<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.19<br />value: 1.4981<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.20<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.21<br />value: 1.5021<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.22<br />value: 1.5044<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.23<br />value: 1.5069<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.24<br />value: 1.5096<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.25<br />value: 1.5125<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.26<br />value: 1.5156<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.27<br />value: 1.5189<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.28<br />value: 1.5224<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.29<br />value: 1.5261<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.30<br />value: 1.5300<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.31<br />value: 1.5341<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.32<br />value: 1.5384<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.33<br />value: 1.5429<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.34<br />value: 1.5476<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.35<br />value: 1.5525<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.36<br />value: 1.5576<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.37<br />value: 1.5629<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.38<br />value: 1.5684<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.39<br />value: 1.5741<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.40<br />value: 1.5800<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.41<br />value: 1.5861<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.42<br />value: 1.5924<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.43<br />value: 1.5989<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.44<br />value: 1.6056<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.45<br />value: 1.6125<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.46<br />value: 1.6196<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.47<br />value: 1.6269<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.48<br />value: 1.6344<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.49<br />value: 1.6421<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.50<br />value: 1.6500<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.51<br />value: 1.6581<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.52<br />value: 1.6664<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.53<br />value: 1.6749<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.54<br />value: 1.6836<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.55<br />value: 1.6925<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.56<br />value: 1.7016<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.57<br />value: 1.7109<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.58<br />value: 1.7204<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.59<br />value: 1.7301<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.60<br />value: 1.7400<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.61<br />value: 1.7501<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.62<br />value: 1.7604<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.63<br />value: 1.7709<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.64<br />value: 1.7816<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.65<br />value: 1.7925<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.66<br />value: 1.8036<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.67<br />value: 1.8149<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.68<br />value: 1.8264<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.69<br />value: 1.8381<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.70<br />value: 1.8500<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.71<br />value: 1.8621<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.72<br />value: 1.8744<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.73<br />value: 1.8869<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.74<br />value: 1.8996<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.75<br />value: 1.9125<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.76<br />value: 1.9256<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.77<br />value: 1.9389<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.78<br />value: 1.9524<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.79<br />value: 1.9661<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.80<br />value: 1.9800<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.81<br />value: 1.9941<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.82<br />value: 2.0084<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.83<br />value: 2.0229<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.84<br />value: 2.0376<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.85<br />value: 2.0525<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.86<br />value: 2.0676<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.87<br />value: 2.0829<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.88<br />value: 2.0984<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.89<br />value: 2.1141<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.90<br />value: 2.1300<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.91<br />value: 2.1461<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.92<br />value: 2.1624<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.93<br />value: 2.1789<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.94<br />value: 2.1956<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.95<br />value: 2.2125<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.96<br />value: 2.2296<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.97<br />value: 2.2469<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.98<br />value: 2.2644<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  0.99<br />value: 2.2821<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.00<br />value: 2.3000<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.01<br />value: 2.3181<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.02<br />value: 2.3364<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.03<br />value: 2.3549<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.04<br />value: 2.3736<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.05<br />value: 2.3925<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.06<br />value: 2.4116<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.07<br />value: 2.4309<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.08<br />value: 2.4504<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.09<br />value: 2.4701<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.10<br />value: 2.4900<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.11<br />value: 2.5101<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.12<br />value: 2.5304<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.13<br />value: 2.5509<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.14<br />value: 2.5716<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.15<br />value: 2.5925<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.16<br />value: 2.6136<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.17<br />value: 2.6349<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.18<br />value: 2.6564<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.19<br />value: 2.6781<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.20<br />value: 2.7000<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.21<br />value: 2.7221<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.22<br />value: 2.7444<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.23<br />value: 2.7669<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.24<br />value: 2.7896<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.25<br />value: 2.8125<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.26<br />value: 2.8356<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.27<br />value: 2.8589<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.28<br />value: 2.8824<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.29<br />value: 2.9061<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.30<br />value: 2.9300<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.31<br />value: 2.9541<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.32<br />value: 2.9784<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.33<br />value: 3.0029<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.34<br />value: 3.0276<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.35<br />value: 3.0525<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.36<br />value: 3.0776<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.37<br />value: 3.1029<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.38<br />value: 3.1284<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.39<br />value: 3.1541<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.40<br />value: 3.1800<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.41<br />value: 3.2061<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.42<br />value: 3.2324<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.43<br />value: 3.2589<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.44<br />value: 3.2856<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.45<br />value: 3.3125<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.46<br />value: 3.3396<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.47<br />value: 3.3669<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.48<br />value: 3.3944<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.49<br />value: 3.4221<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.50<br />value: 3.4500<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.51<br />value: 3.4781<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.52<br />value: 3.5064<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.53<br />value: 3.5349<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.54<br />value: 3.5636<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.55<br />value: 3.5925<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.56<br />value: 3.6216<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.57<br />value: 3.6509<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.58<br />value: 3.6804<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.59<br />value: 3.7101<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.60<br />value: 3.7400<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.61<br />value: 3.7701<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.62<br />value: 3.8004<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.63<br />value: 3.8309<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.64<br />value: 3.8616<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.65<br />value: 3.8925<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.66<br />value: 3.9236<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.67<br />value: 3.9549<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.68<br />value: 3.9864<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.69<br />value: 4.0181<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.70<br />value: 4.0500<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.71<br />value: 4.0821<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.72<br />value: 4.1144<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.73<br />value: 4.1469<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.74<br />value: 4.1796<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.75<br />value: 4.2125<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.76<br />value: 4.2456<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.77<br />value: 4.2789<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.78<br />value: 4.3124<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.79<br />value: 4.3461<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.80<br />value: 4.3800<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.81<br />value: 4.4141<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.82<br />value: 4.4484<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.83<br />value: 4.4829<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.84<br />value: 4.5176<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.85<br />value: 4.5525<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.86<br />value: 4.5876<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.87<br />value: 4.6229<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.88<br />value: 4.6584<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.89<br />value: 4.6941<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.90<br />value: 4.7300<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.91<br />value: 4.7661<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.92<br />value: 4.8024<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.93<br />value: 4.8389<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.94<br />value: 4.8756<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.95<br />value: 4.9125<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.96<br />value: 4.9496<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.97<br />value: 4.9869<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.98<br />value: 5.0244<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  1.99<br />value: 5.0621<br />Function: Loss + Penalty<br />Lambda: 1.8","b:  2.00<br />value: 5.1000<br />Function: Loss + Penalty<br />Lambda: 1.8"],"frame":"1.8","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.9,0.882,0.864,0.846,0.828,0.81,0.792,0.774,0.756,0.738,0.72,0.702,0.684,0.666,0.648,0.63,0.612,0.594,0.576,0.558,0.54,0.522,0.504,0.486,0.468,0.45,0.432,0.414,0.396,0.378,0.36,0.342,0.324,0.306,0.288,0.27,0.252,0.234,0.216,0.198,0.18,0.162,0.144,0.126,0.108,0.09,0.072,0.054,0.036,0.018,0,0.018,0.036,0.054,0.0720000000000001,0.0900000000000001,0.108,0.126,0.144,0.162,0.18,0.198,0.216,0.234,0.252,0.27,0.288,0.306,0.324,0.342,0.36,0.378,0.396,0.414,0.432,0.45,0.468,0.486,0.504,0.522,0.54,0.558,0.576,0.594,0.612,0.63,0.648,0.666,0.684,0.702,0.72,0.738,0.756,0.774,0.792,0.81,0.828,0.846,0.864,0.882,0.9,0.918,0.936,0.954,0.972,0.99,1.008,1.026,1.044,1.062,1.08,1.098,1.116,1.134,1.152,1.17,1.188,1.206,1.224,1.242,1.26,1.278,1.296,1.314,1.332,1.35,1.368,1.386,1.404,1.422,1.44,1.458,1.476,1.494,1.512,1.53,1.548,1.566,1.584,1.602,1.62,1.638,1.656,1.674,1.692,1.71,1.728,1.746,1.764,1.782,1.8,1.818,1.836,1.854,1.872,1.89,1.908,1.926,1.944,1.962,1.98,1.998,2.016,2.034,2.052,2.07,2.088,2.106,2.124,2.142,2.16,2.178,2.196,2.214,2.232,2.25,2.268,2.286,2.304,2.322,2.34,2.358,2.376,2.394,2.412,2.43,2.448,2.466,2.484,2.502,2.52,2.538,2.556,2.574,2.592,2.61,2.628,2.646,2.664,2.682,2.7,2.718,2.736,2.754,2.772,2.79,2.808,2.826,2.844,2.862,2.88,2.898,2.916,2.934,2.952,2.97,2.988,3.006,3.024,3.042,3.06,3.078,3.096,3.114,3.132,3.15,3.168,3.186,3.204,3.222,3.24,3.258,3.276,3.294,3.312,3.33,3.348,3.366,3.384,3.402,3.42,3.438,3.456,3.474,3.492,3.51,3.528,3.546,3.564,3.582,3.6],"text":["b: -0.50<br />value: 0.9000<br />Function: Penalty<br />Lambda: 1.8","b: -0.49<br />value: 0.8820<br />Function: Penalty<br />Lambda: 1.8","b: -0.48<br />value: 0.8640<br />Function: Penalty<br />Lambda: 1.8","b: -0.47<br />value: 0.8460<br />Function: Penalty<br />Lambda: 1.8","b: -0.46<br />value: 0.8280<br />Function: Penalty<br />Lambda: 1.8","b: -0.45<br />value: 0.8100<br />Function: Penalty<br />Lambda: 1.8","b: -0.44<br />value: 0.7920<br />Function: Penalty<br />Lambda: 1.8","b: -0.43<br />value: 0.7740<br />Function: Penalty<br />Lambda: 1.8","b: -0.42<br />value: 0.7560<br />Function: Penalty<br />Lambda: 1.8","b: -0.41<br />value: 0.7380<br />Function: Penalty<br />Lambda: 1.8","b: -0.40<br />value: 0.7200<br />Function: Penalty<br />Lambda: 1.8","b: -0.39<br />value: 0.7020<br />Function: Penalty<br />Lambda: 1.8","b: -0.38<br />value: 0.6840<br />Function: Penalty<br />Lambda: 1.8","b: -0.37<br />value: 0.6660<br />Function: Penalty<br />Lambda: 1.8","b: -0.36<br />value: 0.6480<br />Function: Penalty<br />Lambda: 1.8","b: -0.35<br />value: 0.6300<br />Function: Penalty<br />Lambda: 1.8","b: -0.34<br />value: 0.6120<br />Function: Penalty<br />Lambda: 1.8","b: -0.33<br />value: 0.5940<br />Function: Penalty<br />Lambda: 1.8","b: -0.32<br />value: 0.5760<br />Function: Penalty<br />Lambda: 1.8","b: -0.31<br />value: 0.5580<br />Function: Penalty<br />Lambda: 1.8","b: -0.30<br />value: 0.5400<br />Function: Penalty<br />Lambda: 1.8","b: -0.29<br />value: 0.5220<br />Function: Penalty<br />Lambda: 1.8","b: -0.28<br />value: 0.5040<br />Function: Penalty<br />Lambda: 1.8","b: -0.27<br />value: 0.4860<br />Function: Penalty<br />Lambda: 1.8","b: -0.26<br />value: 0.4680<br />Function: Penalty<br />Lambda: 1.8","b: -0.25<br />value: 0.4500<br />Function: Penalty<br />Lambda: 1.8","b: -0.24<br />value: 0.4320<br />Function: Penalty<br />Lambda: 1.8","b: -0.23<br />value: 0.4140<br />Function: Penalty<br />Lambda: 1.8","b: -0.22<br />value: 0.3960<br />Function: Penalty<br />Lambda: 1.8","b: -0.21<br />value: 0.3780<br />Function: Penalty<br />Lambda: 1.8","b: -0.20<br />value: 0.3600<br />Function: Penalty<br />Lambda: 1.8","b: -0.19<br />value: 0.3420<br />Function: Penalty<br />Lambda: 1.8","b: -0.18<br />value: 0.3240<br />Function: Penalty<br />Lambda: 1.8","b: -0.17<br />value: 0.3060<br />Function: Penalty<br />Lambda: 1.8","b: -0.16<br />value: 0.2880<br />Function: Penalty<br />Lambda: 1.8","b: -0.15<br />value: 0.2700<br />Function: Penalty<br />Lambda: 1.8","b: -0.14<br />value: 0.2520<br />Function: Penalty<br />Lambda: 1.8","b: -0.13<br />value: 0.2340<br />Function: Penalty<br />Lambda: 1.8","b: -0.12<br />value: 0.2160<br />Function: Penalty<br />Lambda: 1.8","b: -0.11<br />value: 0.1980<br />Function: Penalty<br />Lambda: 1.8","b: -0.10<br />value: 0.1800<br />Function: Penalty<br />Lambda: 1.8","b: -0.09<br />value: 0.1620<br />Function: Penalty<br />Lambda: 1.8","b: -0.08<br />value: 0.1440<br />Function: Penalty<br />Lambda: 1.8","b: -0.07<br />value: 0.1260<br />Function: Penalty<br />Lambda: 1.8","b: -0.06<br />value: 0.1080<br />Function: Penalty<br />Lambda: 1.8","b: -0.05<br />value: 0.0900<br />Function: Penalty<br />Lambda: 1.8","b: -0.04<br />value: 0.0720<br />Function: Penalty<br />Lambda: 1.8","b: -0.03<br />value: 0.0540<br />Function: Penalty<br />Lambda: 1.8","b: -0.02<br />value: 0.0360<br />Function: Penalty<br />Lambda: 1.8","b: -0.01<br />value: 0.0180<br />Function: Penalty<br />Lambda: 1.8","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 1.8","b:  0.01<br />value: 0.0180<br />Function: Penalty<br />Lambda: 1.8","b:  0.02<br />value: 0.0360<br />Function: Penalty<br />Lambda: 1.8","b:  0.03<br />value: 0.0540<br />Function: Penalty<br />Lambda: 1.8","b:  0.04<br />value: 0.0720<br />Function: Penalty<br />Lambda: 1.8","b:  0.05<br />value: 0.0900<br />Function: Penalty<br />Lambda: 1.8","b:  0.06<br />value: 0.1080<br />Function: Penalty<br />Lambda: 1.8","b:  0.07<br />value: 0.1260<br />Function: Penalty<br />Lambda: 1.8","b:  0.08<br />value: 0.1440<br />Function: Penalty<br />Lambda: 1.8","b:  0.09<br />value: 0.1620<br />Function: Penalty<br />Lambda: 1.8","b:  0.10<br />value: 0.1800<br />Function: Penalty<br />Lambda: 1.8","b:  0.11<br />value: 0.1980<br />Function: Penalty<br />Lambda: 1.8","b:  0.12<br />value: 0.2160<br />Function: Penalty<br />Lambda: 1.8","b:  0.13<br />value: 0.2340<br />Function: Penalty<br />Lambda: 1.8","b:  0.14<br />value: 0.2520<br />Function: Penalty<br />Lambda: 1.8","b:  0.15<br />value: 0.2700<br />Function: Penalty<br />Lambda: 1.8","b:  0.16<br />value: 0.2880<br />Function: Penalty<br />Lambda: 1.8","b:  0.17<br />value: 0.3060<br />Function: Penalty<br />Lambda: 1.8","b:  0.18<br />value: 0.3240<br />Function: Penalty<br />Lambda: 1.8","b:  0.19<br />value: 0.3420<br />Function: Penalty<br />Lambda: 1.8","b:  0.20<br />value: 0.3600<br />Function: Penalty<br />Lambda: 1.8","b:  0.21<br />value: 0.3780<br />Function: Penalty<br />Lambda: 1.8","b:  0.22<br />value: 0.3960<br />Function: Penalty<br />Lambda: 1.8","b:  0.23<br />value: 0.4140<br />Function: Penalty<br />Lambda: 1.8","b:  0.24<br />value: 0.4320<br />Function: Penalty<br />Lambda: 1.8","b:  0.25<br />value: 0.4500<br />Function: Penalty<br />Lambda: 1.8","b:  0.26<br />value: 0.4680<br />Function: Penalty<br />Lambda: 1.8","b:  0.27<br />value: 0.4860<br />Function: Penalty<br />Lambda: 1.8","b:  0.28<br />value: 0.5040<br />Function: Penalty<br />Lambda: 1.8","b:  0.29<br />value: 0.5220<br />Function: Penalty<br />Lambda: 1.8","b:  0.30<br />value: 0.5400<br />Function: Penalty<br />Lambda: 1.8","b:  0.31<br />value: 0.5580<br />Function: Penalty<br />Lambda: 1.8","b:  0.32<br />value: 0.5760<br />Function: Penalty<br />Lambda: 1.8","b:  0.33<br />value: 0.5940<br />Function: Penalty<br />Lambda: 1.8","b:  0.34<br />value: 0.6120<br />Function: Penalty<br />Lambda: 1.8","b:  0.35<br />value: 0.6300<br />Function: Penalty<br />Lambda: 1.8","b:  0.36<br />value: 0.6480<br />Function: Penalty<br />Lambda: 1.8","b:  0.37<br />value: 0.6660<br />Function: Penalty<br />Lambda: 1.8","b:  0.38<br />value: 0.6840<br />Function: Penalty<br />Lambda: 1.8","b:  0.39<br />value: 0.7020<br />Function: Penalty<br />Lambda: 1.8","b:  0.40<br />value: 0.7200<br />Function: Penalty<br />Lambda: 1.8","b:  0.41<br />value: 0.7380<br />Function: Penalty<br />Lambda: 1.8","b:  0.42<br />value: 0.7560<br />Function: Penalty<br />Lambda: 1.8","b:  0.43<br />value: 0.7740<br />Function: Penalty<br />Lambda: 1.8","b:  0.44<br />value: 0.7920<br />Function: Penalty<br />Lambda: 1.8","b:  0.45<br />value: 0.8100<br />Function: Penalty<br />Lambda: 1.8","b:  0.46<br />value: 0.8280<br />Function: Penalty<br />Lambda: 1.8","b:  0.47<br />value: 0.8460<br />Function: Penalty<br />Lambda: 1.8","b:  0.48<br />value: 0.8640<br />Function: Penalty<br />Lambda: 1.8","b:  0.49<br />value: 0.8820<br />Function: Penalty<br />Lambda: 1.8","b:  0.50<br />value: 0.9000<br />Function: Penalty<br />Lambda: 1.8","b:  0.51<br />value: 0.9180<br />Function: Penalty<br />Lambda: 1.8","b:  0.52<br />value: 0.9360<br />Function: Penalty<br />Lambda: 1.8","b:  0.53<br />value: 0.9540<br />Function: Penalty<br />Lambda: 1.8","b:  0.54<br />value: 0.9720<br />Function: Penalty<br />Lambda: 1.8","b:  0.55<br />value: 0.9900<br />Function: Penalty<br />Lambda: 1.8","b:  0.56<br />value: 1.0080<br />Function: Penalty<br />Lambda: 1.8","b:  0.57<br />value: 1.0260<br />Function: Penalty<br />Lambda: 1.8","b:  0.58<br />value: 1.0440<br />Function: Penalty<br />Lambda: 1.8","b:  0.59<br />value: 1.0620<br />Function: Penalty<br />Lambda: 1.8","b:  0.60<br />value: 1.0800<br />Function: Penalty<br />Lambda: 1.8","b:  0.61<br />value: 1.0980<br />Function: Penalty<br />Lambda: 1.8","b:  0.62<br />value: 1.1160<br />Function: Penalty<br />Lambda: 1.8","b:  0.63<br />value: 1.1340<br />Function: Penalty<br />Lambda: 1.8","b:  0.64<br />value: 1.1520<br />Function: Penalty<br />Lambda: 1.8","b:  0.65<br />value: 1.1700<br />Function: Penalty<br />Lambda: 1.8","b:  0.66<br />value: 1.1880<br />Function: Penalty<br />Lambda: 1.8","b:  0.67<br />value: 1.2060<br />Function: Penalty<br />Lambda: 1.8","b:  0.68<br />value: 1.2240<br />Function: Penalty<br />Lambda: 1.8","b:  0.69<br />value: 1.2420<br />Function: Penalty<br />Lambda: 1.8","b:  0.70<br />value: 1.2600<br />Function: Penalty<br />Lambda: 1.8","b:  0.71<br />value: 1.2780<br />Function: Penalty<br />Lambda: 1.8","b:  0.72<br />value: 1.2960<br />Function: Penalty<br />Lambda: 1.8","b:  0.73<br />value: 1.3140<br />Function: Penalty<br />Lambda: 1.8","b:  0.74<br />value: 1.3320<br />Function: Penalty<br />Lambda: 1.8","b:  0.75<br />value: 1.3500<br />Function: Penalty<br />Lambda: 1.8","b:  0.76<br />value: 1.3680<br />Function: Penalty<br />Lambda: 1.8","b:  0.77<br />value: 1.3860<br />Function: Penalty<br />Lambda: 1.8","b:  0.78<br />value: 1.4040<br />Function: Penalty<br />Lambda: 1.8","b:  0.79<br />value: 1.4220<br />Function: Penalty<br />Lambda: 1.8","b:  0.80<br />value: 1.4400<br />Function: Penalty<br />Lambda: 1.8","b:  0.81<br />value: 1.4580<br />Function: Penalty<br />Lambda: 1.8","b:  0.82<br />value: 1.4760<br />Function: Penalty<br />Lambda: 1.8","b:  0.83<br />value: 1.4940<br />Function: Penalty<br />Lambda: 1.8","b:  0.84<br />value: 1.5120<br />Function: Penalty<br />Lambda: 1.8","b:  0.85<br />value: 1.5300<br />Function: Penalty<br />Lambda: 1.8","b:  0.86<br />value: 1.5480<br />Function: Penalty<br />Lambda: 1.8","b:  0.87<br />value: 1.5660<br />Function: Penalty<br />Lambda: 1.8","b:  0.88<br />value: 1.5840<br />Function: Penalty<br />Lambda: 1.8","b:  0.89<br />value: 1.6020<br />Function: Penalty<br />Lambda: 1.8","b:  0.90<br />value: 1.6200<br />Function: Penalty<br />Lambda: 1.8","b:  0.91<br />value: 1.6380<br />Function: Penalty<br />Lambda: 1.8","b:  0.92<br />value: 1.6560<br />Function: Penalty<br />Lambda: 1.8","b:  0.93<br />value: 1.6740<br />Function: Penalty<br />Lambda: 1.8","b:  0.94<br />value: 1.6920<br />Function: Penalty<br />Lambda: 1.8","b:  0.95<br />value: 1.7100<br />Function: Penalty<br />Lambda: 1.8","b:  0.96<br />value: 1.7280<br />Function: Penalty<br />Lambda: 1.8","b:  0.97<br />value: 1.7460<br />Function: Penalty<br />Lambda: 1.8","b:  0.98<br />value: 1.7640<br />Function: Penalty<br />Lambda: 1.8","b:  0.99<br />value: 1.7820<br />Function: Penalty<br />Lambda: 1.8","b:  1.00<br />value: 1.8000<br />Function: Penalty<br />Lambda: 1.8","b:  1.01<br />value: 1.8180<br />Function: Penalty<br />Lambda: 1.8","b:  1.02<br />value: 1.8360<br />Function: Penalty<br />Lambda: 1.8","b:  1.03<br />value: 1.8540<br />Function: Penalty<br />Lambda: 1.8","b:  1.04<br />value: 1.8720<br />Function: Penalty<br />Lambda: 1.8","b:  1.05<br />value: 1.8900<br />Function: Penalty<br />Lambda: 1.8","b:  1.06<br />value: 1.9080<br />Function: Penalty<br />Lambda: 1.8","b:  1.07<br />value: 1.9260<br />Function: Penalty<br />Lambda: 1.8","b:  1.08<br />value: 1.9440<br />Function: Penalty<br />Lambda: 1.8","b:  1.09<br />value: 1.9620<br />Function: Penalty<br />Lambda: 1.8","b:  1.10<br />value: 1.9800<br />Function: Penalty<br />Lambda: 1.8","b:  1.11<br />value: 1.9980<br />Function: Penalty<br />Lambda: 1.8","b:  1.12<br />value: 2.0160<br />Function: Penalty<br />Lambda: 1.8","b:  1.13<br />value: 2.0340<br />Function: Penalty<br />Lambda: 1.8","b:  1.14<br />value: 2.0520<br />Function: Penalty<br />Lambda: 1.8","b:  1.15<br />value: 2.0700<br />Function: Penalty<br />Lambda: 1.8","b:  1.16<br />value: 2.0880<br />Function: Penalty<br />Lambda: 1.8","b:  1.17<br />value: 2.1060<br />Function: Penalty<br />Lambda: 1.8","b:  1.18<br />value: 2.1240<br />Function: Penalty<br />Lambda: 1.8","b:  1.19<br />value: 2.1420<br />Function: Penalty<br />Lambda: 1.8","b:  1.20<br />value: 2.1600<br />Function: Penalty<br />Lambda: 1.8","b:  1.21<br />value: 2.1780<br />Function: Penalty<br />Lambda: 1.8","b:  1.22<br />value: 2.1960<br />Function: Penalty<br />Lambda: 1.8","b:  1.23<br />value: 2.2140<br />Function: Penalty<br />Lambda: 1.8","b:  1.24<br />value: 2.2320<br />Function: Penalty<br />Lambda: 1.8","b:  1.25<br />value: 2.2500<br />Function: Penalty<br />Lambda: 1.8","b:  1.26<br />value: 2.2680<br />Function: Penalty<br />Lambda: 1.8","b:  1.27<br />value: 2.2860<br />Function: Penalty<br />Lambda: 1.8","b:  1.28<br />value: 2.3040<br />Function: Penalty<br />Lambda: 1.8","b:  1.29<br />value: 2.3220<br />Function: Penalty<br />Lambda: 1.8","b:  1.30<br />value: 2.3400<br />Function: Penalty<br />Lambda: 1.8","b:  1.31<br />value: 2.3580<br />Function: Penalty<br />Lambda: 1.8","b:  1.32<br />value: 2.3760<br />Function: Penalty<br />Lambda: 1.8","b:  1.33<br />value: 2.3940<br />Function: Penalty<br />Lambda: 1.8","b:  1.34<br />value: 2.4120<br />Function: Penalty<br />Lambda: 1.8","b:  1.35<br />value: 2.4300<br />Function: Penalty<br />Lambda: 1.8","b:  1.36<br />value: 2.4480<br />Function: Penalty<br />Lambda: 1.8","b:  1.37<br />value: 2.4660<br />Function: Penalty<br />Lambda: 1.8","b:  1.38<br />value: 2.4840<br />Function: Penalty<br />Lambda: 1.8","b:  1.39<br />value: 2.5020<br />Function: Penalty<br />Lambda: 1.8","b:  1.40<br />value: 2.5200<br />Function: Penalty<br />Lambda: 1.8","b:  1.41<br />value: 2.5380<br />Function: Penalty<br />Lambda: 1.8","b:  1.42<br />value: 2.5560<br />Function: Penalty<br />Lambda: 1.8","b:  1.43<br />value: 2.5740<br />Function: Penalty<br />Lambda: 1.8","b:  1.44<br />value: 2.5920<br />Function: Penalty<br />Lambda: 1.8","b:  1.45<br />value: 2.6100<br />Function: Penalty<br />Lambda: 1.8","b:  1.46<br />value: 2.6280<br />Function: Penalty<br />Lambda: 1.8","b:  1.47<br />value: 2.6460<br />Function: Penalty<br />Lambda: 1.8","b:  1.48<br />value: 2.6640<br />Function: Penalty<br />Lambda: 1.8","b:  1.49<br />value: 2.6820<br />Function: Penalty<br />Lambda: 1.8","b:  1.50<br />value: 2.7000<br />Function: Penalty<br />Lambda: 1.8","b:  1.51<br />value: 2.7180<br />Function: Penalty<br />Lambda: 1.8","b:  1.52<br />value: 2.7360<br />Function: Penalty<br />Lambda: 1.8","b:  1.53<br />value: 2.7540<br />Function: Penalty<br />Lambda: 1.8","b:  1.54<br />value: 2.7720<br />Function: Penalty<br />Lambda: 1.8","b:  1.55<br />value: 2.7900<br />Function: Penalty<br />Lambda: 1.8","b:  1.56<br />value: 2.8080<br />Function: Penalty<br />Lambda: 1.8","b:  1.57<br />value: 2.8260<br />Function: Penalty<br />Lambda: 1.8","b:  1.58<br />value: 2.8440<br />Function: Penalty<br />Lambda: 1.8","b:  1.59<br />value: 2.8620<br />Function: Penalty<br />Lambda: 1.8","b:  1.60<br />value: 2.8800<br />Function: Penalty<br />Lambda: 1.8","b:  1.61<br />value: 2.8980<br />Function: Penalty<br />Lambda: 1.8","b:  1.62<br />value: 2.9160<br />Function: Penalty<br />Lambda: 1.8","b:  1.63<br />value: 2.9340<br />Function: Penalty<br />Lambda: 1.8","b:  1.64<br />value: 2.9520<br />Function: Penalty<br />Lambda: 1.8","b:  1.65<br />value: 2.9700<br />Function: Penalty<br />Lambda: 1.8","b:  1.66<br />value: 2.9880<br />Function: Penalty<br />Lambda: 1.8","b:  1.67<br />value: 3.0060<br />Function: Penalty<br />Lambda: 1.8","b:  1.68<br />value: 3.0240<br />Function: Penalty<br />Lambda: 1.8","b:  1.69<br />value: 3.0420<br />Function: Penalty<br />Lambda: 1.8","b:  1.70<br />value: 3.0600<br />Function: Penalty<br />Lambda: 1.8","b:  1.71<br />value: 3.0780<br />Function: Penalty<br />Lambda: 1.8","b:  1.72<br />value: 3.0960<br />Function: Penalty<br />Lambda: 1.8","b:  1.73<br />value: 3.1140<br />Function: Penalty<br />Lambda: 1.8","b:  1.74<br />value: 3.1320<br />Function: Penalty<br />Lambda: 1.8","b:  1.75<br />value: 3.1500<br />Function: Penalty<br />Lambda: 1.8","b:  1.76<br />value: 3.1680<br />Function: Penalty<br />Lambda: 1.8","b:  1.77<br />value: 3.1860<br />Function: Penalty<br />Lambda: 1.8","b:  1.78<br />value: 3.2040<br />Function: Penalty<br />Lambda: 1.8","b:  1.79<br />value: 3.2220<br />Function: Penalty<br />Lambda: 1.8","b:  1.80<br />value: 3.2400<br />Function: Penalty<br />Lambda: 1.8","b:  1.81<br />value: 3.2580<br />Function: Penalty<br />Lambda: 1.8","b:  1.82<br />value: 3.2760<br />Function: Penalty<br />Lambda: 1.8","b:  1.83<br />value: 3.2940<br />Function: Penalty<br />Lambda: 1.8","b:  1.84<br />value: 3.3120<br />Function: Penalty<br />Lambda: 1.8","b:  1.85<br />value: 3.3300<br />Function: Penalty<br />Lambda: 1.8","b:  1.86<br />value: 3.3480<br />Function: Penalty<br />Lambda: 1.8","b:  1.87<br />value: 3.3660<br />Function: Penalty<br />Lambda: 1.8","b:  1.88<br />value: 3.3840<br />Function: Penalty<br />Lambda: 1.8","b:  1.89<br />value: 3.4020<br />Function: Penalty<br />Lambda: 1.8","b:  1.90<br />value: 3.4200<br />Function: Penalty<br />Lambda: 1.8","b:  1.91<br />value: 3.4380<br />Function: Penalty<br />Lambda: 1.8","b:  1.92<br />value: 3.4560<br />Function: Penalty<br />Lambda: 1.8","b:  1.93<br />value: 3.4740<br />Function: Penalty<br />Lambda: 1.8","b:  1.94<br />value: 3.4920<br />Function: Penalty<br />Lambda: 1.8","b:  1.95<br />value: 3.5100<br />Function: Penalty<br />Lambda: 1.8","b:  1.96<br />value: 3.5280<br />Function: Penalty<br />Lambda: 1.8","b:  1.97<br />value: 3.5460<br />Function: Penalty<br />Lambda: 1.8","b:  1.98<br />value: 3.5640<br />Function: Penalty<br />Lambda: 1.8","b:  1.99<br />value: 3.5820<br />Function: Penalty<br />Lambda: 1.8","b:  2.00<br />value: 3.6000<br />Function: Penalty<br />Lambda: 1.8"],"frame":"1.8","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"1.9","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 1.9","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 1.9","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 1.9","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 1.9","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 1.9","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 1.9","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 1.9","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 1.9","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 1.9","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 1.9","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 1.9","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 1.9","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 1.9","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 1.9","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 1.9","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 1.9","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 1.9","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 1.9","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 1.9","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 1.9","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 1.9","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 1.9","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 1.9","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 1.9","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 1.9","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 1.9","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 1.9","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 1.9","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 1.9","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 1.9","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 1.9","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 1.9","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 1.9","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 1.9","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 1.9","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 1.9","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 1.9","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 1.9","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 1.9","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 1.9","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 1.9","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 1.9","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 1.9","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 1.9","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 1.9","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 1.9","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 1.9","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 1.9","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 1.9","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 1.9","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.9","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.9","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.9","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.9","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.9","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.9","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.9","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.9","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.9","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.9","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.9","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.9","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.9","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.9","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.9","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.9","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.9","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.9","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.9","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.9","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.9","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.9","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.9","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.9","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.9","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.9","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.9","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.9","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.9","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.9","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.9","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.9","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.9","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.9","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.9","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.9","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.9","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.9","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.9","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.9","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.9","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.9","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.9","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.9","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.9","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.9","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.9","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.9","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.9","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.9","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.9","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.9","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.9","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.9","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.9","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.9","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.9","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.9","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.9","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.9","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.9","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.9","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.9","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.9","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.9","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.9","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.9","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.9","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.9","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.9","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.9","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.9","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.9","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.9","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.9","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.9","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.9","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.9","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.9","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.9","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.9","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.9","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.9","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.9","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.9","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.9","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.9","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.9","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.9","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.9","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.9","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.9","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.9","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.9","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.9","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.9","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.9","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.9","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.9","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.9","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 1.9","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 1.9","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 1.9","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 1.9","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 1.9","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 1.9","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 1.9","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 1.9","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 1.9","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 1.9","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 1.9","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 1.9","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 1.9","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 1.9","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 1.9","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 1.9","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 1.9","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 1.9","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 1.9","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 1.9","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 1.9","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 1.9","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 1.9","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 1.9","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 1.9","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 1.9","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 1.9","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 1.9","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 1.9","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 1.9","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 1.9","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 1.9","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 1.9","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 1.9","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 1.9","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 1.9","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 1.9","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 1.9","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 1.9","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 1.9","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 1.9","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 1.9","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 1.9","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 1.9","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 1.9","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 1.9","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 1.9","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 1.9","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 1.9","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 1.9","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 1.9","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 1.9","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 1.9","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 1.9","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 1.9","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 1.9","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 1.9","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 1.9","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 1.9","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 1.9","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 1.9","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 1.9","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 1.9","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 1.9","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 1.9","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 1.9","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 1.9","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 1.9","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 1.9","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 1.9","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 1.9","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 1.9","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 1.9","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 1.9","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 1.9","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 1.9","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 1.9","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 1.9","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 1.9","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 1.9","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 1.9","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 1.9","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 1.9","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 1.9","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 1.9","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 1.9","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 1.9","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 1.9","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 1.9","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 1.9","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 1.9","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 1.9","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 1.9","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 1.9","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 1.9","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 1.9","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 1.9","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 1.9","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 1.9","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 1.9","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 1.9"],"frame":"1.9","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.7,3.6511,3.6024,3.5539,3.5056,3.4575,3.4096,3.3619,3.3144,3.2671,3.22,3.1731,3.1264,3.0799,3.0336,2.9875,2.9416,2.8959,2.8504,2.8051,2.76,2.7151,2.6704,2.6259,2.5816,2.5375,2.4936,2.4499,2.4064,2.3631,2.32,2.2771,2.2344,2.1919,2.1496,2.1075,2.0656,2.0239,1.9824,1.9411,1.9,1.8591,1.8184,1.7779,1.7376,1.6975,1.6576,1.6179,1.5784,1.5391,1.5,1.4991,1.4984,1.4979,1.4976,1.4975,1.4976,1.4979,1.4984,1.4991,1.5,1.5011,1.5024,1.5039,1.5056,1.5075,1.5096,1.5119,1.5144,1.5171,1.52,1.5231,1.5264,1.5299,1.5336,1.5375,1.5416,1.5459,1.5504,1.5551,1.56,1.5651,1.5704,1.5759,1.5816,1.5875,1.5936,1.5999,1.6064,1.6131,1.62,1.6271,1.6344,1.6419,1.6496,1.6575,1.6656,1.6739,1.6824,1.6911,1.7,1.7091,1.7184,1.7279,1.7376,1.7475,1.7576,1.7679,1.7784,1.7891,1.8,1.8111,1.8224,1.8339,1.8456,1.8575,1.8696,1.8819,1.8944,1.9071,1.92,1.9331,1.9464,1.9599,1.9736,1.9875,2.0016,2.0159,2.0304,2.0451,2.06,2.0751,2.0904,2.1059,2.1216,2.1375,2.1536,2.1699,2.1864,2.2031,2.22,2.2371,2.2544,2.2719,2.2896,2.3075,2.3256,2.3439,2.3624,2.3811,2.4,2.4191,2.4384,2.4579,2.4776,2.4975,2.5176,2.5379,2.5584,2.5791,2.6,2.6211,2.6424,2.6639,2.6856,2.7075,2.7296,2.7519,2.7744,2.7971,2.82,2.8431,2.8664,2.8899,2.9136,2.9375,2.9616,2.9859,3.0104,3.0351,3.06,3.0851,3.1104,3.1359,3.1616,3.1875,3.2136,3.2399,3.2664,3.2931,3.32,3.3471,3.3744,3.4019,3.4296,3.4575,3.4856,3.5139,3.5424,3.5711,3.6,3.6291,3.6584,3.6879,3.7176,3.7475,3.7776,3.8079,3.8384,3.8691,3.9,3.9311,3.9624,3.9939,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 3.7000<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.49<br />value: 3.6511<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.48<br />value: 3.6024<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.47<br />value: 3.5539<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.46<br />value: 3.5056<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.45<br />value: 3.4575<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.44<br />value: 3.4096<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.43<br />value: 3.3619<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.42<br />value: 3.3144<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.41<br />value: 3.2671<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.40<br />value: 3.2200<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.39<br />value: 3.1731<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.38<br />value: 3.1264<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.37<br />value: 3.0799<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.36<br />value: 3.0336<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.35<br />value: 2.9875<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.34<br />value: 2.9416<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.33<br />value: 2.8959<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.32<br />value: 2.8504<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.31<br />value: 2.8051<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.30<br />value: 2.7600<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.29<br />value: 2.7151<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.28<br />value: 2.6704<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.27<br />value: 2.6259<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.26<br />value: 2.5816<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.25<br />value: 2.5375<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.24<br />value: 2.4936<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.23<br />value: 2.4499<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.22<br />value: 2.4064<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.21<br />value: 2.3631<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.20<br />value: 2.3200<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.19<br />value: 2.2771<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.18<br />value: 2.2344<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.17<br />value: 2.1919<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.16<br />value: 2.1496<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.15<br />value: 2.1075<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.14<br />value: 2.0656<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.13<br />value: 2.0239<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.12<br />value: 1.9824<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.11<br />value: 1.9411<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.10<br />value: 1.9000<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.09<br />value: 1.8591<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.08<br />value: 1.8184<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.07<br />value: 1.7779<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.06<br />value: 1.7376<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.05<br />value: 1.6975<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.04<br />value: 1.6576<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.03<br />value: 1.6179<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.02<br />value: 1.5784<br />Function: Loss + Penalty<br />Lambda: 1.9","b: -0.01<br />value: 1.5391<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.01<br />value: 1.4991<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.02<br />value: 1.4984<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.03<br />value: 1.4979<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.04<br />value: 1.4976<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.05<br />value: 1.4975<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.06<br />value: 1.4976<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.07<br />value: 1.4979<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.08<br />value: 1.4984<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.09<br />value: 1.4991<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.10<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.11<br />value: 1.5011<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.12<br />value: 1.5024<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.13<br />value: 1.5039<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.14<br />value: 1.5056<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.15<br />value: 1.5075<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.16<br />value: 1.5096<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.17<br />value: 1.5119<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.18<br />value: 1.5144<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.19<br />value: 1.5171<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.20<br />value: 1.5200<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.21<br />value: 1.5231<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.22<br />value: 1.5264<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.23<br />value: 1.5299<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.24<br />value: 1.5336<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.25<br />value: 1.5375<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.26<br />value: 1.5416<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.27<br />value: 1.5459<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.28<br />value: 1.5504<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.29<br />value: 1.5551<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.30<br />value: 1.5600<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.31<br />value: 1.5651<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.32<br />value: 1.5704<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.33<br />value: 1.5759<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.34<br />value: 1.5816<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.35<br />value: 1.5875<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.36<br />value: 1.5936<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.37<br />value: 1.5999<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.38<br />value: 1.6064<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.39<br />value: 1.6131<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.40<br />value: 1.6200<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.41<br />value: 1.6271<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.42<br />value: 1.6344<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.43<br />value: 1.6419<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.44<br />value: 1.6496<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.45<br />value: 1.6575<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.46<br />value: 1.6656<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.47<br />value: 1.6739<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.48<br />value: 1.6824<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.49<br />value: 1.6911<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.50<br />value: 1.7000<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.51<br />value: 1.7091<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.52<br />value: 1.7184<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.53<br />value: 1.7279<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.54<br />value: 1.7376<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.55<br />value: 1.7475<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.56<br />value: 1.7576<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.57<br />value: 1.7679<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.58<br />value: 1.7784<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.59<br />value: 1.7891<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.60<br />value: 1.8000<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.61<br />value: 1.8111<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.62<br />value: 1.8224<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.63<br />value: 1.8339<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.64<br />value: 1.8456<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.65<br />value: 1.8575<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.66<br />value: 1.8696<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.67<br />value: 1.8819<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.68<br />value: 1.8944<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.69<br />value: 1.9071<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.70<br />value: 1.9200<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.71<br />value: 1.9331<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.72<br />value: 1.9464<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.73<br />value: 1.9599<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.74<br />value: 1.9736<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.75<br />value: 1.9875<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.76<br />value: 2.0016<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.77<br />value: 2.0159<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.78<br />value: 2.0304<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.79<br />value: 2.0451<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.80<br />value: 2.0600<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.81<br />value: 2.0751<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.82<br />value: 2.0904<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.83<br />value: 2.1059<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.84<br />value: 2.1216<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.85<br />value: 2.1375<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.86<br />value: 2.1536<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.87<br />value: 2.1699<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.88<br />value: 2.1864<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.89<br />value: 2.2031<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.90<br />value: 2.2200<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.91<br />value: 2.2371<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.92<br />value: 2.2544<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.93<br />value: 2.2719<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.94<br />value: 2.2896<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.95<br />value: 2.3075<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.96<br />value: 2.3256<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.97<br />value: 2.3439<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.98<br />value: 2.3624<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  0.99<br />value: 2.3811<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.00<br />value: 2.4000<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.01<br />value: 2.4191<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.02<br />value: 2.4384<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.03<br />value: 2.4579<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.04<br />value: 2.4776<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.05<br />value: 2.4975<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.06<br />value: 2.5176<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.07<br />value: 2.5379<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.08<br />value: 2.5584<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.09<br />value: 2.5791<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.10<br />value: 2.6000<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.11<br />value: 2.6211<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.12<br />value: 2.6424<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.13<br />value: 2.6639<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.14<br />value: 2.6856<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.15<br />value: 2.7075<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.16<br />value: 2.7296<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.17<br />value: 2.7519<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.18<br />value: 2.7744<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.19<br />value: 2.7971<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.20<br />value: 2.8200<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.21<br />value: 2.8431<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.22<br />value: 2.8664<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.23<br />value: 2.8899<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.24<br />value: 2.9136<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.25<br />value: 2.9375<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.26<br />value: 2.9616<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.27<br />value: 2.9859<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.28<br />value: 3.0104<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.29<br />value: 3.0351<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.30<br />value: 3.0600<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.31<br />value: 3.0851<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.32<br />value: 3.1104<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.33<br />value: 3.1359<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.34<br />value: 3.1616<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.35<br />value: 3.1875<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.36<br />value: 3.2136<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.37<br />value: 3.2399<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.38<br />value: 3.2664<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.39<br />value: 3.2931<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.40<br />value: 3.3200<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.41<br />value: 3.3471<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.42<br />value: 3.3744<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.43<br />value: 3.4019<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.44<br />value: 3.4296<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.45<br />value: 3.4575<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.46<br />value: 3.4856<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.47<br />value: 3.5139<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.48<br />value: 3.5424<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.49<br />value: 3.5711<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.50<br />value: 3.6000<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.51<br />value: 3.6291<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.52<br />value: 3.6584<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.53<br />value: 3.6879<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.54<br />value: 3.7176<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.55<br />value: 3.7475<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.56<br />value: 3.7776<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.57<br />value: 3.8079<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.58<br />value: 3.8384<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.59<br />value: 3.8691<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.60<br />value: 3.9000<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.61<br />value: 3.9311<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.62<br />value: 3.9624<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.63<br />value: 3.9939<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.64<br />value: 4.0256<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.65<br />value: 4.0575<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.66<br />value: 4.0896<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.67<br />value: 4.1219<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.68<br />value: 4.1544<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.69<br />value: 4.1871<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.70<br />value: 4.2200<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.71<br />value: 4.2531<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.72<br />value: 4.2864<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.73<br />value: 4.3199<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.74<br />value: 4.3536<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.75<br />value: 4.3875<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.76<br />value: 4.4216<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.77<br />value: 4.4559<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.78<br />value: 4.4904<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.79<br />value: 4.5251<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.80<br />value: 4.5600<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.81<br />value: 4.5951<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.82<br />value: 4.6304<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.83<br />value: 4.6659<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.84<br />value: 4.7016<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.85<br />value: 4.7375<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.86<br />value: 4.7736<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.87<br />value: 4.8099<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.88<br />value: 4.8464<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.89<br />value: 4.8831<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.90<br />value: 4.9200<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.91<br />value: 4.9571<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.92<br />value: 4.9944<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.93<br />value: 5.0319<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.94<br />value: 5.0696<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.95<br />value: 5.1075<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.96<br />value: 5.1456<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.97<br />value: 5.1839<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.98<br />value: 5.2224<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  1.99<br />value: 5.2611<br />Function: Loss + Penalty<br />Lambda: 1.9","b:  2.00<br />value: 5.3000<br />Function: Loss + Penalty<br />Lambda: 1.9"],"frame":"1.9","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[0.95,0.931,0.912,0.893,0.874,0.855,0.836,0.817,0.798,0.779,0.76,0.741,0.722,0.703,0.684,0.665,0.646,0.627,0.608,0.589,0.57,0.551,0.532,0.513,0.494,0.475,0.456,0.437,0.418,0.399,0.38,0.361,0.342,0.323,0.304,0.285,0.266,0.247,0.228,0.209,0.19,0.171,0.152,0.133,0.114,0.095,0.076,0.0569999999999999,0.038,0.019,0,0.019,0.038,0.0570000000000001,0.0760000000000001,0.0950000000000001,0.114,0.133,0.152,0.171,0.19,0.209,0.228,0.247,0.266,0.285,0.304,0.323,0.342,0.361,0.38,0.399,0.418,0.437,0.456,0.475,0.494,0.513,0.532,0.551,0.57,0.589,0.608,0.627,0.646,0.665,0.684,0.703,0.722,0.741,0.76,0.779,0.798,0.817,0.836,0.855,0.874,0.893,0.912,0.931,0.95,0.969,0.988,1.007,1.026,1.045,1.064,1.083,1.102,1.121,1.14,1.159,1.178,1.197,1.216,1.235,1.254,1.273,1.292,1.311,1.33,1.349,1.368,1.387,1.406,1.425,1.444,1.463,1.482,1.501,1.52,1.539,1.558,1.577,1.596,1.615,1.634,1.653,1.672,1.691,1.71,1.729,1.748,1.767,1.786,1.805,1.824,1.843,1.862,1.881,1.9,1.919,1.938,1.957,1.976,1.995,2.014,2.033,2.052,2.071,2.09,2.109,2.128,2.147,2.166,2.185,2.204,2.223,2.242,2.261,2.28,2.299,2.318,2.337,2.356,2.375,2.394,2.413,2.432,2.451,2.47,2.489,2.508,2.527,2.546,2.565,2.584,2.603,2.622,2.641,2.66,2.679,2.698,2.717,2.736,2.755,2.774,2.793,2.812,2.831,2.85,2.869,2.888,2.907,2.926,2.945,2.964,2.983,3.002,3.021,3.04,3.059,3.078,3.097,3.116,3.135,3.154,3.173,3.192,3.211,3.23,3.249,3.268,3.287,3.306,3.325,3.344,3.363,3.382,3.401,3.42,3.439,3.458,3.477,3.496,3.515,3.534,3.553,3.572,3.591,3.61,3.629,3.648,3.667,3.686,3.705,3.724,3.743,3.762,3.781,3.8],"text":["b: -0.50<br />value: 0.9500<br />Function: Penalty<br />Lambda: 1.9","b: -0.49<br />value: 0.9310<br />Function: Penalty<br />Lambda: 1.9","b: -0.48<br />value: 0.9120<br />Function: Penalty<br />Lambda: 1.9","b: -0.47<br />value: 0.8930<br />Function: Penalty<br />Lambda: 1.9","b: -0.46<br />value: 0.8740<br />Function: Penalty<br />Lambda: 1.9","b: -0.45<br />value: 0.8550<br />Function: Penalty<br />Lambda: 1.9","b: -0.44<br />value: 0.8360<br />Function: Penalty<br />Lambda: 1.9","b: -0.43<br />value: 0.8170<br />Function: Penalty<br />Lambda: 1.9","b: -0.42<br />value: 0.7980<br />Function: Penalty<br />Lambda: 1.9","b: -0.41<br />value: 0.7790<br />Function: Penalty<br />Lambda: 1.9","b: -0.40<br />value: 0.7600<br />Function: Penalty<br />Lambda: 1.9","b: -0.39<br />value: 0.7410<br />Function: Penalty<br />Lambda: 1.9","b: -0.38<br />value: 0.7220<br />Function: Penalty<br />Lambda: 1.9","b: -0.37<br />value: 0.7030<br />Function: Penalty<br />Lambda: 1.9","b: -0.36<br />value: 0.6840<br />Function: Penalty<br />Lambda: 1.9","b: -0.35<br />value: 0.6650<br />Function: Penalty<br />Lambda: 1.9","b: -0.34<br />value: 0.6460<br />Function: Penalty<br />Lambda: 1.9","b: -0.33<br />value: 0.6270<br />Function: Penalty<br />Lambda: 1.9","b: -0.32<br />value: 0.6080<br />Function: Penalty<br />Lambda: 1.9","b: -0.31<br />value: 0.5890<br />Function: Penalty<br />Lambda: 1.9","b: -0.30<br />value: 0.5700<br />Function: Penalty<br />Lambda: 1.9","b: -0.29<br />value: 0.5510<br />Function: Penalty<br />Lambda: 1.9","b: -0.28<br />value: 0.5320<br />Function: Penalty<br />Lambda: 1.9","b: -0.27<br />value: 0.5130<br />Function: Penalty<br />Lambda: 1.9","b: -0.26<br />value: 0.4940<br />Function: Penalty<br />Lambda: 1.9","b: -0.25<br />value: 0.4750<br />Function: Penalty<br />Lambda: 1.9","b: -0.24<br />value: 0.4560<br />Function: Penalty<br />Lambda: 1.9","b: -0.23<br />value: 0.4370<br />Function: Penalty<br />Lambda: 1.9","b: -0.22<br />value: 0.4180<br />Function: Penalty<br />Lambda: 1.9","b: -0.21<br />value: 0.3990<br />Function: Penalty<br />Lambda: 1.9","b: -0.20<br />value: 0.3800<br />Function: Penalty<br />Lambda: 1.9","b: -0.19<br />value: 0.3610<br />Function: Penalty<br />Lambda: 1.9","b: -0.18<br />value: 0.3420<br />Function: Penalty<br />Lambda: 1.9","b: -0.17<br />value: 0.3230<br />Function: Penalty<br />Lambda: 1.9","b: -0.16<br />value: 0.3040<br />Function: Penalty<br />Lambda: 1.9","b: -0.15<br />value: 0.2850<br />Function: Penalty<br />Lambda: 1.9","b: -0.14<br />value: 0.2660<br />Function: Penalty<br />Lambda: 1.9","b: -0.13<br />value: 0.2470<br />Function: Penalty<br />Lambda: 1.9","b: -0.12<br />value: 0.2280<br />Function: Penalty<br />Lambda: 1.9","b: -0.11<br />value: 0.2090<br />Function: Penalty<br />Lambda: 1.9","b: -0.10<br />value: 0.1900<br />Function: Penalty<br />Lambda: 1.9","b: -0.09<br />value: 0.1710<br />Function: Penalty<br />Lambda: 1.9","b: -0.08<br />value: 0.1520<br />Function: Penalty<br />Lambda: 1.9","b: -0.07<br />value: 0.1330<br />Function: Penalty<br />Lambda: 1.9","b: -0.06<br />value: 0.1140<br />Function: Penalty<br />Lambda: 1.9","b: -0.05<br />value: 0.0950<br />Function: Penalty<br />Lambda: 1.9","b: -0.04<br />value: 0.0760<br />Function: Penalty<br />Lambda: 1.9","b: -0.03<br />value: 0.0570<br />Function: Penalty<br />Lambda: 1.9","b: -0.02<br />value: 0.0380<br />Function: Penalty<br />Lambda: 1.9","b: -0.01<br />value: 0.0190<br />Function: Penalty<br />Lambda: 1.9","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 1.9","b:  0.01<br />value: 0.0190<br />Function: Penalty<br />Lambda: 1.9","b:  0.02<br />value: 0.0380<br />Function: Penalty<br />Lambda: 1.9","b:  0.03<br />value: 0.0570<br />Function: Penalty<br />Lambda: 1.9","b:  0.04<br />value: 0.0760<br />Function: Penalty<br />Lambda: 1.9","b:  0.05<br />value: 0.0950<br />Function: Penalty<br />Lambda: 1.9","b:  0.06<br />value: 0.1140<br />Function: Penalty<br />Lambda: 1.9","b:  0.07<br />value: 0.1330<br />Function: Penalty<br />Lambda: 1.9","b:  0.08<br />value: 0.1520<br />Function: Penalty<br />Lambda: 1.9","b:  0.09<br />value: 0.1710<br />Function: Penalty<br />Lambda: 1.9","b:  0.10<br />value: 0.1900<br />Function: Penalty<br />Lambda: 1.9","b:  0.11<br />value: 0.2090<br />Function: Penalty<br />Lambda: 1.9","b:  0.12<br />value: 0.2280<br />Function: Penalty<br />Lambda: 1.9","b:  0.13<br />value: 0.2470<br />Function: Penalty<br />Lambda: 1.9","b:  0.14<br />value: 0.2660<br />Function: Penalty<br />Lambda: 1.9","b:  0.15<br />value: 0.2850<br />Function: Penalty<br />Lambda: 1.9","b:  0.16<br />value: 0.3040<br />Function: Penalty<br />Lambda: 1.9","b:  0.17<br />value: 0.3230<br />Function: Penalty<br />Lambda: 1.9","b:  0.18<br />value: 0.3420<br />Function: Penalty<br />Lambda: 1.9","b:  0.19<br />value: 0.3610<br />Function: Penalty<br />Lambda: 1.9","b:  0.20<br />value: 0.3800<br />Function: Penalty<br />Lambda: 1.9","b:  0.21<br />value: 0.3990<br />Function: Penalty<br />Lambda: 1.9","b:  0.22<br />value: 0.4180<br />Function: Penalty<br />Lambda: 1.9","b:  0.23<br />value: 0.4370<br />Function: Penalty<br />Lambda: 1.9","b:  0.24<br />value: 0.4560<br />Function: Penalty<br />Lambda: 1.9","b:  0.25<br />value: 0.4750<br />Function: Penalty<br />Lambda: 1.9","b:  0.26<br />value: 0.4940<br />Function: Penalty<br />Lambda: 1.9","b:  0.27<br />value: 0.5130<br />Function: Penalty<br />Lambda: 1.9","b:  0.28<br />value: 0.5320<br />Function: Penalty<br />Lambda: 1.9","b:  0.29<br />value: 0.5510<br />Function: Penalty<br />Lambda: 1.9","b:  0.30<br />value: 0.5700<br />Function: Penalty<br />Lambda: 1.9","b:  0.31<br />value: 0.5890<br />Function: Penalty<br />Lambda: 1.9","b:  0.32<br />value: 0.6080<br />Function: Penalty<br />Lambda: 1.9","b:  0.33<br />value: 0.6270<br />Function: Penalty<br />Lambda: 1.9","b:  0.34<br />value: 0.6460<br />Function: Penalty<br />Lambda: 1.9","b:  0.35<br />value: 0.6650<br />Function: Penalty<br />Lambda: 1.9","b:  0.36<br />value: 0.6840<br />Function: Penalty<br />Lambda: 1.9","b:  0.37<br />value: 0.7030<br />Function: Penalty<br />Lambda: 1.9","b:  0.38<br />value: 0.7220<br />Function: Penalty<br />Lambda: 1.9","b:  0.39<br />value: 0.7410<br />Function: Penalty<br />Lambda: 1.9","b:  0.40<br />value: 0.7600<br />Function: Penalty<br />Lambda: 1.9","b:  0.41<br />value: 0.7790<br />Function: Penalty<br />Lambda: 1.9","b:  0.42<br />value: 0.7980<br />Function: Penalty<br />Lambda: 1.9","b:  0.43<br />value: 0.8170<br />Function: Penalty<br />Lambda: 1.9","b:  0.44<br />value: 0.8360<br />Function: Penalty<br />Lambda: 1.9","b:  0.45<br />value: 0.8550<br />Function: Penalty<br />Lambda: 1.9","b:  0.46<br />value: 0.8740<br />Function: Penalty<br />Lambda: 1.9","b:  0.47<br />value: 0.8930<br />Function: Penalty<br />Lambda: 1.9","b:  0.48<br />value: 0.9120<br />Function: Penalty<br />Lambda: 1.9","b:  0.49<br />value: 0.9310<br />Function: Penalty<br />Lambda: 1.9","b:  0.50<br />value: 0.9500<br />Function: Penalty<br />Lambda: 1.9","b:  0.51<br />value: 0.9690<br />Function: Penalty<br />Lambda: 1.9","b:  0.52<br />value: 0.9880<br />Function: Penalty<br />Lambda: 1.9","b:  0.53<br />value: 1.0070<br />Function: Penalty<br />Lambda: 1.9","b:  0.54<br />value: 1.0260<br />Function: Penalty<br />Lambda: 1.9","b:  0.55<br />value: 1.0450<br />Function: Penalty<br />Lambda: 1.9","b:  0.56<br />value: 1.0640<br />Function: Penalty<br />Lambda: 1.9","b:  0.57<br />value: 1.0830<br />Function: Penalty<br />Lambda: 1.9","b:  0.58<br />value: 1.1020<br />Function: Penalty<br />Lambda: 1.9","b:  0.59<br />value: 1.1210<br />Function: Penalty<br />Lambda: 1.9","b:  0.60<br />value: 1.1400<br />Function: Penalty<br />Lambda: 1.9","b:  0.61<br />value: 1.1590<br />Function: Penalty<br />Lambda: 1.9","b:  0.62<br />value: 1.1780<br />Function: Penalty<br />Lambda: 1.9","b:  0.63<br />value: 1.1970<br />Function: Penalty<br />Lambda: 1.9","b:  0.64<br />value: 1.2160<br />Function: Penalty<br />Lambda: 1.9","b:  0.65<br />value: 1.2350<br />Function: Penalty<br />Lambda: 1.9","b:  0.66<br />value: 1.2540<br />Function: Penalty<br />Lambda: 1.9","b:  0.67<br />value: 1.2730<br />Function: Penalty<br />Lambda: 1.9","b:  0.68<br />value: 1.2920<br />Function: Penalty<br />Lambda: 1.9","b:  0.69<br />value: 1.3110<br />Function: Penalty<br />Lambda: 1.9","b:  0.70<br />value: 1.3300<br />Function: Penalty<br />Lambda: 1.9","b:  0.71<br />value: 1.3490<br />Function: Penalty<br />Lambda: 1.9","b:  0.72<br />value: 1.3680<br />Function: Penalty<br />Lambda: 1.9","b:  0.73<br />value: 1.3870<br />Function: Penalty<br />Lambda: 1.9","b:  0.74<br />value: 1.4060<br />Function: Penalty<br />Lambda: 1.9","b:  0.75<br />value: 1.4250<br />Function: Penalty<br />Lambda: 1.9","b:  0.76<br />value: 1.4440<br />Function: Penalty<br />Lambda: 1.9","b:  0.77<br />value: 1.4630<br />Function: Penalty<br />Lambda: 1.9","b:  0.78<br />value: 1.4820<br />Function: Penalty<br />Lambda: 1.9","b:  0.79<br />value: 1.5010<br />Function: Penalty<br />Lambda: 1.9","b:  0.80<br />value: 1.5200<br />Function: Penalty<br />Lambda: 1.9","b:  0.81<br />value: 1.5390<br />Function: Penalty<br />Lambda: 1.9","b:  0.82<br />value: 1.5580<br />Function: Penalty<br />Lambda: 1.9","b:  0.83<br />value: 1.5770<br />Function: Penalty<br />Lambda: 1.9","b:  0.84<br />value: 1.5960<br />Function: Penalty<br />Lambda: 1.9","b:  0.85<br />value: 1.6150<br />Function: Penalty<br />Lambda: 1.9","b:  0.86<br />value: 1.6340<br />Function: Penalty<br />Lambda: 1.9","b:  0.87<br />value: 1.6530<br />Function: Penalty<br />Lambda: 1.9","b:  0.88<br />value: 1.6720<br />Function: Penalty<br />Lambda: 1.9","b:  0.89<br />value: 1.6910<br />Function: Penalty<br />Lambda: 1.9","b:  0.90<br />value: 1.7100<br />Function: Penalty<br />Lambda: 1.9","b:  0.91<br />value: 1.7290<br />Function: Penalty<br />Lambda: 1.9","b:  0.92<br />value: 1.7480<br />Function: Penalty<br />Lambda: 1.9","b:  0.93<br />value: 1.7670<br />Function: Penalty<br />Lambda: 1.9","b:  0.94<br />value: 1.7860<br />Function: Penalty<br />Lambda: 1.9","b:  0.95<br />value: 1.8050<br />Function: Penalty<br />Lambda: 1.9","b:  0.96<br />value: 1.8240<br />Function: Penalty<br />Lambda: 1.9","b:  0.97<br />value: 1.8430<br />Function: Penalty<br />Lambda: 1.9","b:  0.98<br />value: 1.8620<br />Function: Penalty<br />Lambda: 1.9","b:  0.99<br />value: 1.8810<br />Function: Penalty<br />Lambda: 1.9","b:  1.00<br />value: 1.9000<br />Function: Penalty<br />Lambda: 1.9","b:  1.01<br />value: 1.9190<br />Function: Penalty<br />Lambda: 1.9","b:  1.02<br />value: 1.9380<br />Function: Penalty<br />Lambda: 1.9","b:  1.03<br />value: 1.9570<br />Function: Penalty<br />Lambda: 1.9","b:  1.04<br />value: 1.9760<br />Function: Penalty<br />Lambda: 1.9","b:  1.05<br />value: 1.9950<br />Function: Penalty<br />Lambda: 1.9","b:  1.06<br />value: 2.0140<br />Function: Penalty<br />Lambda: 1.9","b:  1.07<br />value: 2.0330<br />Function: Penalty<br />Lambda: 1.9","b:  1.08<br />value: 2.0520<br />Function: Penalty<br />Lambda: 1.9","b:  1.09<br />value: 2.0710<br />Function: Penalty<br />Lambda: 1.9","b:  1.10<br />value: 2.0900<br />Function: Penalty<br />Lambda: 1.9","b:  1.11<br />value: 2.1090<br />Function: Penalty<br />Lambda: 1.9","b:  1.12<br />value: 2.1280<br />Function: Penalty<br />Lambda: 1.9","b:  1.13<br />value: 2.1470<br />Function: Penalty<br />Lambda: 1.9","b:  1.14<br />value: 2.1660<br />Function: Penalty<br />Lambda: 1.9","b:  1.15<br />value: 2.1850<br />Function: Penalty<br />Lambda: 1.9","b:  1.16<br />value: 2.2040<br />Function: Penalty<br />Lambda: 1.9","b:  1.17<br />value: 2.2230<br />Function: Penalty<br />Lambda: 1.9","b:  1.18<br />value: 2.2420<br />Function: Penalty<br />Lambda: 1.9","b:  1.19<br />value: 2.2610<br />Function: Penalty<br />Lambda: 1.9","b:  1.20<br />value: 2.2800<br />Function: Penalty<br />Lambda: 1.9","b:  1.21<br />value: 2.2990<br />Function: Penalty<br />Lambda: 1.9","b:  1.22<br />value: 2.3180<br />Function: Penalty<br />Lambda: 1.9","b:  1.23<br />value: 2.3370<br />Function: Penalty<br />Lambda: 1.9","b:  1.24<br />value: 2.3560<br />Function: Penalty<br />Lambda: 1.9","b:  1.25<br />value: 2.3750<br />Function: Penalty<br />Lambda: 1.9","b:  1.26<br />value: 2.3940<br />Function: Penalty<br />Lambda: 1.9","b:  1.27<br />value: 2.4130<br />Function: Penalty<br />Lambda: 1.9","b:  1.28<br />value: 2.4320<br />Function: Penalty<br />Lambda: 1.9","b:  1.29<br />value: 2.4510<br />Function: Penalty<br />Lambda: 1.9","b:  1.30<br />value: 2.4700<br />Function: Penalty<br />Lambda: 1.9","b:  1.31<br />value: 2.4890<br />Function: Penalty<br />Lambda: 1.9","b:  1.32<br />value: 2.5080<br />Function: Penalty<br />Lambda: 1.9","b:  1.33<br />value: 2.5270<br />Function: Penalty<br />Lambda: 1.9","b:  1.34<br />value: 2.5460<br />Function: Penalty<br />Lambda: 1.9","b:  1.35<br />value: 2.5650<br />Function: Penalty<br />Lambda: 1.9","b:  1.36<br />value: 2.5840<br />Function: Penalty<br />Lambda: 1.9","b:  1.37<br />value: 2.6030<br />Function: Penalty<br />Lambda: 1.9","b:  1.38<br />value: 2.6220<br />Function: Penalty<br />Lambda: 1.9","b:  1.39<br />value: 2.6410<br />Function: Penalty<br />Lambda: 1.9","b:  1.40<br />value: 2.6600<br />Function: Penalty<br />Lambda: 1.9","b:  1.41<br />value: 2.6790<br />Function: Penalty<br />Lambda: 1.9","b:  1.42<br />value: 2.6980<br />Function: Penalty<br />Lambda: 1.9","b:  1.43<br />value: 2.7170<br />Function: Penalty<br />Lambda: 1.9","b:  1.44<br />value: 2.7360<br />Function: Penalty<br />Lambda: 1.9","b:  1.45<br />value: 2.7550<br />Function: Penalty<br />Lambda: 1.9","b:  1.46<br />value: 2.7740<br />Function: Penalty<br />Lambda: 1.9","b:  1.47<br />value: 2.7930<br />Function: Penalty<br />Lambda: 1.9","b:  1.48<br />value: 2.8120<br />Function: Penalty<br />Lambda: 1.9","b:  1.49<br />value: 2.8310<br />Function: Penalty<br />Lambda: 1.9","b:  1.50<br />value: 2.8500<br />Function: Penalty<br />Lambda: 1.9","b:  1.51<br />value: 2.8690<br />Function: Penalty<br />Lambda: 1.9","b:  1.52<br />value: 2.8880<br />Function: Penalty<br />Lambda: 1.9","b:  1.53<br />value: 2.9070<br />Function: Penalty<br />Lambda: 1.9","b:  1.54<br />value: 2.9260<br />Function: Penalty<br />Lambda: 1.9","b:  1.55<br />value: 2.9450<br />Function: Penalty<br />Lambda: 1.9","b:  1.56<br />value: 2.9640<br />Function: Penalty<br />Lambda: 1.9","b:  1.57<br />value: 2.9830<br />Function: Penalty<br />Lambda: 1.9","b:  1.58<br />value: 3.0020<br />Function: Penalty<br />Lambda: 1.9","b:  1.59<br />value: 3.0210<br />Function: Penalty<br />Lambda: 1.9","b:  1.60<br />value: 3.0400<br />Function: Penalty<br />Lambda: 1.9","b:  1.61<br />value: 3.0590<br />Function: Penalty<br />Lambda: 1.9","b:  1.62<br />value: 3.0780<br />Function: Penalty<br />Lambda: 1.9","b:  1.63<br />value: 3.0970<br />Function: Penalty<br />Lambda: 1.9","b:  1.64<br />value: 3.1160<br />Function: Penalty<br />Lambda: 1.9","b:  1.65<br />value: 3.1350<br />Function: Penalty<br />Lambda: 1.9","b:  1.66<br />value: 3.1540<br />Function: Penalty<br />Lambda: 1.9","b:  1.67<br />value: 3.1730<br />Function: Penalty<br />Lambda: 1.9","b:  1.68<br />value: 3.1920<br />Function: Penalty<br />Lambda: 1.9","b:  1.69<br />value: 3.2110<br />Function: Penalty<br />Lambda: 1.9","b:  1.70<br />value: 3.2300<br />Function: Penalty<br />Lambda: 1.9","b:  1.71<br />value: 3.2490<br />Function: Penalty<br />Lambda: 1.9","b:  1.72<br />value: 3.2680<br />Function: Penalty<br />Lambda: 1.9","b:  1.73<br />value: 3.2870<br />Function: Penalty<br />Lambda: 1.9","b:  1.74<br />value: 3.3060<br />Function: Penalty<br />Lambda: 1.9","b:  1.75<br />value: 3.3250<br />Function: Penalty<br />Lambda: 1.9","b:  1.76<br />value: 3.3440<br />Function: Penalty<br />Lambda: 1.9","b:  1.77<br />value: 3.3630<br />Function: Penalty<br />Lambda: 1.9","b:  1.78<br />value: 3.3820<br />Function: Penalty<br />Lambda: 1.9","b:  1.79<br />value: 3.4010<br />Function: Penalty<br />Lambda: 1.9","b:  1.80<br />value: 3.4200<br />Function: Penalty<br />Lambda: 1.9","b:  1.81<br />value: 3.4390<br />Function: Penalty<br />Lambda: 1.9","b:  1.82<br />value: 3.4580<br />Function: Penalty<br />Lambda: 1.9","b:  1.83<br />value: 3.4770<br />Function: Penalty<br />Lambda: 1.9","b:  1.84<br />value: 3.4960<br />Function: Penalty<br />Lambda: 1.9","b:  1.85<br />value: 3.5150<br />Function: Penalty<br />Lambda: 1.9","b:  1.86<br />value: 3.5340<br />Function: Penalty<br />Lambda: 1.9","b:  1.87<br />value: 3.5530<br />Function: Penalty<br />Lambda: 1.9","b:  1.88<br />value: 3.5720<br />Function: Penalty<br />Lambda: 1.9","b:  1.89<br />value: 3.5910<br />Function: Penalty<br />Lambda: 1.9","b:  1.90<br />value: 3.6100<br />Function: Penalty<br />Lambda: 1.9","b:  1.91<br />value: 3.6290<br />Function: Penalty<br />Lambda: 1.9","b:  1.92<br />value: 3.6480<br />Function: Penalty<br />Lambda: 1.9","b:  1.93<br />value: 3.6670<br />Function: Penalty<br />Lambda: 1.9","b:  1.94<br />value: 3.6860<br />Function: Penalty<br />Lambda: 1.9","b:  1.95<br />value: 3.7050<br />Function: Penalty<br />Lambda: 1.9","b:  1.96<br />value: 3.7240<br />Function: Penalty<br />Lambda: 1.9","b:  1.97<br />value: 3.7430<br />Function: Penalty<br />Lambda: 1.9","b:  1.98<br />value: 3.7620<br />Function: Penalty<br />Lambda: 1.9","b:  1.99<br />value: 3.7810<br />Function: Penalty<br />Lambda: 1.9","b:  2.00<br />value: 3.8000<br />Function: Penalty<br />Lambda: 1.9"],"frame":"1.9","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"2","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 2.0","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 2.0","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 2.0","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 2.0","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 2.0","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 2.0","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 2.0","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 2.0","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 2.0","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 2.0","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 2.0","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 2.0","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 2.0","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 2.0","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 2.0","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 2.0","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 2.0","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 2.0","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 2.0","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 2.0","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 2.0","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 2.0","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 2.0","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 2.0","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 2.0","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 2.0","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 2.0","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 2.0","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 2.0","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 2.0","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 2.0","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 2.0","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 2.0","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 2.0","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 2.0","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 2.0","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 2.0","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 2.0","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 2.0","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 2.0","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 2.0","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 2.0","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 2.0","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 2.0","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 2.0","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 2.0","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 2.0","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 2.0","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 2.0","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 2.0","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 2.0","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 2.0","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 2.0","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 2.0","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 2.0","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 2.0","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 2.0","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 2.0","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 2.0","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 2.0","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 2.0","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 2.0","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 2.0","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 2.0","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 2.0","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 2.0","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 2.0","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 2.0","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 2.0","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 2.0","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 2.0","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 2.0","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 2.0","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 2.0","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 2.0","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 2.0","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 2.0","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 2.0","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 2.0","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 2.0","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 2.0","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 2.0","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 2.0","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 2.0","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 2.0","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 2.0","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 2.0","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 2.0","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 2.0","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 2.0","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 2.0","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 2.0","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 2.0","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 2.0","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 2.0","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 2.0","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 2.0","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 2.0","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 2.0","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 2.0","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 2.0","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 2.0","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 2.0","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 2.0","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 2.0","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 2.0","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 2.0","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 2.0","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 2.0","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 2.0","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 2.0","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 2.0","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 2.0","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 2.0","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 2.0","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 2.0","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 2.0","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 2.0","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 2.0","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 2.0","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 2.0","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 2.0","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 2.0","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 2.0","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 2.0","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 2.0","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 2.0","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 2.0","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 2.0","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 2.0","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 2.0","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 2.0","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 2.0","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 2.0","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 2.0","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 2.0","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 2.0","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 2.0","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 2.0","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 2.0","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 2.0","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 2.0","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 2.0","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 2.0","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 2.0","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 2.0","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 2.0","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 2.0","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 2.0","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 2.0","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 2.0","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 2.0","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 2.0","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 2.0","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 2.0","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 2.0","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 2.0","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 2.0","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 2.0","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 2.0","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 2.0","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 2.0","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 2.0","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 2.0","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 2.0","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 2.0","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 2.0","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 2.0","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 2.0","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 2.0","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 2.0","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 2.0","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 2.0","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 2.0","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 2.0","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 2.0","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 2.0","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 2.0","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 2.0","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 2.0","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 2.0","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 2.0","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 2.0","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 2.0","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 2.0","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 2.0","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 2.0","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 2.0","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 2.0","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 2.0","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 2.0","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 2.0","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 2.0","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 2.0","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 2.0","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 2.0","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 2.0","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 2.0","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 2.0","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 2.0","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 2.0","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 2.0","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 2.0","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 2.0","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 2.0","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 2.0","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 2.0","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 2.0","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 2.0","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 2.0","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 2.0","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 2.0","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 2.0","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 2.0","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 2.0","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 2.0","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 2.0","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 2.0","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 2.0","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 2.0","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 2.0","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 2.0","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 2.0","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 2.0","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 2.0","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 2.0","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 2.0","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 2.0","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 2.0","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 2.0","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 2.0","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 2.0","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 2.0","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 2.0","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 2.0","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 2.0","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 2.0","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 2.0","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 2.0","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 2.0","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 2.0","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 2.0","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 2.0","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 2.0","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 2.0","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 2.0","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 2.0","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 2.0","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 2.0","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 2.0","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 2.0"],"frame":"2","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.75,3.7001,3.6504,3.6009,3.5516,3.5025,3.4536,3.4049,3.3564,3.3081,3.26,3.2121,3.1644,3.1169,3.0696,3.0225,2.9756,2.9289,2.8824,2.8361,2.79,2.7441,2.6984,2.6529,2.6076,2.5625,2.5176,2.4729,2.4284,2.3841,2.34,2.2961,2.2524,2.2089,2.1656,2.1225,2.0796,2.0369,1.9944,1.9521,1.91,1.8681,1.8264,1.7849,1.7436,1.7025,1.6616,1.6209,1.5804,1.5401,1.5,1.5001,1.5004,1.5009,1.5016,1.5025,1.5036,1.5049,1.5064,1.5081,1.51,1.5121,1.5144,1.5169,1.5196,1.5225,1.5256,1.5289,1.5324,1.5361,1.54,1.5441,1.5484,1.5529,1.5576,1.5625,1.5676,1.5729,1.5784,1.5841,1.59,1.5961,1.6024,1.6089,1.6156,1.6225,1.6296,1.6369,1.6444,1.6521,1.66,1.6681,1.6764,1.6849,1.6936,1.7025,1.7116,1.7209,1.7304,1.7401,1.75,1.7601,1.7704,1.7809,1.7916,1.8025,1.8136,1.8249,1.8364,1.8481,1.86,1.8721,1.8844,1.8969,1.9096,1.9225,1.9356,1.9489,1.9624,1.9761,1.99,2.0041,2.0184,2.0329,2.0476,2.0625,2.0776,2.0929,2.1084,2.1241,2.14,2.1561,2.1724,2.1889,2.2056,2.2225,2.2396,2.2569,2.2744,2.2921,2.31,2.3281,2.3464,2.3649,2.3836,2.4025,2.4216,2.4409,2.4604,2.4801,2.5,2.5201,2.5404,2.5609,2.5816,2.6025,2.6236,2.6449,2.6664,2.6881,2.71,2.7321,2.7544,2.7769,2.7996,2.8225,2.8456,2.8689,2.8924,2.9161,2.94,2.9641,2.9884,3.0129,3.0376,3.0625,3.0876,3.1129,3.1384,3.1641,3.19,3.2161,3.2424,3.2689,3.2956,3.3225,3.3496,3.3769,3.4044,3.4321,3.46,3.4881,3.5164,3.5449,3.5736,3.6025,3.6316,3.6609,3.6904,3.7201,3.75,3.7801,3.8104,3.8409,3.8716,3.9025,3.9336,3.9649,3.9964,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 3.7500<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.49<br />value: 3.7001<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.48<br />value: 3.6504<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.47<br />value: 3.6009<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.46<br />value: 3.5516<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.45<br />value: 3.5025<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.44<br />value: 3.4536<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.43<br />value: 3.4049<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.42<br />value: 3.3564<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.41<br />value: 3.3081<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.40<br />value: 3.2600<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.39<br />value: 3.2121<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.38<br />value: 3.1644<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.37<br />value: 3.1169<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.36<br />value: 3.0696<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.35<br />value: 3.0225<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.34<br />value: 2.9756<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.33<br />value: 2.9289<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.32<br />value: 2.8824<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.31<br />value: 2.8361<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.30<br />value: 2.7900<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.29<br />value: 2.7441<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.28<br />value: 2.6984<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.27<br />value: 2.6529<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.26<br />value: 2.6076<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.25<br />value: 2.5625<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.24<br />value: 2.5176<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.23<br />value: 2.4729<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.22<br />value: 2.4284<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.21<br />value: 2.3841<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.20<br />value: 2.3400<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.19<br />value: 2.2961<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.18<br />value: 2.2524<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.17<br />value: 2.2089<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.16<br />value: 2.1656<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.15<br />value: 2.1225<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.14<br />value: 2.0796<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.13<br />value: 2.0369<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.12<br />value: 1.9944<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.11<br />value: 1.9521<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.10<br />value: 1.9100<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.09<br />value: 1.8681<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.08<br />value: 1.8264<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.07<br />value: 1.7849<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.06<br />value: 1.7436<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.05<br />value: 1.7025<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.04<br />value: 1.6616<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.03<br />value: 1.6209<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.02<br />value: 1.5804<br />Function: Loss + Penalty<br />Lambda: 2.0","b: -0.01<br />value: 1.5401<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.01<br />value: 1.5001<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.02<br />value: 1.5004<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.03<br />value: 1.5009<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.04<br />value: 1.5016<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.05<br />value: 1.5025<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.06<br />value: 1.5036<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.07<br />value: 1.5049<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.08<br />value: 1.5064<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.09<br />value: 1.5081<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.10<br />value: 1.5100<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.11<br />value: 1.5121<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.12<br />value: 1.5144<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.13<br />value: 1.5169<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.14<br />value: 1.5196<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.15<br />value: 1.5225<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.16<br />value: 1.5256<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.17<br />value: 1.5289<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.18<br />value: 1.5324<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.19<br />value: 1.5361<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.20<br />value: 1.5400<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.21<br />value: 1.5441<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.22<br />value: 1.5484<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.23<br />value: 1.5529<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.24<br />value: 1.5576<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.25<br />value: 1.5625<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.26<br />value: 1.5676<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.27<br />value: 1.5729<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.28<br />value: 1.5784<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.29<br />value: 1.5841<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.30<br />value: 1.5900<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.31<br />value: 1.5961<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.32<br />value: 1.6024<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.33<br />value: 1.6089<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.34<br />value: 1.6156<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.35<br />value: 1.6225<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.36<br />value: 1.6296<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.37<br />value: 1.6369<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.38<br />value: 1.6444<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.39<br />value: 1.6521<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.40<br />value: 1.6600<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.41<br />value: 1.6681<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.42<br />value: 1.6764<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.43<br />value: 1.6849<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.44<br />value: 1.6936<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.45<br />value: 1.7025<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.46<br />value: 1.7116<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.47<br />value: 1.7209<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.48<br />value: 1.7304<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.49<br />value: 1.7401<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.50<br />value: 1.7500<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.51<br />value: 1.7601<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.52<br />value: 1.7704<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.53<br />value: 1.7809<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.54<br />value: 1.7916<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.55<br />value: 1.8025<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.56<br />value: 1.8136<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.57<br />value: 1.8249<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.58<br />value: 1.8364<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.59<br />value: 1.8481<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.60<br />value: 1.8600<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.61<br />value: 1.8721<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.62<br />value: 1.8844<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.63<br />value: 1.8969<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.64<br />value: 1.9096<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.65<br />value: 1.9225<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.66<br />value: 1.9356<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.67<br />value: 1.9489<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.68<br />value: 1.9624<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.69<br />value: 1.9761<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.70<br />value: 1.9900<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.71<br />value: 2.0041<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.72<br />value: 2.0184<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.73<br />value: 2.0329<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.74<br />value: 2.0476<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.75<br />value: 2.0625<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.76<br />value: 2.0776<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.77<br />value: 2.0929<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.78<br />value: 2.1084<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.79<br />value: 2.1241<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.80<br />value: 2.1400<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.81<br />value: 2.1561<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.82<br />value: 2.1724<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.83<br />value: 2.1889<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.84<br />value: 2.2056<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.85<br />value: 2.2225<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.86<br />value: 2.2396<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.87<br />value: 2.2569<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.88<br />value: 2.2744<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.89<br />value: 2.2921<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.90<br />value: 2.3100<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.91<br />value: 2.3281<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.92<br />value: 2.3464<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.93<br />value: 2.3649<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.94<br />value: 2.3836<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.95<br />value: 2.4025<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.96<br />value: 2.4216<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.97<br />value: 2.4409<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.98<br />value: 2.4604<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  0.99<br />value: 2.4801<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.00<br />value: 2.5000<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.01<br />value: 2.5201<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.02<br />value: 2.5404<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.03<br />value: 2.5609<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.04<br />value: 2.5816<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.05<br />value: 2.6025<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.06<br />value: 2.6236<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.07<br />value: 2.6449<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.08<br />value: 2.6664<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.09<br />value: 2.6881<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.10<br />value: 2.7100<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.11<br />value: 2.7321<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.12<br />value: 2.7544<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.13<br />value: 2.7769<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.14<br />value: 2.7996<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.15<br />value: 2.8225<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.16<br />value: 2.8456<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.17<br />value: 2.8689<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.18<br />value: 2.8924<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.19<br />value: 2.9161<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.20<br />value: 2.9400<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.21<br />value: 2.9641<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.22<br />value: 2.9884<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.23<br />value: 3.0129<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.24<br />value: 3.0376<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.25<br />value: 3.0625<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.26<br />value: 3.0876<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.27<br />value: 3.1129<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.28<br />value: 3.1384<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.29<br />value: 3.1641<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.30<br />value: 3.1900<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.31<br />value: 3.2161<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.32<br />value: 3.2424<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.33<br />value: 3.2689<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.34<br />value: 3.2956<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.35<br />value: 3.3225<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.36<br />value: 3.3496<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.37<br />value: 3.3769<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.38<br />value: 3.4044<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.39<br />value: 3.4321<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.40<br />value: 3.4600<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.41<br />value: 3.4881<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.42<br />value: 3.5164<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.43<br />value: 3.5449<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.44<br />value: 3.5736<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.45<br />value: 3.6025<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.46<br />value: 3.6316<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.47<br />value: 3.6609<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.48<br />value: 3.6904<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.49<br />value: 3.7201<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.50<br />value: 3.7500<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.51<br />value: 3.7801<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.52<br />value: 3.8104<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.53<br />value: 3.8409<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.54<br />value: 3.8716<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.55<br />value: 3.9025<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.56<br />value: 3.9336<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.57<br />value: 3.9649<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.58<br />value: 3.9964<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.59<br />value: 4.0281<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.60<br />value: 4.0600<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.61<br />value: 4.0921<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.62<br />value: 4.1244<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.63<br />value: 4.1569<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.64<br />value: 4.1896<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.65<br />value: 4.2225<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.66<br />value: 4.2556<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.67<br />value: 4.2889<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.68<br />value: 4.3224<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.69<br />value: 4.3561<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.70<br />value: 4.3900<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.71<br />value: 4.4241<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.72<br />value: 4.4584<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.73<br />value: 4.4929<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.74<br />value: 4.5276<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.75<br />value: 4.5625<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.76<br />value: 4.5976<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.77<br />value: 4.6329<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.78<br />value: 4.6684<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.79<br />value: 4.7041<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.80<br />value: 4.7400<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.81<br />value: 4.7761<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.82<br />value: 4.8124<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.83<br />value: 4.8489<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.84<br />value: 4.8856<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.85<br />value: 4.9225<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.86<br />value: 4.9596<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.87<br />value: 4.9969<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.88<br />value: 5.0344<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.89<br />value: 5.0721<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.90<br />value: 5.1100<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.91<br />value: 5.1481<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.92<br />value: 5.1864<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.93<br />value: 5.2249<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.94<br />value: 5.2636<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.95<br />value: 5.3025<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.96<br />value: 5.3416<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.97<br />value: 5.3809<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.98<br />value: 5.4204<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  1.99<br />value: 5.4601<br />Function: Loss + Penalty<br />Lambda: 2.0","b:  2.00<br />value: 5.5000<br />Function: Loss + Penalty<br />Lambda: 2.0"],"frame":"2","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[1,0.98,0.96,0.94,0.92,0.9,0.88,0.86,0.84,0.82,0.8,0.78,0.76,0.74,0.72,0.7,0.68,0.66,0.64,0.62,0.6,0.58,0.56,0.54,0.52,0.5,0.48,0.46,0.44,0.42,0.4,0.38,0.36,0.34,0.32,0.3,0.28,0.26,0.24,0.22,0.2,0.18,0.16,0.14,0.12,0.1,0.08,0.0599999999999999,0.04,0.02,0,0.02,0.04,0.0600000000000001,0.0800000000000001,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26,0.28,0.3,0.32,0.34,0.36,0.38,0.4,0.42,0.44,0.46,0.48,0.5,0.52,0.54,0.56,0.58,0.6,0.62,0.64,0.66,0.68,0.7,0.72,0.74,0.76,0.78,0.8,0.82,0.84,0.86,0.88,0.9,0.92,0.94,0.96,0.98,1,1.02,1.04,1.06,1.08,1.1,1.12,1.14,1.16,1.18,1.2,1.22,1.24,1.26,1.28,1.3,1.32,1.34,1.36,1.38,1.4,1.42,1.44,1.46,1.48,1.5,1.52,1.54,1.56,1.58,1.6,1.62,1.64,1.66,1.68,1.7,1.72,1.74,1.76,1.78,1.8,1.82,1.84,1.86,1.88,1.9,1.92,1.94,1.96,1.98,2,2.02,2.04,2.06,2.08,2.1,2.12,2.14,2.16,2.18,2.2,2.22,2.24,2.26,2.28,2.3,2.32,2.34,2.36,2.38,2.4,2.42,2.44,2.46,2.48,2.5,2.52,2.54,2.56,2.58,2.6,2.62,2.64,2.66,2.68,2.7,2.72,2.74,2.76,2.78,2.8,2.82,2.84,2.86,2.88,2.9,2.92,2.94,2.96,2.98,3,3.02,3.04,3.06,3.08,3.1,3.12,3.14,3.16,3.18,3.2,3.22,3.24,3.26,3.28,3.3,3.32,3.34,3.36,3.38,3.4,3.42,3.44,3.46,3.48,3.5,3.52,3.54,3.56,3.58,3.6,3.62,3.64,3.66,3.68,3.7,3.72,3.74,3.76,3.78,3.8,3.82,3.84,3.86,3.88,3.9,3.92,3.94,3.96,3.98,4],"text":["b: -0.50<br />value: 1.0000<br />Function: Penalty<br />Lambda: 2.0","b: -0.49<br />value: 0.9800<br />Function: Penalty<br />Lambda: 2.0","b: -0.48<br />value: 0.9600<br />Function: Penalty<br />Lambda: 2.0","b: -0.47<br />value: 0.9400<br />Function: Penalty<br />Lambda: 2.0","b: -0.46<br />value: 0.9200<br />Function: Penalty<br />Lambda: 2.0","b: -0.45<br />value: 0.9000<br />Function: Penalty<br />Lambda: 2.0","b: -0.44<br />value: 0.8800<br />Function: Penalty<br />Lambda: 2.0","b: -0.43<br />value: 0.8600<br />Function: Penalty<br />Lambda: 2.0","b: -0.42<br />value: 0.8400<br />Function: Penalty<br />Lambda: 2.0","b: -0.41<br />value: 0.8200<br />Function: Penalty<br />Lambda: 2.0","b: -0.40<br />value: 0.8000<br />Function: Penalty<br />Lambda: 2.0","b: -0.39<br />value: 0.7800<br />Function: Penalty<br />Lambda: 2.0","b: -0.38<br />value: 0.7600<br />Function: Penalty<br />Lambda: 2.0","b: -0.37<br />value: 0.7400<br />Function: Penalty<br />Lambda: 2.0","b: -0.36<br />value: 0.7200<br />Function: Penalty<br />Lambda: 2.0","b: -0.35<br />value: 0.7000<br />Function: Penalty<br />Lambda: 2.0","b: -0.34<br />value: 0.6800<br />Function: Penalty<br />Lambda: 2.0","b: -0.33<br />value: 0.6600<br />Function: Penalty<br />Lambda: 2.0","b: -0.32<br />value: 0.6400<br />Function: Penalty<br />Lambda: 2.0","b: -0.31<br />value: 0.6200<br />Function: Penalty<br />Lambda: 2.0","b: -0.30<br />value: 0.6000<br />Function: Penalty<br />Lambda: 2.0","b: -0.29<br />value: 0.5800<br />Function: Penalty<br />Lambda: 2.0","b: -0.28<br />value: 0.5600<br />Function: Penalty<br />Lambda: 2.0","b: -0.27<br />value: 0.5400<br />Function: Penalty<br />Lambda: 2.0","b: -0.26<br />value: 0.5200<br />Function: Penalty<br />Lambda: 2.0","b: -0.25<br />value: 0.5000<br />Function: Penalty<br />Lambda: 2.0","b: -0.24<br />value: 0.4800<br />Function: Penalty<br />Lambda: 2.0","b: -0.23<br />value: 0.4600<br />Function: Penalty<br />Lambda: 2.0","b: -0.22<br />value: 0.4400<br />Function: Penalty<br />Lambda: 2.0","b: -0.21<br />value: 0.4200<br />Function: Penalty<br />Lambda: 2.0","b: -0.20<br />value: 0.4000<br />Function: Penalty<br />Lambda: 2.0","b: -0.19<br />value: 0.3800<br />Function: Penalty<br />Lambda: 2.0","b: -0.18<br />value: 0.3600<br />Function: Penalty<br />Lambda: 2.0","b: -0.17<br />value: 0.3400<br />Function: Penalty<br />Lambda: 2.0","b: -0.16<br />value: 0.3200<br />Function: Penalty<br />Lambda: 2.0","b: -0.15<br />value: 0.3000<br />Function: Penalty<br />Lambda: 2.0","b: -0.14<br />value: 0.2800<br />Function: Penalty<br />Lambda: 2.0","b: -0.13<br />value: 0.2600<br />Function: Penalty<br />Lambda: 2.0","b: -0.12<br />value: 0.2400<br />Function: Penalty<br />Lambda: 2.0","b: -0.11<br />value: 0.2200<br />Function: Penalty<br />Lambda: 2.0","b: -0.10<br />value: 0.2000<br />Function: Penalty<br />Lambda: 2.0","b: -0.09<br />value: 0.1800<br />Function: Penalty<br />Lambda: 2.0","b: -0.08<br />value: 0.1600<br />Function: Penalty<br />Lambda: 2.0","b: -0.07<br />value: 0.1400<br />Function: Penalty<br />Lambda: 2.0","b: -0.06<br />value: 0.1200<br />Function: Penalty<br />Lambda: 2.0","b: -0.05<br />value: 0.1000<br />Function: Penalty<br />Lambda: 2.0","b: -0.04<br />value: 0.0800<br />Function: Penalty<br />Lambda: 2.0","b: -0.03<br />value: 0.0600<br />Function: Penalty<br />Lambda: 2.0","b: -0.02<br />value: 0.0400<br />Function: Penalty<br />Lambda: 2.0","b: -0.01<br />value: 0.0200<br />Function: Penalty<br />Lambda: 2.0","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 2.0","b:  0.01<br />value: 0.0200<br />Function: Penalty<br />Lambda: 2.0","b:  0.02<br />value: 0.0400<br />Function: Penalty<br />Lambda: 2.0","b:  0.03<br />value: 0.0600<br />Function: Penalty<br />Lambda: 2.0","b:  0.04<br />value: 0.0800<br />Function: Penalty<br />Lambda: 2.0","b:  0.05<br />value: 0.1000<br />Function: Penalty<br />Lambda: 2.0","b:  0.06<br />value: 0.1200<br />Function: Penalty<br />Lambda: 2.0","b:  0.07<br />value: 0.1400<br />Function: Penalty<br />Lambda: 2.0","b:  0.08<br />value: 0.1600<br />Function: Penalty<br />Lambda: 2.0","b:  0.09<br />value: 0.1800<br />Function: Penalty<br />Lambda: 2.0","b:  0.10<br />value: 0.2000<br />Function: Penalty<br />Lambda: 2.0","b:  0.11<br />value: 0.2200<br />Function: Penalty<br />Lambda: 2.0","b:  0.12<br />value: 0.2400<br />Function: Penalty<br />Lambda: 2.0","b:  0.13<br />value: 0.2600<br />Function: Penalty<br />Lambda: 2.0","b:  0.14<br />value: 0.2800<br />Function: Penalty<br />Lambda: 2.0","b:  0.15<br />value: 0.3000<br />Function: Penalty<br />Lambda: 2.0","b:  0.16<br />value: 0.3200<br />Function: Penalty<br />Lambda: 2.0","b:  0.17<br />value: 0.3400<br />Function: Penalty<br />Lambda: 2.0","b:  0.18<br />value: 0.3600<br />Function: Penalty<br />Lambda: 2.0","b:  0.19<br />value: 0.3800<br />Function: Penalty<br />Lambda: 2.0","b:  0.20<br />value: 0.4000<br />Function: Penalty<br />Lambda: 2.0","b:  0.21<br />value: 0.4200<br />Function: Penalty<br />Lambda: 2.0","b:  0.22<br />value: 0.4400<br />Function: Penalty<br />Lambda: 2.0","b:  0.23<br />value: 0.4600<br />Function: Penalty<br />Lambda: 2.0","b:  0.24<br />value: 0.4800<br />Function: Penalty<br />Lambda: 2.0","b:  0.25<br />value: 0.5000<br />Function: Penalty<br />Lambda: 2.0","b:  0.26<br />value: 0.5200<br />Function: Penalty<br />Lambda: 2.0","b:  0.27<br />value: 0.5400<br />Function: Penalty<br />Lambda: 2.0","b:  0.28<br />value: 0.5600<br />Function: Penalty<br />Lambda: 2.0","b:  0.29<br />value: 0.5800<br />Function: Penalty<br />Lambda: 2.0","b:  0.30<br />value: 0.6000<br />Function: Penalty<br />Lambda: 2.0","b:  0.31<br />value: 0.6200<br />Function: Penalty<br />Lambda: 2.0","b:  0.32<br />value: 0.6400<br />Function: Penalty<br />Lambda: 2.0","b:  0.33<br />value: 0.6600<br />Function: Penalty<br />Lambda: 2.0","b:  0.34<br />value: 0.6800<br />Function: Penalty<br />Lambda: 2.0","b:  0.35<br />value: 0.7000<br />Function: Penalty<br />Lambda: 2.0","b:  0.36<br />value: 0.7200<br />Function: Penalty<br />Lambda: 2.0","b:  0.37<br />value: 0.7400<br />Function: Penalty<br />Lambda: 2.0","b:  0.38<br />value: 0.7600<br />Function: Penalty<br />Lambda: 2.0","b:  0.39<br />value: 0.7800<br />Function: Penalty<br />Lambda: 2.0","b:  0.40<br />value: 0.8000<br />Function: Penalty<br />Lambda: 2.0","b:  0.41<br />value: 0.8200<br />Function: Penalty<br />Lambda: 2.0","b:  0.42<br />value: 0.8400<br />Function: Penalty<br />Lambda: 2.0","b:  0.43<br />value: 0.8600<br />Function: Penalty<br />Lambda: 2.0","b:  0.44<br />value: 0.8800<br />Function: Penalty<br />Lambda: 2.0","b:  0.45<br />value: 0.9000<br />Function: Penalty<br />Lambda: 2.0","b:  0.46<br />value: 0.9200<br />Function: Penalty<br />Lambda: 2.0","b:  0.47<br />value: 0.9400<br />Function: Penalty<br />Lambda: 2.0","b:  0.48<br />value: 0.9600<br />Function: Penalty<br />Lambda: 2.0","b:  0.49<br />value: 0.9800<br />Function: Penalty<br />Lambda: 2.0","b:  0.50<br />value: 1.0000<br />Function: Penalty<br />Lambda: 2.0","b:  0.51<br />value: 1.0200<br />Function: Penalty<br />Lambda: 2.0","b:  0.52<br />value: 1.0400<br />Function: Penalty<br />Lambda: 2.0","b:  0.53<br />value: 1.0600<br />Function: Penalty<br />Lambda: 2.0","b:  0.54<br />value: 1.0800<br />Function: Penalty<br />Lambda: 2.0","b:  0.55<br />value: 1.1000<br />Function: Penalty<br />Lambda: 2.0","b:  0.56<br />value: 1.1200<br />Function: Penalty<br />Lambda: 2.0","b:  0.57<br />value: 1.1400<br />Function: Penalty<br />Lambda: 2.0","b:  0.58<br />value: 1.1600<br />Function: Penalty<br />Lambda: 2.0","b:  0.59<br />value: 1.1800<br />Function: Penalty<br />Lambda: 2.0","b:  0.60<br />value: 1.2000<br />Function: Penalty<br />Lambda: 2.0","b:  0.61<br />value: 1.2200<br />Function: Penalty<br />Lambda: 2.0","b:  0.62<br />value: 1.2400<br />Function: Penalty<br />Lambda: 2.0","b:  0.63<br />value: 1.2600<br />Function: Penalty<br />Lambda: 2.0","b:  0.64<br />value: 1.2800<br />Function: Penalty<br />Lambda: 2.0","b:  0.65<br />value: 1.3000<br />Function: Penalty<br />Lambda: 2.0","b:  0.66<br />value: 1.3200<br />Function: Penalty<br />Lambda: 2.0","b:  0.67<br />value: 1.3400<br />Function: Penalty<br />Lambda: 2.0","b:  0.68<br />value: 1.3600<br />Function: Penalty<br />Lambda: 2.0","b:  0.69<br />value: 1.3800<br />Function: Penalty<br />Lambda: 2.0","b:  0.70<br />value: 1.4000<br />Function: Penalty<br />Lambda: 2.0","b:  0.71<br />value: 1.4200<br />Function: Penalty<br />Lambda: 2.0","b:  0.72<br />value: 1.4400<br />Function: Penalty<br />Lambda: 2.0","b:  0.73<br />value: 1.4600<br />Function: Penalty<br />Lambda: 2.0","b:  0.74<br />value: 1.4800<br />Function: Penalty<br />Lambda: 2.0","b:  0.75<br />value: 1.5000<br />Function: Penalty<br />Lambda: 2.0","b:  0.76<br />value: 1.5200<br />Function: Penalty<br />Lambda: 2.0","b:  0.77<br />value: 1.5400<br />Function: Penalty<br />Lambda: 2.0","b:  0.78<br />value: 1.5600<br />Function: Penalty<br />Lambda: 2.0","b:  0.79<br />value: 1.5800<br />Function: Penalty<br />Lambda: 2.0","b:  0.80<br />value: 1.6000<br />Function: Penalty<br />Lambda: 2.0","b:  0.81<br />value: 1.6200<br />Function: Penalty<br />Lambda: 2.0","b:  0.82<br />value: 1.6400<br />Function: Penalty<br />Lambda: 2.0","b:  0.83<br />value: 1.6600<br />Function: Penalty<br />Lambda: 2.0","b:  0.84<br />value: 1.6800<br />Function: Penalty<br />Lambda: 2.0","b:  0.85<br />value: 1.7000<br />Function: Penalty<br />Lambda: 2.0","b:  0.86<br />value: 1.7200<br />Function: Penalty<br />Lambda: 2.0","b:  0.87<br />value: 1.7400<br />Function: Penalty<br />Lambda: 2.0","b:  0.88<br />value: 1.7600<br />Function: Penalty<br />Lambda: 2.0","b:  0.89<br />value: 1.7800<br />Function: Penalty<br />Lambda: 2.0","b:  0.90<br />value: 1.8000<br />Function: Penalty<br />Lambda: 2.0","b:  0.91<br />value: 1.8200<br />Function: Penalty<br />Lambda: 2.0","b:  0.92<br />value: 1.8400<br />Function: Penalty<br />Lambda: 2.0","b:  0.93<br />value: 1.8600<br />Function: Penalty<br />Lambda: 2.0","b:  0.94<br />value: 1.8800<br />Function: Penalty<br />Lambda: 2.0","b:  0.95<br />value: 1.9000<br />Function: Penalty<br />Lambda: 2.0","b:  0.96<br />value: 1.9200<br />Function: Penalty<br />Lambda: 2.0","b:  0.97<br />value: 1.9400<br />Function: Penalty<br />Lambda: 2.0","b:  0.98<br />value: 1.9600<br />Function: Penalty<br />Lambda: 2.0","b:  0.99<br />value: 1.9800<br />Function: Penalty<br />Lambda: 2.0","b:  1.00<br />value: 2.0000<br />Function: Penalty<br />Lambda: 2.0","b:  1.01<br />value: 2.0200<br />Function: Penalty<br />Lambda: 2.0","b:  1.02<br />value: 2.0400<br />Function: Penalty<br />Lambda: 2.0","b:  1.03<br />value: 2.0600<br />Function: Penalty<br />Lambda: 2.0","b:  1.04<br />value: 2.0800<br />Function: Penalty<br />Lambda: 2.0","b:  1.05<br />value: 2.1000<br />Function: Penalty<br />Lambda: 2.0","b:  1.06<br />value: 2.1200<br />Function: Penalty<br />Lambda: 2.0","b:  1.07<br />value: 2.1400<br />Function: Penalty<br />Lambda: 2.0","b:  1.08<br />value: 2.1600<br />Function: Penalty<br />Lambda: 2.0","b:  1.09<br />value: 2.1800<br />Function: Penalty<br />Lambda: 2.0","b:  1.10<br />value: 2.2000<br />Function: Penalty<br />Lambda: 2.0","b:  1.11<br />value: 2.2200<br />Function: Penalty<br />Lambda: 2.0","b:  1.12<br />value: 2.2400<br />Function: Penalty<br />Lambda: 2.0","b:  1.13<br />value: 2.2600<br />Function: Penalty<br />Lambda: 2.0","b:  1.14<br />value: 2.2800<br />Function: Penalty<br />Lambda: 2.0","b:  1.15<br />value: 2.3000<br />Function: Penalty<br />Lambda: 2.0","b:  1.16<br />value: 2.3200<br />Function: Penalty<br />Lambda: 2.0","b:  1.17<br />value: 2.3400<br />Function: Penalty<br />Lambda: 2.0","b:  1.18<br />value: 2.3600<br />Function: Penalty<br />Lambda: 2.0","b:  1.19<br />value: 2.3800<br />Function: Penalty<br />Lambda: 2.0","b:  1.20<br />value: 2.4000<br />Function: Penalty<br />Lambda: 2.0","b:  1.21<br />value: 2.4200<br />Function: Penalty<br />Lambda: 2.0","b:  1.22<br />value: 2.4400<br />Function: Penalty<br />Lambda: 2.0","b:  1.23<br />value: 2.4600<br />Function: Penalty<br />Lambda: 2.0","b:  1.24<br />value: 2.4800<br />Function: Penalty<br />Lambda: 2.0","b:  1.25<br />value: 2.5000<br />Function: Penalty<br />Lambda: 2.0","b:  1.26<br />value: 2.5200<br />Function: Penalty<br />Lambda: 2.0","b:  1.27<br />value: 2.5400<br />Function: Penalty<br />Lambda: 2.0","b:  1.28<br />value: 2.5600<br />Function: Penalty<br />Lambda: 2.0","b:  1.29<br />value: 2.5800<br />Function: Penalty<br />Lambda: 2.0","b:  1.30<br />value: 2.6000<br />Function: Penalty<br />Lambda: 2.0","b:  1.31<br />value: 2.6200<br />Function: Penalty<br />Lambda: 2.0","b:  1.32<br />value: 2.6400<br />Function: Penalty<br />Lambda: 2.0","b:  1.33<br />value: 2.6600<br />Function: Penalty<br />Lambda: 2.0","b:  1.34<br />value: 2.6800<br />Function: Penalty<br />Lambda: 2.0","b:  1.35<br />value: 2.7000<br />Function: Penalty<br />Lambda: 2.0","b:  1.36<br />value: 2.7200<br />Function: Penalty<br />Lambda: 2.0","b:  1.37<br />value: 2.7400<br />Function: Penalty<br />Lambda: 2.0","b:  1.38<br />value: 2.7600<br />Function: Penalty<br />Lambda: 2.0","b:  1.39<br />value: 2.7800<br />Function: Penalty<br />Lambda: 2.0","b:  1.40<br />value: 2.8000<br />Function: Penalty<br />Lambda: 2.0","b:  1.41<br />value: 2.8200<br />Function: Penalty<br />Lambda: 2.0","b:  1.42<br />value: 2.8400<br />Function: Penalty<br />Lambda: 2.0","b:  1.43<br />value: 2.8600<br />Function: Penalty<br />Lambda: 2.0","b:  1.44<br />value: 2.8800<br />Function: Penalty<br />Lambda: 2.0","b:  1.45<br />value: 2.9000<br />Function: Penalty<br />Lambda: 2.0","b:  1.46<br />value: 2.9200<br />Function: Penalty<br />Lambda: 2.0","b:  1.47<br />value: 2.9400<br />Function: Penalty<br />Lambda: 2.0","b:  1.48<br />value: 2.9600<br />Function: Penalty<br />Lambda: 2.0","b:  1.49<br />value: 2.9800<br />Function: Penalty<br />Lambda: 2.0","b:  1.50<br />value: 3.0000<br />Function: Penalty<br />Lambda: 2.0","b:  1.51<br />value: 3.0200<br />Function: Penalty<br />Lambda: 2.0","b:  1.52<br />value: 3.0400<br />Function: Penalty<br />Lambda: 2.0","b:  1.53<br />value: 3.0600<br />Function: Penalty<br />Lambda: 2.0","b:  1.54<br />value: 3.0800<br />Function: Penalty<br />Lambda: 2.0","b:  1.55<br />value: 3.1000<br />Function: Penalty<br />Lambda: 2.0","b:  1.56<br />value: 3.1200<br />Function: Penalty<br />Lambda: 2.0","b:  1.57<br />value: 3.1400<br />Function: Penalty<br />Lambda: 2.0","b:  1.58<br />value: 3.1600<br />Function: Penalty<br />Lambda: 2.0","b:  1.59<br />value: 3.1800<br />Function: Penalty<br />Lambda: 2.0","b:  1.60<br />value: 3.2000<br />Function: Penalty<br />Lambda: 2.0","b:  1.61<br />value: 3.2200<br />Function: Penalty<br />Lambda: 2.0","b:  1.62<br />value: 3.2400<br />Function: Penalty<br />Lambda: 2.0","b:  1.63<br />value: 3.2600<br />Function: Penalty<br />Lambda: 2.0","b:  1.64<br />value: 3.2800<br />Function: Penalty<br />Lambda: 2.0","b:  1.65<br />value: 3.3000<br />Function: Penalty<br />Lambda: 2.0","b:  1.66<br />value: 3.3200<br />Function: Penalty<br />Lambda: 2.0","b:  1.67<br />value: 3.3400<br />Function: Penalty<br />Lambda: 2.0","b:  1.68<br />value: 3.3600<br />Function: Penalty<br />Lambda: 2.0","b:  1.69<br />value: 3.3800<br />Function: Penalty<br />Lambda: 2.0","b:  1.70<br />value: 3.4000<br />Function: Penalty<br />Lambda: 2.0","b:  1.71<br />value: 3.4200<br />Function: Penalty<br />Lambda: 2.0","b:  1.72<br />value: 3.4400<br />Function: Penalty<br />Lambda: 2.0","b:  1.73<br />value: 3.4600<br />Function: Penalty<br />Lambda: 2.0","b:  1.74<br />value: 3.4800<br />Function: Penalty<br />Lambda: 2.0","b:  1.75<br />value: 3.5000<br />Function: Penalty<br />Lambda: 2.0","b:  1.76<br />value: 3.5200<br />Function: Penalty<br />Lambda: 2.0","b:  1.77<br />value: 3.5400<br />Function: Penalty<br />Lambda: 2.0","b:  1.78<br />value: 3.5600<br />Function: Penalty<br />Lambda: 2.0","b:  1.79<br />value: 3.5800<br />Function: Penalty<br />Lambda: 2.0","b:  1.80<br />value: 3.6000<br />Function: Penalty<br />Lambda: 2.0","b:  1.81<br />value: 3.6200<br />Function: Penalty<br />Lambda: 2.0","b:  1.82<br />value: 3.6400<br />Function: Penalty<br />Lambda: 2.0","b:  1.83<br />value: 3.6600<br />Function: Penalty<br />Lambda: 2.0","b:  1.84<br />value: 3.6800<br />Function: Penalty<br />Lambda: 2.0","b:  1.85<br />value: 3.7000<br />Function: Penalty<br />Lambda: 2.0","b:  1.86<br />value: 3.7200<br />Function: Penalty<br />Lambda: 2.0","b:  1.87<br />value: 3.7400<br />Function: Penalty<br />Lambda: 2.0","b:  1.88<br />value: 3.7600<br />Function: Penalty<br />Lambda: 2.0","b:  1.89<br />value: 3.7800<br />Function: Penalty<br />Lambda: 2.0","b:  1.90<br />value: 3.8000<br />Function: Penalty<br />Lambda: 2.0","b:  1.91<br />value: 3.8200<br />Function: Penalty<br />Lambda: 2.0","b:  1.92<br />value: 3.8400<br />Function: Penalty<br />Lambda: 2.0","b:  1.93<br />value: 3.8600<br />Function: Penalty<br />Lambda: 2.0","b:  1.94<br />value: 3.8800<br />Function: Penalty<br />Lambda: 2.0","b:  1.95<br />value: 3.9000<br />Function: Penalty<br />Lambda: 2.0","b:  1.96<br />value: 3.9200<br />Function: Penalty<br />Lambda: 2.0","b:  1.97<br />value: 3.9400<br />Function: Penalty<br />Lambda: 2.0","b:  1.98<br />value: 3.9600<br />Function: Penalty<br />Lambda: 2.0","b:  1.99<br />value: 3.9800<br />Function: Penalty<br />Lambda: 2.0","b:  2.00<br />value: 4.0000<br />Function: Penalty<br />Lambda: 2.0"],"frame":"2","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"2.1","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 2.1","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 2.1","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 2.1","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 2.1","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 2.1","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 2.1","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 2.1","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 2.1","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 2.1","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 2.1","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 2.1","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 2.1","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 2.1","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 2.1","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 2.1","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 2.1","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 2.1","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 2.1","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 2.1","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 2.1","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 2.1","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 2.1","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 2.1","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 2.1","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 2.1","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 2.1","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 2.1","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 2.1","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 2.1","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 2.1","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 2.1","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 2.1","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 2.1","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 2.1","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 2.1","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 2.1","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 2.1","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 2.1","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 2.1","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 2.1","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 2.1","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 2.1","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 2.1","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 2.1","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 2.1","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 2.1","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 2.1","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 2.1","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 2.1","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 2.1","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 2.1","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 2.1","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 2.1","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 2.1","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 2.1","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 2.1","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 2.1","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 2.1","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 2.1","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 2.1","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 2.1","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 2.1","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 2.1","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 2.1","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 2.1","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 2.1","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 2.1","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 2.1","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 2.1","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 2.1","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 2.1","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 2.1","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 2.1","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 2.1","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 2.1","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 2.1","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 2.1","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 2.1","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 2.1","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 2.1","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 2.1","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 2.1","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 2.1","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 2.1","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 2.1","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 2.1","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 2.1","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 2.1","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 2.1","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 2.1","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 2.1","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 2.1","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 2.1","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 2.1","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 2.1","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 2.1","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 2.1","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 2.1","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 2.1","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 2.1","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 2.1","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 2.1","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 2.1","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 2.1","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 2.1","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 2.1","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 2.1","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 2.1","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 2.1","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 2.1","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 2.1","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 2.1","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 2.1","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 2.1","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 2.1","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 2.1","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 2.1","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 2.1","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 2.1","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 2.1","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 2.1","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 2.1","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 2.1","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 2.1","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 2.1","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 2.1","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 2.1","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 2.1","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 2.1","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 2.1","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 2.1","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 2.1","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 2.1","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 2.1","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 2.1","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 2.1","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 2.1","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 2.1","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 2.1","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 2.1","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 2.1","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 2.1","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 2.1","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 2.1","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 2.1","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 2.1","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 2.1","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 2.1","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 2.1","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 2.1","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 2.1","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 2.1","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 2.1","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 2.1","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 2.1","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 2.1","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 2.1","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 2.1","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 2.1","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 2.1","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 2.1","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 2.1","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 2.1","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 2.1","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 2.1","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 2.1","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 2.1","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 2.1","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 2.1","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 2.1","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 2.1","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 2.1","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 2.1","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 2.1","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 2.1","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 2.1","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 2.1","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 2.1","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 2.1","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 2.1","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 2.1","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 2.1","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 2.1","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 2.1","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 2.1","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 2.1","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 2.1","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 2.1","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 2.1","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 2.1","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 2.1","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 2.1","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 2.1","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 2.1","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 2.1","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 2.1","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 2.1","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 2.1","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 2.1","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 2.1","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 2.1","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 2.1","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 2.1","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 2.1","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 2.1","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 2.1","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 2.1","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 2.1","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 2.1","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 2.1","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 2.1","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 2.1","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 2.1","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 2.1","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 2.1","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 2.1","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 2.1","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 2.1","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 2.1","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 2.1","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 2.1","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 2.1","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 2.1","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 2.1","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 2.1","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 2.1","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 2.1","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 2.1","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 2.1","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 2.1","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 2.1","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 2.1","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 2.1","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 2.1","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 2.1","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 2.1","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 2.1","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 2.1","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 2.1","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 2.1","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 2.1","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 2.1","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 2.1","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 2.1","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 2.1","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 2.1","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 2.1","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 2.1","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 2.1","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 2.1","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 2.1"],"frame":"2.1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.8,3.7491,3.6984,3.6479,3.5976,3.5475,3.4976,3.4479,3.3984,3.3491,3.3,3.2511,3.2024,3.1539,3.1056,3.0575,3.0096,2.9619,2.9144,2.8671,2.82,2.7731,2.7264,2.6799,2.6336,2.5875,2.5416,2.4959,2.4504,2.4051,2.36,2.3151,2.2704,2.2259,2.1816,2.1375,2.0936,2.0499,2.0064,1.9631,1.92,1.8771,1.8344,1.7919,1.7496,1.7075,1.6656,1.6239,1.5824,1.5411,1.5,1.5011,1.5024,1.5039,1.5056,1.5075,1.5096,1.5119,1.5144,1.5171,1.52,1.5231,1.5264,1.5299,1.5336,1.5375,1.5416,1.5459,1.5504,1.5551,1.56,1.5651,1.5704,1.5759,1.5816,1.5875,1.5936,1.5999,1.6064,1.6131,1.62,1.6271,1.6344,1.6419,1.6496,1.6575,1.6656,1.6739,1.6824,1.6911,1.7,1.7091,1.7184,1.7279,1.7376,1.7475,1.7576,1.7679,1.7784,1.7891,1.8,1.8111,1.8224,1.8339,1.8456,1.8575,1.8696,1.8819,1.8944,1.9071,1.92,1.9331,1.9464,1.9599,1.9736,1.9875,2.0016,2.0159,2.0304,2.0451,2.06,2.0751,2.0904,2.1059,2.1216,2.1375,2.1536,2.1699,2.1864,2.2031,2.22,2.2371,2.2544,2.2719,2.2896,2.3075,2.3256,2.3439,2.3624,2.3811,2.4,2.4191,2.4384,2.4579,2.4776,2.4975,2.5176,2.5379,2.5584,2.5791,2.6,2.6211,2.6424,2.6639,2.6856,2.7075,2.7296,2.7519,2.7744,2.7971,2.82,2.8431,2.8664,2.8899,2.9136,2.9375,2.9616,2.9859,3.0104,3.0351,3.06,3.0851,3.1104,3.1359,3.1616,3.1875,3.2136,3.2399,3.2664,3.2931,3.32,3.3471,3.3744,3.4019,3.4296,3.4575,3.4856,3.5139,3.5424,3.5711,3.6,3.6291,3.6584,3.6879,3.7176,3.7475,3.7776,3.8079,3.8384,3.8691,3.9,3.9311,3.9624,3.9939,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 3.8000<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.49<br />value: 3.7491<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.48<br />value: 3.6984<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.47<br />value: 3.6479<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.46<br />value: 3.5976<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.45<br />value: 3.5475<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.44<br />value: 3.4976<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.43<br />value: 3.4479<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.42<br />value: 3.3984<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.41<br />value: 3.3491<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.40<br />value: 3.3000<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.39<br />value: 3.2511<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.38<br />value: 3.2024<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.37<br />value: 3.1539<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.36<br />value: 3.1056<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.35<br />value: 3.0575<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.34<br />value: 3.0096<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.33<br />value: 2.9619<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.32<br />value: 2.9144<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.31<br />value: 2.8671<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.30<br />value: 2.8200<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.29<br />value: 2.7731<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.28<br />value: 2.7264<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.27<br />value: 2.6799<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.26<br />value: 2.6336<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.25<br />value: 2.5875<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.24<br />value: 2.5416<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.23<br />value: 2.4959<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.22<br />value: 2.4504<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.21<br />value: 2.4051<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.20<br />value: 2.3600<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.19<br />value: 2.3151<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.18<br />value: 2.2704<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.17<br />value: 2.2259<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.16<br />value: 2.1816<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.15<br />value: 2.1375<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.14<br />value: 2.0936<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.13<br />value: 2.0499<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.12<br />value: 2.0064<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.11<br />value: 1.9631<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.10<br />value: 1.9200<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.09<br />value: 1.8771<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.08<br />value: 1.8344<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.07<br />value: 1.7919<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.06<br />value: 1.7496<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.05<br />value: 1.7075<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.04<br />value: 1.6656<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.03<br />value: 1.6239<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.02<br />value: 1.5824<br />Function: Loss + Penalty<br />Lambda: 2.1","b: -0.01<br />value: 1.5411<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.01<br />value: 1.5011<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.02<br />value: 1.5024<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.03<br />value: 1.5039<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.04<br />value: 1.5056<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.05<br />value: 1.5075<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.06<br />value: 1.5096<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.07<br />value: 1.5119<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.08<br />value: 1.5144<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.09<br />value: 1.5171<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.10<br />value: 1.5200<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.11<br />value: 1.5231<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.12<br />value: 1.5264<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.13<br />value: 1.5299<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.14<br />value: 1.5336<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.15<br />value: 1.5375<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.16<br />value: 1.5416<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.17<br />value: 1.5459<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.18<br />value: 1.5504<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.19<br />value: 1.5551<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.20<br />value: 1.5600<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.21<br />value: 1.5651<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.22<br />value: 1.5704<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.23<br />value: 1.5759<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.24<br />value: 1.5816<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.25<br />value: 1.5875<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.26<br />value: 1.5936<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.27<br />value: 1.5999<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.28<br />value: 1.6064<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.29<br />value: 1.6131<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.30<br />value: 1.6200<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.31<br />value: 1.6271<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.32<br />value: 1.6344<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.33<br />value: 1.6419<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.34<br />value: 1.6496<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.35<br />value: 1.6575<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.36<br />value: 1.6656<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.37<br />value: 1.6739<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.38<br />value: 1.6824<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.39<br />value: 1.6911<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.40<br />value: 1.7000<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.41<br />value: 1.7091<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.42<br />value: 1.7184<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.43<br />value: 1.7279<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.44<br />value: 1.7376<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.45<br />value: 1.7475<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.46<br />value: 1.7576<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.47<br />value: 1.7679<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.48<br />value: 1.7784<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.49<br />value: 1.7891<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.50<br />value: 1.8000<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.51<br />value: 1.8111<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.52<br />value: 1.8224<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.53<br />value: 1.8339<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.54<br />value: 1.8456<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.55<br />value: 1.8575<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.56<br />value: 1.8696<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.57<br />value: 1.8819<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.58<br />value: 1.8944<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.59<br />value: 1.9071<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.60<br />value: 1.9200<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.61<br />value: 1.9331<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.62<br />value: 1.9464<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.63<br />value: 1.9599<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.64<br />value: 1.9736<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.65<br />value: 1.9875<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.66<br />value: 2.0016<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.67<br />value: 2.0159<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.68<br />value: 2.0304<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.69<br />value: 2.0451<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.70<br />value: 2.0600<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.71<br />value: 2.0751<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.72<br />value: 2.0904<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.73<br />value: 2.1059<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.74<br />value: 2.1216<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.75<br />value: 2.1375<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.76<br />value: 2.1536<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.77<br />value: 2.1699<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.78<br />value: 2.1864<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.79<br />value: 2.2031<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.80<br />value: 2.2200<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.81<br />value: 2.2371<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.82<br />value: 2.2544<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.83<br />value: 2.2719<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.84<br />value: 2.2896<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.85<br />value: 2.3075<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.86<br />value: 2.3256<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.87<br />value: 2.3439<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.88<br />value: 2.3624<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.89<br />value: 2.3811<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.90<br />value: 2.4000<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.91<br />value: 2.4191<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.92<br />value: 2.4384<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.93<br />value: 2.4579<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.94<br />value: 2.4776<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.95<br />value: 2.4975<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.96<br />value: 2.5176<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.97<br />value: 2.5379<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.98<br />value: 2.5584<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  0.99<br />value: 2.5791<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.00<br />value: 2.6000<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.01<br />value: 2.6211<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.02<br />value: 2.6424<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.03<br />value: 2.6639<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.04<br />value: 2.6856<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.05<br />value: 2.7075<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.06<br />value: 2.7296<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.07<br />value: 2.7519<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.08<br />value: 2.7744<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.09<br />value: 2.7971<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.10<br />value: 2.8200<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.11<br />value: 2.8431<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.12<br />value: 2.8664<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.13<br />value: 2.8899<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.14<br />value: 2.9136<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.15<br />value: 2.9375<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.16<br />value: 2.9616<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.17<br />value: 2.9859<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.18<br />value: 3.0104<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.19<br />value: 3.0351<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.20<br />value: 3.0600<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.21<br />value: 3.0851<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.22<br />value: 3.1104<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.23<br />value: 3.1359<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.24<br />value: 3.1616<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.25<br />value: 3.1875<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.26<br />value: 3.2136<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.27<br />value: 3.2399<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.28<br />value: 3.2664<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.29<br />value: 3.2931<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.30<br />value: 3.3200<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.31<br />value: 3.3471<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.32<br />value: 3.3744<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.33<br />value: 3.4019<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.34<br />value: 3.4296<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.35<br />value: 3.4575<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.36<br />value: 3.4856<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.37<br />value: 3.5139<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.38<br />value: 3.5424<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.39<br />value: 3.5711<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.40<br />value: 3.6000<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.41<br />value: 3.6291<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.42<br />value: 3.6584<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.43<br />value: 3.6879<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.44<br />value: 3.7176<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.45<br />value: 3.7475<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.46<br />value: 3.7776<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.47<br />value: 3.8079<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.48<br />value: 3.8384<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.49<br />value: 3.8691<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.50<br />value: 3.9000<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.51<br />value: 3.9311<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.52<br />value: 3.9624<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.53<br />value: 3.9939<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.54<br />value: 4.0256<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.55<br />value: 4.0575<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.56<br />value: 4.0896<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.57<br />value: 4.1219<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.58<br />value: 4.1544<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.59<br />value: 4.1871<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.60<br />value: 4.2200<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.61<br />value: 4.2531<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.62<br />value: 4.2864<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.63<br />value: 4.3199<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.64<br />value: 4.3536<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.65<br />value: 4.3875<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.66<br />value: 4.4216<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.67<br />value: 4.4559<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.68<br />value: 4.4904<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.69<br />value: 4.5251<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.70<br />value: 4.5600<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.71<br />value: 4.5951<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.72<br />value: 4.6304<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.73<br />value: 4.6659<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.74<br />value: 4.7016<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.75<br />value: 4.7375<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.76<br />value: 4.7736<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.77<br />value: 4.8099<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.78<br />value: 4.8464<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.79<br />value: 4.8831<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.80<br />value: 4.9200<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.81<br />value: 4.9571<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.82<br />value: 4.9944<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.83<br />value: 5.0319<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.84<br />value: 5.0696<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.85<br />value: 5.1075<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.86<br />value: 5.1456<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.87<br />value: 5.1839<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.88<br />value: 5.2224<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.89<br />value: 5.2611<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.90<br />value: 5.3000<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.91<br />value: 5.3391<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.92<br />value: 5.3784<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.93<br />value: 5.4179<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.94<br />value: 5.4576<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.95<br />value: 5.4975<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.96<br />value: 5.5376<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.97<br />value: 5.5779<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.98<br />value: 5.6184<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  1.99<br />value: 5.6591<br />Function: Loss + Penalty<br />Lambda: 2.1","b:  2.00<br />value: 5.7000<br />Function: Loss + Penalty<br />Lambda: 2.1"],"frame":"2.1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[1.05,1.029,1.008,0.987,0.966,0.945,0.924,0.903,0.882,0.861,0.84,0.819,0.798,0.777,0.756,0.735,0.714,0.693,0.672,0.651,0.63,0.609,0.588,0.567,0.546,0.525,0.504,0.483,0.462,0.441,0.42,0.399,0.378,0.357,0.336,0.315,0.294,0.273,0.252,0.231,0.21,0.189,0.168,0.147,0.126,0.105,0.084,0.0629999999999999,0.042,0.021,0,0.021,0.042,0.0630000000000001,0.0840000000000001,0.105,0.126,0.147,0.168,0.189,0.21,0.231,0.252,0.273,0.294,0.315,0.336,0.357,0.378,0.399,0.42,0.441,0.462,0.483,0.504,0.525,0.546,0.567,0.588,0.609,0.63,0.651,0.672,0.693,0.714,0.735,0.756,0.777,0.798,0.819,0.84,0.861,0.882,0.903,0.924,0.945,0.966,0.987,1.008,1.029,1.05,1.071,1.092,1.113,1.134,1.155,1.176,1.197,1.218,1.239,1.26,1.281,1.302,1.323,1.344,1.365,1.386,1.407,1.428,1.449,1.47,1.491,1.512,1.533,1.554,1.575,1.596,1.617,1.638,1.659,1.68,1.701,1.722,1.743,1.764,1.785,1.806,1.827,1.848,1.869,1.89,1.911,1.932,1.953,1.974,1.995,2.016,2.037,2.058,2.079,2.1,2.121,2.142,2.163,2.184,2.205,2.226,2.247,2.268,2.289,2.31,2.331,2.352,2.373,2.394,2.415,2.436,2.457,2.478,2.499,2.52,2.541,2.562,2.583,2.604,2.625,2.646,2.667,2.688,2.709,2.73,2.751,2.772,2.793,2.814,2.835,2.856,2.877,2.898,2.919,2.94,2.961,2.982,3.003,3.024,3.045,3.066,3.087,3.108,3.129,3.15,3.171,3.192,3.213,3.234,3.255,3.276,3.297,3.318,3.339,3.36,3.381,3.402,3.423,3.444,3.465,3.486,3.507,3.528,3.549,3.57,3.591,3.612,3.633,3.654,3.675,3.696,3.717,3.738,3.759,3.78,3.801,3.822,3.843,3.864,3.885,3.906,3.927,3.948,3.969,3.99,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 1.0500<br />Function: Penalty<br />Lambda: 2.1","b: -0.49<br />value: 1.0290<br />Function: Penalty<br />Lambda: 2.1","b: -0.48<br />value: 1.0080<br />Function: Penalty<br />Lambda: 2.1","b: -0.47<br />value: 0.9870<br />Function: Penalty<br />Lambda: 2.1","b: -0.46<br />value: 0.9660<br />Function: Penalty<br />Lambda: 2.1","b: -0.45<br />value: 0.9450<br />Function: Penalty<br />Lambda: 2.1","b: -0.44<br />value: 0.9240<br />Function: Penalty<br />Lambda: 2.1","b: -0.43<br />value: 0.9030<br />Function: Penalty<br />Lambda: 2.1","b: -0.42<br />value: 0.8820<br />Function: Penalty<br />Lambda: 2.1","b: -0.41<br />value: 0.8610<br />Function: Penalty<br />Lambda: 2.1","b: -0.40<br />value: 0.8400<br />Function: Penalty<br />Lambda: 2.1","b: -0.39<br />value: 0.8190<br />Function: Penalty<br />Lambda: 2.1","b: -0.38<br />value: 0.7980<br />Function: Penalty<br />Lambda: 2.1","b: -0.37<br />value: 0.7770<br />Function: Penalty<br />Lambda: 2.1","b: -0.36<br />value: 0.7560<br />Function: Penalty<br />Lambda: 2.1","b: -0.35<br />value: 0.7350<br />Function: Penalty<br />Lambda: 2.1","b: -0.34<br />value: 0.7140<br />Function: Penalty<br />Lambda: 2.1","b: -0.33<br />value: 0.6930<br />Function: Penalty<br />Lambda: 2.1","b: -0.32<br />value: 0.6720<br />Function: Penalty<br />Lambda: 2.1","b: -0.31<br />value: 0.6510<br />Function: Penalty<br />Lambda: 2.1","b: -0.30<br />value: 0.6300<br />Function: Penalty<br />Lambda: 2.1","b: -0.29<br />value: 0.6090<br />Function: Penalty<br />Lambda: 2.1","b: -0.28<br />value: 0.5880<br />Function: Penalty<br />Lambda: 2.1","b: -0.27<br />value: 0.5670<br />Function: Penalty<br />Lambda: 2.1","b: -0.26<br />value: 0.5460<br />Function: Penalty<br />Lambda: 2.1","b: -0.25<br />value: 0.5250<br />Function: Penalty<br />Lambda: 2.1","b: -0.24<br />value: 0.5040<br />Function: Penalty<br />Lambda: 2.1","b: -0.23<br />value: 0.4830<br />Function: Penalty<br />Lambda: 2.1","b: -0.22<br />value: 0.4620<br />Function: Penalty<br />Lambda: 2.1","b: -0.21<br />value: 0.4410<br />Function: Penalty<br />Lambda: 2.1","b: -0.20<br />value: 0.4200<br />Function: Penalty<br />Lambda: 2.1","b: -0.19<br />value: 0.3990<br />Function: Penalty<br />Lambda: 2.1","b: -0.18<br />value: 0.3780<br />Function: Penalty<br />Lambda: 2.1","b: -0.17<br />value: 0.3570<br />Function: Penalty<br />Lambda: 2.1","b: -0.16<br />value: 0.3360<br />Function: Penalty<br />Lambda: 2.1","b: -0.15<br />value: 0.3150<br />Function: Penalty<br />Lambda: 2.1","b: -0.14<br />value: 0.2940<br />Function: Penalty<br />Lambda: 2.1","b: -0.13<br />value: 0.2730<br />Function: Penalty<br />Lambda: 2.1","b: -0.12<br />value: 0.2520<br />Function: Penalty<br />Lambda: 2.1","b: -0.11<br />value: 0.2310<br />Function: Penalty<br />Lambda: 2.1","b: -0.10<br />value: 0.2100<br />Function: Penalty<br />Lambda: 2.1","b: -0.09<br />value: 0.1890<br />Function: Penalty<br />Lambda: 2.1","b: -0.08<br />value: 0.1680<br />Function: Penalty<br />Lambda: 2.1","b: -0.07<br />value: 0.1470<br />Function: Penalty<br />Lambda: 2.1","b: -0.06<br />value: 0.1260<br />Function: Penalty<br />Lambda: 2.1","b: -0.05<br />value: 0.1050<br />Function: Penalty<br />Lambda: 2.1","b: -0.04<br />value: 0.0840<br />Function: Penalty<br />Lambda: 2.1","b: -0.03<br />value: 0.0630<br />Function: Penalty<br />Lambda: 2.1","b: -0.02<br />value: 0.0420<br />Function: Penalty<br />Lambda: 2.1","b: -0.01<br />value: 0.0210<br />Function: Penalty<br />Lambda: 2.1","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 2.1","b:  0.01<br />value: 0.0210<br />Function: Penalty<br />Lambda: 2.1","b:  0.02<br />value: 0.0420<br />Function: Penalty<br />Lambda: 2.1","b:  0.03<br />value: 0.0630<br />Function: Penalty<br />Lambda: 2.1","b:  0.04<br />value: 0.0840<br />Function: Penalty<br />Lambda: 2.1","b:  0.05<br />value: 0.1050<br />Function: Penalty<br />Lambda: 2.1","b:  0.06<br />value: 0.1260<br />Function: Penalty<br />Lambda: 2.1","b:  0.07<br />value: 0.1470<br />Function: Penalty<br />Lambda: 2.1","b:  0.08<br />value: 0.1680<br />Function: Penalty<br />Lambda: 2.1","b:  0.09<br />value: 0.1890<br />Function: Penalty<br />Lambda: 2.1","b:  0.10<br />value: 0.2100<br />Function: Penalty<br />Lambda: 2.1","b:  0.11<br />value: 0.2310<br />Function: Penalty<br />Lambda: 2.1","b:  0.12<br />value: 0.2520<br />Function: Penalty<br />Lambda: 2.1","b:  0.13<br />value: 0.2730<br />Function: Penalty<br />Lambda: 2.1","b:  0.14<br />value: 0.2940<br />Function: Penalty<br />Lambda: 2.1","b:  0.15<br />value: 0.3150<br />Function: Penalty<br />Lambda: 2.1","b:  0.16<br />value: 0.3360<br />Function: Penalty<br />Lambda: 2.1","b:  0.17<br />value: 0.3570<br />Function: Penalty<br />Lambda: 2.1","b:  0.18<br />value: 0.3780<br />Function: Penalty<br />Lambda: 2.1","b:  0.19<br />value: 0.3990<br />Function: Penalty<br />Lambda: 2.1","b:  0.20<br />value: 0.4200<br />Function: Penalty<br />Lambda: 2.1","b:  0.21<br />value: 0.4410<br />Function: Penalty<br />Lambda: 2.1","b:  0.22<br />value: 0.4620<br />Function: Penalty<br />Lambda: 2.1","b:  0.23<br />value: 0.4830<br />Function: Penalty<br />Lambda: 2.1","b:  0.24<br />value: 0.5040<br />Function: Penalty<br />Lambda: 2.1","b:  0.25<br />value: 0.5250<br />Function: Penalty<br />Lambda: 2.1","b:  0.26<br />value: 0.5460<br />Function: Penalty<br />Lambda: 2.1","b:  0.27<br />value: 0.5670<br />Function: Penalty<br />Lambda: 2.1","b:  0.28<br />value: 0.5880<br />Function: Penalty<br />Lambda: 2.1","b:  0.29<br />value: 0.6090<br />Function: Penalty<br />Lambda: 2.1","b:  0.30<br />value: 0.6300<br />Function: Penalty<br />Lambda: 2.1","b:  0.31<br />value: 0.6510<br />Function: Penalty<br />Lambda: 2.1","b:  0.32<br />value: 0.6720<br />Function: Penalty<br />Lambda: 2.1","b:  0.33<br />value: 0.6930<br />Function: Penalty<br />Lambda: 2.1","b:  0.34<br />value: 0.7140<br />Function: Penalty<br />Lambda: 2.1","b:  0.35<br />value: 0.7350<br />Function: Penalty<br />Lambda: 2.1","b:  0.36<br />value: 0.7560<br />Function: Penalty<br />Lambda: 2.1","b:  0.37<br />value: 0.7770<br />Function: Penalty<br />Lambda: 2.1","b:  0.38<br />value: 0.7980<br />Function: Penalty<br />Lambda: 2.1","b:  0.39<br />value: 0.8190<br />Function: Penalty<br />Lambda: 2.1","b:  0.40<br />value: 0.8400<br />Function: Penalty<br />Lambda: 2.1","b:  0.41<br />value: 0.8610<br />Function: Penalty<br />Lambda: 2.1","b:  0.42<br />value: 0.8820<br />Function: Penalty<br />Lambda: 2.1","b:  0.43<br />value: 0.9030<br />Function: Penalty<br />Lambda: 2.1","b:  0.44<br />value: 0.9240<br />Function: Penalty<br />Lambda: 2.1","b:  0.45<br />value: 0.9450<br />Function: Penalty<br />Lambda: 2.1","b:  0.46<br />value: 0.9660<br />Function: Penalty<br />Lambda: 2.1","b:  0.47<br />value: 0.9870<br />Function: Penalty<br />Lambda: 2.1","b:  0.48<br />value: 1.0080<br />Function: Penalty<br />Lambda: 2.1","b:  0.49<br />value: 1.0290<br />Function: Penalty<br />Lambda: 2.1","b:  0.50<br />value: 1.0500<br />Function: Penalty<br />Lambda: 2.1","b:  0.51<br />value: 1.0710<br />Function: Penalty<br />Lambda: 2.1","b:  0.52<br />value: 1.0920<br />Function: Penalty<br />Lambda: 2.1","b:  0.53<br />value: 1.1130<br />Function: Penalty<br />Lambda: 2.1","b:  0.54<br />value: 1.1340<br />Function: Penalty<br />Lambda: 2.1","b:  0.55<br />value: 1.1550<br />Function: Penalty<br />Lambda: 2.1","b:  0.56<br />value: 1.1760<br />Function: Penalty<br />Lambda: 2.1","b:  0.57<br />value: 1.1970<br />Function: Penalty<br />Lambda: 2.1","b:  0.58<br />value: 1.2180<br />Function: Penalty<br />Lambda: 2.1","b:  0.59<br />value: 1.2390<br />Function: Penalty<br />Lambda: 2.1","b:  0.60<br />value: 1.2600<br />Function: Penalty<br />Lambda: 2.1","b:  0.61<br />value: 1.2810<br />Function: Penalty<br />Lambda: 2.1","b:  0.62<br />value: 1.3020<br />Function: Penalty<br />Lambda: 2.1","b:  0.63<br />value: 1.3230<br />Function: Penalty<br />Lambda: 2.1","b:  0.64<br />value: 1.3440<br />Function: Penalty<br />Lambda: 2.1","b:  0.65<br />value: 1.3650<br />Function: Penalty<br />Lambda: 2.1","b:  0.66<br />value: 1.3860<br />Function: Penalty<br />Lambda: 2.1","b:  0.67<br />value: 1.4070<br />Function: Penalty<br />Lambda: 2.1","b:  0.68<br />value: 1.4280<br />Function: Penalty<br />Lambda: 2.1","b:  0.69<br />value: 1.4490<br />Function: Penalty<br />Lambda: 2.1","b:  0.70<br />value: 1.4700<br />Function: Penalty<br />Lambda: 2.1","b:  0.71<br />value: 1.4910<br />Function: Penalty<br />Lambda: 2.1","b:  0.72<br />value: 1.5120<br />Function: Penalty<br />Lambda: 2.1","b:  0.73<br />value: 1.5330<br />Function: Penalty<br />Lambda: 2.1","b:  0.74<br />value: 1.5540<br />Function: Penalty<br />Lambda: 2.1","b:  0.75<br />value: 1.5750<br />Function: Penalty<br />Lambda: 2.1","b:  0.76<br />value: 1.5960<br />Function: Penalty<br />Lambda: 2.1","b:  0.77<br />value: 1.6170<br />Function: Penalty<br />Lambda: 2.1","b:  0.78<br />value: 1.6380<br />Function: Penalty<br />Lambda: 2.1","b:  0.79<br />value: 1.6590<br />Function: Penalty<br />Lambda: 2.1","b:  0.80<br />value: 1.6800<br />Function: Penalty<br />Lambda: 2.1","b:  0.81<br />value: 1.7010<br />Function: Penalty<br />Lambda: 2.1","b:  0.82<br />value: 1.7220<br />Function: Penalty<br />Lambda: 2.1","b:  0.83<br />value: 1.7430<br />Function: Penalty<br />Lambda: 2.1","b:  0.84<br />value: 1.7640<br />Function: Penalty<br />Lambda: 2.1","b:  0.85<br />value: 1.7850<br />Function: Penalty<br />Lambda: 2.1","b:  0.86<br />value: 1.8060<br />Function: Penalty<br />Lambda: 2.1","b:  0.87<br />value: 1.8270<br />Function: Penalty<br />Lambda: 2.1","b:  0.88<br />value: 1.8480<br />Function: Penalty<br />Lambda: 2.1","b:  0.89<br />value: 1.8690<br />Function: Penalty<br />Lambda: 2.1","b:  0.90<br />value: 1.8900<br />Function: Penalty<br />Lambda: 2.1","b:  0.91<br />value: 1.9110<br />Function: Penalty<br />Lambda: 2.1","b:  0.92<br />value: 1.9320<br />Function: Penalty<br />Lambda: 2.1","b:  0.93<br />value: 1.9530<br />Function: Penalty<br />Lambda: 2.1","b:  0.94<br />value: 1.9740<br />Function: Penalty<br />Lambda: 2.1","b:  0.95<br />value: 1.9950<br />Function: Penalty<br />Lambda: 2.1","b:  0.96<br />value: 2.0160<br />Function: Penalty<br />Lambda: 2.1","b:  0.97<br />value: 2.0370<br />Function: Penalty<br />Lambda: 2.1","b:  0.98<br />value: 2.0580<br />Function: Penalty<br />Lambda: 2.1","b:  0.99<br />value: 2.0790<br />Function: Penalty<br />Lambda: 2.1","b:  1.00<br />value: 2.1000<br />Function: Penalty<br />Lambda: 2.1","b:  1.01<br />value: 2.1210<br />Function: Penalty<br />Lambda: 2.1","b:  1.02<br />value: 2.1420<br />Function: Penalty<br />Lambda: 2.1","b:  1.03<br />value: 2.1630<br />Function: Penalty<br />Lambda: 2.1","b:  1.04<br />value: 2.1840<br />Function: Penalty<br />Lambda: 2.1","b:  1.05<br />value: 2.2050<br />Function: Penalty<br />Lambda: 2.1","b:  1.06<br />value: 2.2260<br />Function: Penalty<br />Lambda: 2.1","b:  1.07<br />value: 2.2470<br />Function: Penalty<br />Lambda: 2.1","b:  1.08<br />value: 2.2680<br />Function: Penalty<br />Lambda: 2.1","b:  1.09<br />value: 2.2890<br />Function: Penalty<br />Lambda: 2.1","b:  1.10<br />value: 2.3100<br />Function: Penalty<br />Lambda: 2.1","b:  1.11<br />value: 2.3310<br />Function: Penalty<br />Lambda: 2.1","b:  1.12<br />value: 2.3520<br />Function: Penalty<br />Lambda: 2.1","b:  1.13<br />value: 2.3730<br />Function: Penalty<br />Lambda: 2.1","b:  1.14<br />value: 2.3940<br />Function: Penalty<br />Lambda: 2.1","b:  1.15<br />value: 2.4150<br />Function: Penalty<br />Lambda: 2.1","b:  1.16<br />value: 2.4360<br />Function: Penalty<br />Lambda: 2.1","b:  1.17<br />value: 2.4570<br />Function: Penalty<br />Lambda: 2.1","b:  1.18<br />value: 2.4780<br />Function: Penalty<br />Lambda: 2.1","b:  1.19<br />value: 2.4990<br />Function: Penalty<br />Lambda: 2.1","b:  1.20<br />value: 2.5200<br />Function: Penalty<br />Lambda: 2.1","b:  1.21<br />value: 2.5410<br />Function: Penalty<br />Lambda: 2.1","b:  1.22<br />value: 2.5620<br />Function: Penalty<br />Lambda: 2.1","b:  1.23<br />value: 2.5830<br />Function: Penalty<br />Lambda: 2.1","b:  1.24<br />value: 2.6040<br />Function: Penalty<br />Lambda: 2.1","b:  1.25<br />value: 2.6250<br />Function: Penalty<br />Lambda: 2.1","b:  1.26<br />value: 2.6460<br />Function: Penalty<br />Lambda: 2.1","b:  1.27<br />value: 2.6670<br />Function: Penalty<br />Lambda: 2.1","b:  1.28<br />value: 2.6880<br />Function: Penalty<br />Lambda: 2.1","b:  1.29<br />value: 2.7090<br />Function: Penalty<br />Lambda: 2.1","b:  1.30<br />value: 2.7300<br />Function: Penalty<br />Lambda: 2.1","b:  1.31<br />value: 2.7510<br />Function: Penalty<br />Lambda: 2.1","b:  1.32<br />value: 2.7720<br />Function: Penalty<br />Lambda: 2.1","b:  1.33<br />value: 2.7930<br />Function: Penalty<br />Lambda: 2.1","b:  1.34<br />value: 2.8140<br />Function: Penalty<br />Lambda: 2.1","b:  1.35<br />value: 2.8350<br />Function: Penalty<br />Lambda: 2.1","b:  1.36<br />value: 2.8560<br />Function: Penalty<br />Lambda: 2.1","b:  1.37<br />value: 2.8770<br />Function: Penalty<br />Lambda: 2.1","b:  1.38<br />value: 2.8980<br />Function: Penalty<br />Lambda: 2.1","b:  1.39<br />value: 2.9190<br />Function: Penalty<br />Lambda: 2.1","b:  1.40<br />value: 2.9400<br />Function: Penalty<br />Lambda: 2.1","b:  1.41<br />value: 2.9610<br />Function: Penalty<br />Lambda: 2.1","b:  1.42<br />value: 2.9820<br />Function: Penalty<br />Lambda: 2.1","b:  1.43<br />value: 3.0030<br />Function: Penalty<br />Lambda: 2.1","b:  1.44<br />value: 3.0240<br />Function: Penalty<br />Lambda: 2.1","b:  1.45<br />value: 3.0450<br />Function: Penalty<br />Lambda: 2.1","b:  1.46<br />value: 3.0660<br />Function: Penalty<br />Lambda: 2.1","b:  1.47<br />value: 3.0870<br />Function: Penalty<br />Lambda: 2.1","b:  1.48<br />value: 3.1080<br />Function: Penalty<br />Lambda: 2.1","b:  1.49<br />value: 3.1290<br />Function: Penalty<br />Lambda: 2.1","b:  1.50<br />value: 3.1500<br />Function: Penalty<br />Lambda: 2.1","b:  1.51<br />value: 3.1710<br />Function: Penalty<br />Lambda: 2.1","b:  1.52<br />value: 3.1920<br />Function: Penalty<br />Lambda: 2.1","b:  1.53<br />value: 3.2130<br />Function: Penalty<br />Lambda: 2.1","b:  1.54<br />value: 3.2340<br />Function: Penalty<br />Lambda: 2.1","b:  1.55<br />value: 3.2550<br />Function: Penalty<br />Lambda: 2.1","b:  1.56<br />value: 3.2760<br />Function: Penalty<br />Lambda: 2.1","b:  1.57<br />value: 3.2970<br />Function: Penalty<br />Lambda: 2.1","b:  1.58<br />value: 3.3180<br />Function: Penalty<br />Lambda: 2.1","b:  1.59<br />value: 3.3390<br />Function: Penalty<br />Lambda: 2.1","b:  1.60<br />value: 3.3600<br />Function: Penalty<br />Lambda: 2.1","b:  1.61<br />value: 3.3810<br />Function: Penalty<br />Lambda: 2.1","b:  1.62<br />value: 3.4020<br />Function: Penalty<br />Lambda: 2.1","b:  1.63<br />value: 3.4230<br />Function: Penalty<br />Lambda: 2.1","b:  1.64<br />value: 3.4440<br />Function: Penalty<br />Lambda: 2.1","b:  1.65<br />value: 3.4650<br />Function: Penalty<br />Lambda: 2.1","b:  1.66<br />value: 3.4860<br />Function: Penalty<br />Lambda: 2.1","b:  1.67<br />value: 3.5070<br />Function: Penalty<br />Lambda: 2.1","b:  1.68<br />value: 3.5280<br />Function: Penalty<br />Lambda: 2.1","b:  1.69<br />value: 3.5490<br />Function: Penalty<br />Lambda: 2.1","b:  1.70<br />value: 3.5700<br />Function: Penalty<br />Lambda: 2.1","b:  1.71<br />value: 3.5910<br />Function: Penalty<br />Lambda: 2.1","b:  1.72<br />value: 3.6120<br />Function: Penalty<br />Lambda: 2.1","b:  1.73<br />value: 3.6330<br />Function: Penalty<br />Lambda: 2.1","b:  1.74<br />value: 3.6540<br />Function: Penalty<br />Lambda: 2.1","b:  1.75<br />value: 3.6750<br />Function: Penalty<br />Lambda: 2.1","b:  1.76<br />value: 3.6960<br />Function: Penalty<br />Lambda: 2.1","b:  1.77<br />value: 3.7170<br />Function: Penalty<br />Lambda: 2.1","b:  1.78<br />value: 3.7380<br />Function: Penalty<br />Lambda: 2.1","b:  1.79<br />value: 3.7590<br />Function: Penalty<br />Lambda: 2.1","b:  1.80<br />value: 3.7800<br />Function: Penalty<br />Lambda: 2.1","b:  1.81<br />value: 3.8010<br />Function: Penalty<br />Lambda: 2.1","b:  1.82<br />value: 3.8220<br />Function: Penalty<br />Lambda: 2.1","b:  1.83<br />value: 3.8430<br />Function: Penalty<br />Lambda: 2.1","b:  1.84<br />value: 3.8640<br />Function: Penalty<br />Lambda: 2.1","b:  1.85<br />value: 3.8850<br />Function: Penalty<br />Lambda: 2.1","b:  1.86<br />value: 3.9060<br />Function: Penalty<br />Lambda: 2.1","b:  1.87<br />value: 3.9270<br />Function: Penalty<br />Lambda: 2.1","b:  1.88<br />value: 3.9480<br />Function: Penalty<br />Lambda: 2.1","b:  1.89<br />value: 3.9690<br />Function: Penalty<br />Lambda: 2.1","b:  1.90<br />value: 3.9900<br />Function: Penalty<br />Lambda: 2.1","b:  1.91<br />value: 4.0110<br />Function: Penalty<br />Lambda: 2.1","b:  1.92<br />value: 4.0320<br />Function: Penalty<br />Lambda: 2.1","b:  1.93<br />value: 4.0530<br />Function: Penalty<br />Lambda: 2.1","b:  1.94<br />value: 4.0740<br />Function: Penalty<br />Lambda: 2.1","b:  1.95<br />value: 4.0950<br />Function: Penalty<br />Lambda: 2.1","b:  1.96<br />value: 4.1160<br />Function: Penalty<br />Lambda: 2.1","b:  1.97<br />value: 4.1370<br />Function: Penalty<br />Lambda: 2.1","b:  1.98<br />value: 4.1580<br />Function: Penalty<br />Lambda: 2.1","b:  1.99<br />value: 4.1790<br />Function: Penalty<br />Lambda: 2.1","b:  2.00<br />value: 4.2000<br />Function: Penalty<br />Lambda: 2.1"],"frame":"2.1","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"2.2","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 2.2","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 2.2","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 2.2","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 2.2","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 2.2","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 2.2","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 2.2","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 2.2","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 2.2","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 2.2","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 2.2","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 2.2","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 2.2","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 2.2","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 2.2","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 2.2","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 2.2","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 2.2","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 2.2","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 2.2","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 2.2","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 2.2","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 2.2","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 2.2","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 2.2","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 2.2","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 2.2","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 2.2","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 2.2","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 2.2","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 2.2","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 2.2","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 2.2","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 2.2","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 2.2","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 2.2","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 2.2","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 2.2","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 2.2","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 2.2","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 2.2","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 2.2","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 2.2","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 2.2","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 2.2","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 2.2","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 2.2","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 2.2","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 2.2","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 2.2","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 2.2","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 2.2","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 2.2","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 2.2","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 2.2","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 2.2","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 2.2","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 2.2","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 2.2","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 2.2","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 2.2","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 2.2","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 2.2","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 2.2","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 2.2","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 2.2","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 2.2","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 2.2","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 2.2","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 2.2","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 2.2","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 2.2","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 2.2","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 2.2","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 2.2","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 2.2","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 2.2","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 2.2","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 2.2","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 2.2","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 2.2","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 2.2","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 2.2","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 2.2","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 2.2","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 2.2","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 2.2","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 2.2","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 2.2","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 2.2","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 2.2","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 2.2","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 2.2","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 2.2","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 2.2","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 2.2","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 2.2","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 2.2","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 2.2","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 2.2","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 2.2","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 2.2","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 2.2","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 2.2","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 2.2","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 2.2","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 2.2","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 2.2","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 2.2","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 2.2","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 2.2","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 2.2","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 2.2","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 2.2","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 2.2","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 2.2","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 2.2","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 2.2","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 2.2","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 2.2","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 2.2","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 2.2","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 2.2","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 2.2","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 2.2","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 2.2","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 2.2","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 2.2","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 2.2","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 2.2","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 2.2","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 2.2","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 2.2","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 2.2","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 2.2","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 2.2","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 2.2","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 2.2","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 2.2","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 2.2","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 2.2","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 2.2","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 2.2","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 2.2","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 2.2","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 2.2","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 2.2","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 2.2","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 2.2","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 2.2","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 2.2","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 2.2","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 2.2","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 2.2","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 2.2","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 2.2","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 2.2","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 2.2","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 2.2","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 2.2","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 2.2","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 2.2","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 2.2","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 2.2","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 2.2","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 2.2","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 2.2","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 2.2","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 2.2","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 2.2","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 2.2","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 2.2","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 2.2","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 2.2","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 2.2","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 2.2","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 2.2","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 2.2","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 2.2","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 2.2","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 2.2","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 2.2","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 2.2","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 2.2","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 2.2","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 2.2","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 2.2","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 2.2","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 2.2","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 2.2","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 2.2","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 2.2","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 2.2","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 2.2","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 2.2","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 2.2","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 2.2","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 2.2","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 2.2","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 2.2","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 2.2","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 2.2","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 2.2","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 2.2","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 2.2","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 2.2","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 2.2","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 2.2","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 2.2","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 2.2","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 2.2","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 2.2","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 2.2","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 2.2","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 2.2","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 2.2","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 2.2","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 2.2","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 2.2","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 2.2","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 2.2","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 2.2","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 2.2","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 2.2","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 2.2","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 2.2","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 2.2","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 2.2","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 2.2","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 2.2","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 2.2","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 2.2","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 2.2","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 2.2","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 2.2","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 2.2","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 2.2","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 2.2","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 2.2","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 2.2","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 2.2","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 2.2","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 2.2","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 2.2","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 2.2","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 2.2","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 2.2","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 2.2","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 2.2","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 2.2","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 2.2"],"frame":"2.2","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.85,3.7981,3.7464,3.6949,3.6436,3.5925,3.5416,3.4909,3.4404,3.3901,3.34,3.2901,3.2404,3.1909,3.1416,3.0925,3.0436,2.9949,2.9464,2.8981,2.85,2.8021,2.7544,2.7069,2.6596,2.6125,2.5656,2.5189,2.4724,2.4261,2.38,2.3341,2.2884,2.2429,2.1976,2.1525,2.1076,2.0629,2.0184,1.9741,1.93,1.8861,1.8424,1.7989,1.7556,1.7125,1.6696,1.6269,1.5844,1.5421,1.5,1.5021,1.5044,1.5069,1.5096,1.5125,1.5156,1.5189,1.5224,1.5261,1.53,1.5341,1.5384,1.5429,1.5476,1.5525,1.5576,1.5629,1.5684,1.5741,1.58,1.5861,1.5924,1.5989,1.6056,1.6125,1.6196,1.6269,1.6344,1.6421,1.65,1.6581,1.6664,1.6749,1.6836,1.6925,1.7016,1.7109,1.7204,1.7301,1.74,1.7501,1.7604,1.7709,1.7816,1.7925,1.8036,1.8149,1.8264,1.8381,1.85,1.8621,1.8744,1.8869,1.8996,1.9125,1.9256,1.9389,1.9524,1.9661,1.98,1.9941,2.0084,2.0229,2.0376,2.0525,2.0676,2.0829,2.0984,2.1141,2.13,2.1461,2.1624,2.1789,2.1956,2.2125,2.2296,2.2469,2.2644,2.2821,2.3,2.3181,2.3364,2.3549,2.3736,2.3925,2.4116,2.4309,2.4504,2.4701,2.49,2.5101,2.5304,2.5509,2.5716,2.5925,2.6136,2.6349,2.6564,2.6781,2.7,2.7221,2.7444,2.7669,2.7896,2.8125,2.8356,2.8589,2.8824,2.9061,2.93,2.9541,2.9784,3.0029,3.0276,3.0525,3.0776,3.1029,3.1284,3.1541,3.18,3.2061,3.2324,3.2589,3.2856,3.3125,3.3396,3.3669,3.3944,3.4221,3.45,3.4781,3.5064,3.5349,3.5636,3.5925,3.6216,3.6509,3.6804,3.7101,3.74,3.7701,3.8004,3.8309,3.8616,3.8925,3.9236,3.9549,3.9864,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 3.8500<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.49<br />value: 3.7981<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.48<br />value: 3.7464<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.47<br />value: 3.6949<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.46<br />value: 3.6436<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.45<br />value: 3.5925<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.44<br />value: 3.5416<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.43<br />value: 3.4909<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.42<br />value: 3.4404<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.41<br />value: 3.3901<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.40<br />value: 3.3400<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.39<br />value: 3.2901<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.38<br />value: 3.2404<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.37<br />value: 3.1909<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.36<br />value: 3.1416<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.35<br />value: 3.0925<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.34<br />value: 3.0436<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.33<br />value: 2.9949<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.32<br />value: 2.9464<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.31<br />value: 2.8981<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.30<br />value: 2.8500<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.29<br />value: 2.8021<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.28<br />value: 2.7544<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.27<br />value: 2.7069<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.26<br />value: 2.6596<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.25<br />value: 2.6125<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.24<br />value: 2.5656<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.23<br />value: 2.5189<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.22<br />value: 2.4724<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.21<br />value: 2.4261<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.20<br />value: 2.3800<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.19<br />value: 2.3341<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.18<br />value: 2.2884<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.17<br />value: 2.2429<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.16<br />value: 2.1976<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.15<br />value: 2.1525<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.14<br />value: 2.1076<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.13<br />value: 2.0629<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.12<br />value: 2.0184<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.11<br />value: 1.9741<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.10<br />value: 1.9300<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.09<br />value: 1.8861<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.08<br />value: 1.8424<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.07<br />value: 1.7989<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.06<br />value: 1.7556<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.05<br />value: 1.7125<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.04<br />value: 1.6696<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.03<br />value: 1.6269<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.02<br />value: 1.5844<br />Function: Loss + Penalty<br />Lambda: 2.2","b: -0.01<br />value: 1.5421<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.01<br />value: 1.5021<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.02<br />value: 1.5044<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.03<br />value: 1.5069<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.04<br />value: 1.5096<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.05<br />value: 1.5125<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.06<br />value: 1.5156<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.07<br />value: 1.5189<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.08<br />value: 1.5224<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.09<br />value: 1.5261<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.10<br />value: 1.5300<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.11<br />value: 1.5341<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.12<br />value: 1.5384<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.13<br />value: 1.5429<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.14<br />value: 1.5476<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.15<br />value: 1.5525<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.16<br />value: 1.5576<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.17<br />value: 1.5629<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.18<br />value: 1.5684<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.19<br />value: 1.5741<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.20<br />value: 1.5800<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.21<br />value: 1.5861<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.22<br />value: 1.5924<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.23<br />value: 1.5989<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.24<br />value: 1.6056<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.25<br />value: 1.6125<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.26<br />value: 1.6196<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.27<br />value: 1.6269<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.28<br />value: 1.6344<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.29<br />value: 1.6421<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.30<br />value: 1.6500<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.31<br />value: 1.6581<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.32<br />value: 1.6664<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.33<br />value: 1.6749<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.34<br />value: 1.6836<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.35<br />value: 1.6925<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.36<br />value: 1.7016<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.37<br />value: 1.7109<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.38<br />value: 1.7204<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.39<br />value: 1.7301<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.40<br />value: 1.7400<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.41<br />value: 1.7501<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.42<br />value: 1.7604<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.43<br />value: 1.7709<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.44<br />value: 1.7816<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.45<br />value: 1.7925<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.46<br />value: 1.8036<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.47<br />value: 1.8149<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.48<br />value: 1.8264<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.49<br />value: 1.8381<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.50<br />value: 1.8500<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.51<br />value: 1.8621<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.52<br />value: 1.8744<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.53<br />value: 1.8869<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.54<br />value: 1.8996<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.55<br />value: 1.9125<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.56<br />value: 1.9256<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.57<br />value: 1.9389<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.58<br />value: 1.9524<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.59<br />value: 1.9661<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.60<br />value: 1.9800<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.61<br />value: 1.9941<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.62<br />value: 2.0084<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.63<br />value: 2.0229<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.64<br />value: 2.0376<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.65<br />value: 2.0525<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.66<br />value: 2.0676<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.67<br />value: 2.0829<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.68<br />value: 2.0984<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.69<br />value: 2.1141<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.70<br />value: 2.1300<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.71<br />value: 2.1461<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.72<br />value: 2.1624<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.73<br />value: 2.1789<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.74<br />value: 2.1956<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.75<br />value: 2.2125<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.76<br />value: 2.2296<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.77<br />value: 2.2469<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.78<br />value: 2.2644<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.79<br />value: 2.2821<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.80<br />value: 2.3000<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.81<br />value: 2.3181<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.82<br />value: 2.3364<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.83<br />value: 2.3549<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.84<br />value: 2.3736<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.85<br />value: 2.3925<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.86<br />value: 2.4116<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.87<br />value: 2.4309<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.88<br />value: 2.4504<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.89<br />value: 2.4701<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.90<br />value: 2.4900<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.91<br />value: 2.5101<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.92<br />value: 2.5304<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.93<br />value: 2.5509<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.94<br />value: 2.5716<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.95<br />value: 2.5925<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.96<br />value: 2.6136<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.97<br />value: 2.6349<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.98<br />value: 2.6564<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  0.99<br />value: 2.6781<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.00<br />value: 2.7000<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.01<br />value: 2.7221<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.02<br />value: 2.7444<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.03<br />value: 2.7669<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.04<br />value: 2.7896<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.05<br />value: 2.8125<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.06<br />value: 2.8356<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.07<br />value: 2.8589<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.08<br />value: 2.8824<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.09<br />value: 2.9061<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.10<br />value: 2.9300<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.11<br />value: 2.9541<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.12<br />value: 2.9784<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.13<br />value: 3.0029<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.14<br />value: 3.0276<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.15<br />value: 3.0525<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.16<br />value: 3.0776<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.17<br />value: 3.1029<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.18<br />value: 3.1284<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.19<br />value: 3.1541<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.20<br />value: 3.1800<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.21<br />value: 3.2061<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.22<br />value: 3.2324<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.23<br />value: 3.2589<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.24<br />value: 3.2856<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.25<br />value: 3.3125<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.26<br />value: 3.3396<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.27<br />value: 3.3669<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.28<br />value: 3.3944<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.29<br />value: 3.4221<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.30<br />value: 3.4500<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.31<br />value: 3.4781<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.32<br />value: 3.5064<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.33<br />value: 3.5349<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.34<br />value: 3.5636<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.35<br />value: 3.5925<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.36<br />value: 3.6216<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.37<br />value: 3.6509<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.38<br />value: 3.6804<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.39<br />value: 3.7101<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.40<br />value: 3.7400<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.41<br />value: 3.7701<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.42<br />value: 3.8004<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.43<br />value: 3.8309<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.44<br />value: 3.8616<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.45<br />value: 3.8925<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.46<br />value: 3.9236<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.47<br />value: 3.9549<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.48<br />value: 3.9864<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.49<br />value: 4.0181<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.50<br />value: 4.0500<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.51<br />value: 4.0821<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.52<br />value: 4.1144<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.53<br />value: 4.1469<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.54<br />value: 4.1796<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.55<br />value: 4.2125<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.56<br />value: 4.2456<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.57<br />value: 4.2789<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.58<br />value: 4.3124<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.59<br />value: 4.3461<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.60<br />value: 4.3800<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.61<br />value: 4.4141<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.62<br />value: 4.4484<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.63<br />value: 4.4829<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.64<br />value: 4.5176<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.65<br />value: 4.5525<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.66<br />value: 4.5876<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.67<br />value: 4.6229<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.68<br />value: 4.6584<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.69<br />value: 4.6941<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.70<br />value: 4.7300<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.71<br />value: 4.7661<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.72<br />value: 4.8024<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.73<br />value: 4.8389<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.74<br />value: 4.8756<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.75<br />value: 4.9125<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.76<br />value: 4.9496<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.77<br />value: 4.9869<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.78<br />value: 5.0244<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.79<br />value: 5.0621<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.80<br />value: 5.1000<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.81<br />value: 5.1381<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.82<br />value: 5.1764<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.83<br />value: 5.2149<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.84<br />value: 5.2536<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.85<br />value: 5.2925<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.86<br />value: 5.3316<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.87<br />value: 5.3709<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.88<br />value: 5.4104<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.89<br />value: 5.4501<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.90<br />value: 5.4900<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.91<br />value: 5.5301<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.92<br />value: 5.5704<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.93<br />value: 5.6109<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.94<br />value: 5.6516<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.95<br />value: 5.6925<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.96<br />value: 5.7336<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.97<br />value: 5.7749<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.98<br />value: 5.8164<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  1.99<br />value: 5.8581<br />Function: Loss + Penalty<br />Lambda: 2.2","b:  2.00<br />value: 5.9000<br />Function: Loss + Penalty<br />Lambda: 2.2"],"frame":"2.2","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[1.1,1.078,1.056,1.034,1.012,0.99,0.968,0.946,0.924,0.902,0.88,0.858,0.836,0.814,0.792,0.77,0.748,0.726,0.704,0.682,0.66,0.638,0.616,0.594,0.572,0.55,0.528,0.506,0.484,0.462,0.44,0.418,0.396,0.374,0.352,0.33,0.308,0.286,0.264,0.242,0.22,0.198,0.176,0.154,0.132,0.11,0.088,0.0659999999999999,0.044,0.022,0,0.022,0.044,0.0660000000000001,0.0880000000000001,0.11,0.132,0.154,0.176,0.198,0.22,0.242,0.264,0.286,0.308,0.33,0.352,0.374,0.396,0.418,0.44,0.462,0.484,0.506,0.528,0.55,0.572,0.594,0.616,0.638,0.66,0.682,0.704,0.726,0.748,0.77,0.792,0.814,0.836,0.858,0.88,0.902,0.924,0.946,0.968,0.99,1.012,1.034,1.056,1.078,1.1,1.122,1.144,1.166,1.188,1.21,1.232,1.254,1.276,1.298,1.32,1.342,1.364,1.386,1.408,1.43,1.452,1.474,1.496,1.518,1.54,1.562,1.584,1.606,1.628,1.65,1.672,1.694,1.716,1.738,1.76,1.782,1.804,1.826,1.848,1.87,1.892,1.914,1.936,1.958,1.98,2.002,2.024,2.046,2.068,2.09,2.112,2.134,2.156,2.178,2.2,2.222,2.244,2.266,2.288,2.31,2.332,2.354,2.376,2.398,2.42,2.442,2.464,2.486,2.508,2.53,2.552,2.574,2.596,2.618,2.64,2.662,2.684,2.706,2.728,2.75,2.772,2.794,2.816,2.838,2.86,2.882,2.904,2.926,2.948,2.97,2.992,3.014,3.036,3.058,3.08,3.102,3.124,3.146,3.168,3.19,3.212,3.234,3.256,3.278,3.3,3.322,3.344,3.366,3.388,3.41,3.432,3.454,3.476,3.498,3.52,3.542,3.564,3.586,3.608,3.63,3.652,3.674,3.696,3.718,3.74,3.762,3.784,3.806,3.828,3.85,3.872,3.894,3.916,3.938,3.96,3.982,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 1.1000<br />Function: Penalty<br />Lambda: 2.2","b: -0.49<br />value: 1.0780<br />Function: Penalty<br />Lambda: 2.2","b: -0.48<br />value: 1.0560<br />Function: Penalty<br />Lambda: 2.2","b: -0.47<br />value: 1.0340<br />Function: Penalty<br />Lambda: 2.2","b: -0.46<br />value: 1.0120<br />Function: Penalty<br />Lambda: 2.2","b: -0.45<br />value: 0.9900<br />Function: Penalty<br />Lambda: 2.2","b: -0.44<br />value: 0.9680<br />Function: Penalty<br />Lambda: 2.2","b: -0.43<br />value: 0.9460<br />Function: Penalty<br />Lambda: 2.2","b: -0.42<br />value: 0.9240<br />Function: Penalty<br />Lambda: 2.2","b: -0.41<br />value: 0.9020<br />Function: Penalty<br />Lambda: 2.2","b: -0.40<br />value: 0.8800<br />Function: Penalty<br />Lambda: 2.2","b: -0.39<br />value: 0.8580<br />Function: Penalty<br />Lambda: 2.2","b: -0.38<br />value: 0.8360<br />Function: Penalty<br />Lambda: 2.2","b: -0.37<br />value: 0.8140<br />Function: Penalty<br />Lambda: 2.2","b: -0.36<br />value: 0.7920<br />Function: Penalty<br />Lambda: 2.2","b: -0.35<br />value: 0.7700<br />Function: Penalty<br />Lambda: 2.2","b: -0.34<br />value: 0.7480<br />Function: Penalty<br />Lambda: 2.2","b: -0.33<br />value: 0.7260<br />Function: Penalty<br />Lambda: 2.2","b: -0.32<br />value: 0.7040<br />Function: Penalty<br />Lambda: 2.2","b: -0.31<br />value: 0.6820<br />Function: Penalty<br />Lambda: 2.2","b: -0.30<br />value: 0.6600<br />Function: Penalty<br />Lambda: 2.2","b: -0.29<br />value: 0.6380<br />Function: Penalty<br />Lambda: 2.2","b: -0.28<br />value: 0.6160<br />Function: Penalty<br />Lambda: 2.2","b: -0.27<br />value: 0.5940<br />Function: Penalty<br />Lambda: 2.2","b: -0.26<br />value: 0.5720<br />Function: Penalty<br />Lambda: 2.2","b: -0.25<br />value: 0.5500<br />Function: Penalty<br />Lambda: 2.2","b: -0.24<br />value: 0.5280<br />Function: Penalty<br />Lambda: 2.2","b: -0.23<br />value: 0.5060<br />Function: Penalty<br />Lambda: 2.2","b: -0.22<br />value: 0.4840<br />Function: Penalty<br />Lambda: 2.2","b: -0.21<br />value: 0.4620<br />Function: Penalty<br />Lambda: 2.2","b: -0.20<br />value: 0.4400<br />Function: Penalty<br />Lambda: 2.2","b: -0.19<br />value: 0.4180<br />Function: Penalty<br />Lambda: 2.2","b: -0.18<br />value: 0.3960<br />Function: Penalty<br />Lambda: 2.2","b: -0.17<br />value: 0.3740<br />Function: Penalty<br />Lambda: 2.2","b: -0.16<br />value: 0.3520<br />Function: Penalty<br />Lambda: 2.2","b: -0.15<br />value: 0.3300<br />Function: Penalty<br />Lambda: 2.2","b: -0.14<br />value: 0.3080<br />Function: Penalty<br />Lambda: 2.2","b: -0.13<br />value: 0.2860<br />Function: Penalty<br />Lambda: 2.2","b: -0.12<br />value: 0.2640<br />Function: Penalty<br />Lambda: 2.2","b: -0.11<br />value: 0.2420<br />Function: Penalty<br />Lambda: 2.2","b: -0.10<br />value: 0.2200<br />Function: Penalty<br />Lambda: 2.2","b: -0.09<br />value: 0.1980<br />Function: Penalty<br />Lambda: 2.2","b: -0.08<br />value: 0.1760<br />Function: Penalty<br />Lambda: 2.2","b: -0.07<br />value: 0.1540<br />Function: Penalty<br />Lambda: 2.2","b: -0.06<br />value: 0.1320<br />Function: Penalty<br />Lambda: 2.2","b: -0.05<br />value: 0.1100<br />Function: Penalty<br />Lambda: 2.2","b: -0.04<br />value: 0.0880<br />Function: Penalty<br />Lambda: 2.2","b: -0.03<br />value: 0.0660<br />Function: Penalty<br />Lambda: 2.2","b: -0.02<br />value: 0.0440<br />Function: Penalty<br />Lambda: 2.2","b: -0.01<br />value: 0.0220<br />Function: Penalty<br />Lambda: 2.2","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 2.2","b:  0.01<br />value: 0.0220<br />Function: Penalty<br />Lambda: 2.2","b:  0.02<br />value: 0.0440<br />Function: Penalty<br />Lambda: 2.2","b:  0.03<br />value: 0.0660<br />Function: Penalty<br />Lambda: 2.2","b:  0.04<br />value: 0.0880<br />Function: Penalty<br />Lambda: 2.2","b:  0.05<br />value: 0.1100<br />Function: Penalty<br />Lambda: 2.2","b:  0.06<br />value: 0.1320<br />Function: Penalty<br />Lambda: 2.2","b:  0.07<br />value: 0.1540<br />Function: Penalty<br />Lambda: 2.2","b:  0.08<br />value: 0.1760<br />Function: Penalty<br />Lambda: 2.2","b:  0.09<br />value: 0.1980<br />Function: Penalty<br />Lambda: 2.2","b:  0.10<br />value: 0.2200<br />Function: Penalty<br />Lambda: 2.2","b:  0.11<br />value: 0.2420<br />Function: Penalty<br />Lambda: 2.2","b:  0.12<br />value: 0.2640<br />Function: Penalty<br />Lambda: 2.2","b:  0.13<br />value: 0.2860<br />Function: Penalty<br />Lambda: 2.2","b:  0.14<br />value: 0.3080<br />Function: Penalty<br />Lambda: 2.2","b:  0.15<br />value: 0.3300<br />Function: Penalty<br />Lambda: 2.2","b:  0.16<br />value: 0.3520<br />Function: Penalty<br />Lambda: 2.2","b:  0.17<br />value: 0.3740<br />Function: Penalty<br />Lambda: 2.2","b:  0.18<br />value: 0.3960<br />Function: Penalty<br />Lambda: 2.2","b:  0.19<br />value: 0.4180<br />Function: Penalty<br />Lambda: 2.2","b:  0.20<br />value: 0.4400<br />Function: Penalty<br />Lambda: 2.2","b:  0.21<br />value: 0.4620<br />Function: Penalty<br />Lambda: 2.2","b:  0.22<br />value: 0.4840<br />Function: Penalty<br />Lambda: 2.2","b:  0.23<br />value: 0.5060<br />Function: Penalty<br />Lambda: 2.2","b:  0.24<br />value: 0.5280<br />Function: Penalty<br />Lambda: 2.2","b:  0.25<br />value: 0.5500<br />Function: Penalty<br />Lambda: 2.2","b:  0.26<br />value: 0.5720<br />Function: Penalty<br />Lambda: 2.2","b:  0.27<br />value: 0.5940<br />Function: Penalty<br />Lambda: 2.2","b:  0.28<br />value: 0.6160<br />Function: Penalty<br />Lambda: 2.2","b:  0.29<br />value: 0.6380<br />Function: Penalty<br />Lambda: 2.2","b:  0.30<br />value: 0.6600<br />Function: Penalty<br />Lambda: 2.2","b:  0.31<br />value: 0.6820<br />Function: Penalty<br />Lambda: 2.2","b:  0.32<br />value: 0.7040<br />Function: Penalty<br />Lambda: 2.2","b:  0.33<br />value: 0.7260<br />Function: Penalty<br />Lambda: 2.2","b:  0.34<br />value: 0.7480<br />Function: Penalty<br />Lambda: 2.2","b:  0.35<br />value: 0.7700<br />Function: Penalty<br />Lambda: 2.2","b:  0.36<br />value: 0.7920<br />Function: Penalty<br />Lambda: 2.2","b:  0.37<br />value: 0.8140<br />Function: Penalty<br />Lambda: 2.2","b:  0.38<br />value: 0.8360<br />Function: Penalty<br />Lambda: 2.2","b:  0.39<br />value: 0.8580<br />Function: Penalty<br />Lambda: 2.2","b:  0.40<br />value: 0.8800<br />Function: Penalty<br />Lambda: 2.2","b:  0.41<br />value: 0.9020<br />Function: Penalty<br />Lambda: 2.2","b:  0.42<br />value: 0.9240<br />Function: Penalty<br />Lambda: 2.2","b:  0.43<br />value: 0.9460<br />Function: Penalty<br />Lambda: 2.2","b:  0.44<br />value: 0.9680<br />Function: Penalty<br />Lambda: 2.2","b:  0.45<br />value: 0.9900<br />Function: Penalty<br />Lambda: 2.2","b:  0.46<br />value: 1.0120<br />Function: Penalty<br />Lambda: 2.2","b:  0.47<br />value: 1.0340<br />Function: Penalty<br />Lambda: 2.2","b:  0.48<br />value: 1.0560<br />Function: Penalty<br />Lambda: 2.2","b:  0.49<br />value: 1.0780<br />Function: Penalty<br />Lambda: 2.2","b:  0.50<br />value: 1.1000<br />Function: Penalty<br />Lambda: 2.2","b:  0.51<br />value: 1.1220<br />Function: Penalty<br />Lambda: 2.2","b:  0.52<br />value: 1.1440<br />Function: Penalty<br />Lambda: 2.2","b:  0.53<br />value: 1.1660<br />Function: Penalty<br />Lambda: 2.2","b:  0.54<br />value: 1.1880<br />Function: Penalty<br />Lambda: 2.2","b:  0.55<br />value: 1.2100<br />Function: Penalty<br />Lambda: 2.2","b:  0.56<br />value: 1.2320<br />Function: Penalty<br />Lambda: 2.2","b:  0.57<br />value: 1.2540<br />Function: Penalty<br />Lambda: 2.2","b:  0.58<br />value: 1.2760<br />Function: Penalty<br />Lambda: 2.2","b:  0.59<br />value: 1.2980<br />Function: Penalty<br />Lambda: 2.2","b:  0.60<br />value: 1.3200<br />Function: Penalty<br />Lambda: 2.2","b:  0.61<br />value: 1.3420<br />Function: Penalty<br />Lambda: 2.2","b:  0.62<br />value: 1.3640<br />Function: Penalty<br />Lambda: 2.2","b:  0.63<br />value: 1.3860<br />Function: Penalty<br />Lambda: 2.2","b:  0.64<br />value: 1.4080<br />Function: Penalty<br />Lambda: 2.2","b:  0.65<br />value: 1.4300<br />Function: Penalty<br />Lambda: 2.2","b:  0.66<br />value: 1.4520<br />Function: Penalty<br />Lambda: 2.2","b:  0.67<br />value: 1.4740<br />Function: Penalty<br />Lambda: 2.2","b:  0.68<br />value: 1.4960<br />Function: Penalty<br />Lambda: 2.2","b:  0.69<br />value: 1.5180<br />Function: Penalty<br />Lambda: 2.2","b:  0.70<br />value: 1.5400<br />Function: Penalty<br />Lambda: 2.2","b:  0.71<br />value: 1.5620<br />Function: Penalty<br />Lambda: 2.2","b:  0.72<br />value: 1.5840<br />Function: Penalty<br />Lambda: 2.2","b:  0.73<br />value: 1.6060<br />Function: Penalty<br />Lambda: 2.2","b:  0.74<br />value: 1.6280<br />Function: Penalty<br />Lambda: 2.2","b:  0.75<br />value: 1.6500<br />Function: Penalty<br />Lambda: 2.2","b:  0.76<br />value: 1.6720<br />Function: Penalty<br />Lambda: 2.2","b:  0.77<br />value: 1.6940<br />Function: Penalty<br />Lambda: 2.2","b:  0.78<br />value: 1.7160<br />Function: Penalty<br />Lambda: 2.2","b:  0.79<br />value: 1.7380<br />Function: Penalty<br />Lambda: 2.2","b:  0.80<br />value: 1.7600<br />Function: Penalty<br />Lambda: 2.2","b:  0.81<br />value: 1.7820<br />Function: Penalty<br />Lambda: 2.2","b:  0.82<br />value: 1.8040<br />Function: Penalty<br />Lambda: 2.2","b:  0.83<br />value: 1.8260<br />Function: Penalty<br />Lambda: 2.2","b:  0.84<br />value: 1.8480<br />Function: Penalty<br />Lambda: 2.2","b:  0.85<br />value: 1.8700<br />Function: Penalty<br />Lambda: 2.2","b:  0.86<br />value: 1.8920<br />Function: Penalty<br />Lambda: 2.2","b:  0.87<br />value: 1.9140<br />Function: Penalty<br />Lambda: 2.2","b:  0.88<br />value: 1.9360<br />Function: Penalty<br />Lambda: 2.2","b:  0.89<br />value: 1.9580<br />Function: Penalty<br />Lambda: 2.2","b:  0.90<br />value: 1.9800<br />Function: Penalty<br />Lambda: 2.2","b:  0.91<br />value: 2.0020<br />Function: Penalty<br />Lambda: 2.2","b:  0.92<br />value: 2.0240<br />Function: Penalty<br />Lambda: 2.2","b:  0.93<br />value: 2.0460<br />Function: Penalty<br />Lambda: 2.2","b:  0.94<br />value: 2.0680<br />Function: Penalty<br />Lambda: 2.2","b:  0.95<br />value: 2.0900<br />Function: Penalty<br />Lambda: 2.2","b:  0.96<br />value: 2.1120<br />Function: Penalty<br />Lambda: 2.2","b:  0.97<br />value: 2.1340<br />Function: Penalty<br />Lambda: 2.2","b:  0.98<br />value: 2.1560<br />Function: Penalty<br />Lambda: 2.2","b:  0.99<br />value: 2.1780<br />Function: Penalty<br />Lambda: 2.2","b:  1.00<br />value: 2.2000<br />Function: Penalty<br />Lambda: 2.2","b:  1.01<br />value: 2.2220<br />Function: Penalty<br />Lambda: 2.2","b:  1.02<br />value: 2.2440<br />Function: Penalty<br />Lambda: 2.2","b:  1.03<br />value: 2.2660<br />Function: Penalty<br />Lambda: 2.2","b:  1.04<br />value: 2.2880<br />Function: Penalty<br />Lambda: 2.2","b:  1.05<br />value: 2.3100<br />Function: Penalty<br />Lambda: 2.2","b:  1.06<br />value: 2.3320<br />Function: Penalty<br />Lambda: 2.2","b:  1.07<br />value: 2.3540<br />Function: Penalty<br />Lambda: 2.2","b:  1.08<br />value: 2.3760<br />Function: Penalty<br />Lambda: 2.2","b:  1.09<br />value: 2.3980<br />Function: Penalty<br />Lambda: 2.2","b:  1.10<br />value: 2.4200<br />Function: Penalty<br />Lambda: 2.2","b:  1.11<br />value: 2.4420<br />Function: Penalty<br />Lambda: 2.2","b:  1.12<br />value: 2.4640<br />Function: Penalty<br />Lambda: 2.2","b:  1.13<br />value: 2.4860<br />Function: Penalty<br />Lambda: 2.2","b:  1.14<br />value: 2.5080<br />Function: Penalty<br />Lambda: 2.2","b:  1.15<br />value: 2.5300<br />Function: Penalty<br />Lambda: 2.2","b:  1.16<br />value: 2.5520<br />Function: Penalty<br />Lambda: 2.2","b:  1.17<br />value: 2.5740<br />Function: Penalty<br />Lambda: 2.2","b:  1.18<br />value: 2.5960<br />Function: Penalty<br />Lambda: 2.2","b:  1.19<br />value: 2.6180<br />Function: Penalty<br />Lambda: 2.2","b:  1.20<br />value: 2.6400<br />Function: Penalty<br />Lambda: 2.2","b:  1.21<br />value: 2.6620<br />Function: Penalty<br />Lambda: 2.2","b:  1.22<br />value: 2.6840<br />Function: Penalty<br />Lambda: 2.2","b:  1.23<br />value: 2.7060<br />Function: Penalty<br />Lambda: 2.2","b:  1.24<br />value: 2.7280<br />Function: Penalty<br />Lambda: 2.2","b:  1.25<br />value: 2.7500<br />Function: Penalty<br />Lambda: 2.2","b:  1.26<br />value: 2.7720<br />Function: Penalty<br />Lambda: 2.2","b:  1.27<br />value: 2.7940<br />Function: Penalty<br />Lambda: 2.2","b:  1.28<br />value: 2.8160<br />Function: Penalty<br />Lambda: 2.2","b:  1.29<br />value: 2.8380<br />Function: Penalty<br />Lambda: 2.2","b:  1.30<br />value: 2.8600<br />Function: Penalty<br />Lambda: 2.2","b:  1.31<br />value: 2.8820<br />Function: Penalty<br />Lambda: 2.2","b:  1.32<br />value: 2.9040<br />Function: Penalty<br />Lambda: 2.2","b:  1.33<br />value: 2.9260<br />Function: Penalty<br />Lambda: 2.2","b:  1.34<br />value: 2.9480<br />Function: Penalty<br />Lambda: 2.2","b:  1.35<br />value: 2.9700<br />Function: Penalty<br />Lambda: 2.2","b:  1.36<br />value: 2.9920<br />Function: Penalty<br />Lambda: 2.2","b:  1.37<br />value: 3.0140<br />Function: Penalty<br />Lambda: 2.2","b:  1.38<br />value: 3.0360<br />Function: Penalty<br />Lambda: 2.2","b:  1.39<br />value: 3.0580<br />Function: Penalty<br />Lambda: 2.2","b:  1.40<br />value: 3.0800<br />Function: Penalty<br />Lambda: 2.2","b:  1.41<br />value: 3.1020<br />Function: Penalty<br />Lambda: 2.2","b:  1.42<br />value: 3.1240<br />Function: Penalty<br />Lambda: 2.2","b:  1.43<br />value: 3.1460<br />Function: Penalty<br />Lambda: 2.2","b:  1.44<br />value: 3.1680<br />Function: Penalty<br />Lambda: 2.2","b:  1.45<br />value: 3.1900<br />Function: Penalty<br />Lambda: 2.2","b:  1.46<br />value: 3.2120<br />Function: Penalty<br />Lambda: 2.2","b:  1.47<br />value: 3.2340<br />Function: Penalty<br />Lambda: 2.2","b:  1.48<br />value: 3.2560<br />Function: Penalty<br />Lambda: 2.2","b:  1.49<br />value: 3.2780<br />Function: Penalty<br />Lambda: 2.2","b:  1.50<br />value: 3.3000<br />Function: Penalty<br />Lambda: 2.2","b:  1.51<br />value: 3.3220<br />Function: Penalty<br />Lambda: 2.2","b:  1.52<br />value: 3.3440<br />Function: Penalty<br />Lambda: 2.2","b:  1.53<br />value: 3.3660<br />Function: Penalty<br />Lambda: 2.2","b:  1.54<br />value: 3.3880<br />Function: Penalty<br />Lambda: 2.2","b:  1.55<br />value: 3.4100<br />Function: Penalty<br />Lambda: 2.2","b:  1.56<br />value: 3.4320<br />Function: Penalty<br />Lambda: 2.2","b:  1.57<br />value: 3.4540<br />Function: Penalty<br />Lambda: 2.2","b:  1.58<br />value: 3.4760<br />Function: Penalty<br />Lambda: 2.2","b:  1.59<br />value: 3.4980<br />Function: Penalty<br />Lambda: 2.2","b:  1.60<br />value: 3.5200<br />Function: Penalty<br />Lambda: 2.2","b:  1.61<br />value: 3.5420<br />Function: Penalty<br />Lambda: 2.2","b:  1.62<br />value: 3.5640<br />Function: Penalty<br />Lambda: 2.2","b:  1.63<br />value: 3.5860<br />Function: Penalty<br />Lambda: 2.2","b:  1.64<br />value: 3.6080<br />Function: Penalty<br />Lambda: 2.2","b:  1.65<br />value: 3.6300<br />Function: Penalty<br />Lambda: 2.2","b:  1.66<br />value: 3.6520<br />Function: Penalty<br />Lambda: 2.2","b:  1.67<br />value: 3.6740<br />Function: Penalty<br />Lambda: 2.2","b:  1.68<br />value: 3.6960<br />Function: Penalty<br />Lambda: 2.2","b:  1.69<br />value: 3.7180<br />Function: Penalty<br />Lambda: 2.2","b:  1.70<br />value: 3.7400<br />Function: Penalty<br />Lambda: 2.2","b:  1.71<br />value: 3.7620<br />Function: Penalty<br />Lambda: 2.2","b:  1.72<br />value: 3.7840<br />Function: Penalty<br />Lambda: 2.2","b:  1.73<br />value: 3.8060<br />Function: Penalty<br />Lambda: 2.2","b:  1.74<br />value: 3.8280<br />Function: Penalty<br />Lambda: 2.2","b:  1.75<br />value: 3.8500<br />Function: Penalty<br />Lambda: 2.2","b:  1.76<br />value: 3.8720<br />Function: Penalty<br />Lambda: 2.2","b:  1.77<br />value: 3.8940<br />Function: Penalty<br />Lambda: 2.2","b:  1.78<br />value: 3.9160<br />Function: Penalty<br />Lambda: 2.2","b:  1.79<br />value: 3.9380<br />Function: Penalty<br />Lambda: 2.2","b:  1.80<br />value: 3.9600<br />Function: Penalty<br />Lambda: 2.2","b:  1.81<br />value: 3.9820<br />Function: Penalty<br />Lambda: 2.2","b:  1.82<br />value: 4.0040<br />Function: Penalty<br />Lambda: 2.2","b:  1.83<br />value: 4.0260<br />Function: Penalty<br />Lambda: 2.2","b:  1.84<br />value: 4.0480<br />Function: Penalty<br />Lambda: 2.2","b:  1.85<br />value: 4.0700<br />Function: Penalty<br />Lambda: 2.2","b:  1.86<br />value: 4.0920<br />Function: Penalty<br />Lambda: 2.2","b:  1.87<br />value: 4.1140<br />Function: Penalty<br />Lambda: 2.2","b:  1.88<br />value: 4.1360<br />Function: Penalty<br />Lambda: 2.2","b:  1.89<br />value: 4.1580<br />Function: Penalty<br />Lambda: 2.2","b:  1.90<br />value: 4.1800<br />Function: Penalty<br />Lambda: 2.2","b:  1.91<br />value: 4.2020<br />Function: Penalty<br />Lambda: 2.2","b:  1.92<br />value: 4.2240<br />Function: Penalty<br />Lambda: 2.2","b:  1.93<br />value: 4.2460<br />Function: Penalty<br />Lambda: 2.2","b:  1.94<br />value: 4.2680<br />Function: Penalty<br />Lambda: 2.2","b:  1.95<br />value: 4.2900<br />Function: Penalty<br />Lambda: 2.2","b:  1.96<br />value: 4.3120<br />Function: Penalty<br />Lambda: 2.2","b:  1.97<br />value: 4.3340<br />Function: Penalty<br />Lambda: 2.2","b:  1.98<br />value: 4.3560<br />Function: Penalty<br />Lambda: 2.2","b:  1.99<br />value: 4.3780<br />Function: Penalty<br />Lambda: 2.2","b:  2.00<br />value: 4.4000<br />Function: Penalty<br />Lambda: 2.2"],"frame":"2.2","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"2.3","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 2.3","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 2.3","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 2.3","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 2.3","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 2.3","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 2.3","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 2.3","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 2.3","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 2.3","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 2.3","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 2.3","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 2.3","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 2.3","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 2.3","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 2.3","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 2.3","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 2.3","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 2.3","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 2.3","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 2.3","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 2.3","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 2.3","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 2.3","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 2.3","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 2.3","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 2.3","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 2.3","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 2.3","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 2.3","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 2.3","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 2.3","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 2.3","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 2.3","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 2.3","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 2.3","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 2.3","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 2.3","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 2.3","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 2.3","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 2.3","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 2.3","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 2.3","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 2.3","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 2.3","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 2.3","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 2.3","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 2.3","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 2.3","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 2.3","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 2.3","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 2.3","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 2.3","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 2.3","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 2.3","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 2.3","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 2.3","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 2.3","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 2.3","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 2.3","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 2.3","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 2.3","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 2.3","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 2.3","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 2.3","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 2.3","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 2.3","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 2.3","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 2.3","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 2.3","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 2.3","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 2.3","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 2.3","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 2.3","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 2.3","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 2.3","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 2.3","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 2.3","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 2.3","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 2.3","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 2.3","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 2.3","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 2.3","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 2.3","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 2.3","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 2.3","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 2.3","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 2.3","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 2.3","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 2.3","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 2.3","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 2.3","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 2.3","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 2.3","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 2.3","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 2.3","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 2.3","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 2.3","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 2.3","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 2.3","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 2.3","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 2.3","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 2.3","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 2.3","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 2.3","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 2.3","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 2.3","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 2.3","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 2.3","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 2.3","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 2.3","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 2.3","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 2.3","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 2.3","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 2.3","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 2.3","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 2.3","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 2.3","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 2.3","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 2.3","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 2.3","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 2.3","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 2.3","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 2.3","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 2.3","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 2.3","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 2.3","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 2.3","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 2.3","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 2.3","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 2.3","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 2.3","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 2.3","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 2.3","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 2.3","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 2.3","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 2.3","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 2.3","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 2.3","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 2.3","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 2.3","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 2.3","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 2.3","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 2.3","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 2.3","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 2.3","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 2.3","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 2.3","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 2.3","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 2.3","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 2.3","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 2.3","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 2.3","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 2.3","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 2.3","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 2.3","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 2.3","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 2.3","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 2.3","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 2.3","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 2.3","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 2.3","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 2.3","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 2.3","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 2.3","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 2.3","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 2.3","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 2.3","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 2.3","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 2.3","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 2.3","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 2.3","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 2.3","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 2.3","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 2.3","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 2.3","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 2.3","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 2.3","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 2.3","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 2.3","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 2.3","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 2.3","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 2.3","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 2.3","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 2.3","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 2.3","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 2.3","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 2.3","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 2.3","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 2.3","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 2.3","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 2.3","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 2.3","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 2.3","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 2.3","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 2.3","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 2.3","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 2.3","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 2.3","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 2.3","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 2.3","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 2.3","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 2.3","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 2.3","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 2.3","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 2.3","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 2.3","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 2.3","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 2.3","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 2.3","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 2.3","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 2.3","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 2.3","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 2.3","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 2.3","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 2.3","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 2.3","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 2.3","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 2.3","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 2.3","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 2.3","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 2.3","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 2.3","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 2.3","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 2.3","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 2.3","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 2.3","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 2.3","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 2.3","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 2.3","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 2.3","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 2.3","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 2.3","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 2.3","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 2.3","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 2.3","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 2.3","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 2.3","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 2.3","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 2.3","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 2.3","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 2.3","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 2.3","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 2.3","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 2.3","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 2.3","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 2.3","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 2.3","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 2.3","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 2.3","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 2.3","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 2.3"],"frame":"2.3","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.9,3.8471,3.7944,3.7419,3.6896,3.6375,3.5856,3.5339,3.4824,3.4311,3.38,3.3291,3.2784,3.2279,3.1776,3.1275,3.0776,3.0279,2.9784,2.9291,2.88,2.8311,2.7824,2.7339,2.6856,2.6375,2.5896,2.5419,2.4944,2.4471,2.4,2.3531,2.3064,2.2599,2.2136,2.1675,2.1216,2.0759,2.0304,1.9851,1.94,1.8951,1.8504,1.8059,1.7616,1.7175,1.6736,1.6299,1.5864,1.5431,1.5,1.5031,1.5064,1.5099,1.5136,1.5175,1.5216,1.5259,1.5304,1.5351,1.54,1.5451,1.5504,1.5559,1.5616,1.5675,1.5736,1.5799,1.5864,1.5931,1.6,1.6071,1.6144,1.6219,1.6296,1.6375,1.6456,1.6539,1.6624,1.6711,1.68,1.6891,1.6984,1.7079,1.7176,1.7275,1.7376,1.7479,1.7584,1.7691,1.78,1.7911,1.8024,1.8139,1.8256,1.8375,1.8496,1.8619,1.8744,1.8871,1.9,1.9131,1.9264,1.9399,1.9536,1.9675,1.9816,1.9959,2.0104,2.0251,2.04,2.0551,2.0704,2.0859,2.1016,2.1175,2.1336,2.1499,2.1664,2.1831,2.2,2.2171,2.2344,2.2519,2.2696,2.2875,2.3056,2.3239,2.3424,2.3611,2.38,2.3991,2.4184,2.4379,2.4576,2.4775,2.4976,2.5179,2.5384,2.5591,2.58,2.6011,2.6224,2.6439,2.6656,2.6875,2.7096,2.7319,2.7544,2.7771,2.8,2.8231,2.8464,2.8699,2.8936,2.9175,2.9416,2.9659,2.9904,3.0151,3.04,3.0651,3.0904,3.1159,3.1416,3.1675,3.1936,3.2199,3.2464,3.2731,3.3,3.3271,3.3544,3.3819,3.4096,3.4375,3.4656,3.4939,3.5224,3.5511,3.58,3.6091,3.6384,3.6679,3.6976,3.7275,3.7576,3.7879,3.8184,3.8491,3.88,3.9111,3.9424,3.9739,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 3.9000<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.49<br />value: 3.8471<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.48<br />value: 3.7944<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.47<br />value: 3.7419<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.46<br />value: 3.6896<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.45<br />value: 3.6375<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.44<br />value: 3.5856<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.43<br />value: 3.5339<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.42<br />value: 3.4824<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.41<br />value: 3.4311<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.40<br />value: 3.3800<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.39<br />value: 3.3291<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.38<br />value: 3.2784<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.37<br />value: 3.2279<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.36<br />value: 3.1776<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.35<br />value: 3.1275<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.34<br />value: 3.0776<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.33<br />value: 3.0279<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.32<br />value: 2.9784<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.31<br />value: 2.9291<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.30<br />value: 2.8800<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.29<br />value: 2.8311<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.28<br />value: 2.7824<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.27<br />value: 2.7339<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.26<br />value: 2.6856<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.25<br />value: 2.6375<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.24<br />value: 2.5896<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.23<br />value: 2.5419<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.22<br />value: 2.4944<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.21<br />value: 2.4471<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.20<br />value: 2.4000<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.19<br />value: 2.3531<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.18<br />value: 2.3064<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.17<br />value: 2.2599<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.16<br />value: 2.2136<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.15<br />value: 2.1675<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.14<br />value: 2.1216<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.13<br />value: 2.0759<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.12<br />value: 2.0304<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.11<br />value: 1.9851<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.10<br />value: 1.9400<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.09<br />value: 1.8951<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.08<br />value: 1.8504<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.07<br />value: 1.8059<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.06<br />value: 1.7616<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.05<br />value: 1.7175<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.04<br />value: 1.6736<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.03<br />value: 1.6299<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.02<br />value: 1.5864<br />Function: Loss + Penalty<br />Lambda: 2.3","b: -0.01<br />value: 1.5431<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.01<br />value: 1.5031<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.02<br />value: 1.5064<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.03<br />value: 1.5099<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.04<br />value: 1.5136<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.05<br />value: 1.5175<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.06<br />value: 1.5216<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.07<br />value: 1.5259<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.08<br />value: 1.5304<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.09<br />value: 1.5351<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.10<br />value: 1.5400<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.11<br />value: 1.5451<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.12<br />value: 1.5504<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.13<br />value: 1.5559<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.14<br />value: 1.5616<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.15<br />value: 1.5675<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.16<br />value: 1.5736<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.17<br />value: 1.5799<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.18<br />value: 1.5864<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.19<br />value: 1.5931<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.20<br />value: 1.6000<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.21<br />value: 1.6071<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.22<br />value: 1.6144<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.23<br />value: 1.6219<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.24<br />value: 1.6296<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.25<br />value: 1.6375<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.26<br />value: 1.6456<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.27<br />value: 1.6539<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.28<br />value: 1.6624<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.29<br />value: 1.6711<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.30<br />value: 1.6800<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.31<br />value: 1.6891<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.32<br />value: 1.6984<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.33<br />value: 1.7079<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.34<br />value: 1.7176<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.35<br />value: 1.7275<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.36<br />value: 1.7376<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.37<br />value: 1.7479<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.38<br />value: 1.7584<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.39<br />value: 1.7691<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.40<br />value: 1.7800<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.41<br />value: 1.7911<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.42<br />value: 1.8024<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.43<br />value: 1.8139<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.44<br />value: 1.8256<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.45<br />value: 1.8375<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.46<br />value: 1.8496<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.47<br />value: 1.8619<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.48<br />value: 1.8744<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.49<br />value: 1.8871<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.50<br />value: 1.9000<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.51<br />value: 1.9131<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.52<br />value: 1.9264<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.53<br />value: 1.9399<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.54<br />value: 1.9536<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.55<br />value: 1.9675<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.56<br />value: 1.9816<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.57<br />value: 1.9959<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.58<br />value: 2.0104<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.59<br />value: 2.0251<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.60<br />value: 2.0400<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.61<br />value: 2.0551<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.62<br />value: 2.0704<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.63<br />value: 2.0859<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.64<br />value: 2.1016<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.65<br />value: 2.1175<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.66<br />value: 2.1336<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.67<br />value: 2.1499<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.68<br />value: 2.1664<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.69<br />value: 2.1831<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.70<br />value: 2.2000<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.71<br />value: 2.2171<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.72<br />value: 2.2344<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.73<br />value: 2.2519<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.74<br />value: 2.2696<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.75<br />value: 2.2875<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.76<br />value: 2.3056<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.77<br />value: 2.3239<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.78<br />value: 2.3424<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.79<br />value: 2.3611<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.80<br />value: 2.3800<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.81<br />value: 2.3991<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.82<br />value: 2.4184<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.83<br />value: 2.4379<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.84<br />value: 2.4576<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.85<br />value: 2.4775<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.86<br />value: 2.4976<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.87<br />value: 2.5179<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.88<br />value: 2.5384<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.89<br />value: 2.5591<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.90<br />value: 2.5800<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.91<br />value: 2.6011<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.92<br />value: 2.6224<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.93<br />value: 2.6439<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.94<br />value: 2.6656<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.95<br />value: 2.6875<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.96<br />value: 2.7096<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.97<br />value: 2.7319<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.98<br />value: 2.7544<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  0.99<br />value: 2.7771<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.00<br />value: 2.8000<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.01<br />value: 2.8231<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.02<br />value: 2.8464<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.03<br />value: 2.8699<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.04<br />value: 2.8936<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.05<br />value: 2.9175<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.06<br />value: 2.9416<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.07<br />value: 2.9659<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.08<br />value: 2.9904<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.09<br />value: 3.0151<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.10<br />value: 3.0400<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.11<br />value: 3.0651<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.12<br />value: 3.0904<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.13<br />value: 3.1159<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.14<br />value: 3.1416<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.15<br />value: 3.1675<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.16<br />value: 3.1936<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.17<br />value: 3.2199<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.18<br />value: 3.2464<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.19<br />value: 3.2731<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.20<br />value: 3.3000<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.21<br />value: 3.3271<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.22<br />value: 3.3544<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.23<br />value: 3.3819<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.24<br />value: 3.4096<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.25<br />value: 3.4375<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.26<br />value: 3.4656<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.27<br />value: 3.4939<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.28<br />value: 3.5224<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.29<br />value: 3.5511<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.30<br />value: 3.5800<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.31<br />value: 3.6091<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.32<br />value: 3.6384<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.33<br />value: 3.6679<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.34<br />value: 3.6976<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.35<br />value: 3.7275<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.36<br />value: 3.7576<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.37<br />value: 3.7879<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.38<br />value: 3.8184<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.39<br />value: 3.8491<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.40<br />value: 3.8800<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.41<br />value: 3.9111<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.42<br />value: 3.9424<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.43<br />value: 3.9739<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.44<br />value: 4.0056<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.45<br />value: 4.0375<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.46<br />value: 4.0696<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.47<br />value: 4.1019<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.48<br />value: 4.1344<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.49<br />value: 4.1671<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.50<br />value: 4.2000<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.51<br />value: 4.2331<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.52<br />value: 4.2664<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.53<br />value: 4.2999<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.54<br />value: 4.3336<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.55<br />value: 4.3675<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.56<br />value: 4.4016<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.57<br />value: 4.4359<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.58<br />value: 4.4704<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.59<br />value: 4.5051<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.60<br />value: 4.5400<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.61<br />value: 4.5751<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.62<br />value: 4.6104<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.63<br />value: 4.6459<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.64<br />value: 4.6816<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.65<br />value: 4.7175<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.66<br />value: 4.7536<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.67<br />value: 4.7899<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.68<br />value: 4.8264<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.69<br />value: 4.8631<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.70<br />value: 4.9000<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.71<br />value: 4.9371<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.72<br />value: 4.9744<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.73<br />value: 5.0119<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.74<br />value: 5.0496<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.75<br />value: 5.0875<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.76<br />value: 5.1256<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.77<br />value: 5.1639<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.78<br />value: 5.2024<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.79<br />value: 5.2411<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.80<br />value: 5.2800<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.81<br />value: 5.3191<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.82<br />value: 5.3584<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.83<br />value: 5.3979<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.84<br />value: 5.4376<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.85<br />value: 5.4775<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.86<br />value: 5.5176<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.87<br />value: 5.5579<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.88<br />value: 5.5984<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.89<br />value: 5.6391<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.90<br />value: 5.6800<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.91<br />value: 5.7211<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.92<br />value: 5.7624<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.93<br />value: 5.8039<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.94<br />value: 5.8456<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.95<br />value: 5.8875<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.96<br />value: 5.9296<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.97<br />value: 5.9719<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.98<br />value: 6.0144<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  1.99<br />value: 6.0571<br />Function: Loss + Penalty<br />Lambda: 2.3","b:  2.00<br />value: 6.1000<br />Function: Loss + Penalty<br />Lambda: 2.3"],"frame":"2.3","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[1.15,1.127,1.104,1.081,1.058,1.035,1.012,0.989,0.966,0.943,0.92,0.897,0.874,0.851,0.828,0.805,0.782,0.759,0.736,0.713,0.69,0.667,0.644,0.621,0.598,0.575,0.552,0.529,0.506,0.483,0.46,0.437,0.414,0.391,0.368,0.345,0.322,0.299,0.276,0.253,0.23,0.207,0.184,0.161,0.138,0.115,0.092,0.0689999999999999,0.046,0.023,0,0.023,0.046,0.0690000000000001,0.0920000000000001,0.115,0.138,0.161,0.184,0.207,0.23,0.253,0.276,0.299,0.322,0.345,0.368,0.391,0.414,0.437,0.46,0.483,0.506,0.529,0.552,0.575,0.598,0.621,0.644,0.667,0.69,0.713,0.736,0.759,0.782,0.805,0.828,0.851,0.874,0.897,0.92,0.943,0.966,0.989,1.012,1.035,1.058,1.081,1.104,1.127,1.15,1.173,1.196,1.219,1.242,1.265,1.288,1.311,1.334,1.357,1.38,1.403,1.426,1.449,1.472,1.495,1.518,1.541,1.564,1.587,1.61,1.633,1.656,1.679,1.702,1.725,1.748,1.771,1.794,1.817,1.84,1.863,1.886,1.909,1.932,1.955,1.978,2.001,2.024,2.047,2.07,2.093,2.116,2.139,2.162,2.185,2.208,2.231,2.254,2.277,2.3,2.323,2.346,2.369,2.392,2.415,2.438,2.461,2.484,2.507,2.53,2.553,2.576,2.599,2.622,2.645,2.668,2.691,2.714,2.737,2.76,2.783,2.806,2.829,2.852,2.875,2.898,2.921,2.944,2.967,2.99,3.013,3.036,3.059,3.082,3.105,3.128,3.151,3.174,3.197,3.22,3.243,3.266,3.289,3.312,3.335,3.358,3.381,3.404,3.427,3.45,3.473,3.496,3.519,3.542,3.565,3.588,3.611,3.634,3.657,3.68,3.703,3.726,3.749,3.772,3.795,3.818,3.841,3.864,3.887,3.91,3.933,3.956,3.979,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 1.1500<br />Function: Penalty<br />Lambda: 2.3","b: -0.49<br />value: 1.1270<br />Function: Penalty<br />Lambda: 2.3","b: -0.48<br />value: 1.1040<br />Function: Penalty<br />Lambda: 2.3","b: -0.47<br />value: 1.0810<br />Function: Penalty<br />Lambda: 2.3","b: -0.46<br />value: 1.0580<br />Function: Penalty<br />Lambda: 2.3","b: -0.45<br />value: 1.0350<br />Function: Penalty<br />Lambda: 2.3","b: -0.44<br />value: 1.0120<br />Function: Penalty<br />Lambda: 2.3","b: -0.43<br />value: 0.9890<br />Function: Penalty<br />Lambda: 2.3","b: -0.42<br />value: 0.9660<br />Function: Penalty<br />Lambda: 2.3","b: -0.41<br />value: 0.9430<br />Function: Penalty<br />Lambda: 2.3","b: -0.40<br />value: 0.9200<br />Function: Penalty<br />Lambda: 2.3","b: -0.39<br />value: 0.8970<br />Function: Penalty<br />Lambda: 2.3","b: -0.38<br />value: 0.8740<br />Function: Penalty<br />Lambda: 2.3","b: -0.37<br />value: 0.8510<br />Function: Penalty<br />Lambda: 2.3","b: -0.36<br />value: 0.8280<br />Function: Penalty<br />Lambda: 2.3","b: -0.35<br />value: 0.8050<br />Function: Penalty<br />Lambda: 2.3","b: -0.34<br />value: 0.7820<br />Function: Penalty<br />Lambda: 2.3","b: -0.33<br />value: 0.7590<br />Function: Penalty<br />Lambda: 2.3","b: -0.32<br />value: 0.7360<br />Function: Penalty<br />Lambda: 2.3","b: -0.31<br />value: 0.7130<br />Function: Penalty<br />Lambda: 2.3","b: -0.30<br />value: 0.6900<br />Function: Penalty<br />Lambda: 2.3","b: -0.29<br />value: 0.6670<br />Function: Penalty<br />Lambda: 2.3","b: -0.28<br />value: 0.6440<br />Function: Penalty<br />Lambda: 2.3","b: -0.27<br />value: 0.6210<br />Function: Penalty<br />Lambda: 2.3","b: -0.26<br />value: 0.5980<br />Function: Penalty<br />Lambda: 2.3","b: -0.25<br />value: 0.5750<br />Function: Penalty<br />Lambda: 2.3","b: -0.24<br />value: 0.5520<br />Function: Penalty<br />Lambda: 2.3","b: -0.23<br />value: 0.5290<br />Function: Penalty<br />Lambda: 2.3","b: -0.22<br />value: 0.5060<br />Function: Penalty<br />Lambda: 2.3","b: -0.21<br />value: 0.4830<br />Function: Penalty<br />Lambda: 2.3","b: -0.20<br />value: 0.4600<br />Function: Penalty<br />Lambda: 2.3","b: -0.19<br />value: 0.4370<br />Function: Penalty<br />Lambda: 2.3","b: -0.18<br />value: 0.4140<br />Function: Penalty<br />Lambda: 2.3","b: -0.17<br />value: 0.3910<br />Function: Penalty<br />Lambda: 2.3","b: -0.16<br />value: 0.3680<br />Function: Penalty<br />Lambda: 2.3","b: -0.15<br />value: 0.3450<br />Function: Penalty<br />Lambda: 2.3","b: -0.14<br />value: 0.3220<br />Function: Penalty<br />Lambda: 2.3","b: -0.13<br />value: 0.2990<br />Function: Penalty<br />Lambda: 2.3","b: -0.12<br />value: 0.2760<br />Function: Penalty<br />Lambda: 2.3","b: -0.11<br />value: 0.2530<br />Function: Penalty<br />Lambda: 2.3","b: -0.10<br />value: 0.2300<br />Function: Penalty<br />Lambda: 2.3","b: -0.09<br />value: 0.2070<br />Function: Penalty<br />Lambda: 2.3","b: -0.08<br />value: 0.1840<br />Function: Penalty<br />Lambda: 2.3","b: -0.07<br />value: 0.1610<br />Function: Penalty<br />Lambda: 2.3","b: -0.06<br />value: 0.1380<br />Function: Penalty<br />Lambda: 2.3","b: -0.05<br />value: 0.1150<br />Function: Penalty<br />Lambda: 2.3","b: -0.04<br />value: 0.0920<br />Function: Penalty<br />Lambda: 2.3","b: -0.03<br />value: 0.0690<br />Function: Penalty<br />Lambda: 2.3","b: -0.02<br />value: 0.0460<br />Function: Penalty<br />Lambda: 2.3","b: -0.01<br />value: 0.0230<br />Function: Penalty<br />Lambda: 2.3","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 2.3","b:  0.01<br />value: 0.0230<br />Function: Penalty<br />Lambda: 2.3","b:  0.02<br />value: 0.0460<br />Function: Penalty<br />Lambda: 2.3","b:  0.03<br />value: 0.0690<br />Function: Penalty<br />Lambda: 2.3","b:  0.04<br />value: 0.0920<br />Function: Penalty<br />Lambda: 2.3","b:  0.05<br />value: 0.1150<br />Function: Penalty<br />Lambda: 2.3","b:  0.06<br />value: 0.1380<br />Function: Penalty<br />Lambda: 2.3","b:  0.07<br />value: 0.1610<br />Function: Penalty<br />Lambda: 2.3","b:  0.08<br />value: 0.1840<br />Function: Penalty<br />Lambda: 2.3","b:  0.09<br />value: 0.2070<br />Function: Penalty<br />Lambda: 2.3","b:  0.10<br />value: 0.2300<br />Function: Penalty<br />Lambda: 2.3","b:  0.11<br />value: 0.2530<br />Function: Penalty<br />Lambda: 2.3","b:  0.12<br />value: 0.2760<br />Function: Penalty<br />Lambda: 2.3","b:  0.13<br />value: 0.2990<br />Function: Penalty<br />Lambda: 2.3","b:  0.14<br />value: 0.3220<br />Function: Penalty<br />Lambda: 2.3","b:  0.15<br />value: 0.3450<br />Function: Penalty<br />Lambda: 2.3","b:  0.16<br />value: 0.3680<br />Function: Penalty<br />Lambda: 2.3","b:  0.17<br />value: 0.3910<br />Function: Penalty<br />Lambda: 2.3","b:  0.18<br />value: 0.4140<br />Function: Penalty<br />Lambda: 2.3","b:  0.19<br />value: 0.4370<br />Function: Penalty<br />Lambda: 2.3","b:  0.20<br />value: 0.4600<br />Function: Penalty<br />Lambda: 2.3","b:  0.21<br />value: 0.4830<br />Function: Penalty<br />Lambda: 2.3","b:  0.22<br />value: 0.5060<br />Function: Penalty<br />Lambda: 2.3","b:  0.23<br />value: 0.5290<br />Function: Penalty<br />Lambda: 2.3","b:  0.24<br />value: 0.5520<br />Function: Penalty<br />Lambda: 2.3","b:  0.25<br />value: 0.5750<br />Function: Penalty<br />Lambda: 2.3","b:  0.26<br />value: 0.5980<br />Function: Penalty<br />Lambda: 2.3","b:  0.27<br />value: 0.6210<br />Function: Penalty<br />Lambda: 2.3","b:  0.28<br />value: 0.6440<br />Function: Penalty<br />Lambda: 2.3","b:  0.29<br />value: 0.6670<br />Function: Penalty<br />Lambda: 2.3","b:  0.30<br />value: 0.6900<br />Function: Penalty<br />Lambda: 2.3","b:  0.31<br />value: 0.7130<br />Function: Penalty<br />Lambda: 2.3","b:  0.32<br />value: 0.7360<br />Function: Penalty<br />Lambda: 2.3","b:  0.33<br />value: 0.7590<br />Function: Penalty<br />Lambda: 2.3","b:  0.34<br />value: 0.7820<br />Function: Penalty<br />Lambda: 2.3","b:  0.35<br />value: 0.8050<br />Function: Penalty<br />Lambda: 2.3","b:  0.36<br />value: 0.8280<br />Function: Penalty<br />Lambda: 2.3","b:  0.37<br />value: 0.8510<br />Function: Penalty<br />Lambda: 2.3","b:  0.38<br />value: 0.8740<br />Function: Penalty<br />Lambda: 2.3","b:  0.39<br />value: 0.8970<br />Function: Penalty<br />Lambda: 2.3","b:  0.40<br />value: 0.9200<br />Function: Penalty<br />Lambda: 2.3","b:  0.41<br />value: 0.9430<br />Function: Penalty<br />Lambda: 2.3","b:  0.42<br />value: 0.9660<br />Function: Penalty<br />Lambda: 2.3","b:  0.43<br />value: 0.9890<br />Function: Penalty<br />Lambda: 2.3","b:  0.44<br />value: 1.0120<br />Function: Penalty<br />Lambda: 2.3","b:  0.45<br />value: 1.0350<br />Function: Penalty<br />Lambda: 2.3","b:  0.46<br />value: 1.0580<br />Function: Penalty<br />Lambda: 2.3","b:  0.47<br />value: 1.0810<br />Function: Penalty<br />Lambda: 2.3","b:  0.48<br />value: 1.1040<br />Function: Penalty<br />Lambda: 2.3","b:  0.49<br />value: 1.1270<br />Function: Penalty<br />Lambda: 2.3","b:  0.50<br />value: 1.1500<br />Function: Penalty<br />Lambda: 2.3","b:  0.51<br />value: 1.1730<br />Function: Penalty<br />Lambda: 2.3","b:  0.52<br />value: 1.1960<br />Function: Penalty<br />Lambda: 2.3","b:  0.53<br />value: 1.2190<br />Function: Penalty<br />Lambda: 2.3","b:  0.54<br />value: 1.2420<br />Function: Penalty<br />Lambda: 2.3","b:  0.55<br />value: 1.2650<br />Function: Penalty<br />Lambda: 2.3","b:  0.56<br />value: 1.2880<br />Function: Penalty<br />Lambda: 2.3","b:  0.57<br />value: 1.3110<br />Function: Penalty<br />Lambda: 2.3","b:  0.58<br />value: 1.3340<br />Function: Penalty<br />Lambda: 2.3","b:  0.59<br />value: 1.3570<br />Function: Penalty<br />Lambda: 2.3","b:  0.60<br />value: 1.3800<br />Function: Penalty<br />Lambda: 2.3","b:  0.61<br />value: 1.4030<br />Function: Penalty<br />Lambda: 2.3","b:  0.62<br />value: 1.4260<br />Function: Penalty<br />Lambda: 2.3","b:  0.63<br />value: 1.4490<br />Function: Penalty<br />Lambda: 2.3","b:  0.64<br />value: 1.4720<br />Function: Penalty<br />Lambda: 2.3","b:  0.65<br />value: 1.4950<br />Function: Penalty<br />Lambda: 2.3","b:  0.66<br />value: 1.5180<br />Function: Penalty<br />Lambda: 2.3","b:  0.67<br />value: 1.5410<br />Function: Penalty<br />Lambda: 2.3","b:  0.68<br />value: 1.5640<br />Function: Penalty<br />Lambda: 2.3","b:  0.69<br />value: 1.5870<br />Function: Penalty<br />Lambda: 2.3","b:  0.70<br />value: 1.6100<br />Function: Penalty<br />Lambda: 2.3","b:  0.71<br />value: 1.6330<br />Function: Penalty<br />Lambda: 2.3","b:  0.72<br />value: 1.6560<br />Function: Penalty<br />Lambda: 2.3","b:  0.73<br />value: 1.6790<br />Function: Penalty<br />Lambda: 2.3","b:  0.74<br />value: 1.7020<br />Function: Penalty<br />Lambda: 2.3","b:  0.75<br />value: 1.7250<br />Function: Penalty<br />Lambda: 2.3","b:  0.76<br />value: 1.7480<br />Function: Penalty<br />Lambda: 2.3","b:  0.77<br />value: 1.7710<br />Function: Penalty<br />Lambda: 2.3","b:  0.78<br />value: 1.7940<br />Function: Penalty<br />Lambda: 2.3","b:  0.79<br />value: 1.8170<br />Function: Penalty<br />Lambda: 2.3","b:  0.80<br />value: 1.8400<br />Function: Penalty<br />Lambda: 2.3","b:  0.81<br />value: 1.8630<br />Function: Penalty<br />Lambda: 2.3","b:  0.82<br />value: 1.8860<br />Function: Penalty<br />Lambda: 2.3","b:  0.83<br />value: 1.9090<br />Function: Penalty<br />Lambda: 2.3","b:  0.84<br />value: 1.9320<br />Function: Penalty<br />Lambda: 2.3","b:  0.85<br />value: 1.9550<br />Function: Penalty<br />Lambda: 2.3","b:  0.86<br />value: 1.9780<br />Function: Penalty<br />Lambda: 2.3","b:  0.87<br />value: 2.0010<br />Function: Penalty<br />Lambda: 2.3","b:  0.88<br />value: 2.0240<br />Function: Penalty<br />Lambda: 2.3","b:  0.89<br />value: 2.0470<br />Function: Penalty<br />Lambda: 2.3","b:  0.90<br />value: 2.0700<br />Function: Penalty<br />Lambda: 2.3","b:  0.91<br />value: 2.0930<br />Function: Penalty<br />Lambda: 2.3","b:  0.92<br />value: 2.1160<br />Function: Penalty<br />Lambda: 2.3","b:  0.93<br />value: 2.1390<br />Function: Penalty<br />Lambda: 2.3","b:  0.94<br />value: 2.1620<br />Function: Penalty<br />Lambda: 2.3","b:  0.95<br />value: 2.1850<br />Function: Penalty<br />Lambda: 2.3","b:  0.96<br />value: 2.2080<br />Function: Penalty<br />Lambda: 2.3","b:  0.97<br />value: 2.2310<br />Function: Penalty<br />Lambda: 2.3","b:  0.98<br />value: 2.2540<br />Function: Penalty<br />Lambda: 2.3","b:  0.99<br />value: 2.2770<br />Function: Penalty<br />Lambda: 2.3","b:  1.00<br />value: 2.3000<br />Function: Penalty<br />Lambda: 2.3","b:  1.01<br />value: 2.3230<br />Function: Penalty<br />Lambda: 2.3","b:  1.02<br />value: 2.3460<br />Function: Penalty<br />Lambda: 2.3","b:  1.03<br />value: 2.3690<br />Function: Penalty<br />Lambda: 2.3","b:  1.04<br />value: 2.3920<br />Function: Penalty<br />Lambda: 2.3","b:  1.05<br />value: 2.4150<br />Function: Penalty<br />Lambda: 2.3","b:  1.06<br />value: 2.4380<br />Function: Penalty<br />Lambda: 2.3","b:  1.07<br />value: 2.4610<br />Function: Penalty<br />Lambda: 2.3","b:  1.08<br />value: 2.4840<br />Function: Penalty<br />Lambda: 2.3","b:  1.09<br />value: 2.5070<br />Function: Penalty<br />Lambda: 2.3","b:  1.10<br />value: 2.5300<br />Function: Penalty<br />Lambda: 2.3","b:  1.11<br />value: 2.5530<br />Function: Penalty<br />Lambda: 2.3","b:  1.12<br />value: 2.5760<br />Function: Penalty<br />Lambda: 2.3","b:  1.13<br />value: 2.5990<br />Function: Penalty<br />Lambda: 2.3","b:  1.14<br />value: 2.6220<br />Function: Penalty<br />Lambda: 2.3","b:  1.15<br />value: 2.6450<br />Function: Penalty<br />Lambda: 2.3","b:  1.16<br />value: 2.6680<br />Function: Penalty<br />Lambda: 2.3","b:  1.17<br />value: 2.6910<br />Function: Penalty<br />Lambda: 2.3","b:  1.18<br />value: 2.7140<br />Function: Penalty<br />Lambda: 2.3","b:  1.19<br />value: 2.7370<br />Function: Penalty<br />Lambda: 2.3","b:  1.20<br />value: 2.7600<br />Function: Penalty<br />Lambda: 2.3","b:  1.21<br />value: 2.7830<br />Function: Penalty<br />Lambda: 2.3","b:  1.22<br />value: 2.8060<br />Function: Penalty<br />Lambda: 2.3","b:  1.23<br />value: 2.8290<br />Function: Penalty<br />Lambda: 2.3","b:  1.24<br />value: 2.8520<br />Function: Penalty<br />Lambda: 2.3","b:  1.25<br />value: 2.8750<br />Function: Penalty<br />Lambda: 2.3","b:  1.26<br />value: 2.8980<br />Function: Penalty<br />Lambda: 2.3","b:  1.27<br />value: 2.9210<br />Function: Penalty<br />Lambda: 2.3","b:  1.28<br />value: 2.9440<br />Function: Penalty<br />Lambda: 2.3","b:  1.29<br />value: 2.9670<br />Function: Penalty<br />Lambda: 2.3","b:  1.30<br />value: 2.9900<br />Function: Penalty<br />Lambda: 2.3","b:  1.31<br />value: 3.0130<br />Function: Penalty<br />Lambda: 2.3","b:  1.32<br />value: 3.0360<br />Function: Penalty<br />Lambda: 2.3","b:  1.33<br />value: 3.0590<br />Function: Penalty<br />Lambda: 2.3","b:  1.34<br />value: 3.0820<br />Function: Penalty<br />Lambda: 2.3","b:  1.35<br />value: 3.1050<br />Function: Penalty<br />Lambda: 2.3","b:  1.36<br />value: 3.1280<br />Function: Penalty<br />Lambda: 2.3","b:  1.37<br />value: 3.1510<br />Function: Penalty<br />Lambda: 2.3","b:  1.38<br />value: 3.1740<br />Function: Penalty<br />Lambda: 2.3","b:  1.39<br />value: 3.1970<br />Function: Penalty<br />Lambda: 2.3","b:  1.40<br />value: 3.2200<br />Function: Penalty<br />Lambda: 2.3","b:  1.41<br />value: 3.2430<br />Function: Penalty<br />Lambda: 2.3","b:  1.42<br />value: 3.2660<br />Function: Penalty<br />Lambda: 2.3","b:  1.43<br />value: 3.2890<br />Function: Penalty<br />Lambda: 2.3","b:  1.44<br />value: 3.3120<br />Function: Penalty<br />Lambda: 2.3","b:  1.45<br />value: 3.3350<br />Function: Penalty<br />Lambda: 2.3","b:  1.46<br />value: 3.3580<br />Function: Penalty<br />Lambda: 2.3","b:  1.47<br />value: 3.3810<br />Function: Penalty<br />Lambda: 2.3","b:  1.48<br />value: 3.4040<br />Function: Penalty<br />Lambda: 2.3","b:  1.49<br />value: 3.4270<br />Function: Penalty<br />Lambda: 2.3","b:  1.50<br />value: 3.4500<br />Function: Penalty<br />Lambda: 2.3","b:  1.51<br />value: 3.4730<br />Function: Penalty<br />Lambda: 2.3","b:  1.52<br />value: 3.4960<br />Function: Penalty<br />Lambda: 2.3","b:  1.53<br />value: 3.5190<br />Function: Penalty<br />Lambda: 2.3","b:  1.54<br />value: 3.5420<br />Function: Penalty<br />Lambda: 2.3","b:  1.55<br />value: 3.5650<br />Function: Penalty<br />Lambda: 2.3","b:  1.56<br />value: 3.5880<br />Function: Penalty<br />Lambda: 2.3","b:  1.57<br />value: 3.6110<br />Function: Penalty<br />Lambda: 2.3","b:  1.58<br />value: 3.6340<br />Function: Penalty<br />Lambda: 2.3","b:  1.59<br />value: 3.6570<br />Function: Penalty<br />Lambda: 2.3","b:  1.60<br />value: 3.6800<br />Function: Penalty<br />Lambda: 2.3","b:  1.61<br />value: 3.7030<br />Function: Penalty<br />Lambda: 2.3","b:  1.62<br />value: 3.7260<br />Function: Penalty<br />Lambda: 2.3","b:  1.63<br />value: 3.7490<br />Function: Penalty<br />Lambda: 2.3","b:  1.64<br />value: 3.7720<br />Function: Penalty<br />Lambda: 2.3","b:  1.65<br />value: 3.7950<br />Function: Penalty<br />Lambda: 2.3","b:  1.66<br />value: 3.8180<br />Function: Penalty<br />Lambda: 2.3","b:  1.67<br />value: 3.8410<br />Function: Penalty<br />Lambda: 2.3","b:  1.68<br />value: 3.8640<br />Function: Penalty<br />Lambda: 2.3","b:  1.69<br />value: 3.8870<br />Function: Penalty<br />Lambda: 2.3","b:  1.70<br />value: 3.9100<br />Function: Penalty<br />Lambda: 2.3","b:  1.71<br />value: 3.9330<br />Function: Penalty<br />Lambda: 2.3","b:  1.72<br />value: 3.9560<br />Function: Penalty<br />Lambda: 2.3","b:  1.73<br />value: 3.9790<br />Function: Penalty<br />Lambda: 2.3","b:  1.74<br />value: 4.0020<br />Function: Penalty<br />Lambda: 2.3","b:  1.75<br />value: 4.0250<br />Function: Penalty<br />Lambda: 2.3","b:  1.76<br />value: 4.0480<br />Function: Penalty<br />Lambda: 2.3","b:  1.77<br />value: 4.0710<br />Function: Penalty<br />Lambda: 2.3","b:  1.78<br />value: 4.0940<br />Function: Penalty<br />Lambda: 2.3","b:  1.79<br />value: 4.1170<br />Function: Penalty<br />Lambda: 2.3","b:  1.80<br />value: 4.1400<br />Function: Penalty<br />Lambda: 2.3","b:  1.81<br />value: 4.1630<br />Function: Penalty<br />Lambda: 2.3","b:  1.82<br />value: 4.1860<br />Function: Penalty<br />Lambda: 2.3","b:  1.83<br />value: 4.2090<br />Function: Penalty<br />Lambda: 2.3","b:  1.84<br />value: 4.2320<br />Function: Penalty<br />Lambda: 2.3","b:  1.85<br />value: 4.2550<br />Function: Penalty<br />Lambda: 2.3","b:  1.86<br />value: 4.2780<br />Function: Penalty<br />Lambda: 2.3","b:  1.87<br />value: 4.3010<br />Function: Penalty<br />Lambda: 2.3","b:  1.88<br />value: 4.3240<br />Function: Penalty<br />Lambda: 2.3","b:  1.89<br />value: 4.3470<br />Function: Penalty<br />Lambda: 2.3","b:  1.90<br />value: 4.3700<br />Function: Penalty<br />Lambda: 2.3","b:  1.91<br />value: 4.3930<br />Function: Penalty<br />Lambda: 2.3","b:  1.92<br />value: 4.4160<br />Function: Penalty<br />Lambda: 2.3","b:  1.93<br />value: 4.4390<br />Function: Penalty<br />Lambda: 2.3","b:  1.94<br />value: 4.4620<br />Function: Penalty<br />Lambda: 2.3","b:  1.95<br />value: 4.4850<br />Function: Penalty<br />Lambda: 2.3","b:  1.96<br />value: 4.5080<br />Function: Penalty<br />Lambda: 2.3","b:  1.97<br />value: 4.5310<br />Function: Penalty<br />Lambda: 2.3","b:  1.98<br />value: 4.5540<br />Function: Penalty<br />Lambda: 2.3","b:  1.99<br />value: 4.5770<br />Function: Penalty<br />Lambda: 2.3","b:  2.00<br />value: 4.6000<br />Function: Penalty<br />Lambda: 2.3"],"frame":"2.3","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"2.4","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 2.4","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 2.4","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 2.4","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 2.4","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 2.4","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 2.4","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 2.4","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 2.4","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 2.4","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 2.4","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 2.4","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 2.4","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 2.4","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 2.4","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 2.4","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 2.4","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 2.4","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 2.4","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 2.4","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 2.4","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 2.4","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 2.4","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 2.4","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 2.4","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 2.4","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 2.4","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 2.4","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 2.4","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 2.4","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 2.4","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 2.4","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 2.4","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 2.4","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 2.4","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 2.4","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 2.4","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 2.4","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 2.4","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 2.4","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 2.4","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 2.4","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 2.4","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 2.4","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 2.4","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 2.4","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 2.4","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 2.4","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 2.4","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 2.4","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 2.4","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 2.4","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 2.4","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 2.4","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 2.4","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 2.4","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 2.4","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 2.4","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 2.4","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 2.4","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 2.4","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 2.4","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 2.4","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 2.4","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 2.4","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 2.4","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 2.4","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 2.4","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 2.4","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 2.4","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 2.4","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 2.4","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 2.4","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 2.4","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 2.4","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 2.4","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 2.4","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 2.4","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 2.4","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 2.4","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 2.4","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 2.4","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 2.4","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 2.4","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 2.4","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 2.4","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 2.4","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 2.4","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 2.4","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 2.4","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 2.4","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 2.4","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 2.4","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 2.4","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 2.4","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 2.4","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 2.4","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 2.4","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 2.4","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 2.4","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 2.4","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 2.4","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 2.4","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 2.4","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 2.4","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 2.4","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 2.4","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 2.4","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 2.4","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 2.4","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 2.4","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 2.4","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 2.4","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 2.4","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 2.4","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 2.4","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 2.4","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 2.4","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 2.4","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 2.4","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 2.4","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 2.4","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 2.4","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 2.4","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 2.4","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 2.4","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 2.4","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 2.4","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 2.4","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 2.4","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 2.4","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 2.4","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 2.4","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 2.4","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 2.4","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 2.4","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 2.4","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 2.4","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 2.4","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 2.4","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 2.4","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 2.4","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 2.4","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 2.4","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 2.4","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 2.4","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 2.4","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 2.4","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 2.4","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 2.4","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 2.4","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 2.4","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 2.4","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 2.4","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 2.4","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 2.4","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 2.4","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 2.4","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 2.4","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 2.4","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 2.4","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 2.4","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 2.4","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 2.4","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 2.4","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 2.4","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 2.4","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 2.4","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 2.4","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 2.4","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 2.4","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 2.4","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 2.4","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 2.4","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 2.4","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 2.4","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 2.4","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 2.4","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 2.4","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 2.4","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 2.4","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 2.4","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 2.4","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 2.4","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 2.4","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 2.4","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 2.4","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 2.4","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 2.4","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 2.4","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 2.4","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 2.4","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 2.4","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 2.4","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 2.4","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 2.4","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 2.4","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 2.4","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 2.4","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 2.4","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 2.4","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 2.4","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 2.4","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 2.4","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 2.4","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 2.4","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 2.4","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 2.4","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 2.4","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 2.4","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 2.4","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 2.4","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 2.4","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 2.4","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 2.4","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 2.4","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 2.4","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 2.4","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 2.4","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 2.4","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 2.4","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 2.4","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 2.4","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 2.4","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 2.4","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 2.4","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 2.4","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 2.4","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 2.4","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 2.4","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 2.4","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 2.4","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 2.4","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 2.4","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 2.4","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 2.4","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 2.4","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 2.4","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 2.4","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 2.4","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 2.4","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 2.4","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 2.4","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 2.4","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 2.4","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 2.4","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 2.4","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 2.4","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 2.4","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 2.4","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 2.4","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 2.4"],"frame":"2.4","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[3.95,3.8961,3.8424,3.7889,3.7356,3.6825,3.6296,3.5769,3.5244,3.4721,3.42,3.3681,3.3164,3.2649,3.2136,3.1625,3.1116,3.0609,3.0104,2.9601,2.91,2.8601,2.8104,2.7609,2.7116,2.6625,2.6136,2.5649,2.5164,2.4681,2.42,2.3721,2.3244,2.2769,2.2296,2.1825,2.1356,2.0889,2.0424,1.9961,1.95,1.9041,1.8584,1.8129,1.7676,1.7225,1.6776,1.6329,1.5884,1.5441,1.5,1.5041,1.5084,1.5129,1.5176,1.5225,1.5276,1.5329,1.5384,1.5441,1.55,1.5561,1.5624,1.5689,1.5756,1.5825,1.5896,1.5969,1.6044,1.6121,1.62,1.6281,1.6364,1.6449,1.6536,1.6625,1.6716,1.6809,1.6904,1.7001,1.71,1.7201,1.7304,1.7409,1.7516,1.7625,1.7736,1.7849,1.7964,1.8081,1.82,1.8321,1.8444,1.8569,1.8696,1.8825,1.8956,1.9089,1.9224,1.9361,1.95,1.9641,1.9784,1.9929,2.0076,2.0225,2.0376,2.0529,2.0684,2.0841,2.1,2.1161,2.1324,2.1489,2.1656,2.1825,2.1996,2.2169,2.2344,2.2521,2.27,2.2881,2.3064,2.3249,2.3436,2.3625,2.3816,2.4009,2.4204,2.4401,2.46,2.4801,2.5004,2.5209,2.5416,2.5625,2.5836,2.6049,2.6264,2.6481,2.67,2.6921,2.7144,2.7369,2.7596,2.7825,2.8056,2.8289,2.8524,2.8761,2.9,2.9241,2.9484,2.9729,2.9976,3.0225,3.0476,3.0729,3.0984,3.1241,3.15,3.1761,3.2024,3.2289,3.2556,3.2825,3.3096,3.3369,3.3644,3.3921,3.42,3.4481,3.4764,3.5049,3.5336,3.5625,3.5916,3.6209,3.6504,3.6801,3.71,3.7401,3.7704,3.8009,3.8316,3.8625,3.8936,3.9249,3.9564,3.9881,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 3.9500<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.49<br />value: 3.8961<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.48<br />value: 3.8424<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.47<br />value: 3.7889<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.46<br />value: 3.7356<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.45<br />value: 3.6825<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.44<br />value: 3.6296<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.43<br />value: 3.5769<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.42<br />value: 3.5244<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.41<br />value: 3.4721<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.40<br />value: 3.4200<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.39<br />value: 3.3681<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.38<br />value: 3.3164<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.37<br />value: 3.2649<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.36<br />value: 3.2136<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.35<br />value: 3.1625<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.34<br />value: 3.1116<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.33<br />value: 3.0609<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.32<br />value: 3.0104<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.31<br />value: 2.9601<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.30<br />value: 2.9100<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.29<br />value: 2.8601<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.28<br />value: 2.8104<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.27<br />value: 2.7609<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.26<br />value: 2.7116<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.25<br />value: 2.6625<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.24<br />value: 2.6136<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.23<br />value: 2.5649<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.22<br />value: 2.5164<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.21<br />value: 2.4681<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.20<br />value: 2.4200<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.19<br />value: 2.3721<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.18<br />value: 2.3244<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.17<br />value: 2.2769<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.16<br />value: 2.2296<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.15<br />value: 2.1825<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.14<br />value: 2.1356<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.13<br />value: 2.0889<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.12<br />value: 2.0424<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.11<br />value: 1.9961<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.10<br />value: 1.9500<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.09<br />value: 1.9041<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.08<br />value: 1.8584<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.07<br />value: 1.8129<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.06<br />value: 1.7676<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.05<br />value: 1.7225<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.04<br />value: 1.6776<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.03<br />value: 1.6329<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.02<br />value: 1.5884<br />Function: Loss + Penalty<br />Lambda: 2.4","b: -0.01<br />value: 1.5441<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.01<br />value: 1.5041<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.02<br />value: 1.5084<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.03<br />value: 1.5129<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.04<br />value: 1.5176<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.05<br />value: 1.5225<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.06<br />value: 1.5276<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.07<br />value: 1.5329<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.08<br />value: 1.5384<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.09<br />value: 1.5441<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.10<br />value: 1.5500<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.11<br />value: 1.5561<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.12<br />value: 1.5624<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.13<br />value: 1.5689<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.14<br />value: 1.5756<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.15<br />value: 1.5825<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.16<br />value: 1.5896<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.17<br />value: 1.5969<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.18<br />value: 1.6044<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.19<br />value: 1.6121<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.20<br />value: 1.6200<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.21<br />value: 1.6281<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.22<br />value: 1.6364<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.23<br />value: 1.6449<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.24<br />value: 1.6536<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.25<br />value: 1.6625<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.26<br />value: 1.6716<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.27<br />value: 1.6809<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.28<br />value: 1.6904<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.29<br />value: 1.7001<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.30<br />value: 1.7100<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.31<br />value: 1.7201<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.32<br />value: 1.7304<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.33<br />value: 1.7409<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.34<br />value: 1.7516<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.35<br />value: 1.7625<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.36<br />value: 1.7736<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.37<br />value: 1.7849<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.38<br />value: 1.7964<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.39<br />value: 1.8081<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.40<br />value: 1.8200<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.41<br />value: 1.8321<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.42<br />value: 1.8444<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.43<br />value: 1.8569<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.44<br />value: 1.8696<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.45<br />value: 1.8825<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.46<br />value: 1.8956<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.47<br />value: 1.9089<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.48<br />value: 1.9224<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.49<br />value: 1.9361<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.50<br />value: 1.9500<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.51<br />value: 1.9641<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.52<br />value: 1.9784<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.53<br />value: 1.9929<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.54<br />value: 2.0076<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.55<br />value: 2.0225<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.56<br />value: 2.0376<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.57<br />value: 2.0529<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.58<br />value: 2.0684<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.59<br />value: 2.0841<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.60<br />value: 2.1000<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.61<br />value: 2.1161<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.62<br />value: 2.1324<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.63<br />value: 2.1489<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.64<br />value: 2.1656<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.65<br />value: 2.1825<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.66<br />value: 2.1996<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.67<br />value: 2.2169<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.68<br />value: 2.2344<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.69<br />value: 2.2521<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.70<br />value: 2.2700<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.71<br />value: 2.2881<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.72<br />value: 2.3064<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.73<br />value: 2.3249<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.74<br />value: 2.3436<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.75<br />value: 2.3625<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.76<br />value: 2.3816<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.77<br />value: 2.4009<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.78<br />value: 2.4204<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.79<br />value: 2.4401<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.80<br />value: 2.4600<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.81<br />value: 2.4801<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.82<br />value: 2.5004<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.83<br />value: 2.5209<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.84<br />value: 2.5416<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.85<br />value: 2.5625<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.86<br />value: 2.5836<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.87<br />value: 2.6049<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.88<br />value: 2.6264<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.89<br />value: 2.6481<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.90<br />value: 2.6700<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.91<br />value: 2.6921<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.92<br />value: 2.7144<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.93<br />value: 2.7369<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.94<br />value: 2.7596<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.95<br />value: 2.7825<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.96<br />value: 2.8056<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.97<br />value: 2.8289<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.98<br />value: 2.8524<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  0.99<br />value: 2.8761<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.00<br />value: 2.9000<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.01<br />value: 2.9241<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.02<br />value: 2.9484<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.03<br />value: 2.9729<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.04<br />value: 2.9976<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.05<br />value: 3.0225<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.06<br />value: 3.0476<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.07<br />value: 3.0729<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.08<br />value: 3.0984<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.09<br />value: 3.1241<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.10<br />value: 3.1500<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.11<br />value: 3.1761<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.12<br />value: 3.2024<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.13<br />value: 3.2289<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.14<br />value: 3.2556<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.15<br />value: 3.2825<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.16<br />value: 3.3096<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.17<br />value: 3.3369<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.18<br />value: 3.3644<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.19<br />value: 3.3921<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.20<br />value: 3.4200<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.21<br />value: 3.4481<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.22<br />value: 3.4764<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.23<br />value: 3.5049<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.24<br />value: 3.5336<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.25<br />value: 3.5625<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.26<br />value: 3.5916<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.27<br />value: 3.6209<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.28<br />value: 3.6504<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.29<br />value: 3.6801<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.30<br />value: 3.7100<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.31<br />value: 3.7401<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.32<br />value: 3.7704<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.33<br />value: 3.8009<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.34<br />value: 3.8316<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.35<br />value: 3.8625<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.36<br />value: 3.8936<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.37<br />value: 3.9249<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.38<br />value: 3.9564<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.39<br />value: 3.9881<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.40<br />value: 4.0200<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.41<br />value: 4.0521<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.42<br />value: 4.0844<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.43<br />value: 4.1169<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.44<br />value: 4.1496<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.45<br />value: 4.1825<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.46<br />value: 4.2156<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.47<br />value: 4.2489<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.48<br />value: 4.2824<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.49<br />value: 4.3161<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.50<br />value: 4.3500<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.51<br />value: 4.3841<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.52<br />value: 4.4184<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.53<br />value: 4.4529<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.54<br />value: 4.4876<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.55<br />value: 4.5225<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.56<br />value: 4.5576<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.57<br />value: 4.5929<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.58<br />value: 4.6284<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.59<br />value: 4.6641<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.60<br />value: 4.7000<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.61<br />value: 4.7361<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.62<br />value: 4.7724<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.63<br />value: 4.8089<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.64<br />value: 4.8456<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.65<br />value: 4.8825<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.66<br />value: 4.9196<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.67<br />value: 4.9569<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.68<br />value: 4.9944<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.69<br />value: 5.0321<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.70<br />value: 5.0700<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.71<br />value: 5.1081<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.72<br />value: 5.1464<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.73<br />value: 5.1849<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.74<br />value: 5.2236<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.75<br />value: 5.2625<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.76<br />value: 5.3016<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.77<br />value: 5.3409<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.78<br />value: 5.3804<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.79<br />value: 5.4201<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.80<br />value: 5.4600<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.81<br />value: 5.5001<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.82<br />value: 5.5404<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.83<br />value: 5.5809<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.84<br />value: 5.6216<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.85<br />value: 5.6625<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.86<br />value: 5.7036<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.87<br />value: 5.7449<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.88<br />value: 5.7864<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.89<br />value: 5.8281<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.90<br />value: 5.8700<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.91<br />value: 5.9121<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.92<br />value: 5.9544<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.93<br />value: 5.9969<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.94<br />value: 6.0396<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.95<br />value: 6.0825<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.96<br />value: 6.1256<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.97<br />value: 6.1689<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.98<br />value: 6.2124<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  1.99<br />value: 6.2561<br />Function: Loss + Penalty<br />Lambda: 2.4","b:  2.00<br />value: 6.3000<br />Function: Loss + Penalty<br />Lambda: 2.4"],"frame":"2.4","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[1.2,1.176,1.152,1.128,1.104,1.08,1.056,1.032,1.008,0.984,0.96,0.936,0.912,0.888,0.864,0.84,0.816,0.792,0.768,0.744,0.72,0.696,0.672,0.648,0.624,0.6,0.576,0.552,0.528,0.504,0.48,0.456,0.432,0.408,0.384,0.36,0.336,0.312,0.288,0.264,0.24,0.216,0.192,0.168,0.144,0.12,0.096,0.0719999999999999,0.048,0.024,0,0.024,0.048,0.0720000000000001,0.0960000000000001,0.12,0.144,0.168,0.192,0.216,0.24,0.264,0.288,0.312,0.336,0.36,0.384,0.408,0.432,0.456,0.48,0.504,0.528,0.552,0.576,0.6,0.624,0.648,0.672,0.696,0.72,0.744,0.768,0.792,0.816,0.84,0.864,0.888,0.912,0.936,0.96,0.984,1.008,1.032,1.056,1.08,1.104,1.128,1.152,1.176,1.2,1.224,1.248,1.272,1.296,1.32,1.344,1.368,1.392,1.416,1.44,1.464,1.488,1.512,1.536,1.56,1.584,1.608,1.632,1.656,1.68,1.704,1.728,1.752,1.776,1.8,1.824,1.848,1.872,1.896,1.92,1.944,1.968,1.992,2.016,2.04,2.064,2.088,2.112,2.136,2.16,2.184,2.208,2.232,2.256,2.28,2.304,2.328,2.352,2.376,2.4,2.424,2.448,2.472,2.496,2.52,2.544,2.568,2.592,2.616,2.64,2.664,2.688,2.712,2.736,2.76,2.784,2.808,2.832,2.856,2.88,2.904,2.928,2.952,2.976,3,3.024,3.048,3.072,3.096,3.12,3.144,3.168,3.192,3.216,3.24,3.264,3.288,3.312,3.336,3.36,3.384,3.408,3.432,3.456,3.48,3.504,3.528,3.552,3.576,3.6,3.624,3.648,3.672,3.696,3.72,3.744,3.768,3.792,3.816,3.84,3.864,3.888,3.912,3.936,3.96,3.984,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 1.2000<br />Function: Penalty<br />Lambda: 2.4","b: -0.49<br />value: 1.1760<br />Function: Penalty<br />Lambda: 2.4","b: -0.48<br />value: 1.1520<br />Function: Penalty<br />Lambda: 2.4","b: -0.47<br />value: 1.1280<br />Function: Penalty<br />Lambda: 2.4","b: -0.46<br />value: 1.1040<br />Function: Penalty<br />Lambda: 2.4","b: -0.45<br />value: 1.0800<br />Function: Penalty<br />Lambda: 2.4","b: -0.44<br />value: 1.0560<br />Function: Penalty<br />Lambda: 2.4","b: -0.43<br />value: 1.0320<br />Function: Penalty<br />Lambda: 2.4","b: -0.42<br />value: 1.0080<br />Function: Penalty<br />Lambda: 2.4","b: -0.41<br />value: 0.9840<br />Function: Penalty<br />Lambda: 2.4","b: -0.40<br />value: 0.9600<br />Function: Penalty<br />Lambda: 2.4","b: -0.39<br />value: 0.9360<br />Function: Penalty<br />Lambda: 2.4","b: -0.38<br />value: 0.9120<br />Function: Penalty<br />Lambda: 2.4","b: -0.37<br />value: 0.8880<br />Function: Penalty<br />Lambda: 2.4","b: -0.36<br />value: 0.8640<br />Function: Penalty<br />Lambda: 2.4","b: -0.35<br />value: 0.8400<br />Function: Penalty<br />Lambda: 2.4","b: -0.34<br />value: 0.8160<br />Function: Penalty<br />Lambda: 2.4","b: -0.33<br />value: 0.7920<br />Function: Penalty<br />Lambda: 2.4","b: -0.32<br />value: 0.7680<br />Function: Penalty<br />Lambda: 2.4","b: -0.31<br />value: 0.7440<br />Function: Penalty<br />Lambda: 2.4","b: -0.30<br />value: 0.7200<br />Function: Penalty<br />Lambda: 2.4","b: -0.29<br />value: 0.6960<br />Function: Penalty<br />Lambda: 2.4","b: -0.28<br />value: 0.6720<br />Function: Penalty<br />Lambda: 2.4","b: -0.27<br />value: 0.6480<br />Function: Penalty<br />Lambda: 2.4","b: -0.26<br />value: 0.6240<br />Function: Penalty<br />Lambda: 2.4","b: -0.25<br />value: 0.6000<br />Function: Penalty<br />Lambda: 2.4","b: -0.24<br />value: 0.5760<br />Function: Penalty<br />Lambda: 2.4","b: -0.23<br />value: 0.5520<br />Function: Penalty<br />Lambda: 2.4","b: -0.22<br />value: 0.5280<br />Function: Penalty<br />Lambda: 2.4","b: -0.21<br />value: 0.5040<br />Function: Penalty<br />Lambda: 2.4","b: -0.20<br />value: 0.4800<br />Function: Penalty<br />Lambda: 2.4","b: -0.19<br />value: 0.4560<br />Function: Penalty<br />Lambda: 2.4","b: -0.18<br />value: 0.4320<br />Function: Penalty<br />Lambda: 2.4","b: -0.17<br />value: 0.4080<br />Function: Penalty<br />Lambda: 2.4","b: -0.16<br />value: 0.3840<br />Function: Penalty<br />Lambda: 2.4","b: -0.15<br />value: 0.3600<br />Function: Penalty<br />Lambda: 2.4","b: -0.14<br />value: 0.3360<br />Function: Penalty<br />Lambda: 2.4","b: -0.13<br />value: 0.3120<br />Function: Penalty<br />Lambda: 2.4","b: -0.12<br />value: 0.2880<br />Function: Penalty<br />Lambda: 2.4","b: -0.11<br />value: 0.2640<br />Function: Penalty<br />Lambda: 2.4","b: -0.10<br />value: 0.2400<br />Function: Penalty<br />Lambda: 2.4","b: -0.09<br />value: 0.2160<br />Function: Penalty<br />Lambda: 2.4","b: -0.08<br />value: 0.1920<br />Function: Penalty<br />Lambda: 2.4","b: -0.07<br />value: 0.1680<br />Function: Penalty<br />Lambda: 2.4","b: -0.06<br />value: 0.1440<br />Function: Penalty<br />Lambda: 2.4","b: -0.05<br />value: 0.1200<br />Function: Penalty<br />Lambda: 2.4","b: -0.04<br />value: 0.0960<br />Function: Penalty<br />Lambda: 2.4","b: -0.03<br />value: 0.0720<br />Function: Penalty<br />Lambda: 2.4","b: -0.02<br />value: 0.0480<br />Function: Penalty<br />Lambda: 2.4","b: -0.01<br />value: 0.0240<br />Function: Penalty<br />Lambda: 2.4","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 2.4","b:  0.01<br />value: 0.0240<br />Function: Penalty<br />Lambda: 2.4","b:  0.02<br />value: 0.0480<br />Function: Penalty<br />Lambda: 2.4","b:  0.03<br />value: 0.0720<br />Function: Penalty<br />Lambda: 2.4","b:  0.04<br />value: 0.0960<br />Function: Penalty<br />Lambda: 2.4","b:  0.05<br />value: 0.1200<br />Function: Penalty<br />Lambda: 2.4","b:  0.06<br />value: 0.1440<br />Function: Penalty<br />Lambda: 2.4","b:  0.07<br />value: 0.1680<br />Function: Penalty<br />Lambda: 2.4","b:  0.08<br />value: 0.1920<br />Function: Penalty<br />Lambda: 2.4","b:  0.09<br />value: 0.2160<br />Function: Penalty<br />Lambda: 2.4","b:  0.10<br />value: 0.2400<br />Function: Penalty<br />Lambda: 2.4","b:  0.11<br />value: 0.2640<br />Function: Penalty<br />Lambda: 2.4","b:  0.12<br />value: 0.2880<br />Function: Penalty<br />Lambda: 2.4","b:  0.13<br />value: 0.3120<br />Function: Penalty<br />Lambda: 2.4","b:  0.14<br />value: 0.3360<br />Function: Penalty<br />Lambda: 2.4","b:  0.15<br />value: 0.3600<br />Function: Penalty<br />Lambda: 2.4","b:  0.16<br />value: 0.3840<br />Function: Penalty<br />Lambda: 2.4","b:  0.17<br />value: 0.4080<br />Function: Penalty<br />Lambda: 2.4","b:  0.18<br />value: 0.4320<br />Function: Penalty<br />Lambda: 2.4","b:  0.19<br />value: 0.4560<br />Function: Penalty<br />Lambda: 2.4","b:  0.20<br />value: 0.4800<br />Function: Penalty<br />Lambda: 2.4","b:  0.21<br />value: 0.5040<br />Function: Penalty<br />Lambda: 2.4","b:  0.22<br />value: 0.5280<br />Function: Penalty<br />Lambda: 2.4","b:  0.23<br />value: 0.5520<br />Function: Penalty<br />Lambda: 2.4","b:  0.24<br />value: 0.5760<br />Function: Penalty<br />Lambda: 2.4","b:  0.25<br />value: 0.6000<br />Function: Penalty<br />Lambda: 2.4","b:  0.26<br />value: 0.6240<br />Function: Penalty<br />Lambda: 2.4","b:  0.27<br />value: 0.6480<br />Function: Penalty<br />Lambda: 2.4","b:  0.28<br />value: 0.6720<br />Function: Penalty<br />Lambda: 2.4","b:  0.29<br />value: 0.6960<br />Function: Penalty<br />Lambda: 2.4","b:  0.30<br />value: 0.7200<br />Function: Penalty<br />Lambda: 2.4","b:  0.31<br />value: 0.7440<br />Function: Penalty<br />Lambda: 2.4","b:  0.32<br />value: 0.7680<br />Function: Penalty<br />Lambda: 2.4","b:  0.33<br />value: 0.7920<br />Function: Penalty<br />Lambda: 2.4","b:  0.34<br />value: 0.8160<br />Function: Penalty<br />Lambda: 2.4","b:  0.35<br />value: 0.8400<br />Function: Penalty<br />Lambda: 2.4","b:  0.36<br />value: 0.8640<br />Function: Penalty<br />Lambda: 2.4","b:  0.37<br />value: 0.8880<br />Function: Penalty<br />Lambda: 2.4","b:  0.38<br />value: 0.9120<br />Function: Penalty<br />Lambda: 2.4","b:  0.39<br />value: 0.9360<br />Function: Penalty<br />Lambda: 2.4","b:  0.40<br />value: 0.9600<br />Function: Penalty<br />Lambda: 2.4","b:  0.41<br />value: 0.9840<br />Function: Penalty<br />Lambda: 2.4","b:  0.42<br />value: 1.0080<br />Function: Penalty<br />Lambda: 2.4","b:  0.43<br />value: 1.0320<br />Function: Penalty<br />Lambda: 2.4","b:  0.44<br />value: 1.0560<br />Function: Penalty<br />Lambda: 2.4","b:  0.45<br />value: 1.0800<br />Function: Penalty<br />Lambda: 2.4","b:  0.46<br />value: 1.1040<br />Function: Penalty<br />Lambda: 2.4","b:  0.47<br />value: 1.1280<br />Function: Penalty<br />Lambda: 2.4","b:  0.48<br />value: 1.1520<br />Function: Penalty<br />Lambda: 2.4","b:  0.49<br />value: 1.1760<br />Function: Penalty<br />Lambda: 2.4","b:  0.50<br />value: 1.2000<br />Function: Penalty<br />Lambda: 2.4","b:  0.51<br />value: 1.2240<br />Function: Penalty<br />Lambda: 2.4","b:  0.52<br />value: 1.2480<br />Function: Penalty<br />Lambda: 2.4","b:  0.53<br />value: 1.2720<br />Function: Penalty<br />Lambda: 2.4","b:  0.54<br />value: 1.2960<br />Function: Penalty<br />Lambda: 2.4","b:  0.55<br />value: 1.3200<br />Function: Penalty<br />Lambda: 2.4","b:  0.56<br />value: 1.3440<br />Function: Penalty<br />Lambda: 2.4","b:  0.57<br />value: 1.3680<br />Function: Penalty<br />Lambda: 2.4","b:  0.58<br />value: 1.3920<br />Function: Penalty<br />Lambda: 2.4","b:  0.59<br />value: 1.4160<br />Function: Penalty<br />Lambda: 2.4","b:  0.60<br />value: 1.4400<br />Function: Penalty<br />Lambda: 2.4","b:  0.61<br />value: 1.4640<br />Function: Penalty<br />Lambda: 2.4","b:  0.62<br />value: 1.4880<br />Function: Penalty<br />Lambda: 2.4","b:  0.63<br />value: 1.5120<br />Function: Penalty<br />Lambda: 2.4","b:  0.64<br />value: 1.5360<br />Function: Penalty<br />Lambda: 2.4","b:  0.65<br />value: 1.5600<br />Function: Penalty<br />Lambda: 2.4","b:  0.66<br />value: 1.5840<br />Function: Penalty<br />Lambda: 2.4","b:  0.67<br />value: 1.6080<br />Function: Penalty<br />Lambda: 2.4","b:  0.68<br />value: 1.6320<br />Function: Penalty<br />Lambda: 2.4","b:  0.69<br />value: 1.6560<br />Function: Penalty<br />Lambda: 2.4","b:  0.70<br />value: 1.6800<br />Function: Penalty<br />Lambda: 2.4","b:  0.71<br />value: 1.7040<br />Function: Penalty<br />Lambda: 2.4","b:  0.72<br />value: 1.7280<br />Function: Penalty<br />Lambda: 2.4","b:  0.73<br />value: 1.7520<br />Function: Penalty<br />Lambda: 2.4","b:  0.74<br />value: 1.7760<br />Function: Penalty<br />Lambda: 2.4","b:  0.75<br />value: 1.8000<br />Function: Penalty<br />Lambda: 2.4","b:  0.76<br />value: 1.8240<br />Function: Penalty<br />Lambda: 2.4","b:  0.77<br />value: 1.8480<br />Function: Penalty<br />Lambda: 2.4","b:  0.78<br />value: 1.8720<br />Function: Penalty<br />Lambda: 2.4","b:  0.79<br />value: 1.8960<br />Function: Penalty<br />Lambda: 2.4","b:  0.80<br />value: 1.9200<br />Function: Penalty<br />Lambda: 2.4","b:  0.81<br />value: 1.9440<br />Function: Penalty<br />Lambda: 2.4","b:  0.82<br />value: 1.9680<br />Function: Penalty<br />Lambda: 2.4","b:  0.83<br />value: 1.9920<br />Function: Penalty<br />Lambda: 2.4","b:  0.84<br />value: 2.0160<br />Function: Penalty<br />Lambda: 2.4","b:  0.85<br />value: 2.0400<br />Function: Penalty<br />Lambda: 2.4","b:  0.86<br />value: 2.0640<br />Function: Penalty<br />Lambda: 2.4","b:  0.87<br />value: 2.0880<br />Function: Penalty<br />Lambda: 2.4","b:  0.88<br />value: 2.1120<br />Function: Penalty<br />Lambda: 2.4","b:  0.89<br />value: 2.1360<br />Function: Penalty<br />Lambda: 2.4","b:  0.90<br />value: 2.1600<br />Function: Penalty<br />Lambda: 2.4","b:  0.91<br />value: 2.1840<br />Function: Penalty<br />Lambda: 2.4","b:  0.92<br />value: 2.2080<br />Function: Penalty<br />Lambda: 2.4","b:  0.93<br />value: 2.2320<br />Function: Penalty<br />Lambda: 2.4","b:  0.94<br />value: 2.2560<br />Function: Penalty<br />Lambda: 2.4","b:  0.95<br />value: 2.2800<br />Function: Penalty<br />Lambda: 2.4","b:  0.96<br />value: 2.3040<br />Function: Penalty<br />Lambda: 2.4","b:  0.97<br />value: 2.3280<br />Function: Penalty<br />Lambda: 2.4","b:  0.98<br />value: 2.3520<br />Function: Penalty<br />Lambda: 2.4","b:  0.99<br />value: 2.3760<br />Function: Penalty<br />Lambda: 2.4","b:  1.00<br />value: 2.4000<br />Function: Penalty<br />Lambda: 2.4","b:  1.01<br />value: 2.4240<br />Function: Penalty<br />Lambda: 2.4","b:  1.02<br />value: 2.4480<br />Function: Penalty<br />Lambda: 2.4","b:  1.03<br />value: 2.4720<br />Function: Penalty<br />Lambda: 2.4","b:  1.04<br />value: 2.4960<br />Function: Penalty<br />Lambda: 2.4","b:  1.05<br />value: 2.5200<br />Function: Penalty<br />Lambda: 2.4","b:  1.06<br />value: 2.5440<br />Function: Penalty<br />Lambda: 2.4","b:  1.07<br />value: 2.5680<br />Function: Penalty<br />Lambda: 2.4","b:  1.08<br />value: 2.5920<br />Function: Penalty<br />Lambda: 2.4","b:  1.09<br />value: 2.6160<br />Function: Penalty<br />Lambda: 2.4","b:  1.10<br />value: 2.6400<br />Function: Penalty<br />Lambda: 2.4","b:  1.11<br />value: 2.6640<br />Function: Penalty<br />Lambda: 2.4","b:  1.12<br />value: 2.6880<br />Function: Penalty<br />Lambda: 2.4","b:  1.13<br />value: 2.7120<br />Function: Penalty<br />Lambda: 2.4","b:  1.14<br />value: 2.7360<br />Function: Penalty<br />Lambda: 2.4","b:  1.15<br />value: 2.7600<br />Function: Penalty<br />Lambda: 2.4","b:  1.16<br />value: 2.7840<br />Function: Penalty<br />Lambda: 2.4","b:  1.17<br />value: 2.8080<br />Function: Penalty<br />Lambda: 2.4","b:  1.18<br />value: 2.8320<br />Function: Penalty<br />Lambda: 2.4","b:  1.19<br />value: 2.8560<br />Function: Penalty<br />Lambda: 2.4","b:  1.20<br />value: 2.8800<br />Function: Penalty<br />Lambda: 2.4","b:  1.21<br />value: 2.9040<br />Function: Penalty<br />Lambda: 2.4","b:  1.22<br />value: 2.9280<br />Function: Penalty<br />Lambda: 2.4","b:  1.23<br />value: 2.9520<br />Function: Penalty<br />Lambda: 2.4","b:  1.24<br />value: 2.9760<br />Function: Penalty<br />Lambda: 2.4","b:  1.25<br />value: 3.0000<br />Function: Penalty<br />Lambda: 2.4","b:  1.26<br />value: 3.0240<br />Function: Penalty<br />Lambda: 2.4","b:  1.27<br />value: 3.0480<br />Function: Penalty<br />Lambda: 2.4","b:  1.28<br />value: 3.0720<br />Function: Penalty<br />Lambda: 2.4","b:  1.29<br />value: 3.0960<br />Function: Penalty<br />Lambda: 2.4","b:  1.30<br />value: 3.1200<br />Function: Penalty<br />Lambda: 2.4","b:  1.31<br />value: 3.1440<br />Function: Penalty<br />Lambda: 2.4","b:  1.32<br />value: 3.1680<br />Function: Penalty<br />Lambda: 2.4","b:  1.33<br />value: 3.1920<br />Function: Penalty<br />Lambda: 2.4","b:  1.34<br />value: 3.2160<br />Function: Penalty<br />Lambda: 2.4","b:  1.35<br />value: 3.2400<br />Function: Penalty<br />Lambda: 2.4","b:  1.36<br />value: 3.2640<br />Function: Penalty<br />Lambda: 2.4","b:  1.37<br />value: 3.2880<br />Function: Penalty<br />Lambda: 2.4","b:  1.38<br />value: 3.3120<br />Function: Penalty<br />Lambda: 2.4","b:  1.39<br />value: 3.3360<br />Function: Penalty<br />Lambda: 2.4","b:  1.40<br />value: 3.3600<br />Function: Penalty<br />Lambda: 2.4","b:  1.41<br />value: 3.3840<br />Function: Penalty<br />Lambda: 2.4","b:  1.42<br />value: 3.4080<br />Function: Penalty<br />Lambda: 2.4","b:  1.43<br />value: 3.4320<br />Function: Penalty<br />Lambda: 2.4","b:  1.44<br />value: 3.4560<br />Function: Penalty<br />Lambda: 2.4","b:  1.45<br />value: 3.4800<br />Function: Penalty<br />Lambda: 2.4","b:  1.46<br />value: 3.5040<br />Function: Penalty<br />Lambda: 2.4","b:  1.47<br />value: 3.5280<br />Function: Penalty<br />Lambda: 2.4","b:  1.48<br />value: 3.5520<br />Function: Penalty<br />Lambda: 2.4","b:  1.49<br />value: 3.5760<br />Function: Penalty<br />Lambda: 2.4","b:  1.50<br />value: 3.6000<br />Function: Penalty<br />Lambda: 2.4","b:  1.51<br />value: 3.6240<br />Function: Penalty<br />Lambda: 2.4","b:  1.52<br />value: 3.6480<br />Function: Penalty<br />Lambda: 2.4","b:  1.53<br />value: 3.6720<br />Function: Penalty<br />Lambda: 2.4","b:  1.54<br />value: 3.6960<br />Function: Penalty<br />Lambda: 2.4","b:  1.55<br />value: 3.7200<br />Function: Penalty<br />Lambda: 2.4","b:  1.56<br />value: 3.7440<br />Function: Penalty<br />Lambda: 2.4","b:  1.57<br />value: 3.7680<br />Function: Penalty<br />Lambda: 2.4","b:  1.58<br />value: 3.7920<br />Function: Penalty<br />Lambda: 2.4","b:  1.59<br />value: 3.8160<br />Function: Penalty<br />Lambda: 2.4","b:  1.60<br />value: 3.8400<br />Function: Penalty<br />Lambda: 2.4","b:  1.61<br />value: 3.8640<br />Function: Penalty<br />Lambda: 2.4","b:  1.62<br />value: 3.8880<br />Function: Penalty<br />Lambda: 2.4","b:  1.63<br />value: 3.9120<br />Function: Penalty<br />Lambda: 2.4","b:  1.64<br />value: 3.9360<br />Function: Penalty<br />Lambda: 2.4","b:  1.65<br />value: 3.9600<br />Function: Penalty<br />Lambda: 2.4","b:  1.66<br />value: 3.9840<br />Function: Penalty<br />Lambda: 2.4","b:  1.67<br />value: 4.0080<br />Function: Penalty<br />Lambda: 2.4","b:  1.68<br />value: 4.0320<br />Function: Penalty<br />Lambda: 2.4","b:  1.69<br />value: 4.0560<br />Function: Penalty<br />Lambda: 2.4","b:  1.70<br />value: 4.0800<br />Function: Penalty<br />Lambda: 2.4","b:  1.71<br />value: 4.1040<br />Function: Penalty<br />Lambda: 2.4","b:  1.72<br />value: 4.1280<br />Function: Penalty<br />Lambda: 2.4","b:  1.73<br />value: 4.1520<br />Function: Penalty<br />Lambda: 2.4","b:  1.74<br />value: 4.1760<br />Function: Penalty<br />Lambda: 2.4","b:  1.75<br />value: 4.2000<br />Function: Penalty<br />Lambda: 2.4","b:  1.76<br />value: 4.2240<br />Function: Penalty<br />Lambda: 2.4","b:  1.77<br />value: 4.2480<br />Function: Penalty<br />Lambda: 2.4","b:  1.78<br />value: 4.2720<br />Function: Penalty<br />Lambda: 2.4","b:  1.79<br />value: 4.2960<br />Function: Penalty<br />Lambda: 2.4","b:  1.80<br />value: 4.3200<br />Function: Penalty<br />Lambda: 2.4","b:  1.81<br />value: 4.3440<br />Function: Penalty<br />Lambda: 2.4","b:  1.82<br />value: 4.3680<br />Function: Penalty<br />Lambda: 2.4","b:  1.83<br />value: 4.3920<br />Function: Penalty<br />Lambda: 2.4","b:  1.84<br />value: 4.4160<br />Function: Penalty<br />Lambda: 2.4","b:  1.85<br />value: 4.4400<br />Function: Penalty<br />Lambda: 2.4","b:  1.86<br />value: 4.4640<br />Function: Penalty<br />Lambda: 2.4","b:  1.87<br />value: 4.4880<br />Function: Penalty<br />Lambda: 2.4","b:  1.88<br />value: 4.5120<br />Function: Penalty<br />Lambda: 2.4","b:  1.89<br />value: 4.5360<br />Function: Penalty<br />Lambda: 2.4","b:  1.90<br />value: 4.5600<br />Function: Penalty<br />Lambda: 2.4","b:  1.91<br />value: 4.5840<br />Function: Penalty<br />Lambda: 2.4","b:  1.92<br />value: 4.6080<br />Function: Penalty<br />Lambda: 2.4","b:  1.93<br />value: 4.6320<br />Function: Penalty<br />Lambda: 2.4","b:  1.94<br />value: 4.6560<br />Function: Penalty<br />Lambda: 2.4","b:  1.95<br />value: 4.6800<br />Function: Penalty<br />Lambda: 2.4","b:  1.96<br />value: 4.7040<br />Function: Penalty<br />Lambda: 2.4","b:  1.97<br />value: 4.7280<br />Function: Penalty<br />Lambda: 2.4","b:  1.98<br />value: 4.7520<br />Function: Penalty<br />Lambda: 2.4","b:  1.99<br />value: 4.7760<br />Function: Penalty<br />Lambda: 2.4","b:  2.00<br />value: 4.8000<br />Function: Penalty<br />Lambda: 2.4"],"frame":"2.4","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]},{"name":"2.5","data":[{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[2.75,2.7201,2.6904,2.6609,2.6316,2.6025,2.5736,2.5449,2.5164,2.4881,2.46,2.4321,2.4044,2.3769,2.3496,2.3225,2.2956,2.2689,2.2424,2.2161,2.19,2.1641,2.1384,2.1129,2.0876,2.0625,2.0376,2.0129,1.9884,1.9641,1.94,1.9161,1.8924,1.8689,1.8456,1.8225,1.7996,1.7769,1.7544,1.7321,1.71,1.6881,1.6664,1.6449,1.6236,1.6025,1.5816,1.5609,1.5404,1.5201,1.5,1.4801,1.4604,1.4409,1.4216,1.4025,1.3836,1.3649,1.3464,1.3281,1.31,1.2921,1.2744,1.2569,1.2396,1.2225,1.2056,1.1889,1.1724,1.1561,1.14,1.1241,1.1084,1.0929,1.0776,1.0625,1.0476,1.0329,1.0184,1.0041,0.99,0.9761,0.9624,0.9489,0.9356,0.9225,0.9096,0.8969,0.8844,0.8721,0.86,0.8481,0.8364,0.8249,0.8136,0.8025,0.7916,0.7809,0.7704,0.7601,0.75,0.7401,0.7304,0.7209,0.7116,0.7025,0.6936,0.6849,0.6764,0.6681,0.66,0.6521,0.6444,0.6369,0.6296,0.6225,0.6156,0.6089,0.6024,0.5961,0.59,0.5841,0.5784,0.5729,0.5676,0.5625,0.5576,0.5529,0.5484,0.5441,0.54,0.5361,0.5324,0.5289,0.5256,0.5225,0.5196,0.5169,0.5144,0.5121,0.51,0.5081,0.5064,0.5049,0.5036,0.5025,0.5016,0.5009,0.5004,0.5001,0.5,0.5001,0.5004,0.5009,0.5016,0.5025,0.5036,0.5049,0.5064,0.5081,0.51,0.5121,0.5144,0.5169,0.5196,0.5225,0.5256,0.5289,0.5324,0.5361,0.54,0.5441,0.5484,0.5529,0.5576,0.5625,0.5676,0.5729,0.5784,0.5841,0.59,0.5961,0.6024,0.6089,0.6156,0.6225,0.6296,0.6369,0.6444,0.6521,0.66,0.6681,0.6764,0.6849,0.6936,0.7025,0.7116,0.7209,0.7304,0.7401,0.75,0.7601,0.7704,0.7809,0.7916,0.8025,0.8136,0.8249,0.8364,0.8481,0.86,0.8721,0.8844,0.8969,0.9096,0.9225,0.9356,0.9489,0.9624,0.9761,0.99,1.0041,1.0184,1.0329,1.0476,1.0625,1.0776,1.0929,1.1084,1.1241,1.14,1.1561,1.1724,1.1889,1.2056,1.2225,1.2396,1.2569,1.2744,1.2921,1.31,1.3281,1.3464,1.3649,1.3836,1.4025,1.4216,1.4409,1.4604,1.4801,1.5],"text":["b: -0.50<br />value: 2.7500<br />Function: Loss<br />Lambda: 2.5","b: -0.49<br />value: 2.7201<br />Function: Loss<br />Lambda: 2.5","b: -0.48<br />value: 2.6904<br />Function: Loss<br />Lambda: 2.5","b: -0.47<br />value: 2.6609<br />Function: Loss<br />Lambda: 2.5","b: -0.46<br />value: 2.6316<br />Function: Loss<br />Lambda: 2.5","b: -0.45<br />value: 2.6025<br />Function: Loss<br />Lambda: 2.5","b: -0.44<br />value: 2.5736<br />Function: Loss<br />Lambda: 2.5","b: -0.43<br />value: 2.5449<br />Function: Loss<br />Lambda: 2.5","b: -0.42<br />value: 2.5164<br />Function: Loss<br />Lambda: 2.5","b: -0.41<br />value: 2.4881<br />Function: Loss<br />Lambda: 2.5","b: -0.40<br />value: 2.4600<br />Function: Loss<br />Lambda: 2.5","b: -0.39<br />value: 2.4321<br />Function: Loss<br />Lambda: 2.5","b: -0.38<br />value: 2.4044<br />Function: Loss<br />Lambda: 2.5","b: -0.37<br />value: 2.3769<br />Function: Loss<br />Lambda: 2.5","b: -0.36<br />value: 2.3496<br />Function: Loss<br />Lambda: 2.5","b: -0.35<br />value: 2.3225<br />Function: Loss<br />Lambda: 2.5","b: -0.34<br />value: 2.2956<br />Function: Loss<br />Lambda: 2.5","b: -0.33<br />value: 2.2689<br />Function: Loss<br />Lambda: 2.5","b: -0.32<br />value: 2.2424<br />Function: Loss<br />Lambda: 2.5","b: -0.31<br />value: 2.2161<br />Function: Loss<br />Lambda: 2.5","b: -0.30<br />value: 2.1900<br />Function: Loss<br />Lambda: 2.5","b: -0.29<br />value: 2.1641<br />Function: Loss<br />Lambda: 2.5","b: -0.28<br />value: 2.1384<br />Function: Loss<br />Lambda: 2.5","b: -0.27<br />value: 2.1129<br />Function: Loss<br />Lambda: 2.5","b: -0.26<br />value: 2.0876<br />Function: Loss<br />Lambda: 2.5","b: -0.25<br />value: 2.0625<br />Function: Loss<br />Lambda: 2.5","b: -0.24<br />value: 2.0376<br />Function: Loss<br />Lambda: 2.5","b: -0.23<br />value: 2.0129<br />Function: Loss<br />Lambda: 2.5","b: -0.22<br />value: 1.9884<br />Function: Loss<br />Lambda: 2.5","b: -0.21<br />value: 1.9641<br />Function: Loss<br />Lambda: 2.5","b: -0.20<br />value: 1.9400<br />Function: Loss<br />Lambda: 2.5","b: -0.19<br />value: 1.9161<br />Function: Loss<br />Lambda: 2.5","b: -0.18<br />value: 1.8924<br />Function: Loss<br />Lambda: 2.5","b: -0.17<br />value: 1.8689<br />Function: Loss<br />Lambda: 2.5","b: -0.16<br />value: 1.8456<br />Function: Loss<br />Lambda: 2.5","b: -0.15<br />value: 1.8225<br />Function: Loss<br />Lambda: 2.5","b: -0.14<br />value: 1.7996<br />Function: Loss<br />Lambda: 2.5","b: -0.13<br />value: 1.7769<br />Function: Loss<br />Lambda: 2.5","b: -0.12<br />value: 1.7544<br />Function: Loss<br />Lambda: 2.5","b: -0.11<br />value: 1.7321<br />Function: Loss<br />Lambda: 2.5","b: -0.10<br />value: 1.7100<br />Function: Loss<br />Lambda: 2.5","b: -0.09<br />value: 1.6881<br />Function: Loss<br />Lambda: 2.5","b: -0.08<br />value: 1.6664<br />Function: Loss<br />Lambda: 2.5","b: -0.07<br />value: 1.6449<br />Function: Loss<br />Lambda: 2.5","b: -0.06<br />value: 1.6236<br />Function: Loss<br />Lambda: 2.5","b: -0.05<br />value: 1.6025<br />Function: Loss<br />Lambda: 2.5","b: -0.04<br />value: 1.5816<br />Function: Loss<br />Lambda: 2.5","b: -0.03<br />value: 1.5609<br />Function: Loss<br />Lambda: 2.5","b: -0.02<br />value: 1.5404<br />Function: Loss<br />Lambda: 2.5","b: -0.01<br />value: 1.5201<br />Function: Loss<br />Lambda: 2.5","b:  0.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 2.5","b:  0.01<br />value: 1.4801<br />Function: Loss<br />Lambda: 2.5","b:  0.02<br />value: 1.4604<br />Function: Loss<br />Lambda: 2.5","b:  0.03<br />value: 1.4409<br />Function: Loss<br />Lambda: 2.5","b:  0.04<br />value: 1.4216<br />Function: Loss<br />Lambda: 2.5","b:  0.05<br />value: 1.4025<br />Function: Loss<br />Lambda: 2.5","b:  0.06<br />value: 1.3836<br />Function: Loss<br />Lambda: 2.5","b:  0.07<br />value: 1.3649<br />Function: Loss<br />Lambda: 2.5","b:  0.08<br />value: 1.3464<br />Function: Loss<br />Lambda: 2.5","b:  0.09<br />value: 1.3281<br />Function: Loss<br />Lambda: 2.5","b:  0.10<br />value: 1.3100<br />Function: Loss<br />Lambda: 2.5","b:  0.11<br />value: 1.2921<br />Function: Loss<br />Lambda: 2.5","b:  0.12<br />value: 1.2744<br />Function: Loss<br />Lambda: 2.5","b:  0.13<br />value: 1.2569<br />Function: Loss<br />Lambda: 2.5","b:  0.14<br />value: 1.2396<br />Function: Loss<br />Lambda: 2.5","b:  0.15<br />value: 1.2225<br />Function: Loss<br />Lambda: 2.5","b:  0.16<br />value: 1.2056<br />Function: Loss<br />Lambda: 2.5","b:  0.17<br />value: 1.1889<br />Function: Loss<br />Lambda: 2.5","b:  0.18<br />value: 1.1724<br />Function: Loss<br />Lambda: 2.5","b:  0.19<br />value: 1.1561<br />Function: Loss<br />Lambda: 2.5","b:  0.20<br />value: 1.1400<br />Function: Loss<br />Lambda: 2.5","b:  0.21<br />value: 1.1241<br />Function: Loss<br />Lambda: 2.5","b:  0.22<br />value: 1.1084<br />Function: Loss<br />Lambda: 2.5","b:  0.23<br />value: 1.0929<br />Function: Loss<br />Lambda: 2.5","b:  0.24<br />value: 1.0776<br />Function: Loss<br />Lambda: 2.5","b:  0.25<br />value: 1.0625<br />Function: Loss<br />Lambda: 2.5","b:  0.26<br />value: 1.0476<br />Function: Loss<br />Lambda: 2.5","b:  0.27<br />value: 1.0329<br />Function: Loss<br />Lambda: 2.5","b:  0.28<br />value: 1.0184<br />Function: Loss<br />Lambda: 2.5","b:  0.29<br />value: 1.0041<br />Function: Loss<br />Lambda: 2.5","b:  0.30<br />value: 0.9900<br />Function: Loss<br />Lambda: 2.5","b:  0.31<br />value: 0.9761<br />Function: Loss<br />Lambda: 2.5","b:  0.32<br />value: 0.9624<br />Function: Loss<br />Lambda: 2.5","b:  0.33<br />value: 0.9489<br />Function: Loss<br />Lambda: 2.5","b:  0.34<br />value: 0.9356<br />Function: Loss<br />Lambda: 2.5","b:  0.35<br />value: 0.9225<br />Function: Loss<br />Lambda: 2.5","b:  0.36<br />value: 0.9096<br />Function: Loss<br />Lambda: 2.5","b:  0.37<br />value: 0.8969<br />Function: Loss<br />Lambda: 2.5","b:  0.38<br />value: 0.8844<br />Function: Loss<br />Lambda: 2.5","b:  0.39<br />value: 0.8721<br />Function: Loss<br />Lambda: 2.5","b:  0.40<br />value: 0.8600<br />Function: Loss<br />Lambda: 2.5","b:  0.41<br />value: 0.8481<br />Function: Loss<br />Lambda: 2.5","b:  0.42<br />value: 0.8364<br />Function: Loss<br />Lambda: 2.5","b:  0.43<br />value: 0.8249<br />Function: Loss<br />Lambda: 2.5","b:  0.44<br />value: 0.8136<br />Function: Loss<br />Lambda: 2.5","b:  0.45<br />value: 0.8025<br />Function: Loss<br />Lambda: 2.5","b:  0.46<br />value: 0.7916<br />Function: Loss<br />Lambda: 2.5","b:  0.47<br />value: 0.7809<br />Function: Loss<br />Lambda: 2.5","b:  0.48<br />value: 0.7704<br />Function: Loss<br />Lambda: 2.5","b:  0.49<br />value: 0.7601<br />Function: Loss<br />Lambda: 2.5","b:  0.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 2.5","b:  0.51<br />value: 0.7401<br />Function: Loss<br />Lambda: 2.5","b:  0.52<br />value: 0.7304<br />Function: Loss<br />Lambda: 2.5","b:  0.53<br />value: 0.7209<br />Function: Loss<br />Lambda: 2.5","b:  0.54<br />value: 0.7116<br />Function: Loss<br />Lambda: 2.5","b:  0.55<br />value: 0.7025<br />Function: Loss<br />Lambda: 2.5","b:  0.56<br />value: 0.6936<br />Function: Loss<br />Lambda: 2.5","b:  0.57<br />value: 0.6849<br />Function: Loss<br />Lambda: 2.5","b:  0.58<br />value: 0.6764<br />Function: Loss<br />Lambda: 2.5","b:  0.59<br />value: 0.6681<br />Function: Loss<br />Lambda: 2.5","b:  0.60<br />value: 0.6600<br />Function: Loss<br />Lambda: 2.5","b:  0.61<br />value: 0.6521<br />Function: Loss<br />Lambda: 2.5","b:  0.62<br />value: 0.6444<br />Function: Loss<br />Lambda: 2.5","b:  0.63<br />value: 0.6369<br />Function: Loss<br />Lambda: 2.5","b:  0.64<br />value: 0.6296<br />Function: Loss<br />Lambda: 2.5","b:  0.65<br />value: 0.6225<br />Function: Loss<br />Lambda: 2.5","b:  0.66<br />value: 0.6156<br />Function: Loss<br />Lambda: 2.5","b:  0.67<br />value: 0.6089<br />Function: Loss<br />Lambda: 2.5","b:  0.68<br />value: 0.6024<br />Function: Loss<br />Lambda: 2.5","b:  0.69<br />value: 0.5961<br />Function: Loss<br />Lambda: 2.5","b:  0.70<br />value: 0.5900<br />Function: Loss<br />Lambda: 2.5","b:  0.71<br />value: 0.5841<br />Function: Loss<br />Lambda: 2.5","b:  0.72<br />value: 0.5784<br />Function: Loss<br />Lambda: 2.5","b:  0.73<br />value: 0.5729<br />Function: Loss<br />Lambda: 2.5","b:  0.74<br />value: 0.5676<br />Function: Loss<br />Lambda: 2.5","b:  0.75<br />value: 0.5625<br />Function: Loss<br />Lambda: 2.5","b:  0.76<br />value: 0.5576<br />Function: Loss<br />Lambda: 2.5","b:  0.77<br />value: 0.5529<br />Function: Loss<br />Lambda: 2.5","b:  0.78<br />value: 0.5484<br />Function: Loss<br />Lambda: 2.5","b:  0.79<br />value: 0.5441<br />Function: Loss<br />Lambda: 2.5","b:  0.80<br />value: 0.5400<br />Function: Loss<br />Lambda: 2.5","b:  0.81<br />value: 0.5361<br />Function: Loss<br />Lambda: 2.5","b:  0.82<br />value: 0.5324<br />Function: Loss<br />Lambda: 2.5","b:  0.83<br />value: 0.5289<br />Function: Loss<br />Lambda: 2.5","b:  0.84<br />value: 0.5256<br />Function: Loss<br />Lambda: 2.5","b:  0.85<br />value: 0.5225<br />Function: Loss<br />Lambda: 2.5","b:  0.86<br />value: 0.5196<br />Function: Loss<br />Lambda: 2.5","b:  0.87<br />value: 0.5169<br />Function: Loss<br />Lambda: 2.5","b:  0.88<br />value: 0.5144<br />Function: Loss<br />Lambda: 2.5","b:  0.89<br />value: 0.5121<br />Function: Loss<br />Lambda: 2.5","b:  0.90<br />value: 0.5100<br />Function: Loss<br />Lambda: 2.5","b:  0.91<br />value: 0.5081<br />Function: Loss<br />Lambda: 2.5","b:  0.92<br />value: 0.5064<br />Function: Loss<br />Lambda: 2.5","b:  0.93<br />value: 0.5049<br />Function: Loss<br />Lambda: 2.5","b:  0.94<br />value: 0.5036<br />Function: Loss<br />Lambda: 2.5","b:  0.95<br />value: 0.5025<br />Function: Loss<br />Lambda: 2.5","b:  0.96<br />value: 0.5016<br />Function: Loss<br />Lambda: 2.5","b:  0.97<br />value: 0.5009<br />Function: Loss<br />Lambda: 2.5","b:  0.98<br />value: 0.5004<br />Function: Loss<br />Lambda: 2.5","b:  0.99<br />value: 0.5001<br />Function: Loss<br />Lambda: 2.5","b:  1.00<br />value: 0.5000<br />Function: Loss<br />Lambda: 2.5","b:  1.01<br />value: 0.5001<br />Function: Loss<br />Lambda: 2.5","b:  1.02<br />value: 0.5004<br />Function: Loss<br />Lambda: 2.5","b:  1.03<br />value: 0.5009<br />Function: Loss<br />Lambda: 2.5","b:  1.04<br />value: 0.5016<br />Function: Loss<br />Lambda: 2.5","b:  1.05<br />value: 0.5025<br />Function: Loss<br />Lambda: 2.5","b:  1.06<br />value: 0.5036<br />Function: Loss<br />Lambda: 2.5","b:  1.07<br />value: 0.5049<br />Function: Loss<br />Lambda: 2.5","b:  1.08<br />value: 0.5064<br />Function: Loss<br />Lambda: 2.5","b:  1.09<br />value: 0.5081<br />Function: Loss<br />Lambda: 2.5","b:  1.10<br />value: 0.5100<br />Function: Loss<br />Lambda: 2.5","b:  1.11<br />value: 0.5121<br />Function: Loss<br />Lambda: 2.5","b:  1.12<br />value: 0.5144<br />Function: Loss<br />Lambda: 2.5","b:  1.13<br />value: 0.5169<br />Function: Loss<br />Lambda: 2.5","b:  1.14<br />value: 0.5196<br />Function: Loss<br />Lambda: 2.5","b:  1.15<br />value: 0.5225<br />Function: Loss<br />Lambda: 2.5","b:  1.16<br />value: 0.5256<br />Function: Loss<br />Lambda: 2.5","b:  1.17<br />value: 0.5289<br />Function: Loss<br />Lambda: 2.5","b:  1.18<br />value: 0.5324<br />Function: Loss<br />Lambda: 2.5","b:  1.19<br />value: 0.5361<br />Function: Loss<br />Lambda: 2.5","b:  1.20<br />value: 0.5400<br />Function: Loss<br />Lambda: 2.5","b:  1.21<br />value: 0.5441<br />Function: Loss<br />Lambda: 2.5","b:  1.22<br />value: 0.5484<br />Function: Loss<br />Lambda: 2.5","b:  1.23<br />value: 0.5529<br />Function: Loss<br />Lambda: 2.5","b:  1.24<br />value: 0.5576<br />Function: Loss<br />Lambda: 2.5","b:  1.25<br />value: 0.5625<br />Function: Loss<br />Lambda: 2.5","b:  1.26<br />value: 0.5676<br />Function: Loss<br />Lambda: 2.5","b:  1.27<br />value: 0.5729<br />Function: Loss<br />Lambda: 2.5","b:  1.28<br />value: 0.5784<br />Function: Loss<br />Lambda: 2.5","b:  1.29<br />value: 0.5841<br />Function: Loss<br />Lambda: 2.5","b:  1.30<br />value: 0.5900<br />Function: Loss<br />Lambda: 2.5","b:  1.31<br />value: 0.5961<br />Function: Loss<br />Lambda: 2.5","b:  1.32<br />value: 0.6024<br />Function: Loss<br />Lambda: 2.5","b:  1.33<br />value: 0.6089<br />Function: Loss<br />Lambda: 2.5","b:  1.34<br />value: 0.6156<br />Function: Loss<br />Lambda: 2.5","b:  1.35<br />value: 0.6225<br />Function: Loss<br />Lambda: 2.5","b:  1.36<br />value: 0.6296<br />Function: Loss<br />Lambda: 2.5","b:  1.37<br />value: 0.6369<br />Function: Loss<br />Lambda: 2.5","b:  1.38<br />value: 0.6444<br />Function: Loss<br />Lambda: 2.5","b:  1.39<br />value: 0.6521<br />Function: Loss<br />Lambda: 2.5","b:  1.40<br />value: 0.6600<br />Function: Loss<br />Lambda: 2.5","b:  1.41<br />value: 0.6681<br />Function: Loss<br />Lambda: 2.5","b:  1.42<br />value: 0.6764<br />Function: Loss<br />Lambda: 2.5","b:  1.43<br />value: 0.6849<br />Function: Loss<br />Lambda: 2.5","b:  1.44<br />value: 0.6936<br />Function: Loss<br />Lambda: 2.5","b:  1.45<br />value: 0.7025<br />Function: Loss<br />Lambda: 2.5","b:  1.46<br />value: 0.7116<br />Function: Loss<br />Lambda: 2.5","b:  1.47<br />value: 0.7209<br />Function: Loss<br />Lambda: 2.5","b:  1.48<br />value: 0.7304<br />Function: Loss<br />Lambda: 2.5","b:  1.49<br />value: 0.7401<br />Function: Loss<br />Lambda: 2.5","b:  1.50<br />value: 0.7500<br />Function: Loss<br />Lambda: 2.5","b:  1.51<br />value: 0.7601<br />Function: Loss<br />Lambda: 2.5","b:  1.52<br />value: 0.7704<br />Function: Loss<br />Lambda: 2.5","b:  1.53<br />value: 0.7809<br />Function: Loss<br />Lambda: 2.5","b:  1.54<br />value: 0.7916<br />Function: Loss<br />Lambda: 2.5","b:  1.55<br />value: 0.8025<br />Function: Loss<br />Lambda: 2.5","b:  1.56<br />value: 0.8136<br />Function: Loss<br />Lambda: 2.5","b:  1.57<br />value: 0.8249<br />Function: Loss<br />Lambda: 2.5","b:  1.58<br />value: 0.8364<br />Function: Loss<br />Lambda: 2.5","b:  1.59<br />value: 0.8481<br />Function: Loss<br />Lambda: 2.5","b:  1.60<br />value: 0.8600<br />Function: Loss<br />Lambda: 2.5","b:  1.61<br />value: 0.8721<br />Function: Loss<br />Lambda: 2.5","b:  1.62<br />value: 0.8844<br />Function: Loss<br />Lambda: 2.5","b:  1.63<br />value: 0.8969<br />Function: Loss<br />Lambda: 2.5","b:  1.64<br />value: 0.9096<br />Function: Loss<br />Lambda: 2.5","b:  1.65<br />value: 0.9225<br />Function: Loss<br />Lambda: 2.5","b:  1.66<br />value: 0.9356<br />Function: Loss<br />Lambda: 2.5","b:  1.67<br />value: 0.9489<br />Function: Loss<br />Lambda: 2.5","b:  1.68<br />value: 0.9624<br />Function: Loss<br />Lambda: 2.5","b:  1.69<br />value: 0.9761<br />Function: Loss<br />Lambda: 2.5","b:  1.70<br />value: 0.9900<br />Function: Loss<br />Lambda: 2.5","b:  1.71<br />value: 1.0041<br />Function: Loss<br />Lambda: 2.5","b:  1.72<br />value: 1.0184<br />Function: Loss<br />Lambda: 2.5","b:  1.73<br />value: 1.0329<br />Function: Loss<br />Lambda: 2.5","b:  1.74<br />value: 1.0476<br />Function: Loss<br />Lambda: 2.5","b:  1.75<br />value: 1.0625<br />Function: Loss<br />Lambda: 2.5","b:  1.76<br />value: 1.0776<br />Function: Loss<br />Lambda: 2.5","b:  1.77<br />value: 1.0929<br />Function: Loss<br />Lambda: 2.5","b:  1.78<br />value: 1.1084<br />Function: Loss<br />Lambda: 2.5","b:  1.79<br />value: 1.1241<br />Function: Loss<br />Lambda: 2.5","b:  1.80<br />value: 1.1400<br />Function: Loss<br />Lambda: 2.5","b:  1.81<br />value: 1.1561<br />Function: Loss<br />Lambda: 2.5","b:  1.82<br />value: 1.1724<br />Function: Loss<br />Lambda: 2.5","b:  1.83<br />value: 1.1889<br />Function: Loss<br />Lambda: 2.5","b:  1.84<br />value: 1.2056<br />Function: Loss<br />Lambda: 2.5","b:  1.85<br />value: 1.2225<br />Function: Loss<br />Lambda: 2.5","b:  1.86<br />value: 1.2396<br />Function: Loss<br />Lambda: 2.5","b:  1.87<br />value: 1.2569<br />Function: Loss<br />Lambda: 2.5","b:  1.88<br />value: 1.2744<br />Function: Loss<br />Lambda: 2.5","b:  1.89<br />value: 1.2921<br />Function: Loss<br />Lambda: 2.5","b:  1.90<br />value: 1.3100<br />Function: Loss<br />Lambda: 2.5","b:  1.91<br />value: 1.3281<br />Function: Loss<br />Lambda: 2.5","b:  1.92<br />value: 1.3464<br />Function: Loss<br />Lambda: 2.5","b:  1.93<br />value: 1.3649<br />Function: Loss<br />Lambda: 2.5","b:  1.94<br />value: 1.3836<br />Function: Loss<br />Lambda: 2.5","b:  1.95<br />value: 1.4025<br />Function: Loss<br />Lambda: 2.5","b:  1.96<br />value: 1.4216<br />Function: Loss<br />Lambda: 2.5","b:  1.97<br />value: 1.4409<br />Function: Loss<br />Lambda: 2.5","b:  1.98<br />value: 1.4604<br />Function: Loss<br />Lambda: 2.5","b:  1.99<br />value: 1.4801<br />Function: Loss<br />Lambda: 2.5","b:  2.00<br />value: 1.5000<br />Function: Loss<br />Lambda: 2.5"],"frame":"2.5","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(255,140,0,1)","dash":"solid"},"hoveron":"points","name":"Loss","legendgroup":"Loss","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[4,3.9451,3.8904,3.8359,3.7816,3.7275,3.6736,3.6199,3.5664,3.5131,3.46,3.4071,3.3544,3.3019,3.2496,3.1975,3.1456,3.0939,3.0424,2.9911,2.94,2.8891,2.8384,2.7879,2.7376,2.6875,2.6376,2.5879,2.5384,2.4891,2.44,2.3911,2.3424,2.2939,2.2456,2.1975,2.1496,2.1019,2.0544,2.0071,1.96,1.9131,1.8664,1.8199,1.7736,1.7275,1.6816,1.6359,1.5904,1.5451,1.5,1.5051,1.5104,1.5159,1.5216,1.5275,1.5336,1.5399,1.5464,1.5531,1.56,1.5671,1.5744,1.5819,1.5896,1.5975,1.6056,1.6139,1.6224,1.6311,1.64,1.6491,1.6584,1.6679,1.6776,1.6875,1.6976,1.7079,1.7184,1.7291,1.74,1.7511,1.7624,1.7739,1.7856,1.7975,1.8096,1.8219,1.8344,1.8471,1.86,1.8731,1.8864,1.8999,1.9136,1.9275,1.9416,1.9559,1.9704,1.9851,2,2.0151,2.0304,2.0459,2.0616,2.0775,2.0936,2.1099,2.1264,2.1431,2.16,2.1771,2.1944,2.2119,2.2296,2.2475,2.2656,2.2839,2.3024,2.3211,2.34,2.3591,2.3784,2.3979,2.4176,2.4375,2.4576,2.4779,2.4984,2.5191,2.54,2.5611,2.5824,2.6039,2.6256,2.6475,2.6696,2.6919,2.7144,2.7371,2.76,2.7831,2.8064,2.8299,2.8536,2.8775,2.9016,2.9259,2.9504,2.9751,3,3.0251,3.0504,3.0759,3.1016,3.1275,3.1536,3.1799,3.2064,3.2331,3.26,3.2871,3.3144,3.3419,3.3696,3.3975,3.4256,3.4539,3.4824,3.5111,3.54,3.5691,3.5984,3.6279,3.6576,3.6875,3.7176,3.7479,3.7784,3.8091,3.84,3.8711,3.9024,3.9339,3.9656,3.9975,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 4.0000<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.49<br />value: 3.9451<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.48<br />value: 3.8904<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.47<br />value: 3.8359<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.46<br />value: 3.7816<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.45<br />value: 3.7275<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.44<br />value: 3.6736<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.43<br />value: 3.6199<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.42<br />value: 3.5664<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.41<br />value: 3.5131<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.40<br />value: 3.4600<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.39<br />value: 3.4071<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.38<br />value: 3.3544<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.37<br />value: 3.3019<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.36<br />value: 3.2496<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.35<br />value: 3.1975<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.34<br />value: 3.1456<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.33<br />value: 3.0939<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.32<br />value: 3.0424<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.31<br />value: 2.9911<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.30<br />value: 2.9400<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.29<br />value: 2.8891<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.28<br />value: 2.8384<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.27<br />value: 2.7879<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.26<br />value: 2.7376<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.25<br />value: 2.6875<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.24<br />value: 2.6376<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.23<br />value: 2.5879<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.22<br />value: 2.5384<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.21<br />value: 2.4891<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.20<br />value: 2.4400<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.19<br />value: 2.3911<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.18<br />value: 2.3424<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.17<br />value: 2.2939<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.16<br />value: 2.2456<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.15<br />value: 2.1975<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.14<br />value: 2.1496<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.13<br />value: 2.1019<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.12<br />value: 2.0544<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.11<br />value: 2.0071<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.10<br />value: 1.9600<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.09<br />value: 1.9131<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.08<br />value: 1.8664<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.07<br />value: 1.8199<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.06<br />value: 1.7736<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.05<br />value: 1.7275<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.04<br />value: 1.6816<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.03<br />value: 1.6359<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.02<br />value: 1.5904<br />Function: Loss + Penalty<br />Lambda: 2.5","b: -0.01<br />value: 1.5451<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.00<br />value: 1.5000<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.01<br />value: 1.5051<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.02<br />value: 1.5104<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.03<br />value: 1.5159<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.04<br />value: 1.5216<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.05<br />value: 1.5275<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.06<br />value: 1.5336<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.07<br />value: 1.5399<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.08<br />value: 1.5464<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.09<br />value: 1.5531<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.10<br />value: 1.5600<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.11<br />value: 1.5671<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.12<br />value: 1.5744<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.13<br />value: 1.5819<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.14<br />value: 1.5896<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.15<br />value: 1.5975<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.16<br />value: 1.6056<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.17<br />value: 1.6139<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.18<br />value: 1.6224<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.19<br />value: 1.6311<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.20<br />value: 1.6400<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.21<br />value: 1.6491<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.22<br />value: 1.6584<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.23<br />value: 1.6679<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.24<br />value: 1.6776<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.25<br />value: 1.6875<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.26<br />value: 1.6976<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.27<br />value: 1.7079<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.28<br />value: 1.7184<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.29<br />value: 1.7291<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.30<br />value: 1.7400<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.31<br />value: 1.7511<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.32<br />value: 1.7624<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.33<br />value: 1.7739<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.34<br />value: 1.7856<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.35<br />value: 1.7975<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.36<br />value: 1.8096<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.37<br />value: 1.8219<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.38<br />value: 1.8344<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.39<br />value: 1.8471<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.40<br />value: 1.8600<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.41<br />value: 1.8731<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.42<br />value: 1.8864<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.43<br />value: 1.8999<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.44<br />value: 1.9136<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.45<br />value: 1.9275<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.46<br />value: 1.9416<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.47<br />value: 1.9559<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.48<br />value: 1.9704<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.49<br />value: 1.9851<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.50<br />value: 2.0000<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.51<br />value: 2.0151<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.52<br />value: 2.0304<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.53<br />value: 2.0459<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.54<br />value: 2.0616<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.55<br />value: 2.0775<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.56<br />value: 2.0936<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.57<br />value: 2.1099<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.58<br />value: 2.1264<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.59<br />value: 2.1431<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.60<br />value: 2.1600<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.61<br />value: 2.1771<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.62<br />value: 2.1944<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.63<br />value: 2.2119<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.64<br />value: 2.2296<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.65<br />value: 2.2475<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.66<br />value: 2.2656<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.67<br />value: 2.2839<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.68<br />value: 2.3024<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.69<br />value: 2.3211<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.70<br />value: 2.3400<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.71<br />value: 2.3591<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.72<br />value: 2.3784<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.73<br />value: 2.3979<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.74<br />value: 2.4176<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.75<br />value: 2.4375<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.76<br />value: 2.4576<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.77<br />value: 2.4779<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.78<br />value: 2.4984<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.79<br />value: 2.5191<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.80<br />value: 2.5400<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.81<br />value: 2.5611<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.82<br />value: 2.5824<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.83<br />value: 2.6039<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.84<br />value: 2.6256<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.85<br />value: 2.6475<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.86<br />value: 2.6696<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.87<br />value: 2.6919<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.88<br />value: 2.7144<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.89<br />value: 2.7371<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.90<br />value: 2.7600<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.91<br />value: 2.7831<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.92<br />value: 2.8064<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.93<br />value: 2.8299<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.94<br />value: 2.8536<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.95<br />value: 2.8775<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.96<br />value: 2.9016<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.97<br />value: 2.9259<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.98<br />value: 2.9504<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  0.99<br />value: 2.9751<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.00<br />value: 3.0000<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.01<br />value: 3.0251<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.02<br />value: 3.0504<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.03<br />value: 3.0759<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.04<br />value: 3.1016<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.05<br />value: 3.1275<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.06<br />value: 3.1536<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.07<br />value: 3.1799<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.08<br />value: 3.2064<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.09<br />value: 3.2331<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.10<br />value: 3.2600<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.11<br />value: 3.2871<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.12<br />value: 3.3144<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.13<br />value: 3.3419<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.14<br />value: 3.3696<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.15<br />value: 3.3975<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.16<br />value: 3.4256<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.17<br />value: 3.4539<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.18<br />value: 3.4824<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.19<br />value: 3.5111<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.20<br />value: 3.5400<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.21<br />value: 3.5691<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.22<br />value: 3.5984<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.23<br />value: 3.6279<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.24<br />value: 3.6576<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.25<br />value: 3.6875<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.26<br />value: 3.7176<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.27<br />value: 3.7479<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.28<br />value: 3.7784<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.29<br />value: 3.8091<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.30<br />value: 3.8400<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.31<br />value: 3.8711<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.32<br />value: 3.9024<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.33<br />value: 3.9339<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.34<br />value: 3.9656<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.35<br />value: 3.9975<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.36<br />value: 4.0296<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.37<br />value: 4.0619<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.38<br />value: 4.0944<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.39<br />value: 4.1271<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.40<br />value: 4.1600<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.41<br />value: 4.1931<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.42<br />value: 4.2264<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.43<br />value: 4.2599<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.44<br />value: 4.2936<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.45<br />value: 4.3275<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.46<br />value: 4.3616<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.47<br />value: 4.3959<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.48<br />value: 4.4304<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.49<br />value: 4.4651<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.50<br />value: 4.5000<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.51<br />value: 4.5351<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.52<br />value: 4.5704<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.53<br />value: 4.6059<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.54<br />value: 4.6416<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.55<br />value: 4.6775<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.56<br />value: 4.7136<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.57<br />value: 4.7499<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.58<br />value: 4.7864<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.59<br />value: 4.8231<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.60<br />value: 4.8600<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.61<br />value: 4.8971<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.62<br />value: 4.9344<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.63<br />value: 4.9719<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.64<br />value: 5.0096<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.65<br />value: 5.0475<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.66<br />value: 5.0856<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.67<br />value: 5.1239<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.68<br />value: 5.1624<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.69<br />value: 5.2011<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.70<br />value: 5.2400<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.71<br />value: 5.2791<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.72<br />value: 5.3184<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.73<br />value: 5.3579<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.74<br />value: 5.3976<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.75<br />value: 5.4375<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.76<br />value: 5.4776<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.77<br />value: 5.5179<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.78<br />value: 5.5584<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.79<br />value: 5.5991<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.80<br />value: 5.6400<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.81<br />value: 5.6811<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.82<br />value: 5.7224<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.83<br />value: 5.7639<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.84<br />value: 5.8056<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.85<br />value: 5.8475<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.86<br />value: 5.8896<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.87<br />value: 5.9319<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.88<br />value: 5.9744<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.89<br />value: 6.0171<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.90<br />value: 6.0600<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.91<br />value: 6.1031<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.92<br />value: 6.1464<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.93<br />value: 6.1899<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.94<br />value: 6.2336<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.95<br />value: 6.2775<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.96<br />value: 6.3216<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.97<br />value: 6.3659<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.98<br />value: 6.4104<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  1.99<br />value: 6.4551<br />Function: Loss + Penalty<br />Lambda: 2.5","b:  2.00<br />value: 6.5000<br />Function: Loss + Penalty<br />Lambda: 2.5"],"frame":"2.5","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,0,0,1)","dash":"solid"},"hoveron":"points","name":"Loss + Penalty","legendgroup":"Loss + Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true},{"x":[-0.5,-0.49,-0.48,-0.47,-0.46,-0.45,-0.44,-0.43,-0.42,-0.41,-0.4,-0.39,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.3,-0.29,-0.28,-0.27,-0.26,-0.25,-0.24,-0.23,-0.22,-0.21,-0.2,-0.19,-0.18,-0.17,-0.16,-0.15,-0.14,-0.13,-0.12,-0.11,-0.1,-0.09,-0.08,-0.07,-0.06,-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05,0.0600000000000001,0.0700000000000001,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.36,0.37,0.38,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.66,0.67,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,1,1.01,1.02,1.03,1.04,1.05,1.06,1.07,1.08,1.09,1.1,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.3,1.31,1.32,1.33,1.34,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.43,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.51,1.52,1.53,1.54,1.55,1.56,1.57,1.58,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.67,1.68,1.69,1.7,1.71,1.72,1.73,1.74,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.82,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.91,1.92,1.93,1.94,1.95,1.96,1.97,1.98,1.99,2],"y":[1.25,1.225,1.2,1.175,1.15,1.125,1.1,1.075,1.05,1.025,1,0.975,0.95,0.925,0.9,0.875,0.85,0.825,0.8,0.775,0.75,0.725,0.7,0.675,0.65,0.625,0.6,0.575,0.55,0.525,0.5,0.475,0.45,0.425,0.4,0.375,0.35,0.325,0.3,0.275,0.25,0.225,0.2,0.175,0.15,0.125,0.1,0.0749999999999999,0.05,0.025,0,0.025,0.05,0.0750000000000001,0.1,0.125,0.15,0.175,0.2,0.225,0.25,0.275,0.3,0.325,0.35,0.375,0.4,0.425,0.45,0.475,0.5,0.525,0.55,0.575,0.6,0.625,0.65,0.675,0.7,0.725,0.75,0.775,0.8,0.825,0.85,0.875,0.9,0.925,0.95,0.975,1,1.025,1.05,1.075,1.1,1.125,1.15,1.175,1.2,1.225,1.25,1.275,1.3,1.325,1.35,1.375,1.4,1.425,1.45,1.475,1.5,1.525,1.55,1.575,1.6,1.625,1.65,1.675,1.7,1.725,1.75,1.775,1.8,1.825,1.85,1.875,1.9,1.925,1.95,1.975,2,2.025,2.05,2.075,2.1,2.125,2.15,2.175,2.2,2.225,2.25,2.275,2.3,2.325,2.35,2.375,2.4,2.425,2.45,2.475,2.5,2.525,2.55,2.575,2.6,2.625,2.65,2.675,2.7,2.725,2.75,2.775,2.8,2.825,2.85,2.875,2.9,2.925,2.95,2.975,3,3.025,3.05,3.075,3.1,3.125,3.15,3.175,3.2,3.225,3.25,3.275,3.3,3.325,3.35,3.375,3.4,3.425,3.45,3.475,3.5,3.525,3.55,3.575,3.6,3.625,3.65,3.675,3.7,3.725,3.75,3.775,3.8,3.825,3.85,3.875,3.9,3.925,3.95,3.975,4,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"text":["b: -0.50<br />value: 1.2500<br />Function: Penalty<br />Lambda: 2.5","b: -0.49<br />value: 1.2250<br />Function: Penalty<br />Lambda: 2.5","b: -0.48<br />value: 1.2000<br />Function: Penalty<br />Lambda: 2.5","b: -0.47<br />value: 1.1750<br />Function: Penalty<br />Lambda: 2.5","b: -0.46<br />value: 1.1500<br />Function: Penalty<br />Lambda: 2.5","b: -0.45<br />value: 1.1250<br />Function: Penalty<br />Lambda: 2.5","b: -0.44<br />value: 1.1000<br />Function: Penalty<br />Lambda: 2.5","b: -0.43<br />value: 1.0750<br />Function: Penalty<br />Lambda: 2.5","b: -0.42<br />value: 1.0500<br />Function: Penalty<br />Lambda: 2.5","b: -0.41<br />value: 1.0250<br />Function: Penalty<br />Lambda: 2.5","b: -0.40<br />value: 1.0000<br />Function: Penalty<br />Lambda: 2.5","b: -0.39<br />value: 0.9750<br />Function: Penalty<br />Lambda: 2.5","b: -0.38<br />value: 0.9500<br />Function: Penalty<br />Lambda: 2.5","b: -0.37<br />value: 0.9250<br />Function: Penalty<br />Lambda: 2.5","b: -0.36<br />value: 0.9000<br />Function: Penalty<br />Lambda: 2.5","b: -0.35<br />value: 0.8750<br />Function: Penalty<br />Lambda: 2.5","b: -0.34<br />value: 0.8500<br />Function: Penalty<br />Lambda: 2.5","b: -0.33<br />value: 0.8250<br />Function: Penalty<br />Lambda: 2.5","b: -0.32<br />value: 0.8000<br />Function: Penalty<br />Lambda: 2.5","b: -0.31<br />value: 0.7750<br />Function: Penalty<br />Lambda: 2.5","b: -0.30<br />value: 0.7500<br />Function: Penalty<br />Lambda: 2.5","b: -0.29<br />value: 0.7250<br />Function: Penalty<br />Lambda: 2.5","b: -0.28<br />value: 0.7000<br />Function: Penalty<br />Lambda: 2.5","b: -0.27<br />value: 0.6750<br />Function: Penalty<br />Lambda: 2.5","b: -0.26<br />value: 0.6500<br />Function: Penalty<br />Lambda: 2.5","b: -0.25<br />value: 0.6250<br />Function: Penalty<br />Lambda: 2.5","b: -0.24<br />value: 0.6000<br />Function: Penalty<br />Lambda: 2.5","b: -0.23<br />value: 0.5750<br />Function: Penalty<br />Lambda: 2.5","b: -0.22<br />value: 0.5500<br />Function: Penalty<br />Lambda: 2.5","b: -0.21<br />value: 0.5250<br />Function: Penalty<br />Lambda: 2.5","b: -0.20<br />value: 0.5000<br />Function: Penalty<br />Lambda: 2.5","b: -0.19<br />value: 0.4750<br />Function: Penalty<br />Lambda: 2.5","b: -0.18<br />value: 0.4500<br />Function: Penalty<br />Lambda: 2.5","b: -0.17<br />value: 0.4250<br />Function: Penalty<br />Lambda: 2.5","b: -0.16<br />value: 0.4000<br />Function: Penalty<br />Lambda: 2.5","b: -0.15<br />value: 0.3750<br />Function: Penalty<br />Lambda: 2.5","b: -0.14<br />value: 0.3500<br />Function: Penalty<br />Lambda: 2.5","b: -0.13<br />value: 0.3250<br />Function: Penalty<br />Lambda: 2.5","b: -0.12<br />value: 0.3000<br />Function: Penalty<br />Lambda: 2.5","b: -0.11<br />value: 0.2750<br />Function: Penalty<br />Lambda: 2.5","b: -0.10<br />value: 0.2500<br />Function: Penalty<br />Lambda: 2.5","b: -0.09<br />value: 0.2250<br />Function: Penalty<br />Lambda: 2.5","b: -0.08<br />value: 0.2000<br />Function: Penalty<br />Lambda: 2.5","b: -0.07<br />value: 0.1750<br />Function: Penalty<br />Lambda: 2.5","b: -0.06<br />value: 0.1500<br />Function: Penalty<br />Lambda: 2.5","b: -0.05<br />value: 0.1250<br />Function: Penalty<br />Lambda: 2.5","b: -0.04<br />value: 0.1000<br />Function: Penalty<br />Lambda: 2.5","b: -0.03<br />value: 0.0750<br />Function: Penalty<br />Lambda: 2.5","b: -0.02<br />value: 0.0500<br />Function: Penalty<br />Lambda: 2.5","b: -0.01<br />value: 0.0250<br />Function: Penalty<br />Lambda: 2.5","b:  0.00<br />value: 0.0000<br />Function: Penalty<br />Lambda: 2.5","b:  0.01<br />value: 0.0250<br />Function: Penalty<br />Lambda: 2.5","b:  0.02<br />value: 0.0500<br />Function: Penalty<br />Lambda: 2.5","b:  0.03<br />value: 0.0750<br />Function: Penalty<br />Lambda: 2.5","b:  0.04<br />value: 0.1000<br />Function: Penalty<br />Lambda: 2.5","b:  0.05<br />value: 0.1250<br />Function: Penalty<br />Lambda: 2.5","b:  0.06<br />value: 0.1500<br />Function: Penalty<br />Lambda: 2.5","b:  0.07<br />value: 0.1750<br />Function: Penalty<br />Lambda: 2.5","b:  0.08<br />value: 0.2000<br />Function: Penalty<br />Lambda: 2.5","b:  0.09<br />value: 0.2250<br />Function: Penalty<br />Lambda: 2.5","b:  0.10<br />value: 0.2500<br />Function: Penalty<br />Lambda: 2.5","b:  0.11<br />value: 0.2750<br />Function: Penalty<br />Lambda: 2.5","b:  0.12<br />value: 0.3000<br />Function: Penalty<br />Lambda: 2.5","b:  0.13<br />value: 0.3250<br />Function: Penalty<br />Lambda: 2.5","b:  0.14<br />value: 0.3500<br />Function: Penalty<br />Lambda: 2.5","b:  0.15<br />value: 0.3750<br />Function: Penalty<br />Lambda: 2.5","b:  0.16<br />value: 0.4000<br />Function: Penalty<br />Lambda: 2.5","b:  0.17<br />value: 0.4250<br />Function: Penalty<br />Lambda: 2.5","b:  0.18<br />value: 0.4500<br />Function: Penalty<br />Lambda: 2.5","b:  0.19<br />value: 0.4750<br />Function: Penalty<br />Lambda: 2.5","b:  0.20<br />value: 0.5000<br />Function: Penalty<br />Lambda: 2.5","b:  0.21<br />value: 0.5250<br />Function: Penalty<br />Lambda: 2.5","b:  0.22<br />value: 0.5500<br />Function: Penalty<br />Lambda: 2.5","b:  0.23<br />value: 0.5750<br />Function: Penalty<br />Lambda: 2.5","b:  0.24<br />value: 0.6000<br />Function: Penalty<br />Lambda: 2.5","b:  0.25<br />value: 0.6250<br />Function: Penalty<br />Lambda: 2.5","b:  0.26<br />value: 0.6500<br />Function: Penalty<br />Lambda: 2.5","b:  0.27<br />value: 0.6750<br />Function: Penalty<br />Lambda: 2.5","b:  0.28<br />value: 0.7000<br />Function: Penalty<br />Lambda: 2.5","b:  0.29<br />value: 0.7250<br />Function: Penalty<br />Lambda: 2.5","b:  0.30<br />value: 0.7500<br />Function: Penalty<br />Lambda: 2.5","b:  0.31<br />value: 0.7750<br />Function: Penalty<br />Lambda: 2.5","b:  0.32<br />value: 0.8000<br />Function: Penalty<br />Lambda: 2.5","b:  0.33<br />value: 0.8250<br />Function: Penalty<br />Lambda: 2.5","b:  0.34<br />value: 0.8500<br />Function: Penalty<br />Lambda: 2.5","b:  0.35<br />value: 0.8750<br />Function: Penalty<br />Lambda: 2.5","b:  0.36<br />value: 0.9000<br />Function: Penalty<br />Lambda: 2.5","b:  0.37<br />value: 0.9250<br />Function: Penalty<br />Lambda: 2.5","b:  0.38<br />value: 0.9500<br />Function: Penalty<br />Lambda: 2.5","b:  0.39<br />value: 0.9750<br />Function: Penalty<br />Lambda: 2.5","b:  0.40<br />value: 1.0000<br />Function: Penalty<br />Lambda: 2.5","b:  0.41<br />value: 1.0250<br />Function: Penalty<br />Lambda: 2.5","b:  0.42<br />value: 1.0500<br />Function: Penalty<br />Lambda: 2.5","b:  0.43<br />value: 1.0750<br />Function: Penalty<br />Lambda: 2.5","b:  0.44<br />value: 1.1000<br />Function: Penalty<br />Lambda: 2.5","b:  0.45<br />value: 1.1250<br />Function: Penalty<br />Lambda: 2.5","b:  0.46<br />value: 1.1500<br />Function: Penalty<br />Lambda: 2.5","b:  0.47<br />value: 1.1750<br />Function: Penalty<br />Lambda: 2.5","b:  0.48<br />value: 1.2000<br />Function: Penalty<br />Lambda: 2.5","b:  0.49<br />value: 1.2250<br />Function: Penalty<br />Lambda: 2.5","b:  0.50<br />value: 1.2500<br />Function: Penalty<br />Lambda: 2.5","b:  0.51<br />value: 1.2750<br />Function: Penalty<br />Lambda: 2.5","b:  0.52<br />value: 1.3000<br />Function: Penalty<br />Lambda: 2.5","b:  0.53<br />value: 1.3250<br />Function: Penalty<br />Lambda: 2.5","b:  0.54<br />value: 1.3500<br />Function: Penalty<br />Lambda: 2.5","b:  0.55<br />value: 1.3750<br />Function: Penalty<br />Lambda: 2.5","b:  0.56<br />value: 1.4000<br />Function: Penalty<br />Lambda: 2.5","b:  0.57<br />value: 1.4250<br />Function: Penalty<br />Lambda: 2.5","b:  0.58<br />value: 1.4500<br />Function: Penalty<br />Lambda: 2.5","b:  0.59<br />value: 1.4750<br />Function: Penalty<br />Lambda: 2.5","b:  0.60<br />value: 1.5000<br />Function: Penalty<br />Lambda: 2.5","b:  0.61<br />value: 1.5250<br />Function: Penalty<br />Lambda: 2.5","b:  0.62<br />value: 1.5500<br />Function: Penalty<br />Lambda: 2.5","b:  0.63<br />value: 1.5750<br />Function: Penalty<br />Lambda: 2.5","b:  0.64<br />value: 1.6000<br />Function: Penalty<br />Lambda: 2.5","b:  0.65<br />value: 1.6250<br />Function: Penalty<br />Lambda: 2.5","b:  0.66<br />value: 1.6500<br />Function: Penalty<br />Lambda: 2.5","b:  0.67<br />value: 1.6750<br />Function: Penalty<br />Lambda: 2.5","b:  0.68<br />value: 1.7000<br />Function: Penalty<br />Lambda: 2.5","b:  0.69<br />value: 1.7250<br />Function: Penalty<br />Lambda: 2.5","b:  0.70<br />value: 1.7500<br />Function: Penalty<br />Lambda: 2.5","b:  0.71<br />value: 1.7750<br />Function: Penalty<br />Lambda: 2.5","b:  0.72<br />value: 1.8000<br />Function: Penalty<br />Lambda: 2.5","b:  0.73<br />value: 1.8250<br />Function: Penalty<br />Lambda: 2.5","b:  0.74<br />value: 1.8500<br />Function: Penalty<br />Lambda: 2.5","b:  0.75<br />value: 1.8750<br />Function: Penalty<br />Lambda: 2.5","b:  0.76<br />value: 1.9000<br />Function: Penalty<br />Lambda: 2.5","b:  0.77<br />value: 1.9250<br />Function: Penalty<br />Lambda: 2.5","b:  0.78<br />value: 1.9500<br />Function: Penalty<br />Lambda: 2.5","b:  0.79<br />value: 1.9750<br />Function: Penalty<br />Lambda: 2.5","b:  0.80<br />value: 2.0000<br />Function: Penalty<br />Lambda: 2.5","b:  0.81<br />value: 2.0250<br />Function: Penalty<br />Lambda: 2.5","b:  0.82<br />value: 2.0500<br />Function: Penalty<br />Lambda: 2.5","b:  0.83<br />value: 2.0750<br />Function: Penalty<br />Lambda: 2.5","b:  0.84<br />value: 2.1000<br />Function: Penalty<br />Lambda: 2.5","b:  0.85<br />value: 2.1250<br />Function: Penalty<br />Lambda: 2.5","b:  0.86<br />value: 2.1500<br />Function: Penalty<br />Lambda: 2.5","b:  0.87<br />value: 2.1750<br />Function: Penalty<br />Lambda: 2.5","b:  0.88<br />value: 2.2000<br />Function: Penalty<br />Lambda: 2.5","b:  0.89<br />value: 2.2250<br />Function: Penalty<br />Lambda: 2.5","b:  0.90<br />value: 2.2500<br />Function: Penalty<br />Lambda: 2.5","b:  0.91<br />value: 2.2750<br />Function: Penalty<br />Lambda: 2.5","b:  0.92<br />value: 2.3000<br />Function: Penalty<br />Lambda: 2.5","b:  0.93<br />value: 2.3250<br />Function: Penalty<br />Lambda: 2.5","b:  0.94<br />value: 2.3500<br />Function: Penalty<br />Lambda: 2.5","b:  0.95<br />value: 2.3750<br />Function: Penalty<br />Lambda: 2.5","b:  0.96<br />value: 2.4000<br />Function: Penalty<br />Lambda: 2.5","b:  0.97<br />value: 2.4250<br />Function: Penalty<br />Lambda: 2.5","b:  0.98<br />value: 2.4500<br />Function: Penalty<br />Lambda: 2.5","b:  0.99<br />value: 2.4750<br />Function: Penalty<br />Lambda: 2.5","b:  1.00<br />value: 2.5000<br />Function: Penalty<br />Lambda: 2.5","b:  1.01<br />value: 2.5250<br />Function: Penalty<br />Lambda: 2.5","b:  1.02<br />value: 2.5500<br />Function: Penalty<br />Lambda: 2.5","b:  1.03<br />value: 2.5750<br />Function: Penalty<br />Lambda: 2.5","b:  1.04<br />value: 2.6000<br />Function: Penalty<br />Lambda: 2.5","b:  1.05<br />value: 2.6250<br />Function: Penalty<br />Lambda: 2.5","b:  1.06<br />value: 2.6500<br />Function: Penalty<br />Lambda: 2.5","b:  1.07<br />value: 2.6750<br />Function: Penalty<br />Lambda: 2.5","b:  1.08<br />value: 2.7000<br />Function: Penalty<br />Lambda: 2.5","b:  1.09<br />value: 2.7250<br />Function: Penalty<br />Lambda: 2.5","b:  1.10<br />value: 2.7500<br />Function: Penalty<br />Lambda: 2.5","b:  1.11<br />value: 2.7750<br />Function: Penalty<br />Lambda: 2.5","b:  1.12<br />value: 2.8000<br />Function: Penalty<br />Lambda: 2.5","b:  1.13<br />value: 2.8250<br />Function: Penalty<br />Lambda: 2.5","b:  1.14<br />value: 2.8500<br />Function: Penalty<br />Lambda: 2.5","b:  1.15<br />value: 2.8750<br />Function: Penalty<br />Lambda: 2.5","b:  1.16<br />value: 2.9000<br />Function: Penalty<br />Lambda: 2.5","b:  1.17<br />value: 2.9250<br />Function: Penalty<br />Lambda: 2.5","b:  1.18<br />value: 2.9500<br />Function: Penalty<br />Lambda: 2.5","b:  1.19<br />value: 2.9750<br />Function: Penalty<br />Lambda: 2.5","b:  1.20<br />value: 3.0000<br />Function: Penalty<br />Lambda: 2.5","b:  1.21<br />value: 3.0250<br />Function: Penalty<br />Lambda: 2.5","b:  1.22<br />value: 3.0500<br />Function: Penalty<br />Lambda: 2.5","b:  1.23<br />value: 3.0750<br />Function: Penalty<br />Lambda: 2.5","b:  1.24<br />value: 3.1000<br />Function: Penalty<br />Lambda: 2.5","b:  1.25<br />value: 3.1250<br />Function: Penalty<br />Lambda: 2.5","b:  1.26<br />value: 3.1500<br />Function: Penalty<br />Lambda: 2.5","b:  1.27<br />value: 3.1750<br />Function: Penalty<br />Lambda: 2.5","b:  1.28<br />value: 3.2000<br />Function: Penalty<br />Lambda: 2.5","b:  1.29<br />value: 3.2250<br />Function: Penalty<br />Lambda: 2.5","b:  1.30<br />value: 3.2500<br />Function: Penalty<br />Lambda: 2.5","b:  1.31<br />value: 3.2750<br />Function: Penalty<br />Lambda: 2.5","b:  1.32<br />value: 3.3000<br />Function: Penalty<br />Lambda: 2.5","b:  1.33<br />value: 3.3250<br />Function: Penalty<br />Lambda: 2.5","b:  1.34<br />value: 3.3500<br />Function: Penalty<br />Lambda: 2.5","b:  1.35<br />value: 3.3750<br />Function: Penalty<br />Lambda: 2.5","b:  1.36<br />value: 3.4000<br />Function: Penalty<br />Lambda: 2.5","b:  1.37<br />value: 3.4250<br />Function: Penalty<br />Lambda: 2.5","b:  1.38<br />value: 3.4500<br />Function: Penalty<br />Lambda: 2.5","b:  1.39<br />value: 3.4750<br />Function: Penalty<br />Lambda: 2.5","b:  1.40<br />value: 3.5000<br />Function: Penalty<br />Lambda: 2.5","b:  1.41<br />value: 3.5250<br />Function: Penalty<br />Lambda: 2.5","b:  1.42<br />value: 3.5500<br />Function: Penalty<br />Lambda: 2.5","b:  1.43<br />value: 3.5750<br />Function: Penalty<br />Lambda: 2.5","b:  1.44<br />value: 3.6000<br />Function: Penalty<br />Lambda: 2.5","b:  1.45<br />value: 3.6250<br />Function: Penalty<br />Lambda: 2.5","b:  1.46<br />value: 3.6500<br />Function: Penalty<br />Lambda: 2.5","b:  1.47<br />value: 3.6750<br />Function: Penalty<br />Lambda: 2.5","b:  1.48<br />value: 3.7000<br />Function: Penalty<br />Lambda: 2.5","b:  1.49<br />value: 3.7250<br />Function: Penalty<br />Lambda: 2.5","b:  1.50<br />value: 3.7500<br />Function: Penalty<br />Lambda: 2.5","b:  1.51<br />value: 3.7750<br />Function: Penalty<br />Lambda: 2.5","b:  1.52<br />value: 3.8000<br />Function: Penalty<br />Lambda: 2.5","b:  1.53<br />value: 3.8250<br />Function: Penalty<br />Lambda: 2.5","b:  1.54<br />value: 3.8500<br />Function: Penalty<br />Lambda: 2.5","b:  1.55<br />value: 3.8750<br />Function: Penalty<br />Lambda: 2.5","b:  1.56<br />value: 3.9000<br />Function: Penalty<br />Lambda: 2.5","b:  1.57<br />value: 3.9250<br />Function: Penalty<br />Lambda: 2.5","b:  1.58<br />value: 3.9500<br />Function: Penalty<br />Lambda: 2.5","b:  1.59<br />value: 3.9750<br />Function: Penalty<br />Lambda: 2.5","b:  1.60<br />value: 4.0000<br />Function: Penalty<br />Lambda: 2.5","b:  1.61<br />value: 4.0250<br />Function: Penalty<br />Lambda: 2.5","b:  1.62<br />value: 4.0500<br />Function: Penalty<br />Lambda: 2.5","b:  1.63<br />value: 4.0750<br />Function: Penalty<br />Lambda: 2.5","b:  1.64<br />value: 4.1000<br />Function: Penalty<br />Lambda: 2.5","b:  1.65<br />value: 4.1250<br />Function: Penalty<br />Lambda: 2.5","b:  1.66<br />value: 4.1500<br />Function: Penalty<br />Lambda: 2.5","b:  1.67<br />value: 4.1750<br />Function: Penalty<br />Lambda: 2.5","b:  1.68<br />value: 4.2000<br />Function: Penalty<br />Lambda: 2.5","b:  1.69<br />value: 4.2250<br />Function: Penalty<br />Lambda: 2.5","b:  1.70<br />value: 4.2500<br />Function: Penalty<br />Lambda: 2.5","b:  1.71<br />value: 4.2750<br />Function: Penalty<br />Lambda: 2.5","b:  1.72<br />value: 4.3000<br />Function: Penalty<br />Lambda: 2.5","b:  1.73<br />value: 4.3250<br />Function: Penalty<br />Lambda: 2.5","b:  1.74<br />value: 4.3500<br />Function: Penalty<br />Lambda: 2.5","b:  1.75<br />value: 4.3750<br />Function: Penalty<br />Lambda: 2.5","b:  1.76<br />value: 4.4000<br />Function: Penalty<br />Lambda: 2.5","b:  1.77<br />value: 4.4250<br />Function: Penalty<br />Lambda: 2.5","b:  1.78<br />value: 4.4500<br />Function: Penalty<br />Lambda: 2.5","b:  1.79<br />value: 4.4750<br />Function: Penalty<br />Lambda: 2.5","b:  1.80<br />value: 4.5000<br />Function: Penalty<br />Lambda: 2.5","b:  1.81<br />value: 4.5250<br />Function: Penalty<br />Lambda: 2.5","b:  1.82<br />value: 4.5500<br />Function: Penalty<br />Lambda: 2.5","b:  1.83<br />value: 4.5750<br />Function: Penalty<br />Lambda: 2.5","b:  1.84<br />value: 4.6000<br />Function: Penalty<br />Lambda: 2.5","b:  1.85<br />value: 4.6250<br />Function: Penalty<br />Lambda: 2.5","b:  1.86<br />value: 4.6500<br />Function: Penalty<br />Lambda: 2.5","b:  1.87<br />value: 4.6750<br />Function: Penalty<br />Lambda: 2.5","b:  1.88<br />value: 4.7000<br />Function: Penalty<br />Lambda: 2.5","b:  1.89<br />value: 4.7250<br />Function: Penalty<br />Lambda: 2.5","b:  1.90<br />value: 4.7500<br />Function: Penalty<br />Lambda: 2.5","b:  1.91<br />value: 4.7750<br />Function: Penalty<br />Lambda: 2.5","b:  1.92<br />value: 4.8000<br />Function: Penalty<br />Lambda: 2.5","b:  1.93<br />value: 4.8250<br />Function: Penalty<br />Lambda: 2.5","b:  1.94<br />value: 4.8500<br />Function: Penalty<br />Lambda: 2.5","b:  1.95<br />value: 4.8750<br />Function: Penalty<br />Lambda: 2.5","b:  1.96<br />value: 4.9000<br />Function: Penalty<br />Lambda: 2.5","b:  1.97<br />value: 4.9250<br />Function: Penalty<br />Lambda: 2.5","b:  1.98<br />value: 4.9500<br />Function: Penalty<br />Lambda: 2.5","b:  1.99<br />value: 4.9750<br />Function: Penalty<br />Lambda: 2.5","b:  2.00<br />value: 5.0000<br />Function: Penalty<br />Lambda: 2.5"],"frame":"2.5","type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(0,191,255,1)","dash":"solid"},"hoveron":"points","name":"Penalty","legendgroup":"Penalty","showlegend":true,"xaxis":"x","yaxis":"y","hoverinfo":"text","visible":true}],"traces":[0,1,2]}],"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
<div id="constrained-optimization-view" class="section level2 hasAnchor" number="7.2">
<h2 class="hasAnchor"><span class="header-section-number">7.2</span> Constrained Optimization View<a href="#constrained-optimization-view" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Of course in a multivariate case, this is much more complicated since one variable may affect the optimizer of another. A commonly used alternative interpretation of the Lasso problem is the constrained optimization formulation:</p>
<p><span class="math display">\[\begin{align}
\min_{\boldsymbol \beta} \,\,&amp; \lVert \mathbf{y}- \mathbf{X}\boldsymbol \beta\rVert^2\\
\text{subject to} \,\, &amp; \lVert\boldsymbol \beta\rVert_1 \leq t
\end{align}\]</span></p>
<p>We can see from the left penal of the following figure that, the Lasso penalty imposes a constraint with the rhombus, i.e., the solution has to stay within the shaded area. The objective function is shown with the contour, and once the contained area is sufficiently small, some <span class="math inline">\(\beta\)</span> parameter will be shrunk to exactly zero. On the other hand, the Ridge regression also has a similar interpretation. However, since the constrained areas is a circle, it will never for the estimated parameters to be zero.</p>
<center>
<img src="images/lassoridge2.png" style="width:100.0%" alt="From online sources" /> Figure from online sources.
</center>
</div>
<div id="the-solution-path" class="section level2 hasAnchor" number="7.3">
<h2 class="hasAnchor"><span class="header-section-number">7.3</span> The Solution Path<a href="#the-solution-path" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We are interested in getting the fitted model with a given <span class="math inline">\(\lambda\)</span> value, however, for selecting the tuning parameter, it would be much more stable to obtain the solution on a sequence of <span class="math inline">\(\lambda\)</span> values. The corresponding <span class="math inline">\(\boldsymbol \beta\)</span> parameter estimates are called the solution path, i.e., the path how parameter changes as <span class="math inline">\(\lambda\)</span> changes. We have seen an example of this with the Ridge regression. For Lasso, the the solution path has an interpretation as the <strong>forward-stagewise</strong> regression. This is different than the forward stepwise model we introduced before. A forward stagewise regression works in the following way:</p>
<ul>
<li>Start with the Null model (intercept) and choose the best variable out of all <span class="math inline">\(p\)</span>, such that when its parameter grows by a small magnitude <span class="math inline">\(\epsilon\)</span> (either positive or negative), the RSS reduces the most. Grow the parameter estimate of this variable by <span class="math inline">\(\epsilon\)</span> and repeat.</li>
</ul>
<p>The stage-wise regression solution has been shown to give the same solution path as the Lasso, if we start with a sufficiently large <span class="math inline">\(\lambda\)</span>, and gradually reduces it towards zero. This can be done with the least angle regression (<code>lars</code>) package. Note that the <code>lars</code> package introduces another computationally more efficient approach to obtain the same solution, but we will not discuss it in details. We comparing the two approaches (stagewise and stepwise) using the <code>prostate</code> data from the <code>ElemStatLearn</code> package.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(lars)</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data</span>(prostate)</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>  lars.fit <span class="ot">=</span> <span class="fu">lars</span>(<span class="at">x =</span> <span class="fu">data.matrix</span>(prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]), <span class="at">y =</span> prostate<span class="sc">$</span>lpsa, </span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>                  <span class="at">type =</span> <span class="st">&quot;forward.stagewise&quot;</span>)</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(lars.fit)</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>  lars.fit <span class="ot">=</span> <span class="fu">lars</span>(<span class="at">x =</span> <span class="fu">data.matrix</span>(prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]), <span class="at">y =</span> prostate<span class="sc">$</span>lpsa, </span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>                  <span class="at">type =</span> <span class="st">&quot;stepwise&quot;</span>)</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(lars.fit)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-108-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>At each vertical line, a new variable enters the model by growing its parameter out of zero. You can relate this to our previous animated graph where as <span class="math inline">\(\lambda\)</span> decreases, the parameter estimate eventually comes out of zero. However, they may change their grow rate as a new variable comes. This is due to the covariance structure.</p>
</div>
<div id="path-wise-coordinate-descent" class="section level2 hasAnchor" number="7.4">
<h2 class="hasAnchor"><span class="header-section-number">7.4</span> Path-wise Coordinate Descent<a href="#path-wise-coordinate-descent" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The coordinate descent algorithm <span class="citation">(<a href="#ref-friedman2010regularization" role="doc-biblioref">J. Friedman, Hastie, and Tibshirani 2010</a>)</span> is probably the most efficient way to solve the Lasso solution up to now. The idea shares similarities with the stage-wise regression. However, with some careful analysis, we can obtain coordinate updates exactly, instead of moving a small step size. And this is done on a decreasing grid of <span class="math inline">\(\lambda\)</span> values. A pseudo algorithm proceed in the following way:</p>
<ol style="list-style-type: decimal">
<li>Start with a <span class="math inline">\(\lambda\)</span> value sufficiently large such that all parameter estimates are zero.</li>
<li>Reduce <span class="math inline">\(\lambda\)</span> by a fraction, e.g., 0.05, and perform coordinate descent updates:
<ol style="list-style-type: lower-roman">
<li>For <span class="math inline">\(j = 1, \ldots p\)</span>, update <span class="math inline">\(\beta_j\)</span> using a one-variable penalized formulation.</li>
<li>Repeat i) until convergence.</li>
</ol></li>
<li>Record the corresponding <span class="math inline">\(\widehat{\boldsymbol \beta}^\text{lasso}_\lambda\)</span>.</li>
<li>Repeat steps 2) and 3) until <span class="math inline">\(\lambda\)</span> is sufficiently small or there are already <span class="math inline">\(n\)</span> nonzero parameters entered into the model. Output <span class="math inline">\(\widehat{\boldsymbol \beta}^\text{lasso}_\lambda\)</span> for all <span class="math inline">\(\lambda\)</span> values.</li>
</ol>
<p>The crucial step is then figuring out the explicit formula of the coordinate update. Recall that in a coordinate descent algorithm of OLS at Section @ref(coordinate), we update <span class="math inline">\(\beta_j\)</span> using</p>
<p><span class="math display">\[
\underset{\boldsymbol \beta_j}{\text{argmin}} \,\, \frac{1}{n} ||\mathbf{y}- X_j \beta_j - \mathbf{X}_{(-j)} \boldsymbol \beta_{(-j)} ||^2
\]</span>
Since this is a one-variable OLS problem, the solution is</p>
<p><span class="math display">\[
\beta_j = \frac{X_j^T \mathbf{r}}{X_j^T X_j}
\]</span></p>
<p>with <span class="math inline">\(\mathbf{r}= \mathbf{y}- \mathbf{X}_{(-j)} \boldsymbol \beta_{(-j)}\)</span>. Now, adding the penalty <span class="math inline">\(|\beta_j|\)</span>, we essentially reduces back to the previous example of the single variable lasso problem, where we have the OLS solution. Hence, all we need to do is to apply the soft-thresholding function. The the Lasso coordinate update becomes</p>
<p><span class="math display">\[\beta_j^\text{new} = \text{SoftTH}\left(\frac{X_j^T \mathbf{r}}{X_j^T X_j}, \lambda\right) \]</span>
Incorporate this into the previous algorithm, we can obtain the entire solution path of a Lasso problem. This algorithm is implemented in the <code>glmnet</code> package. We will show an example of it.</p>
</div>
<div id="using-the-glmnet-package" class="section level2 hasAnchor" number="7.5">
<h2 class="hasAnchor"><span class="header-section-number">7.5</span> Using the <code>glmnet</code> package<a href="#using-the-glmnet-package" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We still use the prostate cancer data <code>prostate</code> data. The dataset contains 8 explanatory variables and one outcome <code>lpsa</code>, the log prostate-specific antigen value. We fit the model using the <code>glmnet</code> package. The tuning parameter can be selected using cross-validation with the <code>cv.glmnet</code> function. You can specify <code>nfolds</code> for the number of folds in the cross-validation. The default is 10. For Lasso, we should use <code>alpha = 1</code>, while <code>alpha = 0</code> is for Ridge. However, it is the default value that you do not need to specify.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(glmnet)</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">3</span>)</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>    fit2 <span class="ot">=</span> <span class="fu">cv.glmnet</span>(<span class="fu">data.matrix</span>(prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]), prostate<span class="sc">$</span>lpsa, <span class="at">nfolds =</span> <span class="dv">10</span>, <span class="at">alpha =</span> <span class="dv">1</span>)</span></code></pre></div>
<p>The left plot demonstrates how <span class="math inline">\(\lambda\)</span> changes the cross-validation error. There are two vertical lines, which represents <code>lambda.min</code> and <code>lambda.1se</code> respectively. The right plot shows how <span class="math inline">\(\lambda\)</span> changes the parameter values, with each line representing a variable. The x-axis in the figure is in terms of <span class="math inline">\(\log(\lambda)\)</span>, hence their is a larger penalty to the right. Note that the <code>glmnet</code> package uses <span class="math inline">\(1/(2n)\)</span> in the loss function instead of <span class="math inline">\(1/n\)</span>, hence the corresponding soft-thresholding function would reduce the magnitude of <span class="math inline">\(\lambda\)</span> by <span class="math inline">\(\lambda\)</span> instead of half of it. Moreover, the package will perform scaling before the model fitting, which essentially changes the corresponding one-variable OLS solution. The solution on the original scale will be retrieved once the entire solution path is finished. However, we usually do not need to worry about these computationally issues in practice. The main advantage of Lasso is shown here that the model can be sparse, with some parameter estimates shrunk to exactly 0.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(fit2)</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(fit2<span class="sc">$</span>glmnet.fit, <span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-110-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>We can obtain the estimated coefficients from the best <span class="math inline">\(\lambda\)</span> value. Similar to the ridge regression example, there are two popular options, <code>lambda.min</code> and <code>lambda.1se</code>. The first one is the value that minimizes the cross-validation error, the second one is slightly more conservative, which gives larger penalty value with more shrinkage. You can notice that <code>lambda.min</code> contains more nonzero parameters.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">coef</span>(fit2, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>)</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot;</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="do">##                        s1</span></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  0.1537694867</span></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a><span class="do">## lcavol       0.5071477800</span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a><span class="do">## lweight      0.5455934491</span></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a><span class="do">## age         -0.0084065349</span></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a><span class="do">## lbph         0.0618168146</span></span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a><span class="do">## svi          0.5899942923</span></span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a><span class="do">## lcp          .           </span></span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a><span class="do">## gleason      0.0009732887</span></span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a><span class="do">## pgg45        0.0023140828</span></span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">coef</span>(fit2, <span class="at">s =</span> <span class="st">&quot;lambda.1se&quot;</span>)</span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot;</span></span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a><span class="do">##                    s1</span></span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 0.6435469</span></span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a><span class="do">## lcavol      0.4553889</span></span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a><span class="do">## lweight     0.3142829</span></span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a><span class="do">## age         .        </span></span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a><span class="do">## lbph        .        </span></span>
<span id="cb89-21"><a href="#cb89-21" aria-hidden="true" tabindex="-1"></a><span class="do">## svi         0.3674270</span></span>
<span id="cb89-22"><a href="#cb89-22" aria-hidden="true" tabindex="-1"></a><span class="do">## lcp         .        </span></span>
<span id="cb89-23"><a href="#cb89-23" aria-hidden="true" tabindex="-1"></a><span class="do">## gleason     .        </span></span>
<span id="cb89-24"><a href="#cb89-24" aria-hidden="true" tabindex="-1"></a><span class="do">## pgg45       .</span></span></code></pre></div>
<p>Prediction can be done using the <code>predict()</code> function.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">=</span> <span class="fu">predict</span>(fit2, <span class="fu">data.matrix</span>(prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]), <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>)</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># training error</span></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mean</span>((pred <span class="sc">-</span> prostate<span class="sc">$</span>lpsa)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.4594359</span></span></code></pre></div>
</div>
<div id="elastic-net" class="section level2 hasAnchor" number="7.6">
<h2 class="hasAnchor"><span class="header-section-number">7.6</span> Elastic-Net<a href="#elastic-net" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lasso may suffer in the case where two variables are strongly correlated. The situation is similar to OLS, however, in Lasso, it would only select one out of the two, instead of letting both parameter estimates to be large. This is not preferred in some practical situations such as genetic studies because expressions of genes from the same pathway may have large correlation, but biologist want to identify all of them instead of just one. The Ridge penalty may help in this case because it naturally considers the correlation structure. Hence the <strong>Elastic-Net</strong> <span class="citation">(<a href="#ref-zou2005regularization" role="doc-biblioref">Zou and Hastie 2005</a>)</span> penalty has been proposed to address this issue: the data contains many correlated variables and we want to select them together if they are important for prediction. The <code>glmnet</code> package uses the following definition of an Elastic-Net penalty, which is a mixture of <span class="math inline">\(\ell_1\)</span> and <span class="math inline">\(\ell_2\)</span> penalties:</p>
<p><span class="math display">\[\lambda \left[ (1 - \alpha)/2 \lVert \boldsymbol \beta\rVert_2^2 + \alpha |\boldsymbol \beta|_1 \right],\]</span>
which involves two tuning parameters. However, in practice, it is very common to simply use <span class="math inline">\(\alpha = 0.5\)</span>.</p>
<!--chapter:end:02.3-lasso.Rmd-->
</div>
</div>
<div id="part-nonparametric-models" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART) Nonparametric Models<a href="#part-nonparametric-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="spline" class="section level1 hasAnchor" number="8">
<h1 class="hasAnchor"><span class="header-section-number">8</span> Spline<a href="#spline" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="from-linear-to-nonlinear" class="section level2 hasAnchor" number="8.1">
<h2 class="hasAnchor"><span class="header-section-number">8.1</span> From Linear to Nonlinear<a href="#from-linear-to-nonlinear" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In previous chapters, we mainly focused linear models. Modeling nonlinear trends can still be done with linear model by introducing higher-order terms, or nonlinear transformations. For example, <span class="math inline">\(x^2\)</span>, <span class="math inline">\(\log(x)\)</span> are all very commonly used approaches to model nonlinear effects. There is another class of approaches that is more flexible with nice theoretical properties, the splines. In this chapter, we mainly focus on univariate regression problems.</p>
</div>
<div id="a-motivating-example-and-polynomials" class="section level2 hasAnchor" number="8.2">
<h2 class="hasAnchor"><span class="header-section-number">8.2</span> A Motivating Example and Polynomials<a href="#a-motivating-example-and-polynomials" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use the U.S. birth rate data as an example. The data records birth rates from 1917 to 2003. The birth rate trend is obviously very nonlinear.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>    birthrates<span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/birthrate2.csv&quot;</span>, <span class="at">row.names =</span> <span class="dv">1</span>)</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">head</span>(birthrates)</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="do">##   Year Birthrate</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 1917     183.1</span></span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 1918     183.9</span></span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 3 1919     163.1</span></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 1920     179.5</span></span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 5 1921     181.4</span></span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 6 1922     173.4</span></span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(birthrates, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-115-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>It might be interesting to fit a linear regression with high order polynomials to approximate this curve. This can be carried out using the <code>poly()</code> function, which calculates all polynomials up to a certain power. Please note that this is a more stable method compared with writing out the powers such as <code>I(Year^2)</code>, <code>I(Year^3)</code> etc because the <code>Year</code> variable is very large, and is numerically unstable.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>    lmfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Birthrate <span class="sc">~</span> <span class="fu">poly</span>(Year, <span class="dv">3</span>), <span class="at">data =</span> birthrates)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(birthrates, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(birthrates<span class="sc">$</span>Year, lmfit<span class="sc">$</span>fitted.values, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;degree = 3&quot;</span>)</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>    lmfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Birthrate <span class="sc">~</span> <span class="fu">poly</span>(Year, <span class="dv">5</span>), <span class="at">data =</span> birthrates)</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(birthrates, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(birthrates<span class="sc">$</span>Year, lmfit<span class="sc">$</span>fitted.values, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;degree = 5&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-116-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>These fittings do not seem to perform very well. How about we take a different approach to model the curve locally. Well, we know there is an approach that works in a similar way – <span class="math inline">\(k\)</span>NN. But we will try something new. Let’s first divide the year range into several non-overlapping intervals, say, every 10 years. Then we will estimate the regression coefficients within each interval by averaging the observations, just like <span class="math inline">\(k\)</span>NN. The only difference is that for prediction, we do not recalculate the neighbors anymore, just check the intervals.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>    mybasis <span class="ot">=</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="fu">nrow</span>(birthrates), <span class="dv">8</span>)</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>)</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>        mybasis[, l] <span class="ot">=</span> birthrates<span class="sc">$</span>Year<span class="sc">*</span>(birthrates<span class="sc">$</span>Year <span class="sc">&gt;=</span> <span class="dv">1917</span> <span class="sc">+</span> l<span class="sc">*</span><span class="dv">10</span>)</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>    lmfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthrates<span class="sc">$</span>Birthrate <span class="sc">~</span> ., <span class="at">data =</span> <span class="fu">data.frame</span>(mybasis))</span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(birthrates, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(birthrates<span class="sc">$</span>Year, lmfit<span class="sc">$</span>fitted.values, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Histgram Regression&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-117-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>The method is called a histogram regression. Suppose the interval that contains a given testing point <span class="math inline">\(x\)</span> is <span class="math inline">\(\phi(x)\)</span>, then, we are fitting a model with</p>
<p><span class="math display">\[\widehat{f}(x) = \frac{\sum_{i=1}^n Y_i \,\, I\{X_i \in \phi(x)\} }{ \sum_{i=1}^n I\{X_i \in \phi(x)\}}\]</span></p>
<p>You may know the word histogram from the plotting the density of a set of observations. Yes, these two are actually motivated by the same philosophy. We will discuss the connection later on. For the purpose of fitting a regression function, the histogram regression does not seem to perform ideally since there will be jumps at the edge of an interval. Hence we need a more flexible framework.</p>
</div>
<div id="piecewise-polynomials" class="section level2 hasAnchor" number="8.3">
<h2 class="hasAnchor"><span class="header-section-number">8.3</span> Piecewise Polynomials<a href="#piecewise-polynomials" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Instead of fitting constant functions within each interval (between two knots), we may consider fitting a line. Consider a simpler case, where we just use 3 knots at 1938, 1960, 1978, which gives 4 intervals.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>    myknots <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1936</span>, <span class="dv">1960</span>, <span class="dv">1978</span>)</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>    bounds <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1917</span>, myknots, <span class="dv">2003</span>)  </span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># piecewise constant</span></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>    mybasis <span class="ot">=</span> <span class="fu">cbind</span>(<span class="st">&quot;x_1&quot;</span> <span class="ot">=</span> (birthrates<span class="sc">$</span>Year <span class="sc">&lt;</span> myknots[<span class="dv">1</span>]), </span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_2&quot;</span> <span class="ot">=</span> (birthrates<span class="sc">$</span>Year <span class="sc">&gt;=</span> myknots[<span class="dv">1</span>])<span class="sc">*</span>(birthrates<span class="sc">$</span>Year <span class="sc">&lt;</span> myknots[<span class="dv">2</span>]), </span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_3&quot;</span> <span class="ot">=</span> (birthrates<span class="sc">$</span>Year <span class="sc">&gt;=</span> myknots[<span class="dv">2</span>])<span class="sc">*</span>(birthrates<span class="sc">$</span>Year <span class="sc">&lt;</span> myknots[<span class="dv">3</span>]),</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_4&quot;</span> <span class="ot">=</span> (birthrates<span class="sc">$</span>Year <span class="sc">&gt;=</span> myknots[<span class="dv">3</span>]))</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>    lmfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthrates<span class="sc">$</span>Birthrate <span class="sc">~</span> . <span class="sc">-</span><span class="dv">1</span>, <span class="at">data =</span> <span class="fu">data.frame</span>(mybasis))</span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">0</span>))    </span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(birthrates, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">v =</span> myknots, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Piecewise constant&quot;</span>)</span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a>        <span class="fu">points</span>(<span class="fu">c</span>(bounds[k], bounds[k<span class="sc">+</span><span class="dv">1</span>]), <span class="fu">rep</span>(lmfit<span class="sc">$</span>coefficients[k], <span class="dv">2</span>), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, </span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>               <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># piecewise linear</span></span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a>    mybasis <span class="ot">=</span> <span class="fu">cbind</span>(<span class="st">&quot;x_1&quot;</span> <span class="ot">=</span> (birthrates<span class="sc">$</span>Year <span class="sc">&lt;</span> myknots[<span class="dv">1</span>]), </span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_2&quot;</span> <span class="ot">=</span> (birthrates<span class="sc">$</span>Year <span class="sc">&gt;=</span> myknots[<span class="dv">1</span>])<span class="sc">*</span>(birthrates<span class="sc">$</span>Year <span class="sc">&lt;</span> myknots[<span class="dv">2</span>]), </span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_3&quot;</span> <span class="ot">=</span> (birthrates<span class="sc">$</span>Year <span class="sc">&gt;=</span> myknots[<span class="dv">2</span>])<span class="sc">*</span>(birthrates<span class="sc">$</span>Year <span class="sc">&lt;</span> myknots[<span class="dv">3</span>]),</span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_4&quot;</span> <span class="ot">=</span> (birthrates<span class="sc">$</span>Year <span class="sc">&gt;=</span> myknots[<span class="dv">3</span>]),</span>
<span id="cb94-27"><a href="#cb94-27" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_11&quot;</span> <span class="ot">=</span> birthrates<span class="sc">$</span>Year<span class="sc">*</span>(birthrates<span class="sc">$</span>Year <span class="sc">&lt;</span> myknots[<span class="dv">1</span>]), </span>
<span id="cb94-28"><a href="#cb94-28" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_21&quot;</span> <span class="ot">=</span> birthrates<span class="sc">$</span>Year<span class="sc">*</span>(birthrates<span class="sc">$</span>Year <span class="sc">&gt;=</span> myknots[<span class="dv">1</span>])<span class="sc">*</span>(birthrates<span class="sc">$</span>Year <span class="sc">&lt;</span> myknots[<span class="dv">2</span>]), </span>
<span id="cb94-29"><a href="#cb94-29" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_31&quot;</span> <span class="ot">=</span> birthrates<span class="sc">$</span>Year<span class="sc">*</span>(birthrates<span class="sc">$</span>Year <span class="sc">&gt;=</span> myknots[<span class="dv">2</span>])<span class="sc">*</span>(birthrates<span class="sc">$</span>Year <span class="sc">&lt;</span> myknots[<span class="dv">3</span>]),</span>
<span id="cb94-30"><a href="#cb94-30" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_41&quot;</span> <span class="ot">=</span> birthrates<span class="sc">$</span>Year<span class="sc">*</span>(birthrates<span class="sc">$</span>Year <span class="sc">&gt;=</span> myknots[<span class="dv">3</span>]))</span>
<span id="cb94-31"><a href="#cb94-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb94-32"><a href="#cb94-32" aria-hidden="true" tabindex="-1"></a>    lmfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthrates<span class="sc">$</span>Birthrate <span class="sc">~</span> .<span class="sc">-</span><span class="dv">1</span>, <span class="at">data =</span> <span class="fu">data.frame</span>(mybasis))</span>
<span id="cb94-33"><a href="#cb94-33" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">0</span>))  </span>
<span id="cb94-34"><a href="#cb94-34" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(birthrates, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb94-35"><a href="#cb94-35" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">v =</span> myknots, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb94-36"><a href="#cb94-36" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Piecewise linear&quot;</span>)</span>
<span id="cb94-37"><a href="#cb94-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb94-38"><a href="#cb94-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb94-39"><a href="#cb94-39" aria-hidden="true" tabindex="-1"></a>        <span class="fu">points</span>(<span class="fu">c</span>(bounds[k], bounds[k<span class="sc">+</span><span class="dv">1</span>]), </span>
<span id="cb94-40"><a href="#cb94-40" aria-hidden="true" tabindex="-1"></a>               lmfit<span class="sc">$</span>coefficients[k] <span class="sc">+</span> <span class="fu">c</span>(bounds[k], bounds[k<span class="sc">+</span><span class="dv">1</span>])<span class="sc">*</span>lmfit<span class="sc">$</span>coefficients[k<span class="sc">+</span><span class="dv">4</span>], </span>
<span id="cb94-41"><a href="#cb94-41" aria-hidden="true" tabindex="-1"></a>               <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-118-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="splines" class="section level2 hasAnchor" number="8.4">
<h2 class="hasAnchor"><span class="header-section-number">8.4</span> Splines<a href="#splines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>However, these functions are not continuous. Hence we use a trick to construct continuous basis:</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>    pos <span class="ot">&lt;-</span> <span class="cf">function</span>(x) x<span class="sc">*</span>(x<span class="sc">&gt;</span><span class="dv">0</span>)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>    mybasis <span class="ot">=</span> <span class="fu">cbind</span>(<span class="st">&quot;int&quot;</span> <span class="ot">=</span> <span class="dv">1</span>, <span class="st">&quot;x_1&quot;</span> <span class="ot">=</span> birthrates<span class="sc">$</span>Year, </span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_2&quot;</span> <span class="ot">=</span> <span class="fu">pos</span>(birthrates<span class="sc">$</span>Year <span class="sc">-</span> myknots[<span class="dv">1</span>]), </span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_3&quot;</span> <span class="ot">=</span> <span class="fu">pos</span>(birthrates<span class="sc">$</span>Year <span class="sc">-</span> myknots[<span class="dv">2</span>]),</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>                    <span class="st">&quot;x_4&quot;</span> <span class="ot">=</span> <span class="fu">pos</span>(birthrates<span class="sc">$</span>Year <span class="sc">-</span> myknots[<span class="dv">3</span>]))</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">matplot</span>(birthrates<span class="sc">$</span>Year, mybasis[, <span class="sc">-</span><span class="dv">1</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lty =</span> <span class="dv">1</span>, </span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>            <span class="at">yaxt =</span> <span class="st">&#39;n&#39;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Spline Basis Functions&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-119-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>With this definition, any fitted model will be</p>
<ul>
<li>Continuous everywhere</li>
<li>Linear everywhere except the knots</li>
<li>Has a different slot for each region</li>
</ul>
<p>The resulted model is called a spline.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>    lmfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(birthrates<span class="sc">$</span>Birthrate <span class="sc">~</span> .<span class="sc">-</span><span class="dv">1</span>, <span class="at">data =</span> <span class="fu">data.frame</span>(mybasis))</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">0</span>))  </span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(birthrates, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(birthrates<span class="sc">$</span>Year, lmfit<span class="sc">$</span>fitted.values, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">v =</span> myknots, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Linear Spline&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-120-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Of course, writing this out explicitly is very tedious, hence we have the <code>bs</code> function in the <code>splines</code> package to help us.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>    lmfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Birthrate <span class="sc">~</span> splines<span class="sc">::</span><span class="fu">bs</span>(Year, <span class="at">degree =</span> <span class="dv">1</span>, <span class="at">knots =</span> myknots), <span class="at">data =</span> birthrates)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(birthrates, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(birthrates<span class="sc">$</span>Year, lmfit<span class="sc">$</span>fitted.values, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Linear spline with the bs() function&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-121-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>The next step is to increase the degree to account for more complicated functions. A few things we need to consider here:</p>
<ul>
<li>How many knots should be used</li>
<li>Where to place the knots</li>
<li>What is the degree of functions in each region</li>
</ul>
<p>For example, we consider this setting</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>    lmfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Birthrate <span class="sc">~</span> splines<span class="sc">::</span><span class="fu">bs</span>(Year, <span class="at">degree =</span> <span class="dv">3</span>, <span class="at">knots =</span> myknots), <span class="at">data =</span> birthrates)</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(birthrates, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(birthrates<span class="sc">$</span>Year, lmfit<span class="sc">$</span>fitted.values, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Cubic spline with 3 knots&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-122-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>All of them affects the performance. In particular, the number of knots and the number of degrees in each region will determine the total number of degrees of freedom. For simplicity, we can control that using the <code>df</code> parameter. We use a total of 6 parameters, chosen by the function automatically. However, this does not seems to perform better than the knots we implemented. The choice of knots can be crucial.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>    lmfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Birthrate <span class="sc">~</span> splines<span class="sc">::</span><span class="fu">bs</span>(Year, <span class="at">df =</span> <span class="dv">5</span>), <span class="at">data =</span> birthrates)</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(birthrates, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(birthrates<span class="sc">$</span>Year, lmfit<span class="sc">$</span>fitted.values, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Linear spline with 6 degrees of parameters&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-123-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="spline-basis" class="section level2 hasAnchor" number="8.5">
<h2 class="hasAnchor"><span class="header-section-number">8.5</span> Spline Basis<a href="#spline-basis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are different ways to construct spline basis. We used two techniques previously, the regression spline and basis spline (B-spline). The B-spline has slight more advantages computationally. Here is a comparision of B-spline with different degrees.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">3</span>)</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>        bs_d <span class="ot">=</span> splines2<span class="sc">::</span><span class="fu">bSpline</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="at">degree =</span> d, <span class="at">knots =</span> <span class="fu">seq</span>(<span class="dv">10</span>, <span class="dv">90</span>, <span class="dv">10</span>), <span class="at">intercept =</span> <span class="cn">TRUE</span>)</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>        <span class="fu">matplot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, bs_d, <span class="at">type =</span> <span class="fu">ifelse</span>(d <span class="sc">==</span> <span class="dv">0</span>, <span class="st">&quot;s&quot;</span>, <span class="st">&quot;l&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">ylab =</span> <span class="st">&quot;spline&quot;</span>, </span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">xaxt =</span> <span class="st">&#39;n&#39;</span>, <span class="at">yaxt =</span> <span class="st">&#39;n&#39;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.05</span>, <span class="fl">1.05</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">title</span>(<span class="fu">paste</span>(<span class="st">&quot;degree =&quot;</span>, d))</span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a>    }</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-124-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="natural-cubic-spline" class="section level2 hasAnchor" number="8.6">
<h2 class="hasAnchor"><span class="header-section-number">8.6</span> Natural Cubic Spline<a href="#natural-cubic-spline" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Extrapolations are generally dangerous because the functions could be extream outside the range of the observed data. In linear models fit by <code>bs()</code>, extrapolations outside the boundaries will trigger a warning.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(splines)</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>    fit.bs <span class="ot">=</span> <span class="fu">lm</span>(Birthrate <span class="sc">~</span> <span class="fu">bs</span>(Year, <span class="at">df=</span><span class="dv">6</span>), <span class="at">data=</span>birthrates)</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(birthrates<span class="sc">$</span>Year, birthrates<span class="sc">$</span>Birthrate, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">280</span>), <span class="at">pch =</span> <span class="dv">19</span>, </span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">1900</span>, <span class="dv">2020</span>), <span class="at">xlab =</span> <span class="st">&quot;year&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;rate&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)   </span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">seq</span>(<span class="dv">1900</span>, <span class="dv">2020</span>), <span class="fu">predict</span>(fit.bs, <span class="fu">data.frame</span>(<span class="st">&quot;Year&quot;</span><span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">1900</span>, <span class="dv">2020</span>))),</span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">col=</span><span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">3</span>)    </span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning in bs(Year, degree = 3L, knots = c(`25%` = 1938.5, `50%` = 1960, : some &#39;x&#39; values beyond</span></span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a><span class="do">## boundary knots may cause ill-conditioned bases</span></span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb101-11"><a href="#cb101-11" aria-hidden="true" tabindex="-1"></a>    fit.ns <span class="ot">=</span> <span class="fu">lm</span>(Birthrate <span class="sc">~</span> <span class="fu">ns</span>(Year, <span class="at">df=</span><span class="dv">6</span>), <span class="at">data=</span>birthrates)    </span>
<span id="cb101-12"><a href="#cb101-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">seq</span>(<span class="dv">1900</span>, <span class="dv">2020</span>), <span class="fu">predict</span>(fit.ns, <span class="fu">data.frame</span>(<span class="st">&quot;Year&quot;</span><span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">1900</span>, <span class="dv">2020</span>))), </span>
<span id="cb101-13"><a href="#cb101-13" aria-hidden="true" tabindex="-1"></a>          <span class="at">col=</span><span class="st">&quot;darkgreen&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb101-14"><a href="#cb101-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Cubic B-Spline&quot;</span>, <span class="st">&quot;Natural Cubic Spline&quot;</span>), </span>
<span id="cb101-15"><a href="#cb101-15" aria-hidden="true" tabindex="-1"></a>           <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkgreen&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">cex =</span> <span class="fl">1.2</span>)</span>
<span id="cb101-16"><a href="#cb101-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Birth rate extrapolation&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-125-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Hence this motivates us to consider setting additional constrains that forces the extrapolations to be come more regular. This is done by forcing the second and third derivatives to be 0 if beyond the two extreme knots.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>    ncs <span class="ot">=</span> <span class="fu">ns</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="at">df =</span> <span class="dv">6</span>, <span class="at">intercept =</span> <span class="cn">TRUE</span>)</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">matplot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, ncs, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">ylab =</span> <span class="st">&quot;spline&quot;</span>,</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">xaxt =</span> <span class="st">&#39;n&#39;</span>, <span class="at">yaxt =</span> <span class="st">&#39;n&#39;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Natural Cubic Spline&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-126-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="smoothing-spline" class="section level2 hasAnchor" number="8.7">
<h2 class="hasAnchor"><span class="header-section-number">8.7</span> Smoothing Spline<a href="#smoothing-spline" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The motivation is to trying to solve the knots selection issue. Instead, let’s start with a “horrible” idea, by putting knots at all each observed data point <span class="math inline">\((x_1, x_2, \ldots, x_n)\)</span>. Then, we can create <span class="math inline">\(n\)</span> natural cubic spline basis. However, we also know that this leads to over-fitting since there are too many parameters. Let’s utilize the ridge regression idea by adding some penalties. This leads to the objective function</p>
<p><span class="math display">\[\begin{equation}
\underset{\boldsymbol \beta}{\text{min}} \,\, \lVert \mathbf{y}- \mathbf{F}\boldsymbol \beta\rVert^2 + \lambda \boldsymbol \beta^\text{T}\Omega \boldsymbol \beta, (\#eq:penalized)
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{F}\)</span> is the matrix of all <span class="math inline">\(n\)</span> natural cubic spline basis, and <span class="math inline">\(\Omega\)</span> is some covariance matrix that takes care of some form of relationship among different basis, which we will define later, and <span class="math inline">\(\lambda\)</span> is similar to the ridge regression. The question is, will this type of regression problem provide a good solution with nice properties?</p>
<p>Let’s consider fitting a regression model by solving a regression function <span class="math inline">\(g(x)\)</span> with the following penalized criteria:</p>
<p><span class="math display">\[\begin{equation}
\frac{1}{n} \sum_{i=1}^n \big(y_i - g(x_i)\big)^2 + \lambda \int_a^b \big[g&#39;&#39;(x) \big]^2 dx. (\#eq:roughness)
\end{equation}\]</span></p>
<p>This is the sum of <span class="math inline">\(\ell_2\)</span> loss and a roughness penalty that enforce certain smoothness on <span class="math inline">\(g(x)\)</span>. And we shall show that the optimal <span class="math inline">\(g(x)\)</span> that minimize this objective function will take the ridge penalty form in @ref(eq:penalized) mentioned previously.</p>
<p>We will consider all absolutely continuous functions on <span class="math inline">\([a, b] = [\min(x_i), \max(x_i)]\)</span>, with finite roughness, i.e., <span class="math inline">\(\int_a^b \big[g&#39;&#39;(x) \big]^2 dx &lt; \infty\)</span>. This is known as the second order Sobolev space. Let’s first define <span class="math inline">\(g(\cdot)\)</span> as the optimal solution to Equation @ref(eq:roughness). Since the loss part in @ref(eq:roughness) only involves <span class="math inline">\(n\)</span> data points, we can find define a natural cubic spline (NCS) fit <span class="math inline">\(\tilde{g}(\cdot)\)</span> such that it matches with <span class="math inline">\(g(\cdot)\)</span> on all the observed data, i.e.,</p>
<p><span class="math display">\[g(x_i) = \widetilde{g}(x_i), \quad i = 1, \ldots, n.\]</span></p>
<p>Note that the roughness of NCS fit <span class="math inline">\(\widetilde{g}\)</span> is also finite, hence <span class="math inline">\(\widetilde{g}\)</span> is within the space of functions we are considering. Also, such matching on all observed data points is doable when we have <span class="math inline">\(n\)</span> basis in the natural cubic spline. In this case, the loss corresponds to <span class="math inline">\(\tilde{g}\)</span> and <span class="math inline">\(g\)</span> are identical. Hence, it only matters if the penalty part of <span class="math inline">\(\tilde{g}(\cdot)\)</span> is the same. To analyze this, we define the difference between these two functions as</p>
<p><span class="math display">\[h(x) = g(x) - \tilde{g}(x).\]</span>
It is then obvious that <span class="math inline">\(h(x_i) = 0\)</span> for all observed <span class="math inline">\(i\)</span>. Then we have</p>
<p><span class="math display">\[\int g&#39;&#39;^2 dx = \int \widetilde{g}&#39;&#39;^2 dx + \int h&#39;&#39;^2 dx + 2 \int \widetilde{g}&#39;&#39; h&#39;&#39; dx\]</span>
The first and second term on the right hand side are both non-negative. Hence, only the third term matters. WLOG, we assume that <span class="math inline">\(x_i\)</span>’s are ordered from the smallest to the largest. Then</p>
<p><span class="math display">\[\begin{align}
\int \tilde{g}&#39;&#39; h&#39;&#39; dx =&amp; ~\tilde{g}&#39;&#39; h&#39; \Big|_a^b - \int_a^b h&#39; \tilde{g}^{(3)} dx \nonumber \\
=&amp;~ 0 - \int_a^b h&#39; \tilde{g}^{(3)} dx \nonumber \\
=&amp;~ - \sum_{i=1}^{n-1} \tilde{g}^{(3)}(x_j^+) \int_{x_j}^{x_{j+1}} h&#39; dx \quad \nonumber \\
=&amp;~ - \sum_{i=1}^{n-1} \tilde{g}^{(3)}(x_j^+) \big(h(x_{j+1}) - h(x_j)\big) \nonumber \\
=&amp;~ 0
\end{align}\]</span></p>
<p>The second equation is because <span class="math inline">\(\tilde{g}\)</span> is a NCS and suppose to have 0 second derivative on the two boundaries <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The third equation is because <span class="math inline">\(\tilde{g}\)</span> is at most <span class="math inline">\(x^3\)</span> on any regions and have constant third derivatives, which we can pull out of the integration. And the last equation is because <span class="math inline">\(h(x) = 0\)</span> on all <span class="math inline">\(x_i\)</span>’s.</p>
<p>Hence, this shows that the roughness penalty <span class="math display">\[\int \widetilde{g}&#39;&#39;^2 dx\]</span> of our NCS solution is no larger than the best solution <span class="math inline">\(g\)</span>. Noticing that <span class="math inline">\(\widetilde{g}\)</span> is also with the space of functions we are considering, then <span class="math inline">\(g\)</span> must be our NCS solution.</p>
<p>Hence, <span class="math inline">\(g\)</span> has a finite sample representation</p>
<p><span class="math display">\[\widehat g(x) = \sum_{j=1}^n \beta_j N_j(x)\]</span></p>
<p>where <span class="math inline">\(N_j\)</span>’s are a set of natural cubic spline basis functions with knots at each of the unique <span class="math inline">\(x\)</span> values. Then Equation @ref(eq:roughness) becomes</p>
<p><span class="math display">\[\begin{align}
&amp; \lVert \mathbf{y}- \sum_{j=1}^n \beta_j N_j(x) \rVert^2 + \int \Big( \sum_{j=1}^n \beta_j N_j&#39;&#39;(x)\Big)^2 dx \\
=&amp; \lVert \mathbf{y}- \mathbf{F}\boldsymbol \beta\rVert^2 + \boldsymbol \beta^\text{T}\Omega \boldsymbol \beta,
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{F}\)</span> is the design matrix corresponds to the <span class="math inline">\(n\)</span> NCS basis <span class="math inline">\(N_j\)</span>’s, and <span class="math inline">\(\Omega\)</span> is an <span class="math inline">\(n \times n\)</span> matrix with <span class="math inline">\(\Omega_{ij} = \int N_i&#39;&#39;(x) N_j(x) dx.\)</span> The solution is essentially a ridge solution:</p>
<p><span class="math display">\[\widehat{\boldsymbol \beta} = (\mathbf{F}^\text{T}\mathbf{F}+ \lambda \Omega)^{-1} \mathbf{F}^\text{T}\mathbf{y}.\]</span>
and the penalty <span class="math inline">\(\lambda\)</span> can be tuned using GCV.</p>
</div>
<div id="fitting-smoothing-splines" class="section level2 hasAnchor" number="8.8">
<h2 class="hasAnchor"><span class="header-section-number">8.8</span> Fitting Smoothing Splines<a href="#fitting-smoothing-splines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Fitting a smoothing spline can be done by using the <code>smooth.spline</code> package. However, since the birthrate data has little variation in adjacent years, over-fitting is quite severe. The function will automatically use GCV to tune the parameter.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># smoothing spline </span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>    fit <span class="ot">=</span> <span class="fu">smooth.spline</span>(birthrates<span class="sc">$</span>Year, birthrates<span class="sc">$</span>Birthrate)</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(birthrates<span class="sc">$</span>Year, birthrates<span class="sc">$</span>Birthrate, <span class="at">pch =</span> <span class="dv">19</span>, </span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;Year&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;BirthRates&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">seq</span>(<span class="dv">1917</span>, <span class="dv">2003</span>), <span class="fu">predict</span>(fit, <span class="fu">seq</span>(<span class="dv">1917</span>, <span class="dv">2003</span>))<span class="sc">$</span>y, <span class="at">col=</span><span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-127-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the degrees of freedom is very large</span></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>    fit<span class="sc">$</span>df</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 60.7691</span></span></code></pre></div>
<p>Let’s look at another simulation example, where this method performs resonabaly well.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>    n <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> n)</span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">sin</span>(<span class="dv">12</span><span class="sc">*</span>(x<span class="fl">+0.2</span>))<span class="sc">/</span>(x<span class="fl">+0.2</span>) <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fit smoothing spline</span></span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>    fit <span class="ot">=</span> <span class="fu">smooth.spline</span>(x, y)</span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the degrees of freedom</span></span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>    fit<span class="sc">$</span>df    </span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 9.96443</span></span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fitted model</span></span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;y&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(x, <span class="fu">sin</span>(<span class="dv">12</span><span class="sc">*</span>(x<span class="fl">+0.2</span>))<span class="sc">/</span>(x<span class="fl">+0.2</span>), <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">3</span>)    </span>
<span id="cb105-16"><a href="#cb105-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(x, <span class="fu">predict</span>(fit, x)<span class="sc">$</span>y, <span class="at">col=</span><span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb105-17"><a href="#cb105-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Truth&quot;</span>, <span class="st">&quot;Smoothing Splines&quot;</span>), </span>
<span id="cb105-18"><a href="#cb105-18" aria-hidden="true" tabindex="-1"></a>           <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">cex =</span> <span class="fl">1.2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-128-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="extending-splines-to-multiple-varibles" class="section level2 hasAnchor" number="8.9">
<h2 class="hasAnchor"><span class="header-section-number">8.9</span> Extending Splines to Multiple Varibles<a href="#extending-splines-to-multiple-varibles" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Since all spline approaches can be transformed into some kind of linear model, if we postulate an additive structure, we can fit a multivariate model with</p>
<p><span class="math display">\[f(x) = \sum_j h_j(x_j) = \sum_j \sum_k N_{jk}(x_j) \beta_{jk}\]</span>
where <span class="math inline">\(h_j(x_j)\)</span> is a univariate function for <span class="math inline">\(x_j\)</span> that can be approximated by splines basis <span class="math inline">\(N_{jk}(\cdot), k = 1, \ldots, K\)</span>. This works for both linear regression and generalized linear regressions. For the South Africa Heart Disease data, we use the <code>gam()</code> function in the <code>gam</code> (generalized additive models) package. We compute a logistic regression model using natural splines (note <code>famhist</code> is included as a factor).</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(gam)</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: package &#39;gam&#39; was built under R version 4.2.2</span></span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Loading required package: foreach</span></span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Loaded gam 1.20.2</span></span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>    form <span class="ot">=</span> <span class="fu">formula</span>(<span class="st">&quot;chd ~ ns(sbp,df=4) + ns(tobacco,df=4) + </span></span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a><span class="st">                          ns(ldl,df=4) + famhist + ns(obesity,df=4) + </span></span>
<span id="cb106-9"><a href="#cb106-9" aria-hidden="true" tabindex="-1"></a><span class="st">                          ns(alcohol,df=4) + ns(age,df=4)&quot;</span>)</span>
<span id="cb106-10"><a href="#cb106-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb106-11"><a href="#cb106-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># note that we can also do </span></span>
<span id="cb106-12"><a href="#cb106-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># m = glm(form, data=SAheart, family=binomial)</span></span>
<span id="cb106-13"><a href="#cb106-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(summary(m), digits=3)</span></span>
<span id="cb106-14"><a href="#cb106-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># however, the gam function provides more information </span></span>
<span id="cb106-15"><a href="#cb106-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb106-16"><a href="#cb106-16" aria-hidden="true" tabindex="-1"></a>    m <span class="ot">=</span> <span class="fu">gam</span>(form, <span class="at">data=</span>SAheart, <span class="at">family=</span>binomial)</span>
<span id="cb106-17"><a href="#cb106-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summary</span>(m)</span>
<span id="cb106-18"><a href="#cb106-18" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb106-19"><a href="#cb106-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Call: gam(formula = form, family = binomial, data = SAheart)</span></span>
<span id="cb106-20"><a href="#cb106-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Deviance Residuals:</span></span>
<span id="cb106-21"><a href="#cb106-21" aria-hidden="true" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb106-22"><a href="#cb106-22" aria-hidden="true" tabindex="-1"></a><span class="do">## -1.7245 -0.8265 -0.3884  0.8870  2.9589 </span></span>
<span id="cb106-23"><a href="#cb106-23" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb106-24"><a href="#cb106-24" aria-hidden="true" tabindex="-1"></a><span class="do">## (Dispersion Parameter for binomial family taken to be 1)</span></span>
<span id="cb106-25"><a href="#cb106-25" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb106-26"><a href="#cb106-26" aria-hidden="true" tabindex="-1"></a><span class="do">##     Null Deviance: 596.1084 on 461 degrees of freedom</span></span>
<span id="cb106-27"><a href="#cb106-27" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual Deviance: 457.6318 on 436 degrees of freedom</span></span>
<span id="cb106-28"><a href="#cb106-28" aria-hidden="true" tabindex="-1"></a><span class="do">## AIC: 509.6318 </span></span>
<span id="cb106-29"><a href="#cb106-29" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb106-30"><a href="#cb106-30" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Local Scoring Iterations: 6 </span></span>
<span id="cb106-31"><a href="#cb106-31" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb106-32"><a href="#cb106-32" aria-hidden="true" tabindex="-1"></a><span class="do">## Anova for Parametric Effects</span></span>
<span id="cb106-33"><a href="#cb106-33" aria-hidden="true" tabindex="-1"></a><span class="do">##                      Df Sum Sq Mean Sq F value    Pr(&gt;F)    </span></span>
<span id="cb106-34"><a href="#cb106-34" aria-hidden="true" tabindex="-1"></a><span class="do">## ns(sbp, df = 4)       4   6.31  1.5783  1.4242  0.224956    </span></span>
<span id="cb106-35"><a href="#cb106-35" aria-hidden="true" tabindex="-1"></a><span class="do">## ns(tobacco, df = 4)   4  18.09  4.5218  4.0802  0.002941 ** </span></span>
<span id="cb106-36"><a href="#cb106-36" aria-hidden="true" tabindex="-1"></a><span class="do">## ns(ldl, df = 4)       4  12.05  3.0137  2.7194  0.029290 *  </span></span>
<span id="cb106-37"><a href="#cb106-37" aria-hidden="true" tabindex="-1"></a><span class="do">## famhist               1  19.70 19.7029 17.7788 3.019e-05 ***</span></span>
<span id="cb106-38"><a href="#cb106-38" aria-hidden="true" tabindex="-1"></a><span class="do">## ns(obesity, df = 4)   4   3.66  0.9161  0.8266  0.508701    </span></span>
<span id="cb106-39"><a href="#cb106-39" aria-hidden="true" tabindex="-1"></a><span class="do">## ns(alcohol, df = 4)   4   1.28  0.3200  0.2887  0.885278    </span></span>
<span id="cb106-40"><a href="#cb106-40" aria-hidden="true" tabindex="-1"></a><span class="do">## ns(age, df = 4)       4  17.64  4.4100  3.9794  0.003496 ** </span></span>
<span id="cb106-41"><a href="#cb106-41" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals           436 483.19  1.1082                      </span></span>
<span id="cb106-42"><a href="#cb106-42" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb106-43"><a href="#cb106-43" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb106-44"><a href="#cb106-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb106-45"><a href="#cb106-45" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">3</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="cb106-46"><a href="#cb106-46" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(m, <span class="at">se =</span> <span class="cn">TRUE</span>, <span class="at">residuals =</span> <span class="cn">TRUE</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-129-1.png" width="90%" style="display: block; margin: auto;" /></p>
<!--chapter:end:03.1-spline.Rmd-->
</div>
</div>
<div id="k-neariest-neighber" class="section level1 hasAnchor" number="9">
<h1 class="hasAnchor"><span class="header-section-number">9</span> K-Neariest Neighber<a href="#k-neariest-neighber" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><span class="math inline">\(K\)</span> nearest neighbor (KNN) is a simple nonparametric method. It can be used for both regression and classification problems. However, the idea is quite different from models we introduced before. In a linear model, we have a set of parameters <span class="math inline">\(\boldsymbol \beta\)</span> and our estimated function value, for any target point <span class="math inline">\(x_0\)</span> is <span class="math inline">\(x_0^\text{T}\boldsymbol \beta\)</span>. In KNN, we don’t really specify these parameters. Instead, we directed estimate <span class="math inline">\(f(x_0)\)</span>. This is traditionally called <strong>nonparametric models</strong> in statistics. Usually these models perform a local averaging technique to estimate <span class="math inline">\(f(x_0)\)</span> using observations close to <span class="math inline">\(x_0\)</span>.</p>
<div id="definition-1" class="section level2 hasAnchor" number="9.1">
<h2 class="hasAnchor"><span class="header-section-number">9.1</span> Definition<a href="#definition-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we collect a set of observations <span class="math inline">\(\{x_i, y_i\}_{i=1}^n\)</span>, the prediction at a new target point <span class="math inline">\(x_0\)</span> is</p>
<p><span class="math display">\[\widehat y = \frac{1}{k} \sum_{x_i \in N_k(x_0)} y_i,\]</span>
where <span class="math inline">\(N_k(x_0)\)</span> defines the <span class="math inline">\(k\)</span> samples from the training data that are closest to <span class="math inline">\(x_0\)</span>. As default, closeness is defined using a distance measure, such as the Euclidean distance. Here is a demonstration of the fitted regression function.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate training data with 2*sin(x) and random Gaussian errors</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)    </span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">15</span>, <span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x))</span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate testing data points where we evaluate the prediction function</span></span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>    test.x <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.001</span>)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi</span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># &quot;1-nearest neighbor&quot; regression using kknn package</span></span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(kknn)</span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a>    knn.fit <span class="ot">=</span> <span class="fu">kknn</span>(y <span class="sc">~</span> x, <span class="at">train =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), </span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a>                   <span class="at">test =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> test.x),</span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a>                   <span class="at">k =</span> <span class="dv">1</span>, <span class="at">kernel =</span> <span class="st">&quot;rectangular&quot;</span>)</span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a>    test.pred <span class="ot">=</span> knn.fit<span class="sc">$</span>fitted.values</span>
<span id="cb107-15"><a href="#cb107-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb107-16"><a href="#cb107-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot the data</span></span>
<span id="cb107-17"><a href="#cb107-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">rep</span>(<span class="dv">2</span>,<span class="dv">4</span>))</span>
<span id="cb107-18"><a href="#cb107-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x, y, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi), <span class="at">pch =</span> <span class="st">&quot;o&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>, </span>
<span id="cb107-19"><a href="#cb107-19" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb107-20"><a href="#cb107-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="at">main=</span><span class="st">&quot;1-Nearest Neighbor Regression&quot;</span>, <span class="at">cex.main =</span> <span class="fl">1.5</span>)</span>
<span id="cb107-21"><a href="#cb107-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb107-22"><a href="#cb107-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot the true regression line</span></span>
<span id="cb107-23"><a href="#cb107-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(test.x, <span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(test.x), <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb107-24"><a href="#cb107-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb107-25"><a href="#cb107-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot the fitted line</span></span>
<span id="cb107-26"><a href="#cb107-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(test.x, test.pred, <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb107-27"><a href="#cb107-27" aria-hidden="true" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Fitted line&quot;</span>, <span class="st">&quot;True function&quot;</span>), </span>
<span id="cb107-28"><a href="#cb107-28" aria-hidden="true" tabindex="-1"></a>           <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-132-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="tuning-k" class="section level2 hasAnchor" number="9.2">
<h2 class="hasAnchor"><span class="header-section-number">9.2</span> Tuning <span class="math inline">\(k\)</span><a href="#tuning-k" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Tuning <span class="math inline">\(k\)</span> is crucial for <span class="math inline">\(k\)</span>NN. Let’s observe its effect. This time, I generate 200 observations. For 1NN, the fitted regression line is very jumpy.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate more data</span></span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">200</span></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x))</span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a>  test.y <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(test.x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(test.x))</span>
<span id="cb108-8"><a href="#cb108-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb108-9"><a href="#cb108-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 1-nearest neighbor</span></span>
<span id="cb108-10"><a href="#cb108-10" aria-hidden="true" tabindex="-1"></a>  knn.fit <span class="ot">=</span> <span class="fu">kknn</span>(y <span class="sc">~</span> x, <span class="at">train =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), </span>
<span id="cb108-11"><a href="#cb108-11" aria-hidden="true" tabindex="-1"></a>                 <span class="at">test =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> test.x),</span>
<span id="cb108-12"><a href="#cb108-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">k =</span> <span class="dv">1</span>, <span class="at">kernel =</span> <span class="st">&quot;rectangular&quot;</span>)</span>
<span id="cb108-13"><a href="#cb108-13" aria-hidden="true" tabindex="-1"></a>  test.pred <span class="ot">=</span> knn.fit<span class="sc">$</span>fitted.values</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-134-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We can evaluate the prediction error of this model:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prediction error</span></span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>((test.pred <span class="sc">-</span> test.y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2.097488</span></span></code></pre></div>
<p>If we consider different values of <span class="math inline">\(k\)</span>, we can observe the trade-off between bias and variance.</p>
<pre><code>## Prediction Error for K = 1 : 2.09748780881932
## Prediction Error for K = 5 : 1.39071867992277
## Prediction Error for K = 10 : 1.24696415340282
## Prediction Error for K = 33 : 1.21589627474692
## Prediction Error for K = 66 : 1.37604707375972
## Prediction Error for K = 100 : 1.42868908518756</code></pre>
<p><img src="SMLR_files/figure-html/unnamed-chunk-136-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>As <span class="math inline">\(k\)</span> increases, we have a more stable model, i.e., smaller variance. However, the bias is also increased. As <span class="math inline">\(k\)</span> decreases, the bias decreases, but the model is less stable.</p>
</div>
<div id="the-bias-variance-trade-off" class="section level2 hasAnchor" number="9.3">
<h2 class="hasAnchor"><span class="header-section-number">9.3</span> The Bias-variance Trade-off<a href="#the-bias-variance-trade-off" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Formally, the prediction error (at a given target point <span class="math inline">\(x_0\)</span>) can be broke down into three parts: the <strong>irreducible error</strong>, the <strong>bias squared</strong>, and the <strong>variance</strong>.</p>
<span class="math display">\[\begin{aligned}
\text{E}\Big[ \big( Y - \widehat f(x_0) \big)^2 \Big] &amp;= \text{E}\Big[ \big( Y - f(x_0) + f(x_0) -  \text{E}[\widehat f(x_0)] + \text{E}[\widehat f(x_0)] - \widehat f(x_0) \big)^2 \Big] \\
&amp;= \text{E}\Big[ \big( Y - f(x_0) \big)^2 \Big] + \text{E}\Big[ \big(f(x_0) - \text{E}[\widehat f(x_0)] \big)^2 \Big] + \text{E}\Big[ \big(E[\widehat f(x_0)] - \widehat f(x_0) \big)^2 \Big] + \text{Cross Terms}\\
&amp;= \underbrace{\text{E}\Big[ ( Y - f(x_0))^2 \big]}_{\text{Irreducible Error}} +
\underbrace{\Big(f(x_0) - \text{E}[\widehat f(x_0)]\Big)^2}_{\text{Bias}^2} +
\underbrace{\text{E}\Big[ \big(\widehat f(x_0) - \text{E}[\widehat f(x_0)] \big)^2 \Big]}_{\text{Variance}}
\end{aligned}\]</span>
<p>As we can see from the previous example, when <span class="math inline">\(k=1\)</span>, the prediction error is about 2. This is because for all the testing points, the theoretical irreducible error is 1 (variance of the error term), the bias is almost 0 since the function is smooth, and the variance is the variance of 1 nearest neighbor, which is again 1. On the other extreme side, when <span class="math inline">\(k = n\)</span>, the variance should be in the level of <span class="math inline">\(1/n\)</span>, the bias is the difference between the sin function and the overall average. Overall, we can expect the trend:</p>
<ul>
<li>As <span class="math inline">\(k\)</span> increases, bias increases and variance decreases</li>
<li>As <span class="math inline">\(k\)</span> decreases, bias decreases and variance increases</li>
</ul>
</div>
<div id="knn-for-classification" class="section level2 hasAnchor" number="9.4">
<h2 class="hasAnchor"><span class="header-section-number">9.4</span> KNN for Classification<a href="#knn-for-classification" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For classification, kNN is different from the regression model in term of finding neighbers. The only difference is to majority voting instead of averaging. Majority voting means that we look for the most popular class label among its neighbors. For 1NN, it is simply the class of the closest neighbor. The visualization of 1NN is a Voronoi tessellation. The plot on the left is some randomly observed data in <span class="math inline">\([0, 1]^2\)</span>, and the plot on the right is the corresponding 1NN classification model.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-137-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="example-1-an-artificial-data" class="section level2 hasAnchor" number="9.5">
<h2 class="hasAnchor"><span class="header-section-number">9.5</span> Example 1: An artificial data<a href="#example-1-an-artificial-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use artificial data from the <code>ElemStatLearn</code> package.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> mixture.example<span class="sc">$</span>x</span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> mixture.example<span class="sc">$</span>y</span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a>  xnew <span class="ot">&lt;-</span> mixture.example<span class="sc">$</span>xnew</span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">rep</span>(<span class="dv">2</span>,<span class="dv">4</span>))</span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, <span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), </span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">axes =</span> <span class="cn">FALSE</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-139-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>The decision boundary is highly nonlinear. We can utilize the <code>contour()</code> function to demonstrate the result.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># knn classification </span></span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>  k <span class="ot">=</span> <span class="dv">15</span></span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>  knn.fit <span class="ot">&lt;-</span> <span class="fu">knn</span>(x, xnew, y, <span class="at">k=</span>k)</span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>  px1 <span class="ot">&lt;-</span> mixture.example<span class="sc">$</span>px1</span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>  px2 <span class="ot">&lt;-</span> mixture.example<span class="sc">$</span>px2</span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">matrix</span>(knn.fit <span class="sc">==</span> <span class="st">&quot;1&quot;</span>, <span class="fu">length</span>(px1), <span class="fu">length</span>(px2))</span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb112-9"><a href="#cb112-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(px1, px2, pred, <span class="at">levels=</span><span class="fl">0.5</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>,<span class="at">axes=</span><span class="cn">FALSE</span>)</span>
<span id="cb112-10"><a href="#cb112-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span>
<span id="cb112-11"><a href="#cb112-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">title</span>(<span class="fu">paste</span>(k, <span class="st">&quot;-Nearest Neighbor&quot;</span>, <span class="at">sep=</span> <span class="st">&quot;&quot;</span>))</span>
<span id="cb112-12"><a href="#cb112-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(x, <span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb112-13"><a href="#cb112-13" aria-hidden="true" tabindex="-1"></a>  mesh <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(px1, px2)</span>
<span id="cb112-14"><a href="#cb112-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(mesh, <span class="at">pch=</span><span class="st">&quot;.&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>, <span class="at">col=</span><span class="fu">ifelse</span>(pred, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>))</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-140-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We can evaluate the in-sample prediction result of this model using a confusion matrix:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the confusion matrix</span></span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>  knn.fit <span class="ot">&lt;-</span> <span class="fu">knn</span>(x, x, y, <span class="at">k =</span> <span class="dv">15</span>)</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>  xtab <span class="ot">=</span> <span class="fu">table</span>(knn.fit, y)</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(caret)</span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">confusionMatrix</span>(xtab)</span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb113-9"><a href="#cb113-9" aria-hidden="true" tabindex="-1"></a><span class="do">##        y</span></span>
<span id="cb113-10"><a href="#cb113-10" aria-hidden="true" tabindex="-1"></a><span class="do">## knn.fit  0  1</span></span>
<span id="cb113-11"><a href="#cb113-11" aria-hidden="true" tabindex="-1"></a><span class="do">##       0 82 13</span></span>
<span id="cb113-12"><a href="#cb113-12" aria-hidden="true" tabindex="-1"></a><span class="do">##       1 18 87</span></span>
<span id="cb113-13"><a href="#cb113-13" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb113-14"><a href="#cb113-14" aria-hidden="true" tabindex="-1"></a><span class="do">##                Accuracy : 0.845           </span></span>
<span id="cb113-15"><a href="#cb113-15" aria-hidden="true" tabindex="-1"></a><span class="do">##                  95% CI : (0.7873, 0.8922)</span></span>
<span id="cb113-16"><a href="#cb113-16" aria-hidden="true" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5             </span></span>
<span id="cb113-17"><a href="#cb113-17" aria-hidden="true" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : &lt;2e-16          </span></span>
<span id="cb113-18"><a href="#cb113-18" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb113-19"><a href="#cb113-19" aria-hidden="true" tabindex="-1"></a><span class="do">##                   Kappa : 0.69            </span></span>
<span id="cb113-20"><a href="#cb113-20" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb113-21"><a href="#cb113-21" aria-hidden="true" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 0.4725          </span></span>
<span id="cb113-22"><a href="#cb113-22" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb113-23"><a href="#cb113-23" aria-hidden="true" tabindex="-1"></a><span class="do">##             Sensitivity : 0.8200          </span></span>
<span id="cb113-24"><a href="#cb113-24" aria-hidden="true" tabindex="-1"></a><span class="do">##             Specificity : 0.8700          </span></span>
<span id="cb113-25"><a href="#cb113-25" aria-hidden="true" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.8632          </span></span>
<span id="cb113-26"><a href="#cb113-26" aria-hidden="true" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.8286          </span></span>
<span id="cb113-27"><a href="#cb113-27" aria-hidden="true" tabindex="-1"></a><span class="do">##              Prevalence : 0.5000          </span></span>
<span id="cb113-28"><a href="#cb113-28" aria-hidden="true" tabindex="-1"></a><span class="do">##          Detection Rate : 0.4100          </span></span>
<span id="cb113-29"><a href="#cb113-29" aria-hidden="true" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.4750          </span></span>
<span id="cb113-30"><a href="#cb113-30" aria-hidden="true" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.8450          </span></span>
<span id="cb113-31"><a href="#cb113-31" aria-hidden="true" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb113-32"><a href="#cb113-32" aria-hidden="true" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : 0               </span></span>
<span id="cb113-33"><a href="#cb113-33" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span></code></pre></div>
</div>
<div id="tuning-with-the-caret-package" class="section level2 hasAnchor" number="9.6">
<h2 class="hasAnchor"><span class="header-section-number">9.6</span> Tuning with the <code>caret</code> Package<a href="#tuning-with-the-caret-package" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <code>caret</code> package has some built-in feature that can tune some popular machine learning models using cross-validation. The cross-validation settings need to be specified using the <span class="math inline">\(trainControl()\)</span> function.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(caret)</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>  control <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>There are other cross-validation methods, such as <code>repeatedcv</code> the repeats the CV several times, and leave-one-out CV <code>LOOCV</code>. For more details, you can read the <a href="https://www.rdocumentation.org/packages/caret/versions/6.0-88/topics/trainControl">documentation</a>. We can then setup the training by specifying a grid of <span class="math inline">\(k\)</span> values, and also the CV setup. Make sure that you specify <code>method = "knn"</code> and also construct the outcome as a factor in a data frame.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>  knn.cvfit <span class="ot">&lt;-</span> <span class="fu">train</span>(y <span class="sc">~</span> ., <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>, </span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> <span class="fu">data.frame</span>(<span class="st">&quot;x&quot;</span> <span class="ot">=</span> x, <span class="st">&quot;y&quot;</span> <span class="ot">=</span> <span class="fu">as.factor</span>(y)),</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">tuneGrid =</span> <span class="fu">data.frame</span>(<span class="at">k =</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">40</span>, <span class="dv">1</span>)),</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">trControl =</span> control)</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(knn.cvfit<span class="sc">$</span>results<span class="sc">$</span>k, <span class="dv">1</span><span class="sc">-</span>knn.cvfit<span class="sc">$</span>results<span class="sc">$</span>Accuracy,</span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;K&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Classification Error&quot;</span>, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>,</span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-144-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Print out the fitted object, we can see that the best <span class="math inline">\(k\)</span> is 6. And there is a clear “U” shaped pattern that shows the potential bias-variance trade-off.</p>
</div>
<div id="distance-measures" class="section level2 hasAnchor" number="9.7">
<h2 class="hasAnchor"><span class="header-section-number">9.7</span> Distance Measures<a href="#distance-measures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Closeness between two points needs to be defined based on some distance measures. By default, we use the squared Euclidean distance (<span class="math inline">\(\ell_2\)</span> norm) for continuous variables:</p>
<p><span class="math display">\[d^2(\mathbf{u}, \mathbf{v}) = \lVert \mathbf{u}- \mathbf{v}\rVert_2^2 = \sum_{j=1}^p (u_j, v_j)^2.\]</span>
However, this measure is not scale invariant. A variable with large scale can dominate this measure. Hence, we often consider a normalized version:</p>
<p><span class="math display">\[d^2(\mathbf{u}, \mathbf{v}) = \sum_{j=1}^p \frac{(u_j, v_j)^2}{\sigma_j^2},\]</span>
where <span class="math inline">\(\sigma_j^2\)</span> can be estimated using the sample variance of variable <span class="math inline">\(j\)</span>. Another choice that further taking the covariance structure into consideration is the <strong>Mahalanobis distance</strong>:</p>
<p><span class="math display">\[d^2(\mathbf{u}, \mathbf{v}) = (\mathbf{u}- \mathbf{v})^\text{T}\Sigma^{-1} (\mathbf{u}- \mathbf{v}),\]</span>
where <span class="math inline">\(\Sigma\)</span> is the covariance matrix, and can be estimated using the sample covariance. In the following plot, the red cross and orange cross have the same Euclidean distance to the center. However, the red cross is more of a “outlier” based on the joint distribution. The Mahalanobis distance would reflect this.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>  x<span class="ot">=</span><span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>  y<span class="ot">=</span><span class="dv">1</span> <span class="sc">+</span> <span class="fl">0.3</span><span class="sc">*</span>x <span class="sc">+</span> <span class="fl">0.3</span><span class="sc">*</span><span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(car)</span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: package &#39;car&#39; was built under R version 4.2.2</span></span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: package &#39;carData&#39; was built under R version 4.2.2</span></span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dataEllipse</span>(x, y, <span class="at">levels=</span><span class="fu">c</span>(<span class="fl">0.6</span>, <span class="fl">0.9</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb116-8"><a href="#cb116-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">4</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb116-9"><a href="#cb116-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="fl">1.5</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">pch =</span> <span class="dv">4</span>, <span class="at">cex =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-145-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>For categorical variables, the Hamming distance is commonly used:</p>
<p><span class="math display">\[d(\mathbf{u}, \mathbf{v}) = \sum_{j=1}^p I(u_j \neq v_j).\]</span>
It simply counts how many entries are not the same.</p>
</div>
<div id="nn-error-bound" class="section level2 hasAnchor" number="9.8">
<h2 class="hasAnchor"><span class="header-section-number">9.8</span> 1NN Error Bound<a href="#nn-error-bound" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can show that 1NN can achieve reasonable performance for fixed <span class="math inline">\(p\)</span>, as <span class="math inline">\(n \rightarrow \infty\)</span> by showing that the 1NN error is no more than twice of the Bayes error, which is the smaller one of <span class="math inline">\(P(Y = 1 | X = x_0)\)</span> and <span class="math inline">\(1 - P(Y = 1 | X = x_0)\)</span>.</p>
<p>Let’s denote <span class="math inline">\(x_{1nn}\)</span> the closest neighbor of <span class="math inline">\(x_0\)</span>, we have <span class="math inline">\(d(x_0, x_{1nn}) \rightarrow 0\)</span> by the law of large numbers. Assuming smoothness, we have <span class="math inline">\(P(Y = 1 | x_{1nn}) \rightarrow P(Y = 1 | x_0)\)</span>. Hence, the 1NN error is the chance we make a wrong prediction, which can happen in two situations. WLOG, let’s assume that <span class="math inline">\(P(Y = 1 | X = x_0)\)</span> is larger than <span class="math inline">\(1-P(Y = 1 | X = x_0)\)</span>, then</p>
<p><span class="math display">\[\begin{align}
&amp; \,P(Y=1|x_0)[ 1 - P(Y=1|x_{1nn})] + P(Y=1|x_0)[ 1 - P(Y=1|x_{1nn})]\\
\leq &amp; \,[ 1 - P(Y=1|x_{1nn})] + [ 1 - P(Y=1|x_{1nn})]\\
\rightarrow &amp; \, 2[ 1 - P(Y=1|x_0)]\\
= &amp; \,2 \times \text{Bayes Error}\\
\end{align}\]</span></p>
<p>This is a crude bound, but shows that 1NN can still be a reasonable estimator when the noise is small (Bayes error small).</p>
</div>
<div id="example-2-handwritten-digit-data" class="section level2 hasAnchor" number="9.9">
<h2 class="hasAnchor"><span class="header-section-number">9.9</span> Example 2: Handwritten Digit Data<a href="#example-2-handwritten-digit-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s consider another example using the handwritten digit data. Each observation in this data is a <span class="math inline">\(16 \times 16\)</span> pixel image. Hence, the total number of variables is <span class="math inline">\(256\)</span>. At each pixel, we have the gray scale as the numerical value.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Handwritten Digit Recognition Data</span></span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the first column is the true digit</span></span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dim</span>(zip.train)</span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 7291  257</span></span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dim</span>(zip.test)</span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2007  257</span></span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># look at one sample</span></span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">image</span>(<span class="fu">zip2image</span>(zip.train, <span class="dv">1</span>), <span class="at">col=</span><span class="fu">gray</span>(<span class="dv">256</span><span class="sc">:</span><span class="dv">0</span><span class="sc">/</span><span class="dv">256</span>), <span class="at">zlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), </span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">axes =</span> <span class="cn">FALSE</span>)</span>
<span id="cb117-12"><a href="#cb117-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] &quot;digit  6  taken&quot;</span></span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-146-1.png" width="20%" style="display: block; margin: auto;" /></p>
<p>We use 3NN to predict all samples in the testing data. The model is fairly accurate.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit 3nn model and calculate the error</span></span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>  knn.fit <span class="ot">&lt;-</span> <span class="fu">knn</span>(zip.train[, <span class="dv">2</span><span class="sc">:</span><span class="dv">257</span>], zip.test[, <span class="dv">2</span><span class="sc">:</span><span class="dv">257</span>], zip.train[, <span class="dv">1</span>], <span class="at">k=</span><span class="dv">3</span>)</span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># overall prediction error</span></span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(knn.fit <span class="sc">!=</span> zip.test[,<span class="dv">1</span>])</span>
<span id="cb118-6"><a href="#cb118-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.05430992</span></span>
<span id="cb118-7"><a href="#cb118-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb118-8"><a href="#cb118-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the confusion matrix</span></span>
<span id="cb118-9"><a href="#cb118-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">table</span>(knn.fit, zip.test[,<span class="dv">1</span>])</span>
<span id="cb118-10"><a href="#cb118-10" aria-hidden="true" tabindex="-1"></a><span class="do">##        </span></span>
<span id="cb118-11"><a href="#cb118-11" aria-hidden="true" tabindex="-1"></a><span class="do">## knn.fit   0   1   2   3   4   5   6   7   8   9</span></span>
<span id="cb118-12"><a href="#cb118-12" aria-hidden="true" tabindex="-1"></a><span class="do">##       0 355   0   6   1   0   3   3   0   4   2</span></span>
<span id="cb118-13"><a href="#cb118-13" aria-hidden="true" tabindex="-1"></a><span class="do">##       1   0 258   0   0   2   0   0   1   0   0</span></span>
<span id="cb118-14"><a href="#cb118-14" aria-hidden="true" tabindex="-1"></a><span class="do">##       2   1   0 182   2   0   2   1   1   2   0</span></span>
<span id="cb118-15"><a href="#cb118-15" aria-hidden="true" tabindex="-1"></a><span class="do">##       3   0   0   1 153   0   4   0   1   5   0</span></span>
<span id="cb118-16"><a href="#cb118-16" aria-hidden="true" tabindex="-1"></a><span class="do">##       4   0   3   1   0 183   0   2   4   0   3</span></span>
<span id="cb118-17"><a href="#cb118-17" aria-hidden="true" tabindex="-1"></a><span class="do">##       5   0   0   0   7   2 146   0   0   1   0</span></span>
<span id="cb118-18"><a href="#cb118-18" aria-hidden="true" tabindex="-1"></a><span class="do">##       6   0   2   2   0   2   0 164   0   0   0</span></span>
<span id="cb118-19"><a href="#cb118-19" aria-hidden="true" tabindex="-1"></a><span class="do">##       7   2   1   2   1   2   0   0 138   1   4</span></span>
<span id="cb118-20"><a href="#cb118-20" aria-hidden="true" tabindex="-1"></a><span class="do">##       8   0   0   4   0   1   1   0   1 151   0</span></span>
<span id="cb118-21"><a href="#cb118-21" aria-hidden="true" tabindex="-1"></a><span class="do">##       9   1   0   0   2   8   4   0   1   2 168</span></span></code></pre></div>
</div>
<div id="curse-of-dimensionality" class="section level2 hasAnchor" number="9.10">
<h2 class="hasAnchor"><span class="header-section-number">9.10</span> Curse of Dimensionality<a href="#curse-of-dimensionality" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Many of the practical problems we encounter today are high-dimensional. The resolution of the handwritten digit example is <span class="math inline">\(16 \times 16 = 256\)</span>. Genetic studies often involves more than 25K gene expressions, etc. For a given sample size <span class="math inline">\(n\)</span>, as the number of variables <span class="math inline">\(p\)</span> increases, the data becomes very sparse. Nearest neighbor methods usually do not perform very well on high-dimensional data. This is because for any given target point, there will not be enough training data that lies close enough. To see this, let’s consider a <span class="math inline">\(p\)</span>-dimensional hyper-cube. Suppose we have <span class="math inline">\(n=1000\)</span> observations uniformly spread out in this cube, and we are interested in predicting a target point with <span class="math inline">\(k=10\)</span> neighbors. If these neighbors are really close to the target point, then this would be a good estimation with small bias. Suppose <span class="math inline">\(p=2\)</span>, then we know that if we take a cube (square) with height and width both <span class="math inline">\(l = 0.1\)</span>, then there will be <span class="math inline">\(1000 \times 0.1^2 = 10\)</span> observations within the square. In general, we have the relationship</p>
<p><span class="math display">\[l^p = \frac{k}{n}\]</span>
Try different <span class="math inline">\(p\)</span>, we have</p>
<ul>
<li>If <span class="math inline">\(p = 2\)</span>, <span class="math inline">\(l = 0.1\)</span></li>
<li>If <span class="math inline">\(p = 10\)</span>, <span class="math inline">\(l = 0.63\)</span></li>
<li>If <span class="math inline">\(p = 100\)</span>, <span class="math inline">\(l = 0.955\)</span></li>
</ul>
<p>This implies that if we have 100 dimensions, then the nearest 10 observations would be 0.955 away from the target point at each dimension, this is almost at the other corner of the cube. Hence there will be a very large bias. Decreasing <span class="math inline">\(k\)</span> does not help much in this situation since even the closest point can be really far away, and the model would have large variance.</p>
<center>
<img src="images/highd.png" style="width:30.0%" />
</center>
<p>However, why our model performs well in the handwritten digit example? There is possibly (approximately) a lower dimensional representation of the data so that when you evaluate the distance on the high-dimensional space, it is just as effective as working on the low dimensional space. Dimension reduction is an important topic in statistical learning and machine learning. Many methods, such as sliced inverse regression <span class="citation">(<a href="#ref-li1991sliced" role="doc-biblioref">Li 1991</a>)</span> and UMAP <span class="citation">(<a href="#ref-mcinnes2018umap" role="doc-biblioref">McInnes et al. 2018</a>)</span> have been developed based on this concept.</p>
<center>
<p><img src="images/manifold.png" style="width:70.0%" /></p>
Image from <span class="citation">Cayton (<a href="#ref-cayton2005algorithms" role="doc-biblioref">2005</a>)</span>.
</center>
<!--chapter:end:03.2-knn.Rmd-->
</div>
</div>
<div id="kernel-smoothing" class="section level1 hasAnchor" number="10">
<h1 class="hasAnchor"><span class="header-section-number">10</span> Kernel Smoothing<a href="#kernel-smoothing" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Fundamental ideas of local regression approaches are similar to <span class="math inline">\(k\)</span>NN. But most approaches would address a fundamental drawback of <span class="math inline">\(k\)</span>NN that the estimated function is not smooth. Having a smoothed estimation would also allow us to estimate the derivative, which is essentially used when estimating the density function. We will start with the intuition of the kernel estimator and then discuss the bias-variance trade-off using kernel density estimation as an example.</p>
<div id="knn-vs.-kernel" class="section level2 hasAnchor" number="10.1">
<h2 class="hasAnchor"><span class="header-section-number">10.1</span> KNN vs. Kernel<a href="#knn-vs.-kernel" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We first compare the <span class="math inline">\(K\)</span>NN method with a Gaussian kernel regression. <span class="math inline">\(K\)</span>NN has jumps while Gaussian kernel regression is smooth.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-151-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="kernel-density-estimations" class="section level2 hasAnchor" number="10.2">
<h2 class="hasAnchor"><span class="header-section-number">10.2</span> Kernel Density Estimations<a href="#kernel-density-estimations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A natural estimator, by using the counts, is</p>
<p><span class="math display">\[\widehat f(x) = \frac{\#\big\{x_i: x_i \in [x - \frac{h}{2}, x + \frac{h}{2}]\big\}}{h n}\]</span>
This maybe compared with the histogram estimator</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(ggplot2)</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data</span>(mpg)</span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># histogram </span></span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">hist</span>(mpg<span class="sc">$</span>hwy, <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">6</span>, <span class="dv">50</span>, <span class="dv">2</span>))</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># uniform kernel</span></span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a>    xgrid <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">6</span>, <span class="dv">50</span>, <span class="fl">0.1</span>)</span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true" tabindex="-1"></a>    histden <span class="ot">=</span> <span class="fu">sapply</span>(xgrid, <span class="at">FUN =</span> <span class="cf">function</span>(x, obs, h) <span class="fu">sum</span>( ((x<span class="sc">-</span>h<span class="sc">/</span><span class="dv">2</span>) <span class="sc">&lt;=</span> obs) <span class="sc">*</span> ((x<span class="sc">+</span>h<span class="sc">/</span><span class="dv">2</span>) <span class="sc">&gt;</span> obs))<span class="sc">/</span>h<span class="sc">/</span><span class="fu">length</span>(obs), </span>
<span id="cb119-10"><a href="#cb119-10" aria-hidden="true" tabindex="-1"></a>                     <span class="at">obs =</span> mpg<span class="sc">$</span>hwy, <span class="at">h =</span> <span class="dv">2</span>)</span>
<span id="cb119-11"><a href="#cb119-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb119-12"><a href="#cb119-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(xgrid, histden, <span class="at">type =</span> <span class="st">&quot;s&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-152-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>This can be view in two ways. The easier interpretation is that, for each target point, we count how many observations are close-by. We can also interpret it as evenly distributing the point-mass of each observation to a close-by region with width <span class="math inline">\(h\)</span>, and then stack them all.</p>
<p><span class="math display">\[\widehat f(x) = \frac{1}{h n} \sum_i \,\underbrace{ \mathbf{1} \Big(x \in [x_i - \frac{h}{2}, x_i + \frac{h}{2}]}_{\text{uniform density centered at } x_i} \Big)\]</span>
Here is a close-up demonstration of how those uniform density functions are stacked for all observations.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-154-1.png" width="55%" style="display: block; margin: auto;" /></p>
<p>However, this is will lead to jumps at the end of each small uniform density. Let’s consider using a smooth function instead. Naturally, we can use the Gaussian kernel function to calculate the numerator in the above equation.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-155-1.png" width="55%" style="display: block; margin: auto;" /></p>
<p>We apply this to the <code>mpg</code> dataset.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">6</span>, <span class="dv">50</span>, <span class="fl">0.1</span>)</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>  kernelfun <span class="ot">&lt;-</span> <span class="cf">function</span>(x, obs, h) <span class="fu">sum</span>(<span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span><span class="sc">*</span>((x<span class="sc">-</span>obs)<span class="sc">/</span>h)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">2</span><span class="sc">*</span>pi)<span class="sc">/</span>h)</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(xgrid, <span class="fu">sapply</span>(xgrid, <span class="at">FUN =</span> kernelfun, <span class="at">obs =</span> mpg<span class="sc">$</span>hwy, <span class="at">h =</span> <span class="fl">1.5</span>)<span class="sc">/</span><span class="fu">length</span>(mpg<span class="sc">$</span>hwy), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>,</span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;MPG&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Estimated Density&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-156-1.png" width="55%" style="display: block; margin: auto;" /></p>
<p>The <code>ggplot2</code> packages provides some convenient features to plot the density and histogram.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(mpg, <span class="fu">aes</span>(<span class="at">x=</span>hwy)) <span class="sc">+</span> </span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span>..density..), <span class="at">colour=</span><span class="st">&quot;black&quot;</span>, <span class="at">fill=</span><span class="st">&quot;white&quot;</span>)<span class="sc">+</span></span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>(<span class="at">alpha=</span>.<span class="dv">2</span>, <span class="at">fill=</span><span class="st">&quot;#ff8c00&quot;</span>)</span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.</span></span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a><span class="do">## ℹ Please use `after_stat(density)` instead.</span></span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-157-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="bias-variance-trade-off" class="section level2 hasAnchor" number="10.3">
<h2 class="hasAnchor"><span class="header-section-number">10.3</span> Bias-variance trade-off<a href="#bias-variance-trade-off" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s consider estimating a density, using the Parzen estimator</p>
<p><span class="math display">\[\widehat f(x) = \frac{1}{n} \sum_{i=1}^n K_{h} (x, x_i)\]</span>
here, <span class="math inline">\(K_h(\mathbf{u}, \mathbf{v}) = K(|\mathbf{u}- \mathbf{v}|/h)/h\)</span> is a kernel function that satisfies</p>
<ul>
<li><span class="math inline">\(\int K(u)du = 1\)</span> (a proper density)</li>
<li><span class="math inline">\(K(-u) = K(u)\)</span> (symmetric)</li>
<li><span class="math inline">\(\int K(u) u^2 du \leq \infty\)</span> (finite second moment)</li>
</ul>
<p>Note that <span class="math inline">\(h\)</span> simply scales the covariate and adjust the density accordingly. Our goal is to estimate a target point <span class="math inline">\(x\)</span> using a set of iid data. First, we can analyze the bias:</p>
<p><span class="math display">\[\begin{align}
    \text{E}\big[ \widehat f(x) \big] &amp;= \text{E}\left[ K\left( \frac{x - x_1}{h} \right) \Big/ h \right] \\
    &amp;= \int_{-\infty}^\infty \frac{1}{h} K\left(\frac{x-x_1}{h}\right) f(x_1) d x_1 \\
    &amp;= \int_{\infty}^{-\infty} \frac{1}{h} K(t) f(x - th) d (x-th) \\
    (\text{Taylor expansion}) \quad &amp;= f(x) + \frac{h^2}{2} f&#39;&#39;(x) \int_{-\infty}^\infty K(t) t^2 dt + o(h^2) \\
    (\text{as } ha \rightarrow 0) \quad &amp;\rightarrow f(x)
\end{align}\]</span></p>
<p>Since the density is over the entire domain, we can define the integrated Bias<span class="math inline">\(^2\)</span>:</p>
<p><span class="math display">\[\begin{align}
\text{Bias}^2 &amp;= \int \left( E[\widehat f(x)] - f(x)\right)^2 dx \\
    &amp;\approx \frac{h^4 \sigma_K^4}{4} \int \big[ f&#39;&#39;(x)\big]^2 dx
\end{align}\]</span>
where <span class="math inline">\(\sigma_K^2 = \int_{-\infty}^\infty K(t) t^2 dt\)</span>.</p>
<p>On the other hand, the variance term is</p>
<p><span class="math display">\[\begin{align}
  \text{Var}\big[ \widehat f(x) \big] &amp;= \frac{1}{n} \text{Var}\Big[\frac{1}{h}K\big( \frac{x - x_1}{h} \big) \Big] \\
  &amp;= \frac{1}{n} \text{E}\bigg[ \frac{1}{h^2} K^2\big( \frac{x - x_1}{h}\big) - \text{E}\Big[ \frac{1}{h} K\big( \frac{x - x_1}{h} \big)\Big]^2 \bigg]\\
  &amp;= \frac{1}{n} \Big[ \int \frac{1}{h} K^2( \frac{x - x_1}{h} ) f(x_1) dx_1 + O(1) \Big] \\
  &amp;= \frac{1}{n} \Big[ \frac{1}{h} \int K^2( u ) f(x) du + O(1) \Big] \\
  &amp;= \frac{f(x)}{nh} \int K^2( u ) du
\end{align}\]</span></p>
<p>with the integrated variance being</p>
<p><span class="math display">\[\frac{1}{nh} \int K^2( u ) dt \]</span></p>
<p>By minimizing the asymptotic mean integrated squared error (AMISE), defined as the sum of integrated Bias<span class="math inline">\(^2\)</span> and variance, we have the optimal <span class="math inline">\(h\)</span> being</p>
<p><span class="math display">\[h^\text{opt} = \bigg[\frac{1}{n} \frac{\int K^2(u)du}{ \sigma^2_K \int f&#39;&#39;(u)du} \bigg]^{1/5},\]</span></p>
<p>and the optimal <span class="math inline">\(h\)</span> is in the order of <span class="math inline">\(\cal O(n^{-4/5})\)</span>.</p>
</div>
<div id="gaussian-kernel-regression" class="section level2 hasAnchor" number="10.4">
<h2 class="hasAnchor"><span class="header-section-number">10.4</span> Gaussian Kernel Regression<a href="#gaussian-kernel-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A Nadaraya-Watson kernel regression model has the following formula. Note that we use <span class="math inline">\(h\)</span> as the bandwidth instead of <span class="math inline">\(h\)</span>.</p>
<p><span class="math display">\[\widehat f(x) = \frac{\sum_i K_h(x, x_i) y_i}{\sum_i K_h(x, x_i)},\]</span>
where <span class="math inline">\(h\)</span> is the bandwidth. At each target point <span class="math inline">\(x\)</span>, training data <span class="math inline">\(x_i\)</span>s that are closer to <span class="math inline">\(x\)</span> receives higher weights <span class="math inline">\(K_h(x, x_i)\)</span>, hence their <span class="math inline">\(y_i\)</span> values are more influential in terms of estimating <span class="math inline">\(f(x)\)</span>. For the Gaussian kernel, we use</p>
<p><span class="math display">\[K_h(x, x_i) = \frac{1}{h\sqrt{2\pi}} \exp\left\{ -\frac{(x - x_i)^2}{2 h^2}\right\}\]</span></p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-158-1.png" width="90%" style="display: block; margin: auto;" /></p>
<div id="bias-variance-trade-off-1" class="section level3 hasAnchor" number="10.4.1">
<h3 class="hasAnchor"><span class="header-section-number">10.4.1</span> Bias-variance Trade-off<a href="#bias-variance-trade-off-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The bandwidth <span class="math inline">\(h\)</span> is an important tuning parameter that controls the bias-variance trade-off. It behaves the same as the density estimation. By setting a large <span class="math inline">\(h\)</span>, the estimator is more stable but has more bias.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># a small bandwidth</span></span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>  ksmooth.fit1 <span class="ot">=</span> <span class="fu">ksmooth</span>(x, y, <span class="at">bandwidth =</span> <span class="fl">0.5</span>, <span class="at">kernel =</span> <span class="st">&quot;normal&quot;</span>, <span class="at">x.points =</span> testx)</span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># a large bandwidth</span></span>
<span id="cb122-5"><a href="#cb122-5" aria-hidden="true" tabindex="-1"></a>  ksmooth.fit2 <span class="ot">=</span> <span class="fu">ksmooth</span>(x, y, <span class="at">bandwidth =</span> <span class="dv">2</span>, <span class="at">kernel =</span> <span class="st">&quot;normal&quot;</span>, <span class="at">x.points =</span> testx)</span>
<span id="cb122-6"><a href="#cb122-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb122-7"><a href="#cb122-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot both</span></span>
<span id="cb122-8"><a href="#cb122-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, y, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="at">yaxt=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb122-9"><a href="#cb122-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(testx, <span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(testx), <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb122-10"><a href="#cb122-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(testx, ksmooth.fit1<span class="sc">$</span>y, <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb122-11"><a href="#cb122-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(testx, ksmooth.fit2<span class="sc">$</span>y, <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb122-12"><a href="#cb122-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;h = 0.5&quot;</span>, <span class="st">&quot;h = 2&quot;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;red&quot;</span>), </span>
<span id="cb122-13"><a href="#cb122-13" aria-hidden="true" tabindex="-1"></a>         <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-160-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="choice-of-kernel-functions" class="section level2 hasAnchor" number="10.5">
<h2 class="hasAnchor"><span class="header-section-number">10.5</span> Choice of Kernel Functions<a href="#choice-of-kernel-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Other kernel functions can also be used. The most efficient kernel is the Epanechnikov kernel, which will minimize the mean integrated squared error (MISE). The efficiency is defined as</p>
<p><span class="math display">\[ \Big(\int u^2K(u) du\Big)^\frac{1}{2}  \int K^2(u) du, \]</span>
Different kernel functions can be visualized in the following. Most kernels are bounded within <span class="math inline">\([-h/2, h/2]\)</span>, except the Gaussian kernel.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-161-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="local-linear-regression" class="section level2 hasAnchor" number="10.6">
<h2 class="hasAnchor"><span class="header-section-number">10.6</span> Local Linear Regression<a href="#local-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Local averaging will suffer severe bias at the boundaries. One solution is to use the local polynomial regression. The following examples are local linear regressions, evaluated as different target points. We are solving for a linear model weighted by the kernel weights</p>
<p><span class="math display">\[\sum_{i = 1}^n K_h(x, x_i) \big( y_i - \beta_0 - \beta_1 x_i \big)^2\]</span></p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-162-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="local-polynomial-regression" class="section level2 hasAnchor" number="10.7">
<h2 class="hasAnchor"><span class="header-section-number">10.7</span> Local Polynomial Regression<a href="#local-polynomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The following examples are local polynomial regressions, evaluated as different target points. We can easily extend the local linear model to inccorperate higher orders terms:</p>
<p><span class="math display">\[\sum_{i=1}^n K_h(x, x_i) \Big[ y_i - \beta_0(x) - \sum_{r=1}^d \beta_j(x) x_i^r \Big]^2\]</span></p>
<p>The followings are local quadratic fittings, which will further correct the bias.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-164-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="r-implementations" class="section level2 hasAnchor" number="10.8">
<h2 class="hasAnchor"><span class="header-section-number">10.8</span> R Implementations<a href="#r-implementations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Some popular <code>R</code> functions implements the local polynomial regressions: <code>loess</code>, <code>locfit</code>, <code>locploy</code>, etc. These functions automatically calculate the fitted value for each target point (essentially all the observed points). This can be used in combination with <code>ggplot2</code>. The point-wise confidence intervals are also calculated.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(mpg, <span class="fu">aes</span>(displ, hwy)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_smooth</span>(<span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="at">span =</span> <span class="fl">0.5</span>)</span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a><span class="do">## `geom_smooth()` using formula = &#39;y ~ x&#39;</span></span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-165-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>A toy example that compares different bandwidth. Be careful that different methods may formulat the bandwidth parameter in different ways.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># local polynomial fitting using locfit and locpoly</span></span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(KernSmooth)</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(locfit)</span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: package &#39;locfit&#39; was built under R version 4.2.2</span></span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true" tabindex="-1"></a>    n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb124-8"><a href="#cb124-8" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb124-9"><a href="#cb124-9" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> <span class="fu">sin</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>x)<span class="sc">+</span><span class="fu">rnorm</span>(n,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb124-10"><a href="#cb124-10" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> y[<span class="fu">order</span>(x)]</span>
<span id="cb124-11"><a href="#cb124-11" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> <span class="fu">sort</span>(x)</span>
<span id="cb124-12"><a href="#cb124-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb124-13"><a href="#cb124-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb124-14"><a href="#cb124-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(x, <span class="fu">sin</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>x), <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="dv">1</span>)</span>
<span id="cb124-15"><a href="#cb124-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">locpoly</span>(x, y, <span class="at">bandwidth=</span><span class="fl">0.15</span>, <span class="at">degree=</span><span class="dv">2</span>), <span class="at">col=</span><span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb124-16"><a href="#cb124-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">locfit</span>(y<span class="sc">~</span><span class="fu">lp</span>(x, <span class="at">nn =</span> <span class="fl">0.3</span>, <span class="at">h=</span><span class="fl">0.05</span>, <span class="at">deg=</span><span class="dv">2</span>)), <span class="at">col=</span><span class="dv">4</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb124-17"><a href="#cb124-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb124-18"><a href="#cb124-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;locpoly&quot;</span>, <span class="st">&quot;locfit&quot;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">cex =</span> <span class="fl">1.5</span>, <span class="at">lwd =</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-167-1.png" width="45%" style="display: block; margin: auto;" /></p>
<!--chapter:end:03.3-kernel.Rmd-->
</div>
</div>
<div id="part-classification-models" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART) Classification Models<a href="#part-classification-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="logistic-regression" class="section level1 hasAnchor" number="11">
<h1 class="hasAnchor"><span class="header-section-number">11</span> Logistic Regression<a href="#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="modeling-binary-outcomes" class="section level2 hasAnchor" number="11.1">
<h2 class="hasAnchor"><span class="header-section-number">11.1</span> Modeling Binary Outcomes<a href="#modeling-binary-outcomes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To model binary outcomes using a logistic regression, we will use the 0/1 coding of <span class="math inline">\(Y\)</span>. We need to set its connection with covariates. Recall in a linear regression, the outcome is continuous, and we set</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X + \epsilon\]</span>
However, this does not work for classification since <span class="math inline">\(Y\)</span> can only be 0 or 1. Hence we turn to consider modeling the probability <span class="math inline">\(P(Y = 1 | X = \mathbf{x})\)</span>. Hence, <span class="math inline">\(Y\)</span> is a Bernoulli random variable given <span class="math inline">\(X\)</span>, and this is modeled by a function of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[ P(Y = 1 | X = \mathbf{x}) = \frac{\exp(\mathbf{x}^\text{T}\boldsymbol \beta)}{1 + \exp(\mathbf{x}^\text{T}\boldsymbol \beta)}\]</span>
Note that although <span class="math inline">\(\mathbf{x}^\text{T}\boldsymbol \beta\)</span> may ranges from 0 to infinity as <span class="math inline">\(X\)</span> changes, the probability will still be bounded between 0 and 1. This is an example of <strong>Generalized Linear Models</strong>. The relationship is still represented using a linear function of <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(\mathbf{x}^\text{T}\boldsymbol \beta\)</span>. This is called a <strong>logit link</strong> function (a function to connect the conditional expectation of <span class="math inline">\(Y\)</span> with <span class="math inline">\(\boldsymbol \beta^\text{T}\mathbf{x}\)</span>):</p>
<p><span class="math display">\[\eta(a) = \frac{\exp(a)}{1 + \exp(a)}\]</span>
Hence, <span class="math inline">\(P(Y = 1 | X = \mathbf{x}) = \eta(\mathbf{x}^\text{T}\boldsymbol \beta)\)</span>. We can reversely solve this and get</p>
<span class="math display">\[\begin{aligned}
P(Y = 1 | X = \mathbf{x}) = \eta(\mathbf{x}^\text{T}\boldsymbol \beta) &amp;= \frac{\exp(\mathbf{x}^\text{T}\boldsymbol \beta)}{1 + \exp(\mathbf{x}^\text{T}\boldsymbol \beta)}\\
1 - \eta(\mathbf{x}^\text{T}\boldsymbol \beta) &amp;= \frac{1}{1 + \exp(\mathbf{x}^\text{T}\boldsymbol \beta)} \\
\text{Odds} = \frac{\eta(\mathbf{x}^\text{T}\boldsymbol \beta)}{1-\eta(\mathbf{x}^\text{T}\boldsymbol \beta)} &amp;= \exp(\mathbf{x}^\text{T}\boldsymbol \beta)\\
\log(\text{Odds}) = \mathbf{x}^\text{T}\boldsymbol \beta
\end{aligned}\]</span>
<p>Hence, the parameters in a logistic regression is explained as <strong>log odds</strong>. Let’s look at a concrete example.</p>
</div>
<div id="example-cleveland-clinic-heart-disease-data" class="section level2 hasAnchor" number="11.2">
<h2 class="hasAnchor"><span class="header-section-number">11.2</span> Example: Cleveland Clinic Heart Disease Data<a href="#example-cleveland-clinic-heart-disease-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use use the <a href="https://www.kaggle.com/aavigan/cleveland-clinic-heart-disease-dataset">Cleveland clinic heart disease dataset</a>. The goal is to model and predict a class label of whether the patient has a hearth disease or not. This is indicated by whether the <code>num</code> variable is <span class="math inline">\(0\)</span> (no presence) or <span class="math inline">\(&gt;0\)</span> (presence).</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>  heart <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/processed_cleveland.csv&quot;</span>)</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>  heart<span class="sc">$</span>Y <span class="ot">=</span> <span class="fu">as.factor</span>(heart<span class="sc">$</span>num <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">table</span>(heart<span class="sc">$</span>Y)</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a><span class="do">## FALSE  TRUE </span></span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a><span class="do">##   164   139</span></span></code></pre></div>
<p>Let’s model the probability of heart disease using the <code>Age</code> variable. This can be done using the <code>glm()</code> function, which stands for the Generalized Linear Model. The syntax of <code>glm()</code> is almost the same as a linear model. Note that it is important to use <code>family = binomial</code> to specify the logistic regression.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>  logistic.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Y<span class="sc">~</span>age, <span class="at">data =</span> heart, <span class="at">family =</span> binomial)</span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summary</span>(logistic.fit)</span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true" tabindex="-1"></a><span class="do">## glm(formula = Y ~ age, family = binomial, data = heart)</span></span>
<span id="cb126-6"><a href="#cb126-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb126-7"><a href="#cb126-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Deviance Residuals: </span></span>
<span id="cb126-8"><a href="#cb126-8" aria-hidden="true" tabindex="-1"></a><span class="do">##    Min      1Q  Median      3Q     Max  </span></span>
<span id="cb126-9"><a href="#cb126-9" aria-hidden="true" tabindex="-1"></a><span class="do">## -1.596  -1.073  -0.835   1.173   1.705  </span></span>
<span id="cb126-10"><a href="#cb126-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb126-11"><a href="#cb126-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb126-12"><a href="#cb126-12" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb126-13"><a href="#cb126-13" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) -3.00591    0.75913  -3.960  7.5e-05 ***</span></span>
<span id="cb126-14"><a href="#cb126-14" aria-hidden="true" tabindex="-1"></a><span class="do">## age          0.05199    0.01367   3.803 0.000143 ***</span></span>
<span id="cb126-15"><a href="#cb126-15" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb126-16"><a href="#cb126-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb126-17"><a href="#cb126-17" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb126-18"><a href="#cb126-18" aria-hidden="true" tabindex="-1"></a><span class="do">## (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb126-19"><a href="#cb126-19" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb126-20"><a href="#cb126-20" aria-hidden="true" tabindex="-1"></a><span class="do">##     Null deviance: 417.98  on 302  degrees of freedom</span></span>
<span id="cb126-21"><a href="#cb126-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual deviance: 402.54  on 301  degrees of freedom</span></span>
<span id="cb126-22"><a href="#cb126-22" aria-hidden="true" tabindex="-1"></a><span class="do">## AIC: 406.54</span></span>
<span id="cb126-23"><a href="#cb126-23" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb126-24"><a href="#cb126-24" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of Fisher Scoring iterations: 4</span></span></code></pre></div>
<p>The result is similar to a linear regression, with some differences. The parameter estimate of age is 0.05199. It is positive, meaning that increasing age would increase the change of having heart disease. However, this does not mean that 1 year older would increase the change by 0.05. Since, by our previous formula, the probably is not directly expressed as <span class="math inline">\(\mathbf{x}^\text{T}\boldsymbol \beta\)</span>.</p>
<p>This calculation can be realized when predicting a new target point. Let’s consider a new subject with <code>Age = 55</code>. What is the predicted probability of heart disease? Based on our formula, we have</p>
<p><span class="math display">\[\beta_0 + \beta_1 X = -3.00591 + 0.05199 \times 55 = -0.14646\]</span>
And the estimated probability is</p>
<p><span class="math display">\[ P(Y = 1 | X) = \frac{\exp(\beta_0 + \beta_1 X)}{1 + \exp(\beta_0 + \beta_1 X)} = \frac{\exp(-0.14646)}{1 + \exp(-0.14646)} = 0.4634503\]</span>
Hence, the estimated probability for this subject is 46.3%. This can be done using R code. Please note that if you want to predict the probability, you need to specify <code>type = "response"</code>. Otherwise, only <span class="math inline">\(\beta_0 + \beta_1 X\)</span> is provided.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>  testdata <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="st">&quot;age&quot;</span> <span class="ot">=</span> <span class="dv">55</span>)</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>(logistic.fit, <span class="at">newdata =</span> testdata)</span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a><span class="do">##          1 </span></span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a><span class="do">## -0.1466722</span></span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>(logistic.fit, <span class="at">newdata =</span> testdata, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb127-6"><a href="#cb127-6" aria-hidden="true" tabindex="-1"></a><span class="do">##         1 </span></span>
<span id="cb127-7"><a href="#cb127-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.4633975</span></span></code></pre></div>
<p>If we need to make a 0/1 decision about this subject, a natural idea is to see if the predicted probability is greater than 0.5. In this case, we would predict this subject as 0.</p>
</div>
<div id="interpretation-of-the-parameters" class="section level2 hasAnchor" number="11.3">
<h2 class="hasAnchor"><span class="header-section-number">11.3</span> Interpretation of the Parameters<a href="#interpretation-of-the-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall that <span class="math inline">\(\mathbf{x}^\text{T}\boldsymbol \beta\)</span> is the log odds, we can further interpret the effect of a single variable. Let’s define the following two, with an arbitrary age value <span class="math inline">\(a\)</span>:</p>
<ul>
<li>A subject with <code>age</code> <span class="math inline">\(= a\)</span></li>
<li>A subject with <code>age</code> <span class="math inline">\(= a + 1\)</span></li>
</ul>
<p>Then, if we look at the <strong>odds ratio</strong> corresponding to these two target points, we have</p>
<span class="math display">\[\begin{aligned}
\text{Odds Ratio} &amp;= \frac{\text{Odds in Group 2}}{\text{Odds in Group 1}}\\
&amp;= \frac{\exp(\beta_0 + \beta_1 (a+1))}{\exp(\beta_0 + \beta_1 a)}\\
&amp;= \frac{\exp(\beta_0 + \beta_1 a) \times \exp(\beta_1)}{\exp(\beta_0 + \beta_1 a)}\\
&amp;= \exp(\beta_1)
\end{aligned}\]</span>
<p>Taking <span class="math inline">\(\log\)</span> on both sides, we have</p>
<p><span class="math display">\[\log(\text{Odds Ratio}) = \beta_1\]</span></p>
<p>Hence, the odds ratio between these two subjects (<strong>they differ only with one unit of <code>age</code></strong>) can be directly interpreted as the exponential of the parameter of <code>age</code>. After taking the log, we can also say that</p>
<blockquote>
<p>The parameter <span class="math inline">\(\beta\)</span> of a varaible in a logistic regression represents the <strong>log of odds ratio</strong> associated with one-unit increase of this variable.</p>
</blockquote>
<p>Please note that we usually do not be explicit about what this odds ratio is about (what two subject we are comparing). Because the interpretation of the parameter does not change regardless of the value <span class="math inline">\(a\)</span>, as long as the two subjects differ in one unit.</p>
<p>And also note that this conclusion is regardless of the values of other covaraites. When we have a multivariate model, as long as all other covariates are held the same, the previous derivation will remain the same.</p>
</div>
<div id="solving-a-logistic-regression" class="section level2 hasAnchor" number="11.4">
<h2 class="hasAnchor"><span class="header-section-number">11.4</span> Solving a Logistic Regression<a href="#solving-a-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The logistic regression is solved by maximizing the log-likelihood function. Note that the log-likelihood is given by</p>
<p><span class="math display">\[\ell(\boldsymbol \beta) = \sum_{i=1}^n \log \, p(y_i | x_i, \boldsymbol \beta).\]</span>
Using the probabilities of Bernoulli distribution, we have</p>
<p><span class="math display">\[\begin{align}
\ell(\boldsymbol \beta) =&amp; \sum_{i=1}^n \log \left\{ \eta(\mathbf{x}_i)^{y_i} [1-\eta(\mathbf{x}_i)]^{1-y_i} \right\}\\
    =&amp; \sum_{i=1}^n y_i \log \frac{\eta(\mathbf{x}_i)}{1-\eta(\mathbf{x}_i)} + \log [1-\eta(\mathbf{x}_i)] \\
    =&amp; \sum_{i=1}^n y_i \mathbf{x}_i^\text{T}\boldsymbol \beta- \log [ 1 + \exp(\mathbf{x}_i^\text{T}\boldsymbol \beta)]
\end{align}\]</span></p>
<p>Since this objective function is relatively simple, we can use Newton’s method to update. The gradient is given by</p>
<p><span class="math display">\[\frac{\partial \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta} =~ \sum_{i=1}^n y_i \mathbf{x}_i^\text{T}- \sum_{i=1}^n \frac{\exp(\mathbf{x}_i^\text{T}\boldsymbol \beta) \mathbf{x}_i^\text{T}}{1 + \exp(\mathbf{x}_i^\text{T}\boldsymbol \beta)},\]</span></p>
<p>and the Hessian matrix is given by</p>
<p><span class="math display">\[\frac{\partial^2 \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta\partial \boldsymbol \beta^\text{T}} =~ - \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^\text{T}\eta(\mathbf{x}_i) [1- \eta(\mathbf{x}_i)].\]</span>
This leads to the update</p>
<p><span class="math display">\[\boldsymbol \beta^{\,\text{new}} = \boldsymbol \beta^{\,\text{old}} - \left[\frac{\partial^2 \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta\partial \boldsymbol \beta^\text{T}}\right]^{-1} \frac{\partial \ell(\boldsymbol \beta)}{\partial \boldsymbol \beta}\]</span></p>
</div>
<div id="example-south-africa-heart-data" class="section level2 hasAnchor" number="11.5">
<h2 class="hasAnchor"><span class="header-section-number">11.5</span> Example: South Africa Heart Data<a href="#example-south-africa-heart-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use the South Africa heart data as a demonstration. The goal is to estimate the probability of <code>chd</code>, the indicator of coronary heart disease.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data</span>(SAheart)</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb128-4"><a href="#cb128-4" aria-hidden="true" tabindex="-1"></a>    heart <span class="ot">=</span> SAheart</span>
<span id="cb128-5"><a href="#cb128-5" aria-hidden="true" tabindex="-1"></a>    heart<span class="sc">$</span>famhist <span class="ot">=</span> <span class="fu">as.numeric</span>(heart<span class="sc">$</span>famhist)<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb128-6"><a href="#cb128-6" aria-hidden="true" tabindex="-1"></a>    n <span class="ot">=</span> <span class="fu">nrow</span>(heart)</span>
<span id="cb128-7"><a href="#cb128-7" aria-hidden="true" tabindex="-1"></a>    p <span class="ot">=</span> <span class="fu">ncol</span>(heart)</span>
<span id="cb128-8"><a href="#cb128-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb128-9"><a href="#cb128-9" aria-hidden="true" tabindex="-1"></a>    heart.full <span class="ot">=</span> <span class="fu">glm</span>(chd<span class="sc">~</span>., <span class="at">data=</span>heart, <span class="at">family=</span>binomial)</span>
<span id="cb128-10"><a href="#cb128-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">summary</span>(heart.full)<span class="sc">$</span>coef, <span class="at">dig=</span><span class="dv">3</span>)</span>
<span id="cb128-11"><a href="#cb128-11" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error z value Pr(&gt;|z|)</span></span>
<span id="cb128-12"><a href="#cb128-12" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)   -6.151      1.308  -4.701    0.000</span></span>
<span id="cb128-13"><a href="#cb128-13" aria-hidden="true" tabindex="-1"></a><span class="do">## sbp            0.007      0.006   1.135    0.256</span></span>
<span id="cb128-14"><a href="#cb128-14" aria-hidden="true" tabindex="-1"></a><span class="do">## tobacco        0.079      0.027   2.984    0.003</span></span>
<span id="cb128-15"><a href="#cb128-15" aria-hidden="true" tabindex="-1"></a><span class="do">## ldl            0.174      0.060   2.915    0.004</span></span>
<span id="cb128-16"><a href="#cb128-16" aria-hidden="true" tabindex="-1"></a><span class="do">## adiposity      0.019      0.029   0.635    0.526</span></span>
<span id="cb128-17"><a href="#cb128-17" aria-hidden="true" tabindex="-1"></a><span class="do">## famhist        0.925      0.228   4.061    0.000</span></span>
<span id="cb128-18"><a href="#cb128-18" aria-hidden="true" tabindex="-1"></a><span class="do">## typea          0.040      0.012   3.214    0.001</span></span>
<span id="cb128-19"><a href="#cb128-19" aria-hidden="true" tabindex="-1"></a><span class="do">## obesity       -0.063      0.044  -1.422    0.155</span></span>
<span id="cb128-20"><a href="#cb128-20" aria-hidden="true" tabindex="-1"></a><span class="do">## alcohol        0.000      0.004   0.027    0.978</span></span>
<span id="cb128-21"><a href="#cb128-21" aria-hidden="true" tabindex="-1"></a><span class="do">## age            0.045      0.012   3.728    0.000</span></span>
<span id="cb128-22"><a href="#cb128-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb128-23"><a href="#cb128-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fitted value </span></span>
<span id="cb128-24"><a href="#cb128-24" aria-hidden="true" tabindex="-1"></a>    yhat <span class="ot">=</span> (heart.full<span class="sc">$</span>fitted.values<span class="sc">&gt;</span><span class="fl">0.5</span>)</span>
<span id="cb128-25"><a href="#cb128-25" aria-hidden="true" tabindex="-1"></a>    <span class="fu">table</span>(yhat, SAheart<span class="sc">$</span>chd)</span>
<span id="cb128-26"><a href="#cb128-26" aria-hidden="true" tabindex="-1"></a><span class="do">##        </span></span>
<span id="cb128-27"><a href="#cb128-27" aria-hidden="true" tabindex="-1"></a><span class="do">## yhat      0   1</span></span>
<span id="cb128-28"><a href="#cb128-28" aria-hidden="true" tabindex="-1"></a><span class="do">##   FALSE 256  77</span></span>
<span id="cb128-29"><a href="#cb128-29" aria-hidden="true" tabindex="-1"></a><span class="do">##   TRUE   46  83</span></span></code></pre></div>
<p>Based on what we learned in class, we can solve this problem ourselves using numerical optimization. Here we will demonstrate an approach that uses general solver <code>optim()</code>. First, write the objective function of the logistic regression, for any value of <span class="math inline">\(\boldsymbol \beta\)</span>, <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the negative log-likelihood function of logistic regression </span></span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>    my.loglik <span class="ot">&lt;-</span> <span class="cf">function</span>(b, x, y)</span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a>        bm <span class="ot">=</span> <span class="fu">as.matrix</span>(b)</span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>        xb <span class="ot">=</span>  x <span class="sc">%*%</span> bm</span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this returns the negative loglikelihood</span></span>
<span id="cb129-7"><a href="#cb129-7" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span>(<span class="fu">sum</span>(y<span class="sc">*</span>xb) <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">log</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(xb))))</span>
<span id="cb129-8"><a href="#cb129-8" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb129-9"><a href="#cb129-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-10"><a href="#cb129-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient</span></span>
<span id="cb129-11"><a href="#cb129-11" aria-hidden="true" tabindex="-1"></a>    my.gradient <span class="ot">&lt;-</span> <span class="cf">function</span>(b, x, y)</span>
<span id="cb129-12"><a href="#cb129-12" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb129-13"><a href="#cb129-13" aria-hidden="true" tabindex="-1"></a>        bm <span class="ot">=</span> <span class="fu">as.matrix</span>(b) </span>
<span id="cb129-14"><a href="#cb129-14" aria-hidden="true" tabindex="-1"></a>        expxb <span class="ot">=</span>  <span class="fu">exp</span>(x <span class="sc">%*%</span> bm)</span>
<span id="cb129-15"><a href="#cb129-15" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span>(<span class="fu">t</span>(x) <span class="sc">%*%</span> (y <span class="sc">-</span> expxb<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>expxb)))</span>
<span id="cb129-16"><a href="#cb129-16" aria-hidden="true" tabindex="-1"></a>    }</span></code></pre></div>
<p>Let’s check the result of this function for some arbitrary <span class="math inline">\(\boldsymbol \beta\)</span> value.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># prepare the data matrix, I am adding a column of 1 for intercept</span></span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(<span class="st">&quot;intercept&quot;</span> <span class="ot">=</span> <span class="dv">1</span>, heart[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>]))</span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">as.matrix</span>(heart[,<span class="dv">10</span>])</span>
<span id="cb130-5"><a href="#cb130-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb130-6"><a href="#cb130-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># check my function</span></span>
<span id="cb130-7"><a href="#cb130-7" aria-hidden="true" tabindex="-1"></a>    b <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">ncol</span>(x))</span>
<span id="cb130-8"><a href="#cb130-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">my.loglik</span>(b, x, y) <span class="co"># scalar</span></span>
<span id="cb130-9"><a href="#cb130-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -320.234</span></span>
<span id="cb130-10"><a href="#cb130-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb130-11"><a href="#cb130-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># check the optimal value and the likelihood</span></span>
<span id="cb130-12"><a href="#cb130-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">my.loglik</span>(heart.full<span class="sc">$</span>coefficients, x, y)</span>
<span id="cb130-13"><a href="#cb130-13" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -236.07</span></span></code></pre></div>
<p>Then we optimize this objective function</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use a general solver to get the optimal value</span></span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Note that we are doing maximization instead of minimization, </span></span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we need to specify &quot;fnscale&quot; = -1</span></span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">optim</span>(b, <span class="at">fn =</span> my.loglik, <span class="at">gr =</span> my.gradient, </span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>          <span class="at">method =</span> <span class="st">&quot;BFGS&quot;</span>, <span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">control =</span> <span class="fu">list</span>(<span class="st">&quot;fnscale&quot;</span> <span class="ot">=</span> <span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a><span class="do">## $par</span></span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] -6.150733305  0.006504017  0.079376464  0.173923988  0.018586578  0.925372019  0.039595096</span></span>
<span id="cb131-8"><a href="#cb131-8" aria-hidden="true" tabindex="-1"></a><span class="do">##  [8] -0.062909867  0.000121675  0.045225500</span></span>
<span id="cb131-9"><a href="#cb131-9" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb131-10"><a href="#cb131-10" aria-hidden="true" tabindex="-1"></a><span class="do">## $value</span></span>
<span id="cb131-11"><a href="#cb131-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] -236.07</span></span>
<span id="cb131-12"><a href="#cb131-12" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb131-13"><a href="#cb131-13" aria-hidden="true" tabindex="-1"></a><span class="do">## $counts</span></span>
<span id="cb131-14"><a href="#cb131-14" aria-hidden="true" tabindex="-1"></a><span class="do">## function gradient </span></span>
<span id="cb131-15"><a href="#cb131-15" aria-hidden="true" tabindex="-1"></a><span class="do">##       74       16 </span></span>
<span id="cb131-16"><a href="#cb131-16" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb131-17"><a href="#cb131-17" aria-hidden="true" tabindex="-1"></a><span class="do">## $convergence</span></span>
<span id="cb131-18"><a href="#cb131-18" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0</span></span>
<span id="cb131-19"><a href="#cb131-19" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb131-20"><a href="#cb131-20" aria-hidden="true" tabindex="-1"></a><span class="do">## $message</span></span>
<span id="cb131-21"><a href="#cb131-21" aria-hidden="true" tabindex="-1"></a><span class="do">## NULL</span></span></code></pre></div>
<p>This matches our <code>glm()</code> solution. Now, if we do not have a general solver, we should consider using the Newton-Raphson. You need to write a function to calculate the Hessian matrix and proceed with an optimization update.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># my Newton-Raphson method</span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set up an initial value</span></span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this is sometimes crucial...</span></span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a>    b <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">ncol</span>(x))</span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a>    mybeta <span class="ot">=</span> <span class="fu">my.logistic</span>(b, x, y, <span class="at">tol =</span> <span class="fl">1e-10</span>, <span class="at">maxitr =</span> <span class="dv">20</span>, </span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a>                         <span class="at">gr =</span> my.gradient, <span class="at">hess =</span> my.hessian, <span class="at">verbose =</span> <span class="cn">TRUE</span>)</span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true" tabindex="-1"></a><span class="do">## at iteration 1, current beta is </span></span>
<span id="cb132-10"><a href="#cb132-10" aria-hidden="true" tabindex="-1"></a><span class="do">## -4.032 0.005 0.066 0.133 0.009 0.694 0.024 -0.045 -0.001 0.027</span></span>
<span id="cb132-11"><a href="#cb132-11" aria-hidden="true" tabindex="-1"></a><span class="do">## at iteration 2, current beta is </span></span>
<span id="cb132-12"><a href="#cb132-12" aria-hidden="true" tabindex="-1"></a><span class="do">## -5.684 0.006 0.077 0.167 0.017 0.884 0.037 -0.061 0 0.041</span></span>
<span id="cb132-13"><a href="#cb132-13" aria-hidden="true" tabindex="-1"></a><span class="do">## at iteration 3, current beta is </span></span>
<span id="cb132-14"><a href="#cb132-14" aria-hidden="true" tabindex="-1"></a><span class="do">## -6.127 0.007 0.079 0.174 0.019 0.924 0.039 -0.063 0 0.045</span></span>
<span id="cb132-15"><a href="#cb132-15" aria-hidden="true" tabindex="-1"></a><span class="do">## at iteration 4, current beta is </span></span>
<span id="cb132-16"><a href="#cb132-16" aria-hidden="true" tabindex="-1"></a><span class="do">## -6.151 0.007 0.079 0.174 0.019 0.925 0.04 -0.063 0 0.045</span></span>
<span id="cb132-17"><a href="#cb132-17" aria-hidden="true" tabindex="-1"></a><span class="do">## at iteration 5, current beta is </span></span>
<span id="cb132-18"><a href="#cb132-18" aria-hidden="true" tabindex="-1"></a><span class="do">## -6.151 0.007 0.079 0.174 0.019 0.925 0.04 -0.063 0 0.045</span></span>
<span id="cb132-19"><a href="#cb132-19" aria-hidden="true" tabindex="-1"></a><span class="do">## at iteration 6, current beta is </span></span>
<span id="cb132-20"><a href="#cb132-20" aria-hidden="true" tabindex="-1"></a><span class="do">## -6.151 0.007 0.079 0.174 0.019 0.925 0.04 -0.063 0 0.045</span></span>
<span id="cb132-21"><a href="#cb132-21" aria-hidden="true" tabindex="-1"></a><span class="do">## at iteration 7, current beta is </span></span>
<span id="cb132-22"><a href="#cb132-22" aria-hidden="true" tabindex="-1"></a><span class="do">## -6.151 0.007 0.079 0.174 0.019 0.925 0.04 -0.063 0 0.045</span></span>
<span id="cb132-23"><a href="#cb132-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb132-24"><a href="#cb132-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the parameter value</span></span>
<span id="cb132-25"><a href="#cb132-25" aria-hidden="true" tabindex="-1"></a>    mybeta</span>
<span id="cb132-26"><a href="#cb132-26" aria-hidden="true" tabindex="-1"></a><span class="do">##                    [,1]</span></span>
<span id="cb132-27"><a href="#cb132-27" aria-hidden="true" tabindex="-1"></a><span class="do">## intercept -6.1507208650</span></span>
<span id="cb132-28"><a href="#cb132-28" aria-hidden="true" tabindex="-1"></a><span class="do">## sbp        0.0065040171</span></span>
<span id="cb132-29"><a href="#cb132-29" aria-hidden="true" tabindex="-1"></a><span class="do">## tobacco    0.0793764457</span></span>
<span id="cb132-30"><a href="#cb132-30" aria-hidden="true" tabindex="-1"></a><span class="do">## ldl        0.1739238981</span></span>
<span id="cb132-31"><a href="#cb132-31" aria-hidden="true" tabindex="-1"></a><span class="do">## adiposity  0.0185865682</span></span>
<span id="cb132-32"><a href="#cb132-32" aria-hidden="true" tabindex="-1"></a><span class="do">## famhist    0.9253704194</span></span>
<span id="cb132-33"><a href="#cb132-33" aria-hidden="true" tabindex="-1"></a><span class="do">## typea      0.0395950250</span></span>
<span id="cb132-34"><a href="#cb132-34" aria-hidden="true" tabindex="-1"></a><span class="do">## obesity   -0.0629098693</span></span>
<span id="cb132-35"><a href="#cb132-35" aria-hidden="true" tabindex="-1"></a><span class="do">## alcohol    0.0001216624</span></span>
<span id="cb132-36"><a href="#cb132-36" aria-hidden="true" tabindex="-1"></a><span class="do">## age        0.0452253496</span></span>
<span id="cb132-37"><a href="#cb132-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get the standard error estimation </span></span>
<span id="cb132-38"><a href="#cb132-38" aria-hidden="true" tabindex="-1"></a>    mysd <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(<span class="fu">solve</span>(<span class="sc">-</span><span class="fu">my.hessian</span>(mybeta, x, y))))    </span></code></pre></div>
<p>With this solution, I can then get the standard errors and the p-value. You can check them with the <code>glm()</code> function solution.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># my summary matrix</span></span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">data.frame</span>(<span class="st">&quot;beta&quot;</span> <span class="ot">=</span> mybeta, <span class="st">&quot;sd&quot;</span> <span class="ot">=</span> mysd, <span class="st">&quot;z&quot;</span> <span class="ot">=</span> mybeta<span class="sc">/</span>mysd, </span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>          <span class="st">&quot;pvalue&quot;</span> <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(<span class="fu">abs</span>(mybeta<span class="sc">/</span>mysd)))), <span class="at">dig=</span><span class="dv">5</span>)</span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a><span class="do">##               beta      sd        z  pvalue</span></span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a><span class="do">## intercept -6.15072 1.30826 -4.70145 0.00000</span></span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a><span class="do">## sbp        0.00650 0.00573  1.13500 0.25637</span></span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a><span class="do">## tobacco    0.07938 0.02660  2.98376 0.00285</span></span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a><span class="do">## ldl        0.17392 0.05966  2.91517 0.00355</span></span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true" tabindex="-1"></a><span class="do">## adiposity  0.01859 0.02929  0.63458 0.52570</span></span>
<span id="cb133-10"><a href="#cb133-10" aria-hidden="true" tabindex="-1"></a><span class="do">## famhist    0.92537 0.22789  4.06053 0.00005</span></span>
<span id="cb133-11"><a href="#cb133-11" aria-hidden="true" tabindex="-1"></a><span class="do">## typea      0.03960 0.01232  3.21382 0.00131</span></span>
<span id="cb133-12"><a href="#cb133-12" aria-hidden="true" tabindex="-1"></a><span class="do">## obesity   -0.06291 0.04425 -1.42176 0.15509</span></span>
<span id="cb133-13"><a href="#cb133-13" aria-hidden="true" tabindex="-1"></a><span class="do">## alcohol    0.00012 0.00448  0.02714 0.97835</span></span>
<span id="cb133-14"><a href="#cb133-14" aria-hidden="true" tabindex="-1"></a><span class="do">## age        0.04523 0.01213  3.72846 0.00019</span></span>
<span id="cb133-15"><a href="#cb133-15" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb133-16"><a href="#cb133-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># check that with the glm fitting </span></span>
<span id="cb133-17"><a href="#cb133-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">summary</span>(heart.full)<span class="sc">$</span>coef, <span class="at">dig=</span><span class="dv">5</span>)</span>
<span id="cb133-18"><a href="#cb133-18" aria-hidden="true" tabindex="-1"></a><span class="do">##             Estimate Std. Error  z value Pr(&gt;|z|)</span></span>
<span id="cb133-19"><a href="#cb133-19" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) -6.15072    1.30826 -4.70145  0.00000</span></span>
<span id="cb133-20"><a href="#cb133-20" aria-hidden="true" tabindex="-1"></a><span class="do">## sbp          0.00650    0.00573  1.13500  0.25637</span></span>
<span id="cb133-21"><a href="#cb133-21" aria-hidden="true" tabindex="-1"></a><span class="do">## tobacco      0.07938    0.02660  2.98376  0.00285</span></span>
<span id="cb133-22"><a href="#cb133-22" aria-hidden="true" tabindex="-1"></a><span class="do">## ldl          0.17392    0.05966  2.91517  0.00355</span></span>
<span id="cb133-23"><a href="#cb133-23" aria-hidden="true" tabindex="-1"></a><span class="do">## adiposity    0.01859    0.02929  0.63458  0.52570</span></span>
<span id="cb133-24"><a href="#cb133-24" aria-hidden="true" tabindex="-1"></a><span class="do">## famhist      0.92537    0.22789  4.06053  0.00005</span></span>
<span id="cb133-25"><a href="#cb133-25" aria-hidden="true" tabindex="-1"></a><span class="do">## typea        0.03960    0.01232  3.21382  0.00131</span></span>
<span id="cb133-26"><a href="#cb133-26" aria-hidden="true" tabindex="-1"></a><span class="do">## obesity     -0.06291    0.04425 -1.42176  0.15509</span></span>
<span id="cb133-27"><a href="#cb133-27" aria-hidden="true" tabindex="-1"></a><span class="do">## alcohol      0.00012    0.00448  0.02714  0.97835</span></span>
<span id="cb133-28"><a href="#cb133-28" aria-hidden="true" tabindex="-1"></a><span class="do">## age          0.04523    0.01213  3.72846  0.00019</span></span></code></pre></div>
</div>
<div id="penalized-logistic-regression" class="section level2 hasAnchor" number="11.6">
<h2 class="hasAnchor"><span class="header-section-number">11.6</span> Penalized Logistic Regression<a href="#penalized-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Similar to a linear regression, we can also apply penalties to a logistic regression to address collinearity problems or select variables in a high-dimensional setting. For example, if we use the Lasso penalty, the objective function is</p>
<p><span class="math display">\[\sum_{i=1}^n \log \, p(y_i | x_i, \boldsymbol \beta) + \lambda |\boldsymbol \beta|_1\]</span>
This can be done using the <code>glmnet</code> package. Specifying <code>family = "binomial"</code> will ensure that a logistic regression is used, even your <code>y</code> is not a factor (but as numerical 0/1).</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(glmnet)</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>  lasso.fit <span class="ot">=</span> <span class="fu">cv.glmnet</span>(<span class="at">x =</span> <span class="fu">data.matrix</span>(SAheart[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>]), <span class="at">y =</span> SAheart[,<span class="dv">10</span>], </span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">nfold =</span> <span class="dv">10</span>, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(lasso.fit)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-180-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>The procedure is essentially the same as in a linear regression. And we could obtain the estimated parameters by selecting the best <span class="math inline">\(\lambda\)</span> value.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>(lasso.fit, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>)</span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 10 x 1 sparse Matrix of class &quot;dgCMatrix&quot;</span></span>
<span id="cb135-3"><a href="#cb135-3" aria-hidden="true" tabindex="-1"></a><span class="do">##                       s1</span></span>
<span id="cb135-4"><a href="#cb135-4" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) -6.319899679</span></span>
<span id="cb135-5"><a href="#cb135-5" aria-hidden="true" tabindex="-1"></a><span class="do">## sbp          0.003381401</span></span>
<span id="cb135-6"><a href="#cb135-6" aria-hidden="true" tabindex="-1"></a><span class="do">## tobacco      0.067870752</span></span>
<span id="cb135-7"><a href="#cb135-7" aria-hidden="true" tabindex="-1"></a><span class="do">## ldl          0.137863725</span></span>
<span id="cb135-8"><a href="#cb135-8" aria-hidden="true" tabindex="-1"></a><span class="do">## adiposity    .          </span></span>
<span id="cb135-9"><a href="#cb135-9" aria-hidden="true" tabindex="-1"></a><span class="do">## famhist      0.777513058</span></span>
<span id="cb135-10"><a href="#cb135-10" aria-hidden="true" tabindex="-1"></a><span class="do">## typea        0.026945871</span></span>
<span id="cb135-11"><a href="#cb135-11" aria-hidden="true" tabindex="-1"></a><span class="do">## obesity     -0.008357962</span></span>
<span id="cb135-12"><a href="#cb135-12" aria-hidden="true" tabindex="-1"></a><span class="do">## alcohol      .          </span></span>
<span id="cb135-13"><a href="#cb135-13" aria-hidden="true" tabindex="-1"></a><span class="do">## age          0.042535589</span></span></code></pre></div>
<!--chapter:end:04.1-logistic.Rmd-->
</div>
</div>
<div id="discriminant-analysis" class="section level1 hasAnchor" number="12">
<h1 class="hasAnchor"><span class="header-section-number">12</span> Discriminant Analysis<a href="#discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>When we model the probability of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, such as using a logistic regression, the approach is often called a soft classification, meaning that we do not directly produce the class label for prediction. However, we can also view the task as finding a function, with 0/1 as the output. In this case, the function is called a <strong>classifier</strong>:</p>
<p><span class="math display">\[f : \mathbb{R}^p \longrightarrow \{0, 1\}\]</span>
In this case, we can directly evaluate the prediction error, which is calculated from the <strong>0-1 loss</strong>:</p>
<p><span class="math display">\[L\big(f(\mathbf{x}), y \big) = \begin{cases}
0 \quad \text{if} \quad y = f(\mathbf{x})\\
1 \quad \text{o.w.}
\end{cases}\]</span></p>
<p>The goal is to minimize the overall <strong>risk</strong>, the integrated loss:</p>
<p><span class="math display">\[\text{R}(f) = \text{E}_{X, Y} \left[ L\big(f(X), Y\big) \right]\]</span>
Continuing the notation from the logistic regression, with <span class="math inline">\(\eta(\mathbf{x}) = \text{P}(Y = 1 | X = \mathbf{x})\)</span>, we can easily see the decision rule to minimize the risk is to take the dominate label for any given <span class="math inline">\(\mathbf{x}\)</span>, this leads to the <strong>Bayes rule</strong>:</p>
<p><span class="math display">\[\begin{align}
f_B(\mathbf{x}) = \underset{f}{\arg\min} \,\, \text{R}(f) =
    \begin{cases}
    1 &amp; \text{if} \quad \eta(\mathbf{x}) \geq 1/2 \\
    0 &amp; \text{if} \quad \eta(\mathbf{x}) &lt; 1/2. \\
    \end{cases}
\end{align}\]</span></p>
<p>Note that it doesn’t matter when <span class="math inline">\(\eta(\mathbf{x}) = 1/2\)</span> since we will make 50% mistake anyway. The risk associated with this rule is called the <strong>Bayes risk</strong>, which is the best risk we could achieve with a classification model with 0/1 loss.</p>
<div id="bayes-rule" class="section level2 hasAnchor" number="12.1">
<h2 class="hasAnchor"><span class="header-section-number">12.1</span> Bayes Rule<a href="#bayes-rule" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The essential idea of Discriminant Analysis is to estimate the densities functions of each class, and compare the densities at any given target point to perform classification. Let’s construct the Bayes rule from the Bayes prospective:</p>
<p><span class="math display">\[\begin{align}
  \text{P}(Y = 1 | X = \mathbf{x}) &amp;= \frac{\text{P}(X = \mathbf{x}| Y = 1)\text{P}(Y = 1)}{\text{P}(X = \mathbf{x})} \\
  \text{P}(Y = 0 | X = \mathbf{x}) &amp;= \frac{\text{P}(X = \mathbf{x}| Y = 0)\text{P}(Y = 0)}{\text{P}(X = \mathbf{x})}
\end{align}\]</span></p>
<p>Lets further define marginal probabilities (<strong>prior</strong>) <span class="math inline">\(\pi_1 = P(Y = 1)\)</span> and <span class="math inline">\(\pi_0 = 1 - \pi_1 = P(Y = 0)\)</span>, then, denote the conditional densities of <span class="math inline">\(X\)</span> as</p>
<p><span class="math display">\[\begin{align}
  f_1 = \text{P}(X = \mathbf{x}| Y = 1)\\
  f_0 = \text{P}(X = \mathbf{x}| Y = 0)\\
\end{align}\]</span></p>
<p>Note that the Bayes rule suggests to make the decision 1 when <span class="math inline">\(\eta(\mathbf{x}) \geq 1/2\)</span>, this is equivalent to <span class="math inline">\(\pi_1 &gt; \pi_0\)</span>. Utilizing the <strong>Bayes Theorem</strong>, we have</p>
<p><span class="math display">\[\begin{align}
f_B(\mathbf{x}) = \underset{f}{\arg\min} \,\, \text{R}(f) =
    \begin{cases}
    1 &amp; \text{if} \quad \pi_1 f_1(\mathbf{x}) \geq \pi_0 f_0(\mathbf{x}) \\
    0 &amp; \text{if} \quad \pi_1 f_1(\mathbf{x}) &lt; \pi_0 f_0(\mathbf{x}). \\
    \end{cases}
\end{align}\]</span></p>
<p>This suggests that we can model the conditional density of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> instead of modeling <span class="math inline">\(P(Y | X)\)</span> to make the decision.</p>
</div>
<div id="example-linear-discriminant-analysis-lda" class="section level2 hasAnchor" number="12.2">
<h2 class="hasAnchor"><span class="header-section-number">12.2</span> Example: Linear Discriminant Analysis (LDA)<a href="#example-linear-discriminant-analysis-lda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We create two density functions that use the <strong>same covariance matrix</strong>: <span class="math inline">\(X_1 \sim \cal{N}(\mu_1, \Sigma)\)</span> and <span class="math inline">\(X_2 \sim \cal{N}(\mu_2, \Sigma)\)</span>, with <span class="math inline">\(\mu_1 = (0.5, -1)^\text{T}\)</span>, <span class="math inline">\(\mu_2 = (-0.5, 0.5)^\text{T}\)</span>, and <span class="math inline">\(\Sigma_{2\times2}\)</span> has diagonal elements 1 and off diagonal elements 0.5. Let’s first generate some observations.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(mvtnorm)</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate two sets of samples</span></span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a>  mu1 <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb136-7"><a href="#cb136-7" aria-hidden="true" tabindex="-1"></a>  mu2 <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb136-8"><a href="#cb136-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb136-9"><a href="#cb136-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># define prior</span></span>
<span id="cb136-10"><a href="#cb136-10" aria-hidden="true" tabindex="-1"></a>  p1 <span class="ot">=</span> <span class="fl">0.4</span> </span>
<span id="cb136-11"><a href="#cb136-11" aria-hidden="true" tabindex="-1"></a>  p2 <span class="ot">=</span> <span class="fl">0.6</span></span>
<span id="cb136-12"><a href="#cb136-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb136-13"><a href="#cb136-13" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb136-14"><a href="#cb136-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb136-15"><a href="#cb136-15" aria-hidden="true" tabindex="-1"></a>  Class1 <span class="ot">=</span> <span class="fu">rmvnorm</span>(n<span class="sc">*</span>p1, <span class="at">mean =</span> mu1, <span class="at">sigma =</span> Sigma)</span>
<span id="cb136-16"><a href="#cb136-16" aria-hidden="true" tabindex="-1"></a>  Class2 <span class="ot">=</span> <span class="fu">rmvnorm</span>(n<span class="sc">*</span>p2, <span class="at">mean =</span> mu2, <span class="at">sigma =</span> Sigma)</span>
<span id="cb136-17"><a href="#cb136-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-18"><a href="#cb136-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(Class1, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb136-19"><a href="#cb136-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(Class2, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-184-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>If we know their true density functions, then the decision line is linear.</p>
<div id="htmlwidget-7455d817246b0e0deed0" style="width:75%;height:576px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-7455d817246b0e0deed0">{"x":{"visdat":{"46f4453d7f00":["function () ","plotlyVisDat"]},"cur_data":"46f4453d7f00","attrs":{"46f4453d7f00":{"x":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"y":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[0.00444359861529692,0.00441407315209244,0.00432666864755998,0.00418482357737072,0.00399401855396117,0.00376142502059808,0.00349545847418932,0.00320527496896476,0.00290025272701336,0.00258949932731576,0.00228141959920296,0.00198337091696734,0.00170142229792605,0.00144022288151579,0.00120297527072305,0.000991500870666543,0.000806378438850811,0.000647133855449412,0.000512458548153684,0.000400435682027572,0.000308756567639692,0.000234914079018396,0.000176364549958642,0.000130654072091693,9.55089371747855e-05,6.88929023843241e-05,4.90359233661433e-05,3.44400394865361e-05,2.38683463215505e-05,1.63226409510241e-05,1.10145797398989e-05,7.334235350181e-06,4.8189360126116e-06,3.1243298934251e-06,1.99881231051827e-06,1.26181752206809e-06,7.86014393869231e-07,4.83140952598617e-07,2.93039800713828e-07,1.75383517583013e-07,1.03576289562571e-07,6.03589017299704e-08,3.47081699703865e-08,1.9693890060127e-08,1.10265768759837e-08,6.09199189788505e-09,3.32114113225121e-09,1.78658939856142e-09,9.48356402924291e-10,4.96738493394313e-10,2.56739953204248e-10],[0.00595837552163676,0.00595837552163676,0.00587945780196071,0.00572474427569231,0.0055002738419329,0.00521461128137509,0.00487830527795108,0.00450324334474084,0.00410195851856161,0.00368694381493308,0.00327002581849207,0.00286183934892583,0.00247143234775675,0.0021060157249911,0.0017708586591326,0.00146931731365961,0.00120297527072305,0.000971867838009839,0.000774759888007996,0.000609447713543893,0.00047305886264623,0.000362329188615297,0.000273842509810286,0.000204224489205769,0.000150287955841078,0.000109131454417772,7.819610405879e-05,5.52878492096141e-05,3.85730235129327e-05,2.65550470612311e-05,1.80393080852625e-05,1.20921098261642e-05,7.99822647189894e-06,5.22029106241527e-06,3.36205757285024e-06,2.13660864376199e-06,1.33984396000321e-06,8.29073190061756e-07,5.06221897147374e-07,3.04998981850144e-07,1.81328161819899e-07,1.06375480622347e-07,6.15782324274592e-08,3.51740444983538e-08,1.982562127669e-08,1.10265768759837e-08,6.05151369582517e-09,3.2771531553391e-09,1.75121255821382e-09,9.23401114835194e-10,4.80453468540155e-10],[0.00788370428577923,0.00793643789774812,0.00788370428577923,0.00772759648166358,0.00747425573509311,0.0071334706305035,0.00671804964117307,0.00624302316799314,0.00572474427569231,0.00517996282933357,0.00462494531498742,0.00407470308084292,0.00354237668013568,0.00303880561103103,0.00257229341523282,0.00214856006475334,0.0017708586591326,0.0014402228815158,0.00115580593567211,0.000915270661168869,0.000715193517144861,0.000551451095550976,0.000419565573051116,0.000314993864048076,0.000233353193889754,0.000170582632274262,0.000123045371264316,8.75800436192475e-05,6.15112340795035e-05,4.26297838087414e-05,2.9152864029864e-05,1.96724627140205e-05,1.3099226195589e-05,8.60680491384983e-06,5.58017325999851e-06,3.56995560244198e-06,2.25365458700744e-06,1.40385191457317e-06,8.62908308810329e-07,5.23380345814241e-07,3.13241702523392e-07,1.84991233673249e-07,1.07803318127594e-07,6.1990125428116e-08,3.51740444983537e-08,1.9693890060127e-08,1.08805316494408e-08,5.93168569614551e-09,3.19091732580136e-09,1.69380098167069e-09,8.8719403923514e-10],[0.010293005090544,0.0104311641721669,0.0104311641721669,0.010293005090544,0.010022152374682,0.00962917815916989,0.00912907657003718,0.00854031489815827,0.00788370428577922,0.00718118597580114,0.00645463114694013,0.0057247442756923,0.00501014344842975,0.00432666864755998,0.00368694381493307,0.00310019355645468,0.00257229341523282,0.0021060157249911,0.00170142229792605,0.00135635083026808,0.00106694335246963,0.000828171141856104,0.000634319746563399,0.000479408551336933,0.000357530197139557,0.000263104991416689,0.000191053436166511,0.000136895769006016,9.67909172117888e-05,6.75287315173662e-05,4.64891906341662e-05,3.15809205892381e-05,2.11693241432259e-05,1.40022751354938e-05,9.13902000651191e-06,5.88586173720116e-06,3.74050199652905e-06,2.34562797546544e-06,1.45143563457388e-06,8.86228754383629e-07,5.33953330143356e-07,3.17446226427337e-07,1.86228628631323e-07,1.07803318127594e-07,6.1578232427459e-08,3.47081699703865e-08,1.93039249091855e-08,1.05942186124464e-08,5.73722190831072e-09,3.06579966783812e-09,1.61657293848473e-09],[0.0132606085907303,0.0135284906538333,0.0136189818938397,0.0135284906538333,0.0132606085907303,0.0128258741311443,0.0122410845531381,0.0115282192847515,0.0107130705970553,0.00982370046134149,0.0088888517609989,0.00793643789774811,0.00699221844809223,0.00607874269155644,0.00521461128137509,0.00441407315209244,0.00368694381493307,0.00303880561103102,0.00247143234775675,0.00198337091696734,0.00157061073532225,0.0012272769832107,0.00094629386412483,0.000719977402492236,0.000540531632252621,0.000400435682027572,0.000292720967552189,0.00021114670144965,0.000150287955841077,0.000105553699781887,7.31530015492573e-05,5.00265147275886e-05,3.3758081013335e-05,2.24783620409423e-05,1.47693362936527e-05,9.57561560633791e-06,6.12606831148571e-06,3.86728673629915e-06,2.40901951889891e-06,1.48075657935485e-06,8.98124264949923e-07,5.37524911052607e-07,3.17446226427338e-07,1.84991233673249e-07,1.06375480622347e-07,6.03589017299704e-08,3.3794850486292e-08,1.86710669746599e-08,1.01788133589094e-08,5.47563602773968e-09,2.90657410772236e-09],[0.0168575374037106,0.0173131191607864,0.0175455065561331,0.0175455065561331,0.0173131191607864,0.0168575374037106,0.0161965439076004,0.0153553592070208,0.0143650457958376,0.0132606085907303,0.0120789533689274,0.0108568680577609,0.00962917815916989,0.00842719980921658,0.00727757629628117,0.00620154143960373,0.00521461128137509,0.00432666864755998,0.00354237668013568,0.00286183934892583,0.00228141959920296,0.00179462821951634,0.00139300675928467,0.00106694335246963,0.00080637843885081,0.00060137567719103,0.00044254987033666,0.000321357162202053,0.00023026246859491,0.000162804998989945,0.000113585193561303,7.819610405879e-05,5.31199816340433e-05,3.56073885343443e-05,2.35522139365917e-05,1.53720843421044e-05,9.9001821842771e-06,6.29162778191576e-06,3.94541111064009e-06,2.44135486911151e-06,1.49066126883147e-06,8.98124264949922e-07,5.33953330143356e-07,3.13241702523392e-07,1.81328161819899e-07,1.03576289562571e-07,5.83801015606385e-08,3.24697354449611e-08,1.78197686330893e-08,9.65016588223688e-09,5.1567598097414e-09],[0.0211462914953401,0.0218630474459658,0.0223047103014429,0.0224539051336535,0.0223047103014429,0.0218630474459658,0.0211462914953402,0.0201821364791977,0.0190068203480652,0.0176628673678771,0.0161965439076004,0.0146552389704592,0.0130849739756079,0.0115282192847515,0.0100221523746821,0.00859744053803597,0.00727757629628117,0.00607874269155644,0.00501014344842975,0.00407470308084292,0.00327002581849207,0.00258949932731576,0.00202343766726017,0.00156017482211562,0.00118704205791238,0.000891185999581158,0.000660206826506172,0.000482615285583216,0.000348122057918207,0.000247782949525226,0.000174028630031493,0.00012060890966982,8.24797789303687e-05,5.56576662247035e-05,3.70605536273999e-05,2.43505189014685e-05,1.57875211302174e-05,1.01001791309085e-05,6.37607790203289e-06,3.97180172234043e-06,2.44135486911152e-06,1.48075657935485e-06,8.8622875438363e-07,5.23380345814242e-07,3.04998981850144e-07,1.75383517583013e-07,9.9515005158301e-08,5.57182888368804e-08,3.07833852677884e-08,1.67820260953206e-08,9.02779758954641e-09],[0.0261748200130662,0.0272430346821459,0.0279792886978622,0.0283548440188743,0.0283548440188744,0.0279792886978622,0.0272430346821459,0.0261748200130662,0.0248154029509433,0.0232149827969817,0.0214301301009784,0.0195204873448052,0.0175455065561331,0.0155614683371894,0.0136189818938397,0.0117611047624258,0.0100221523746821,0.00842719980921658,0.00699221844809223,0.00572474427569231,0.00462494531498741,0.00368694381493308,0.00290025272701337,0.00225120256576163,0.00172425984051604,0.00130316755350706,0.000971867838009839,0.000715193517144861,0.000519337083796087,0.00037212128128198,0.00026310499141669,0.000183562123782546,0.000126370722120622,8.5845842563734e-05,5.75441891392905e-05,3.80621300578968e-05,2.4842432013625e-05,1.59994310050563e-05,1.01677386064797e-05,6.37607790203289e-06,3.94541111064011e-06,2.40901951889891e-06,1.45143563457388e-06,8.62908308810332e-07,5.06221897147374e-07,2.93039800713829e-07,1.67386990236189e-07,9.43465877295696e-08,5.2473508298577e-08,2.87980719409356e-08,1.55953860591983e-08],[0.0319699973583051,0.0334972902368784,0.0346326846918545,0.0353323113313541,0.0355686469837545,0.0353323113313541,0.0346326846918545,0.0334972902368784,0.0319699973583051,0.0301082096508339,0.0279792886978621,0.0256565238508506,0.0232149827969817,0.0207275668690903,0.0182615522622865,0.0158758308503845,0.0136189818938397,0.0115282192847515,0.00962917815916988,0.00793643789774811,0.00645463114694013,0.00517996282933357,0.00410195851856161,0.00320527496896476,0.00247143234775675,0.00188036244303331,0.00141170455772781,0.00104581645858419,0.000764498318653054,0.000551451095550974,0.000392506524268112,0.000275674225477117,0.000191053436166511,0.000130654072091693,8.81658611320587e-05,5.87066588706572e-05,3.85730235129327e-05,2.50086015099281e-05,1.59994310050563e-05,1.01001791309085e-05,6.29162778191576e-06,3.86728673629915e-06,2.34562797546544e-06,1.40385191457317e-06,8.29073190061753e-07,4.83140952598617e-07,2.77820467600072e-07,1.57639130787856e-07,8.82618918349101e-08,4.87631597727661e-08,2.65839709530959e-08],[0.0385310552921972,0.0406418319775229,0.0423004566051226,0.0434436435299832,0.0440267709878692,0.0440267709878692,0.0434436435299832,0.0423004566051226,0.0406418319775229,0.0385310552921972,0.03604607136649,0.0332747177014261,0.0303096016091382,0.0272430346821459,0.0241624042177895,0.0211462914953401,0.0182615522622865,0.0155614683371893,0.0130849739756079,0.0108568680577609,0.0088888517609989,0.00718118597580115,0.00572474427569231,0.00450324334474084,0.00349545847418932,0.00267727070095846,0.00202343766726017,0.00150902620751688,0.00111048613670333,0.000806378438850809,0.000577795399608218,0.000408525019399057,0.000285018234642431,0.00019621673244518,0.000133293459427897,8.93492778599579e-05,5.90993440928136e-05,3.85730235129327e-05,2.4842432013625e-05,1.57875211302174e-05,9.9001821842771e-06,6.12606831148569e-06,3.74050199652905e-06,2.25365458700744e-06,1.3398439600032e-06,7.86014393869231e-07,4.55005013879485e-07,2.59902988024618e-07,1.46492627809615e-07,8.14759951158149e-08,4.47149741818993e-08],[0.0458235375970751,0.0486571069125697,0.0509815879579283,0.0527096146688646,0.0537744195174696,0.0541341132946451,0.0537744195174696,0.0527096146688646,0.0509815879579283,0.0486571069125697,0.0458235375970751,0.0425834017517011,0.0390482429518072,0.0353323113313541,0.03154655991627,0.0277933804891206,0.0241624042177895,0.0207275668690903,0.0175455065561331,0.0146552389704592,0.0120789533689274,0.00982370046134149,0.00788370428577922,0.00624302316799313,0.00487830527795107,0.00376142502059809,0.00286183934892583,0.00214856006475334,0.00159169244419837,0.00116353705032503,0.000839287367232362,0.000597379840265559,0.000419565573051116,0.000290775984914268,0.000198850474808582,0.000134185051161005,8.93492778599579e-05,5.87066588706572e-05,3.80621300578966e-05,2.43505189014685e-05,1.53720843421044e-05,9.57561560633789e-06,5.88586173720116e-06,3.56995560244198e-06,2.13660864376198e-06,1.26181752206809e-06,7.35321955800059e-07,4.22832071051303e-07,2.39920696720254e-07,1.34331206198904e-07,7.42156545046393e-08],[0.0537744195174695,0.0574815799110812,0.0606304887014555,0.0631048658850247,0.0648103003735523,0.0656802243170994,0.0656802243170994,0.0648103003735523,0.0631048658850247,0.0606304887014555,0.0574815799110812,0.0537744195174695,0.0496400456740287,0.04521661225618,0.0406418319775229,0.03604607136649,0.03154655991627,0.0272430346821459,0.0232149827969817,0.0195204873448052,0.0161965439076004,0.0132606085907303,0.0107130705970553,0.00854031489815828,0.00671804964117306,0.00521461128137509,0.00399401855396118,0.00301861428615636,0.00225120256576162,0.00165665064512227,0.00120297527072305,0.000861969447319046,0.000609447713543893,0.000425197241717693,0.000292720967552189,0.000198850474808582,0.000133293459427898,8.81658611320587e-05,5.75441891392904e-05,3.70605536274e-05,2.35522139365918e-05,1.47693362936526e-05,9.13902000651192e-06,5.58017325999852e-06,3.36205757285024e-06,1.99881231051827e-06,1.17259568547565e-06,6.78787717256027e-07,3.87729696525888e-07,2.18541320188754e-07,1.21547901778672e-07],[0.062269052147199,0.0670070528615627,0.0711505376047041,0.074549590415764,0.0770764572452968,0.0786335049661667,0.0791594796334459,0.0786335049661668,0.0770764572452968,0.074549590415764,0.0711505376047041,0.0670070528615627,0.0622690521471989,0.0570996439129113,0.0516658943689313,0.046130048415225,0.0406418319775229,0.0353323113313541,0.0303096016091382,0.0256565238508507,0.0214301301009784,0.0176628673678772,0.0143650457958376,0.0115282192847515,0.00912907657003719,0.00713347063050351,0.00550027384193289,0.00418482357737072,0.00314180627216261,0.00232750733227017,0.00170142229792605,0.00122727698321071,0.000873539334459006,0.000613524271729785,0.000425197241717693,0.000290775984914269,0.000196216732445181,0.000130654072091693,8.58458425637342e-05,5.56576662247035e-05,3.56073885343443e-05,2.24783620409423e-05,1.40022751354938e-05,8.60680491384984e-06,5.22029106241527e-06,3.12432989342511e-06,1.84513631719825e-06,1.07524996423906e-06,6.18300821416875e-07,3.50832337500352e-07,1.96430452704769e-07],[0.0711505376047041,0.0770764572452968,0.0823900392819534,0.0869034628749644,0.0904500604748912,0.0928945098919035,0.0941413974686199,0.0941413974686199,0.0928945098919035,0.0904500604748912,0.0869034628749645,0.0823900392819534,0.0770764572452968,0.0711505376047041,0.0648103003735523,0.0582531332350027,0.0516658943689313,0.0452166122561799,0.0390482429518072,0.0332747177014261,0.0279792886978621,0.0232149827969817,0.0190068203480652,0.0153553592070208,0.0122410845531381,0.00962917815916989,0.00747425573509311,0.00572474427569231,0.00432666864755998,0.00322671485563984,0.00237452609930412,0.00172425984051604,0.00123548616329471,0.000873539334459005,0.000609447713543892,0.000419565573051116,0.000285018234642431,0.000191053436166511,0.000126370722120622,8.24797789303686e-05,5.31199816340433e-05,3.37580810133349e-05,2.11693241432259e-05,1.3099226195589e-05,7.99822647189894e-06,4.8189360126116e-06,2.86495647885114e-06,1.6807158873788e-06,9.72926401385327e-07,5.55744378929111e-07,3.13241702523392e-07],[0.0802220071374839,0.087484754780886,0.0941413974686199,0.0999627763750334,0.10473826743213,0.108288382895958,0.110475953339786,0.111214920181278,0.110475953339786,0.108288382895958,0.10473826743213,0.0999627763750334,0.0941413974686199,0.087484754780886,0.0802220071374839,0.0725878738079432,0.0648103003735523,0.0570996439129113,0.0496400456740287,0.0425834017517012,0.03604607136649,0.0301082096508339,0.0248154029509433,0.0201821364791977,0.0161965439076004,0.0128258741311443,0.0100221523746821,0.00772759648166358,0.00587945780196071,0.00441407315209244,0.00327002581849207,0.00239040915800238,0.00172425984051604,0.0012272769832107,0.000861969447319045,0.000597379840265559,0.000408525019399058,0.000275674225477117,0.000183562123782546,0.00012060890966982,7.819610405879e-05,5.00265147275886e-05,3.15809205892381e-05,1.96724627140206e-05,1.20921098261642e-05,7.334235350181e-06,4.38951975589341e-06,2.59231982310907e-06,1.51066984651126e-06,8.68680249293133e-07,4.92901047262913e-07],[0.0892520640593719,0.0979833807828676,0.106144129249152,0.113461610599908,0.119677171542053,0.124561289565839,0.127927608726522,0.129644732227423,0.129644732227423,0.127927608726522,0.124561289565839,0.119677171542053,0.113461610599908,0.106144129249152,0.0979833807828676,0.0892520640593719,0.0802220071374838,0.0711505376047041,0.0622690521471989,0.0537744195174695,0.0458235375970751,0.0385310552921972,0.0319699973583051,0.0261748200130662,0.0211462914953401,0.0168575374037106,0.0132606085907303,0.010293005090544,0.00788370428577922,0.00595837552163675,0.00444359861529692,0.00327002581849207,0.00237452609930412,0.00170142229792605,0.00120297527072305,0.000839287367232362,0.000577795399608218,0.000392506524268112,0.00026310499141669,0.000174028630031493,0.000113585193561303,7.3153001549257e-05,4.64891906341662e-05,2.9152864029864e-05,1.80393080852624e-05,1.10145797398989e-05,6.63629057752583e-06,3.94541111064009e-06,2.31456051246988e-06,1.3398439600032e-06,7.65331014096921e-07],[0.0979833807828676,0.108288382895958,0.118092066769606,0.12707759453053,0.134935634119589,0.141381872783512,0.146174027410271,0.149126938641066,0.15012443954056,0.149126938641066,0.146174027410271,0.141381872783512,0.134935634119589,0.12707759453053,0.118092066769606,0.108288382895958,0.0979833807828676,0.0874847547808859,0.0770764572452968,0.0670070528615627,0.0574815799110812,0.0486571069125697,0.0406418319775229,0.0334972902368784,0.0272430346821459,0.0218630474459658,0.0173131191607864,0.0135284906538333,0.0104311641721669,0.00793643789774811,0.00595837552163676,0.00441407315209245,0.00322671485563985,0.00232750733227017,0.00165665064512228,0.00116353705032503,0.00080637843885081,0.000551451095550975,0.00037212128128198,0.000247782949525226,0.000162804998989945,0.000105553699781887,6.75287315173662e-05,4.26297838087415e-05,2.65550470612311e-05,1.63226409510241e-05,9.90018218427707e-06,5.92523190352158e-06,3.49926574527763e-06,2.03919099765271e-06,1.17259568547565e-06],[0.106144129249152,0.118092066769606,0.129644732227423,0.140442461814151,0.15012443954056,0.158348433523511,0.164810755687758,0.1692648329271,0.171536810222608,0.171536810222608,0.1692648329271,0.164810755687758,0.158348433523511,0.15012443954056,0.140442461814151,0.129644732227423,0.118092066769606,0.106144129249152,0.0941413974686199,0.0823900392819534,0.0711505376047041,0.0606304887014555,0.0509815879579283,0.0423004566051226,0.0346326846918545,0.0279792886978622,0.0223047103014429,0.0175455065561331,0.0136189818938397,0.0104311641721669,0.00788370428577923,0.00587945780196071,0.00432666864755999,0.00314180627216261,0.00225120256576162,0.00159169244419837,0.00111048613670334,0.000764498318653055,0.000519337083796086,0.000348122057918207,0.000230262468594911,0.000150287955841077,9.67909172117889e-05,6.15112340795036e-05,3.85730235129327e-05,2.38683463215506e-05,1.45737188242281e-05,8.78067390645846e-06,5.22029106241526e-06,3.06246401650793e-06,1.7727874873605e-06],[0.113461610599908,0.12707759453053,0.140442461814151,0.153157154390045,0.164810755687758,0.17500208365302,0.183362404522089,0.189577494037488,0.193407213455959,0.194700902383989,0.193407213455959,0.189577494037488,0.183362404522089,0.17500208365302,0.164810755687758,0.153157154390045,0.140442461814151,0.12707759453053,0.113461610599908,0.0999627763750334,0.0869034628749645,0.074549590415764,0.0631048658850247,0.0527096146688645,0.0434436435299832,0.0353323113313541,0.0283548440188743,0.0224539051336535,0.0175455065561331,0.0135284906538333,0.010293005090544,0.00772759648166358,0.00572474427569231,0.00418482357737072,0.00301861428615636,0.00214856006475334,0.00150902620751688,0.00104581645858419,0.00071519351714486,0.000482615285583216,0.000321357162202053,0.00021114670144965,0.000136895769006016,8.75800436192475e-05,5.52878492096141e-05,3.44400394865361e-05,2.11693241432259e-05,1.28398441382614e-05,7.68461152614904e-06,4.5383030262895e-06,2.6446881573138e-06],[0.119677171542053,0.134935634119589,0.15012443954056,0.164810755687758,0.178537360250285,0.190845566208414,0.201300311571489,0.209515533131891,0.21517777503787,0.218066024244334,0.218066024244334,0.21517777503787,0.209515533131891,0.201300311571489,0.190845566208414,0.178537360250285,0.164810755687758,0.15012443954056,0.134935634119589,0.119677171542053,0.10473826743213,0.0904500604748913,0.0770764572452968,0.0648103003735523,0.0537744195174695,0.0440267709878692,0.0355686469837545,0.0283548440188743,0.0223047103014429,0.0173131191607864,0.0132606085907303,0.0100221523746821,0.00747425573509311,0.00550027384193289,0.00399401855396117,0.00286183934892583,0.00202343766726017,0.00141170455772781,0.000971867838009839,0.000660206826506171,0.00044254987033666,0.000292720967552189,0.000191053436166511,0.000123045371264316,7.81961040587898e-05,4.90359233661433e-05,3.03426149808481e-05,1.8526827672323e-05,1.11624242428346e-05,6.63629057752583e-06,3.89315477887558e-06],[0.124561289565839,0.141381872783512,0.158348433523511,0.17500208365302,0.190845566208414,0.205366847613037,0.218066024244334,0.228483625539526,0.236228103963181,0.241000228215217,0.242612263885053,0.241000228215217,0.236228103963181,0.228483625539526,0.218066024244334,0.205366847613037,0.190845566208414,0.17500208365302,0.158348433523511,0.141381872783512,0.124561289565839,0.108288382895958,0.0928945098919035,0.0786335049661667,0.0656802243170993,0.0541341132946451,0.0440267709878692,0.0353323113313541,0.0279792886978621,0.0218630474459658,0.0168575374037106,0.0128258741311443,0.00962917815916989,0.00713347063050349,0.00521461128137509,0.00376142502059809,0.00267727070095846,0.00188036244303331,0.00130316755350706,0.000891185999581157,0.00060137567719103,0.000400435682027572,0.000263104991416689,0.000170582632274262,0.000109131454417772,6.88929023843241e-05,4.29149318046831e-05,2.63786022171354e-05,1.59994310050563e-05,9.57561560633791e-06,5.65507379741364e-06],[0.127927608726522,0.146174027410271,0.164810755687758,0.183362404522089,0.201300311571489,0.218066024244334,0.233099300949596,0.245868755771919,0.25590285004469,0.262818727926023,0.266346443473052,0.266346443473052,0.262818727926023,0.25590285004469,0.245868755771919,0.233099300949596,0.218066024244334,0.201300311571489,0.183362404522089,0.164810755687758,0.146174027410271,0.127927608726522,0.110475953339786,0.0941413974686199,0.0791594796334459,0.0656802243170994,0.0537744195174696,0.0434436435299832,0.0346326846918545,0.0272430346821459,0.0211462914953402,0.0161965439076004,0.0122410845531382,0.00912907657003719,0.00671804964117306,0.00487830527795108,0.00349545847418932,0.00247143234775676,0.00172425984051603,0.00118704205791238,0.000806378438850812,0.000540531632252621,0.000357530197139557,0.000233353193889755,0.000150287955841078,9.55089371747857e-05,5.9892612048338e-05,3.70605536274e-05,2.26287184189335e-05,1.36338157579522e-05,8.10558361534242e-06],[0.129644732227423,0.149126938641066,0.1692648329271,0.189577494037488,0.209515533131891,0.228483625539526,0.245868755771919,0.261072430532258,0.273544563684942,0.282816387722065,0.288529657735577,0.290459614829476,0.288529657735577,0.282816387722065,0.273544563684942,0.261072430532258,0.245868755771919,0.228483625539526,0.209515533131891,0.189577494037488,0.1692648329271,0.149126938641066,0.129644732227423,0.111214920181278,0.0941413974686199,0.0786335049661668,0.0648103003735523,0.0527096146688645,0.0423004566051226,0.0334972902368784,0.0261748200130662,0.0201821364791977,0.0153553592070208,0.0115282192847515,0.00854031489815827,0.00624302316799314,0.00450324334474084,0.00320527496896476,0.00225120256576163,0.00156017482211562,0.00106694335246963,0.000719977402492236,0.000479408551336934,0.000314993864048076,0.000204224489205768,0.000130654072091693,8.24797789303686e-05,5.13785014937552e-05,3.1580920589238e-05,1.91547965993229e-05,1.1464093266488e-05],[0.129644732227423,0.15012443954056,0.171536810222608,0.193407213455959,0.21517777503787,0.236228103963181,0.25590285004469,0.273544563684942,0.288529657735577,0.300304776445231,0.308420634321427,0.312560446865633,0.312560446865633,0.308420634321427,0.300304776445231,0.288529657735577,0.273544563684942,0.25590285004469,0.236228103963181,0.21517777503787,0.193407213455959,0.171536810222608,0.15012443954056,0.129644732227423,0.110475953339786,0.0928945098919036,0.0770764572452968,0.0631048658850247,0.0509815879579283,0.0406418319775229,0.0319699973583051,0.0248154029509433,0.0190068203480652,0.0143650457958375,0.0107130705970553,0.00788370428577923,0.00572474427569231,0.00410195851856161,0.00290025272701337,0.00202343766726017,0.00139300675928467,0.00094629386412483,0.0006343197465634,0.000419565573051116,0.000273842509810286,0.000176364549958642,0.000112080776051485,7.02846313308199e-05,4.34909625667288e-05,2.65550470612311e-05,1.59994310050564e-05],[0.127927608726522,0.149126938641066,0.171536810222608,0.194700902383989,0.218066024244334,0.241000228215217,0.262818727926023,0.282816387722065,0.300304776445231,0.314651144426621,0.325316280684138,0.331888105486478,0.334108084564509,0.331888105486478,0.325316280684138,0.314651144426621,0.300304776445231,0.282816387722065,0.262818727926023,0.241000228215217,0.218066024244334,0.194700902383989,0.171536810222608,0.149126938641066,0.127927608726522,0.108288382895958,0.0904500604748913,0.074549590415764,0.0606304887014555,0.0486571069125697,0.0385310552921972,0.0301082096508339,0.0232149827969817,0.0176628673678771,0.0132606085907303,0.0098237004613415,0.00718118597580115,0.00517996282933357,0.00368694381493308,0.00258949932731576,0.00179462821951634,0.0012272769832107,0.000828171141856104,0.000551451095550976,0.000362329188615297,0.000234914079018397,0.000150287955841078,9.48743286391396e-05,5.90993440928135e-05,3.63267054975913e-05,2.20332606506407e-05],[0.124561289565839,0.146174027410271,0.1692648329271,0.193407213455959,0.218066024244334,0.242612263885053,0.266346443473052,0.288529657735577,0.308420634321427,0.325316280684138,0.338592689956246,0.347743294159522,0.352410919769243,0.352410919769243,0.347743294159522,0.338592689956246,0.325316280684138,0.308420634321427,0.288529657735577,0.266346443473052,0.242612263885053,0.218066024244334,0.193407213455959,0.169264832927099,0.146174027410271,0.124561289565839,0.10473826743213,0.0869034628749644,0.0711505376047041,0.0574815799110811,0.0458235375970751,0.03604607136649,0.0279792886978621,0.0214301301009783,0.0161965439076004,0.0120789533689274,0.0088888517609989,0.00645463114694013,0.00462494531498741,0.00327002581849206,0.00228141959920296,0.00157061073532225,0.00106694335246963,0.000715193517144861,0.000473058862646229,0.000308756567639692,0.000198850474808582,0.000126370722120622,7.92457005399165e-05,4.90359233661433e-05,2.99407319550803e-05],[0.119677171542053,0.141381872783512,0.164810755687758,0.189577494037488,0.21517777503787,0.241000228215217,0.266346443473052,0.290459614829476,0.312560446865632,0.331888105486478,0.347743294159522,0.359530092588644,0.366793082238878,0.369246538554654,0.366793082238878,0.359530092588644,0.347743294159522,0.331888105486478,0.312560446865632,0.290459614829476,0.266346443473052,0.241000228215217,0.21517777503787,0.189577494037488,0.164810755687758,0.141381872783512,0.119677171542053,0.0999627763750334,0.0823900392819534,0.0670070528615627,0.0537744195174696,0.0425834017517011,0.0332747177014262,0.0256565238508506,0.0195204873448052,0.0146552389704592,0.0108568680577609,0.00793643789774811,0.00572474427569231,0.00407470308084291,0.00286183934892583,0.00198337091696733,0.00135635083026808,0.000915270661168869,0.000609447713543891,0.000400435682027572,0.000259620208377656,0.000166093878132596,0.000104852348883928,6.53148763687232e-05,4.01472184654367e-05],[0.113461610599908,0.134935634119589,0.158348433523511,0.183362404522089,0.209515533131891,0.236228103963181,0.262818727926023,0.288529657735577,0.312560446865632,0.334108084564509,0.352410919769243,0.366793082238878,0.3767058134337,0.381762191893459,0.381762191893459,0.3767058134337,0.366793082238878,0.352410919769243,0.334108084564509,0.312560446865632,0.288529657735577,0.262818727926023,0.236228103963181,0.209515533131891,0.183362404522089,0.158348433523511,0.134935634119589,0.113461610599908,0.0941413974686199,0.0770764572452967,0.0622690521471989,0.0496400456740287,0.0390482429518072,0.0303096016091382,0.0232149827969817,0.0175455065561331,0.0130849739756079,0.00962917815916989,0.00699221844809223,0.00501014344842975,0.00354237668013568,0.00247143234775675,0.00170142229792605,0.00115580593567212,0.000774759888007995,0.000512458548153685,0.000334471996783786,0.000215412147761162,0.000136895769006016,8.5845842563734e-05,5.31199816340434e-05],[0.106144129249151,0.12707759453053,0.15012443954056,0.17500208365302,0.201300311571489,0.228483625539526,0.25590285004469,0.282816387722065,0.308420634321427,0.331888105486478,0.352410919769243,0.369246538554654,0.381762191893459,0.389474299741258,0.392079469322702,0.389474299741258,0.381762191893459,0.369246538554654,0.352410919769243,0.331888105486478,0.308420634321427,0.282816387722065,0.25590285004469,0.228483625539526,0.201300311571489,0.17500208365302,0.15012443954056,0.12707759453053,0.106144129249152,0.0874847547808859,0.0711505376047041,0.0570996439129113,0.04521661225618,0.0353323113313541,0.0272430346821459,0.0207275668690903,0.0155614683371893,0.0115282192847515,0.00842719980921657,0.00607874269155644,0.00432666864755998,0.00303880561103102,0.0021060157249911,0.0014402228815158,0.000971867838009838,0.000647133855449413,0.000425197241717693,0.000275674225477118,0.000176364549958642,0.00011133605603608,6.93537227614023e-05],[0.0979833807828676,0.118092066769606,0.140442461814151,0.164810755687758,0.190845566208414,0.218066024244334,0.245868755771919,0.273544563684942,0.30030477644523,0.325316280684138,0.347743294159522,0.366793082238878,0.381762191893459,0.392079469322702,0.397342202502014,0.397342202502014,0.392079469322702,0.381762191893459,0.366793082238878,0.347743294159522,0.325316280684138,0.300304776445231,0.273544563684942,0.245868755771919,0.218066024244334,0.190845566208414,0.164810755687758,0.140442461814151,0.118092066769606,0.0979833807828675,0.0802220071374839,0.0648103003735523,0.0516658943689313,0.0406418319775229,0.03154655991627,0.0241624042177895,0.0182615522622866,0.0136189818938397,0.0100221523746821,0.00727757629628117,0.0052146112813751,0.00368694381493307,0.00257229341523282,0.00177085865913261,0.00120297527072305,0.000806378438850811,0.000533372378245343,0.000348122057918207,0.00022420328428174,0.000142482591322814,8.93492778599581e-05],[0.0892520640593719,0.108288382895958,0.129644732227423,0.153157154390045,0.178537360250285,0.205366847613037,0.233099300949596,0.261072430532258,0.288529657735577,0.314651144426621,0.338592689956246,0.359530092588644,0.3767058134337,0.389474299741258,0.397342202502014,0.4,0.397342202502014,0.389474299741258,0.3767058134337,0.359530092588644,0.338592689956246,0.314651144426621,0.288529657735577,0.261072430532258,0.233099300949596,0.205366847613037,0.178537360250285,0.153157154390045,0.129644732227423,0.108288382895957,0.0892520640593719,0.0725878738079432,0.0582531332350026,0.046130048415225,0.0360460713664899,0.0277933804891206,0.0211462914953401,0.0158758308503845,0.0117611047624258,0.00859744053803596,0.00620154143960373,0.00441407315209244,0.00310019355645468,0.00214856006475334,0.00146931731365961,0.000991500870666543,0.00066020682650617,0.000433786795776071,0.000281243214242592,0.000179927350201022,0.000113585193561303],[0.0802220071374838,0.0979833807828676,0.118092066769606,0.14044246181415,0.164810755687758,0.190845566208414,0.218066024244334,0.245868755771919,0.273544563684942,0.30030477644523,0.325316280684138,0.347743294159522,0.366793082238878,0.381762191893459,0.392079469322702,0.397342202502014,0.397342202502014,0.392079469322702,0.381762191893459,0.366793082238878,0.347743294159522,0.325316280684138,0.30030477644523,0.273544563684942,0.245868755771918,0.218066024244334,0.190845566208414,0.164810755687758,0.14044246181415,0.118092066769606,0.0979833807828676,0.0802220071374838,0.0648103003735523,0.0516658943689313,0.0406418319775229,0.03154655991627,0.0241624042177895,0.0182615522622865,0.0136189818938397,0.010022152374682,0.00727757629628118,0.00521461128137509,0.00368694381493307,0.00257229341523282,0.0017708586591326,0.00120297527072305,0.000806378438850809,0.000533372378245343,0.000348122057918207,0.00022420328428174,0.000142482591322814],[0.0711505376047041,0.0874847547808859,0.106144129249152,0.12707759453053,0.15012443954056,0.17500208365302,0.201300311571489,0.228483625539526,0.25590285004469,0.282816387722065,0.308420634321427,0.331888105486478,0.352410919769243,0.369246538554654,0.381762191893459,0.389474299741258,0.392079469322702,0.389474299741258,0.381762191893459,0.369246538554654,0.352410919769243,0.331888105486478,0.308420634321426,0.282816387722065,0.25590285004469,0.228483625539526,0.201300311571489,0.17500208365302,0.15012443954056,0.12707759453053,0.106144129249152,0.0874847547808859,0.0711505376047041,0.0570996439129112,0.0452166122561799,0.0353323113313541,0.0272430346821459,0.0207275668690903,0.0155614683371893,0.0115282192847515,0.00842719980921658,0.00607874269155643,0.00432666864755998,0.00303880561103103,0.0021060157249911,0.0014402228815158,0.000971867838009837,0.000647133855449412,0.000425197241717692,0.000275674225477117,0.000176364549958642],[0.0622690521471989,0.0770764572452968,0.0941413974686199,0.113461610599908,0.134935634119589,0.158348433523511,0.183362404522089,0.209515533131891,0.236228103963181,0.262818727926023,0.288529657735577,0.312560446865632,0.334108084564509,0.352410919769243,0.366793082238878,0.376705813433699,0.381762191893459,0.381762191893459,0.376705813433699,0.366793082238878,0.352410919769243,0.334108084564509,0.312560446865632,0.288529657735577,0.262818727926023,0.236228103963181,0.209515533131891,0.183362404522089,0.158348433523511,0.134935634119589,0.113461610599908,0.0941413974686199,0.0770764572452968,0.0622690521471989,0.0496400456740287,0.0390482429518072,0.0303096016091382,0.0232149827969817,0.0175455065561331,0.0130849739756079,0.0096291781591699,0.00699221844809222,0.00501014344842975,0.00354237668013568,0.00247143234775675,0.00170142229792605,0.00115580593567211,0.000774759888007996,0.000512458548153684,0.000334471996783787,0.000215412147761162],[0.0537744195174695,0.0670070528615627,0.0823900392819534,0.0999627763750333,0.119677171542053,0.141381872783512,0.164810755687758,0.189577494037488,0.21517777503787,0.241000228215217,0.266346443473052,0.290459614829476,0.312560446865632,0.331888105486477,0.347743294159522,0.359530092588643,0.366793082238878,0.369246538554654,0.366793082238878,0.359530092588644,0.347743294159522,0.331888105486478,0.312560446865632,0.290459614829476,0.266346443473052,0.241000228215217,0.21517777503787,0.189577494037488,0.164810755687758,0.141381872783512,0.119677171542053,0.0999627763750334,0.0823900392819534,0.0670070528615627,0.0537744195174695,0.0425834017517012,0.0332747177014262,0.0256565238508506,0.0195204873448052,0.0146552389704592,0.0108568680577609,0.00793643789774811,0.00572474427569231,0.00407470308084292,0.00286183934892583,0.00198337091696734,0.00135635083026808,0.000915270661168869,0.000609447713543891,0.000400435682027572,0.000259620208377657],[0.0458235375970751,0.0574815799110812,0.0711505376047041,0.0869034628749644,0.10473826743213,0.124561289565839,0.146174027410271,0.1692648329271,0.193407213455959,0.218066024244334,0.242612263885053,0.266346443473052,0.288529657735577,0.308420634321427,0.325316280684138,0.338592689956246,0.347743294159522,0.352410919769243,0.352410919769243,0.347743294159522,0.338592689956246,0.325316280684138,0.308420634321426,0.288529657735577,0.266346443473052,0.242612263885053,0.218066024244334,0.193407213455959,0.169264832927099,0.146174027410271,0.124561289565839,0.10473826743213,0.0869034628749644,0.0711505376047041,0.0574815799110811,0.0458235375970751,0.03604607136649,0.0279792886978621,0.0214301301009783,0.0161965439076004,0.0120789533689274,0.00888885176099889,0.00645463114694013,0.00462494531498742,0.00327002581849206,0.00228141959920296,0.00157061073532225,0.00106694335246963,0.000715193517144859,0.000473058862646229,0.000308756567639692],[0.0385310552921972,0.0486571069125697,0.0606304887014555,0.074549590415764,0.0904500604748912,0.108288382895958,0.127927608726522,0.149126938641066,0.171536810222608,0.194700902383989,0.218066024244334,0.241000228215217,0.262818727926023,0.282816387722065,0.300304776445231,0.314651144426621,0.325316280684138,0.331888105486478,0.334108084564509,0.331888105486478,0.325316280684138,0.314651144426621,0.30030477644523,0.282816387722065,0.262818727926023,0.241000228215217,0.218066024244334,0.194700902383989,0.171536810222608,0.149126938641066,0.127927608726522,0.108288382895958,0.0904500604748912,0.0745495904157639,0.0606304887014554,0.0486571069125697,0.0385310552921972,0.0301082096508339,0.0232149827969817,0.0176628673678771,0.0132606085907303,0.00982370046134148,0.00718118597580114,0.00517996282933357,0.00368694381493307,0.00258949932731576,0.00179462821951633,0.0012272769832107,0.000828171141856103,0.000551451095550975,0.000362329188615298],[0.0319699973583051,0.0406418319775229,0.0509815879579283,0.0631048658850247,0.0770764572452968,0.0928945098919035,0.110475953339786,0.129644732227423,0.15012443954056,0.171536810222608,0.193407213455959,0.21517777503787,0.236228103963181,0.25590285004469,0.273544563684942,0.288529657735577,0.30030477644523,0.308420634321426,0.312560446865632,0.312560446865632,0.308420634321426,0.30030477644523,0.288529657735577,0.273544563684942,0.25590285004469,0.236228103963181,0.21517777503787,0.193407213455959,0.171536810222608,0.15012443954056,0.129644732227423,0.110475953339786,0.0928945098919035,0.0770764572452967,0.0631048658850247,0.0509815879579283,0.0406418319775229,0.0319699973583051,0.0248154029509433,0.0190068203480652,0.0143650457958376,0.0107130705970553,0.00788370428577922,0.00572474427569231,0.0041019585185616,0.00290025272701336,0.00202343766726017,0.00139300675928467,0.000946293864124829,0.000634319746563399,0.000419565573051116],[0.0261748200130662,0.0334972902368784,0.0423004566051226,0.0527096146688645,0.0648103003735523,0.0786335049661667,0.0941413974686199,0.111214920181278,0.129644732227423,0.149126938641066,0.169264832927099,0.189577494037488,0.209515533131891,0.228483625539526,0.245868755771918,0.261072430532258,0.273544563684942,0.282816387722065,0.288529657735577,0.290459614829476,0.288529657735577,0.282816387722065,0.273544563684942,0.261072430532258,0.245868755771918,0.228483625539526,0.209515533131891,0.189577494037487,0.169264832927099,0.149126938641066,0.129644732227423,0.111214920181278,0.0941413974686199,0.0786335049661667,0.0648103003735523,0.0527096146688646,0.0423004566051226,0.0334972902368784,0.0261748200130662,0.0201821364791976,0.0153553592070208,0.0115282192847515,0.00854031489815828,0.00624302316799314,0.00450324334474083,0.00320527496896476,0.00225120256576162,0.00156017482211562,0.00106694335246963,0.000719977402492236,0.000479408551336934],[0.0211462914953401,0.0272430346821459,0.0346326846918545,0.0434436435299832,0.0537744195174695,0.0656802243170993,0.0791594796334459,0.0941413974686199,0.110475953339786,0.127927608726522,0.146174027410271,0.164810755687758,0.183362404522089,0.201300311571489,0.218066024244334,0.233099300949596,0.245868755771918,0.25590285004469,0.262818727926023,0.266346443473052,0.266346443473052,0.262818727926023,0.25590285004469,0.245868755771918,0.233099300949596,0.218066024244334,0.201300311571489,0.183362404522089,0.164810755687758,0.146174027410271,0.127927608726522,0.110475953339786,0.0941413974686199,0.0791594796334458,0.0656802243170993,0.0537744195174695,0.0434436435299832,0.0346326846918545,0.0272430346821459,0.0211462914953401,0.0161965439076004,0.0122410845531381,0.00912907657003718,0.00671804964117307,0.00487830527795107,0.00349545847418932,0.00247143234775675,0.00172425984051603,0.00118704205791238,0.00080637843885081,0.000540531632252623],[0.0168575374037106,0.0218630474459658,0.0279792886978622,0.0353323113313541,0.0440267709878692,0.0541341132946451,0.0656802243170994,0.0786335049661668,0.0928945098919035,0.108288382895958,0.124561289565839,0.141381872783512,0.158348433523511,0.17500208365302,0.190845566208414,0.205366847613037,0.218066024244334,0.228483625539526,0.236228103963181,0.241000228215217,0.242612263885053,0.241000228215217,0.236228103963181,0.228483625539526,0.218066024244334,0.205366847613037,0.190845566208414,0.17500208365302,0.158348433523511,0.141381872783512,0.124561289565839,0.108288382895958,0.0928945098919035,0.0786335049661667,0.0656802243170993,0.0541341132946451,0.0440267709878692,0.0353323113313541,0.0279792886978621,0.0218630474459658,0.0168575374037106,0.0128258741311443,0.00962917815916989,0.0071334706305035,0.00521461128137509,0.00376142502059808,0.00267727070095845,0.00188036244303331,0.00130316755350706,0.000891185999581157,0.00060137567719103],[0.0132606085907303,0.0173131191607864,0.0223047103014429,0.0283548440188743,0.0355686469837545,0.0440267709878691,0.0537744195174695,0.0648103003735523,0.0770764572452967,0.0904500604748911,0.10473826743213,0.119677171542053,0.134935634119589,0.15012443954056,0.164810755687758,0.178537360250285,0.190845566208414,0.201300311571489,0.209515533131891,0.21517777503787,0.218066024244334,0.218066024244334,0.21517777503787,0.209515533131891,0.201300311571489,0.190845566208414,0.178537360250285,0.164810755687758,0.15012443954056,0.134935634119589,0.119677171542053,0.10473826743213,0.0904500604748912,0.0770764572452967,0.0648103003735523,0.0537744195174695,0.0440267709878692,0.0355686469837545,0.0283548440188743,0.0223047103014429,0.0173131191607864,0.0132606085907302,0.0100221523746821,0.00747425573509312,0.00550027384193288,0.00399401855396117,0.00286183934892583,0.00202343766726017,0.00141170455772781,0.000971867838009838,0.000660206826506172],[0.010293005090544,0.0135284906538333,0.0175455065561331,0.0224539051336535,0.0283548440188743,0.0353323113313541,0.0434436435299832,0.0527096146688645,0.0631048658850247,0.074549590415764,0.0869034628749644,0.0999627763750334,0.113461610599908,0.12707759453053,0.14044246181415,0.153157154390045,0.164810755687758,0.17500208365302,0.183362404522089,0.189577494037488,0.193407213455959,0.194700902383989,0.193407213455959,0.189577494037487,0.183362404522089,0.17500208365302,0.164810755687758,0.153157154390045,0.14044246181415,0.12707759453053,0.113461610599908,0.0999627763750334,0.0869034628749644,0.074549590415764,0.0631048658850247,0.0527096146688646,0.0434436435299832,0.0353323113313541,0.0283548440188743,0.0224539051336535,0.0175455065561331,0.0135284906538333,0.010293005090544,0.00772759648166358,0.0057247442756923,0.00418482357737072,0.00301861428615636,0.00214856006475334,0.00150902620751688,0.00104581645858419,0.000715193517144861],[0.00788370428577923,0.0104311641721669,0.0136189818938398,0.0175455065561331,0.0223047103014429,0.0279792886978622,0.0346326846918545,0.0423004566051226,0.0509815879579283,0.0606304887014555,0.0711505376047041,0.0823900392819534,0.09414139746862,0.106144129249152,0.118092066769606,0.129644732227423,0.140442461814151,0.15012443954056,0.158348433523511,0.164810755687758,0.1692648329271,0.171536810222608,0.171536810222608,0.1692648329271,0.164810755687758,0.158348433523511,0.15012443954056,0.140442461814151,0.129644732227423,0.118092066769606,0.106144129249152,0.0941413974686199,0.0823900392819534,0.0711505376047041,0.0606304887014554,0.0509815879579283,0.0423004566051226,0.0346326846918545,0.0279792886978621,0.0223047103014429,0.0175455065561331,0.0136189818938397,0.0104311641721669,0.00788370428577923,0.00587945780196071,0.00432666864755998,0.0031418062721626,0.00225120256576162,0.00159169244419836,0.00111048613670333,0.000764498318653055],[0.00595837552163675,0.00793643789774811,0.0104311641721669,0.0135284906538333,0.0173131191607863,0.0218630474459658,0.0272430346821459,0.0334972902368784,0.0406418319775229,0.0486571069125696,0.0574815799110811,0.0670070528615627,0.0770764572452967,0.0874847547808859,0.0979833807828675,0.108288382895957,0.118092066769606,0.12707759453053,0.134935634119589,0.141381872783512,0.146174027410271,0.149126938641066,0.15012443954056,0.149126938641066,0.146174027410271,0.141381872783512,0.134935634119589,0.12707759453053,0.118092066769606,0.108288382895957,0.0979833807828676,0.0874847547808859,0.0770764572452967,0.0670070528615627,0.0574815799110811,0.0486571069125697,0.0406418319775229,0.0334972902368784,0.0272430346821459,0.0218630474459658,0.0173131191607864,0.0135284906538333,0.0104311641721669,0.00793643789774812,0.00595837552163675,0.00441407315209244,0.00322671485563984,0.00232750733227017,0.00165665064512227,0.00116353705032503,0.000806378438850812],[0.00444359861529692,0.00595837552163676,0.00788370428577923,0.010293005090544,0.0132606085907303,0.0168575374037106,0.0211462914953402,0.0261748200130662,0.0319699973583051,0.0385310552921972,0.0458235375970751,0.0537744195174696,0.0622690521471989,0.0711505376047041,0.0802220071374839,0.0892520640593719,0.0979833807828676,0.106144129249152,0.113461610599908,0.119677171542053,0.124561289565839,0.127927608726522,0.129644732227423,0.129644732227423,0.127927608726522,0.124561289565839,0.119677171542053,0.113461610599908,0.106144129249151,0.0979833807828676,0.0892520640593719,0.0802220071374838,0.0711505376047041,0.0622690521471989,0.0537744195174695,0.0458235375970751,0.0385310552921972,0.0319699973583051,0.0261748200130662,0.0211462914953401,0.0168575374037106,0.0132606085907302,0.010293005090544,0.00788370428577923,0.00595837552163675,0.00444359861529692,0.00327002581849206,0.00237452609930412,0.00170142229792605,0.00120297527072305,0.000839287367232362],[0.00327002581849206,0.00441407315209244,0.00587945780196071,0.00772759648166356,0.010022152374682,0.0128258741311443,0.0161965439076004,0.0201821364791976,0.0248154029509433,0.0301082096508338,0.0360460713664899,0.0425834017517011,0.0496400456740287,0.0570996439129112,0.0648103003735523,0.0725878738079431,0.0802220071374838,0.0874847547808858,0.0941413974686198,0.0999627763750333,0.10473826743213,0.108288382895957,0.110475953339786,0.111214920181278,0.110475953339786,0.108288382895957,0.10473826743213,0.0999627763750333,0.0941413974686198,0.0874847547808859,0.0802220071374838,0.0725878738079432,0.0648103003735523,0.0570996439129112,0.0496400456740286,0.0425834017517011,0.03604607136649,0.0301082096508339,0.0248154029509433,0.0201821364791976,0.0161965439076004,0.0128258741311443,0.010022152374682,0.00772759648166358,0.00587945780196071,0.00441407315209244,0.00327002581849206,0.00239040915800238,0.00172425984051603,0.0012272769832107,0.000861969447319046],[0.00237452609930412,0.00322671485563985,0.00432666864755999,0.00572474427569231,0.00747425573509311,0.00962917815916989,0.0122410845531382,0.0153553592070208,0.0190068203480652,0.0232149827969817,0.0279792886978621,0.0332747177014262,0.0390482429518072,0.0452166122561799,0.0516658943689313,0.0582531332350026,0.0648103003735523,0.0711505376047041,0.0770764572452968,0.0823900392819534,0.0869034628749644,0.0904500604748912,0.0928945098919035,0.0941413974686199,0.0941413974686199,0.0928945098919035,0.0904500604748912,0.0869034628749644,0.0823900392819534,0.0770764572452967,0.0711505376047041,0.0648103003735523,0.0582531332350026,0.0516658943689313,0.0452166122561799,0.0390482429518072,0.0332747177014261,0.0279792886978621,0.0232149827969817,0.0190068203480652,0.0153553592070208,0.0122410845531381,0.00962917815916989,0.00747425573509311,0.0057247442756923,0.00432666864755998,0.00322671485563984,0.00237452609930412,0.00172425984051603,0.00123548616329471,0.000873539334459006],[0.00170142229792605,0.00232750733227016,0.0031418062721626,0.00418482357737071,0.00550027384193288,0.00713347063050349,0.00912907657003717,0.0115282192847515,0.0143650457958375,0.0176628673678771,0.0214301301009783,0.0256565238508506,0.0303096016091382,0.035332311331354,0.0406418319775229,0.0461300484152249,0.0516658943689312,0.0570996439129112,0.0622690521471988,0.0670070528615626,0.071150537604704,0.0745495904157639,0.0770764572452967,0.0786335049661666,0.0791594796334458,0.0786335049661666,0.0770764572452967,0.0745495904157639,0.071150537604704,0.0670070528615626,0.0622690521471989,0.0570996439129112,0.0516658943689313,0.046130048415225,0.0406418319775229,0.0353323113313541,0.0303096016091382,0.0256565238508506,0.0214301301009783,0.0176628673678771,0.0143650457958375,0.0115282192847515,0.00912907657003719,0.0071334706305035,0.00550027384193288,0.00418482357737072,0.0031418062721626,0.00232750733227016,0.00170142229792605,0.0012272769832107,0.000873539334459005],[0.00120297527072305,0.00165665064512228,0.00225120256576162,0.00301861428615636,0.00399401855396117,0.00521461128137509,0.00671804964117306,0.00854031489815827,0.0107130705970553,0.0132606085907302,0.0161965439076004,0.0195204873448052,0.0232149827969817,0.0272430346821459,0.03154655991627,0.0360460713664899,0.0406418319775229,0.0452166122561799,0.0496400456740287,0.0537744195174695,0.0574815799110811,0.0606304887014554,0.0631048658850247,0.0648103003735523,0.0656802243170993,0.0656802243170993,0.0648103003735523,0.0631048658850247,0.0606304887014554,0.0574815799110811,0.0537744195174695,0.0496400456740287,0.0452166122561799,0.0406418319775229,0.0360460713664899,0.03154655991627,0.0272430346821459,0.0232149827969817,0.0195204873448052,0.0161965439076004,0.0132606085907303,0.0107130705970553,0.00854031489815827,0.00671804964117307,0.00521461128137509,0.00399401855396118,0.00301861428615636,0.00225120256576162,0.00165665064512227,0.00120297527072305,0.000861969447319046],[0.000839287367232362,0.00116353705032503,0.00159169244419837,0.00214856006475334,0.00286183934892583,0.00376142502059809,0.00487830527795108,0.00624302316799314,0.00788370428577922,0.00982370046134149,0.0120789533689274,0.0146552389704592,0.0175455065561331,0.0207275668690903,0.0241624042177895,0.0277933804891206,0.03154655991627,0.0353323113313541,0.0390482429518072,0.0425834017517011,0.0458235375970751,0.0486571069125697,0.0509815879579283,0.0527096146688646,0.0537744195174695,0.0541341132946451,0.0537744195174696,0.0527096146688646,0.0509815879579283,0.0486571069125697,0.0458235375970751,0.0425834017517011,0.0390482429518072,0.0353323113313541,0.03154655991627,0.0277933804891206,0.0241624042177895,0.0207275668690903,0.0175455065561331,0.0146552389704592,0.0120789533689274,0.00982370046134149,0.00788370428577922,0.00624302316799314,0.00487830527795107,0.00376142502059809,0.00286183934892583,0.00214856006475334,0.00159169244419836,0.00116353705032503,0.000839287367232362]],"type":"surface","colorscale":[["0","rgb(255,112,183)"],["1","rgb(128,0,64)"]],"inherit":true},"46f4453d7f00.1":{"x":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"y":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[0.00564213753089713,0.00731745791692661,0.00936453475198971,0.0118255564286688,0.0147355506920122,0.0181184300533911,0.0219828584556888,0.0263182598341997,0.0310913503036355,0.0362436063266842,0.0416900707336809,0.047319839874405,0.0529984669970311,0.0585723644277108,0.0638751026275517,0.0687353063956126,0.0729856603688545,0.0764723819368925,0.0790644220032968,0.0806616292762043,0.0812011699419676,0.0806616292762043,0.0790644220032968,0.0764723819368924,0.0729856603688545,0.0687353063956126,0.0638751026275517,0.0585723644277108,0.0529984669970311,0.0473198398744049,0.0416900707336809,0.0362436063266842,0.0310913503036355,0.0263182598341997,0.0219828584556887,0.0181184300533911,0.0147355506920122,0.0118255564286688,0.0093645347519897,0.00731745791692661,0.00564213753089713,0.00429275902338874,0.00322284009713001,0.00238753866629755,0.00174530557548755,0.00125893105084854,0.000896069760398337,0.000629348359576674,0.000436163977371401,0.000298275712212873,0.000201277576741507],[0.00599102783094177,0.00782191692206264,0.0100770744617596,0.0128104723472374,0.0160696058955829,0.0198909128860954,0.0242948158614006,0.0292807310172078,0.0348224741954725,0.0408645520232188,0.047319839874405,0.054069107049735,0.0609627479662844,0.0678249183842699,0.0744600685110431,0.0806616292762043,0.0862223698666218,0.0909457330521832,0.094657298827537,0.0972154505603285,0.0985203364756491,0.0985203364756491,0.0972154505603285,0.094657298827537,0.0909457330521832,0.0862223698666218,0.0806616292762043,0.074460068511043,0.0678249183842699,0.0609627479662844,0.054069107049735,0.047319839874405,0.0408645520232188,0.0348224741954725,0.0292807310172078,0.0242948158614006,0.0198909128860954,0.0160696058955829,0.0128104723472374,0.0100770744617596,0.00782191692206264,0.00599102783094176,0.00452792142923455,0.00337680384864244,0.00248497596768341,0.00180446290608457,0.00129295417097857,0.000914171570315839,0.000637795862576538,0.000439081451328283,0.000298275712212874],[0.00627723536605608,0.00825041076289934,0.0107002059457552,0.0136936148550558,0.0172923289271272,0.0215475686937563,0.0264943010518157,0.0321451951514675,0.0384847857762759,0.0454644024137073,0.0529984669970311,0.0609627479662844,0.0691950726228376,0.077498841553397,0.0856494658693669,0.0934035782207984,0.100510579292344,0.106725806407056,0.111824385623646,0.115614685867945,0.11795025744925,0.118739219450169,0.11795025744925,0.115614685867945,0.111824385623646,0.106725806407056,0.100510579292344,0.0934035782207984,0.0856494658693669,0.077498841553397,0.0691950726228376,0.0609627479662844,0.0529984669970311,0.0454644024137073,0.0384847857762759,0.0321451951514675,0.0264943010518157,0.0215475686937563,0.0172923289271273,0.0136936148550558,0.0107002059457553,0.00825041076289933,0.00627723536605609,0.00471270940824392,0.00349126099840525,0.00255213344688908,0.00184091547481606,0.00131030900168851,0.000920286407594678,0.00063779586257654,0.000436163977371403],[0.00649000297133997,0.00858711641353847,0.0112113836026397,0.0144437672387548,0.0183616268297072,0.0230330388105312,0.0285102305220978,0.0348224741954726,0.0419689330467932,0.0499120765521392,0.0585723644277108,0.0678249183842699,0.077498841553397,0.087379699852504,0.0972154505603285,0.106725806407056,0.115614685867945,0.12358505892293,0.130355194312447,0.135675090712337,0.139341764837855,0.14121209620293,0.14121209620293,0.139341764837855,0.135675090712337,0.130355194312447,0.12358505892293,0.115614685867945,0.106725806407056,0.0972154505603284,0.087379699852504,0.0774988415533969,0.0678249183842699,0.0585723644277108,0.0499120765521392,0.0419689330467932,0.0348224741954725,0.0285102305220978,0.0230330388105312,0.0183616268297072,0.0144437672387548,0.0112113836026396,0.00858711641353846,0.00649000297133998,0.00484007228345976,0.00356178914895618,0.00258638976077405,0.00185322924494206,0.00131030900168851,0.000914171570315837,0.000629348359576674],[0.00662110972813867,0.00881918670294107,0.0115913947224954,0.0150332285620231,0.0192388111967165,0.0242948158614006,0.0302732047187965,0.037223104426415,0.0451623144762508,0.054069107049735,0.0638751026275517,0.074460068511043,0.0856494658693669,0.0972154505603284,0.108881810711915,0.120333010706226,0.131227132171329,0.14121209620293,0.14994416456255,0.157107401148196,0.162432574343936,0.165713930009679,0.166822380271916,0.165713930009679,0.162432574343936,0.157107401148196,0.14994416456255,0.14121209620293,0.131227132171329,0.120333010706226,0.108881810711915,0.0972154505603284,0.0856494658693669,0.074460068511043,0.0638751026275517,0.0540691070497349,0.0451623144762508,0.037223104426415,0.0302732047187964,0.0242948158614005,0.0192388111967165,0.015033228562023,0.0115913947224954,0.00881918670294107,0.00662110972813866,0.0049050387277381,0.00358561373700356,0.00258638976077405,0.00184091547481605,0.00129295417097857,0.000896069760398338],[0.00666539792294538,0.00893756328245514,0.0118255564286688,0.015439507635816,0.0198909128860954,0.0252863061055659,0.0317194372430102,0.0392622300195994,0.0479549960374577,0.0577965829382958,0.0687353063956126,0.0806616292762043,0.0934035782207984,0.106725806407056,0.120333010706226,0.133878096089058,0.146975071174301,0.159216193873727,0.170192415899862,0.179515757313079,0.186841934348759,0.191891413089782,0.194467098341135,0.194467098341135,0.191891413089782,0.186841934348759,0.179515757313079,0.170192415899862,0.159216193873727,0.146975071174301,0.133878096089058,0.120333010706226,0.106725806407056,0.0934035782207984,0.0806616292762043,0.0687353063956126,0.0577965829382958,0.0479549960374576,0.0392622300195993,0.0317194372430102,0.0252863061055658,0.0198909128860954,0.015439507635816,0.0118255564286688,0.00893756328245512,0.00666539792294538,0.00490503872773809,0.00356178914895618,0.00255213344688907,0.00180446290608457,0.00125893105084854],[0.00662110972813867,0.00893756328245514,0.0119046568466222,0.0156467462582504,0.02029273598075,0.0259696787411795,0.0327945711689488,0.0408645520232188,0.0502459353553176,0.0609627479662844,0.0729856603688545,0.0862223698666218,0.100510579292344,0.115614685867945,0.131227132171329,0.146975071174301,0.162432574343936,0.177138100154409,0.190616391795795,0.202403451179384,0.212072809175268,0.219261041115406,0.223690407961599,0.22518665931084,0.223690407961599,0.219261041115406,0.212072809175268,0.202403451179384,0.190616391795795,0.177138100154408,0.162432574343936,0.146975071174301,0.131227132171329,0.115614685867945,0.100510579292344,0.0862223698666218,0.0729856603688545,0.0609627479662844,0.0502459353553176,0.0408645520232188,0.0327945711689488,0.0259696787411795,0.02029273598075,0.0156467462582504,0.0119046568466222,0.00893756328245514,0.00662110972813866,0.00484007228345977,0.00349126099840524,0.00248497596768341,0.00174530557548755],[0.00649000297133997,0.00881918670294107,0.0118255564286688,0.0156467462582504,0.0204284728407596,0.0263182598341997,0.0334570654521643,0.0419689330467932,0.0519490270377817,0.0634506849076839,0.0764723819368925,0.0909457330521832,0.106725806407056,0.12358505892293,0.14121209620293,0.159216193873727,0.177138100154409,0.194467098341135,0.210663692721226,0.22518665931084,0.237522650285267,0.247216133531637,0.253897249390649,0.257305215333912,0.257305215333912,0.253897249390649,0.247216133531637,0.237522650285267,0.22518665931084,0.210663692721226,0.194467098341135,0.177138100154409,0.159216193873727,0.14121209620293,0.12358505892293,0.106725806407056,0.0909457330521832,0.0764723819368924,0.0634506849076839,0.0519490270377818,0.0419689330467932,0.0334570654521643,0.0263182598341997,0.0204284728407596,0.0156467462582504,0.0118255564286688,0.00881918670294106,0.00649000297133998,0.0047127094082439,0.00337680384864244,0.00238753866629755],[0.00627723536605608,0.00858711641353847,0.0115913947224954,0.015439507635816,0.02029273598075,0.0263182598341997,0.0336808577004802,0.0425322660283115,0.0529984669970311,0.0651654652949748,0.0790644220032968,0.094657298827537,0.111824385623646,0.130355194312447,0.14994416456255,0.170192415899862,0.190616391795795,0.210663692721226,0.229735731585067,0.247216133531637,0.26250312547953,0.275043606783134,0.284366241056231,0.290110820183939,0.292051353575983,0.290110820183939,0.284366241056231,0.275043606783134,0.26250312547953,0.247216133531637,0.229735731585067,0.210663692721226,0.190616391795795,0.170192415899862,0.14994416456255,0.130355194312447,0.111824385623646,0.094657298827537,0.0790644220032968,0.0651654652949747,0.0529984669970311,0.0425322660283115,0.0336808577004802,0.0263182598341997,0.02029273598075,0.015439507635816,0.0115913947224953,0.00858711641353846,0.00627723536605607,0.00452792142923454,0.00322284009713001],[0.00599102783094176,0.00825041076289934,0.0112113836026397,0.0150332285620231,0.0198909128860954,0.0259696787411795,0.0334570654521643,0.0425322660283115,0.0533529704756318,0.0660401564818038,0.0806616292762043,0.0972154505603285,0.115614685867945,0.135675090712337,0.157107401148196,0.179515757313079,0.202403451179384,0.22518665931084,0.247216133531637,0.267806040375428,0.286268349312621,0.301950467357234,0.314273299697836,0.322766662556805,0.327099036366501,0.327099036366501,0.322766662556805,0.314273299697836,0.301950467357234,0.286268349312621,0.267806040375428,0.247216133531637,0.22518665931084,0.202403451179384,0.179515757313079,0.157107401148196,0.135675090712337,0.115614685867945,0.0972154505603284,0.0806616292762042,0.0660401564818038,0.0533529704756318,0.0425322660283115,0.0334570654521643,0.0259696787411795,0.0198909128860954,0.0150332285620231,0.0112113836026397,0.00825041076289931,0.00599102783094176,0.00429275902338875],[0.00564213753089712,0.00782191692206264,0.0107002059457552,0.0144437672387548,0.0192388111967165,0.0252863061055658,0.0327945711689488,0.0419689330467932,0.0529984669970311,0.0660401564818037,0.0812011699419676,0.0985203364756491,0.11795025744925,0.139341764837855,0.162432574343936,0.186841934348759,0.212072809175268,0.237522650285267,0.26250312547953,0.286268349312621,0.308050271419555,0.327099036366501,0.342725438309289,0.354342155944772,0.361500342322826,0.36391839582758,0.361500342322826,0.354342155944772,0.342725438309289,0.327099036366501,0.308050271419555,0.286268349312621,0.26250312547953,0.237522650285267,0.212072809175268,0.186841934348759,0.162432574343936,0.139341764837855,0.11795025744925,0.098520336475649,0.0812011699419676,0.0660401564818037,0.0529984669970311,0.0419689330467932,0.0327945711689487,0.0252863061055658,0.0192388111967164,0.0144437672387548,0.0107002059457552,0.00782191692206263,0.00564213753089713],[0.00524318771128397,0.00731745791692661,0.0100770744617596,0.0136936148550558,0.0183616268297072,0.0242948158614006,0.0317194372430102,0.0408645520232188,0.0519490270377817,0.0651654652949747,0.0806616292762043,0.0985203364756491,0.118739219450169,0.14121209620293,0.165713930009679,0.191891413089782,0.219261041115406,0.247216133531637,0.275043606783134,0.301950467357234,0.327099036366501,0.349648951424394,0.368803133657878,0.383854275067035,0.394228091889034,0.399519665209578,0.399519665209578,0.394228091889034,0.383854275067035,0.368803133657878,0.349648951424394,0.327099036366501,0.301950467357234,0.275043606783134,0.247216133531637,0.219261041115406,0.191891413089782,0.165713930009679,0.14121209620293,0.118739219450169,0.0985203364756491,0.0806616292762042,0.0651654652949748,0.0519490270377818,0.0408645520232188,0.0317194372430102,0.0242948158614006,0.0183616268297072,0.0136936148550558,0.0100770744617596,0.00731745791692661],[0.00480791245344714,0.00675486501711126,0.00936453475198971,0.0128104723472374,0.0172923289271272,0.0230330388105312,0.0302732047187965,0.0392622300195994,0.0502459353553176,0.0634506849076839,0.0790644220032968,0.0972154505603285,0.11795025744925,0.14121209620293,0.166822380271917,0.194467098341135,0.223690407961599,0.253897249390649,0.284366241056231,0.314273299697836,0.342725438309289,0.368803133657878,0.391608645798388,0.410316845527414,0.424224581583097,0.432794486603365,0.435689422244215,0.432794486603365,0.424224581583097,0.410316845527414,0.391608645798388,0.368803133657878,0.342725438309289,0.314273299697836,0.284366241056231,0.253897249390649,0.223690407961599,0.194467098341135,0.166822380271916,0.14121209620293,0.11795025744925,0.0972154505603284,0.0790644220032968,0.0634506849076839,0.0502459353553176,0.0392622300195994,0.0302732047187964,0.0230330388105312,0.0172923289271272,0.0128104723472374,0.00936453475198971],[0.00435037909052005,0.00615293777784242,0.00858711641353847,0.0118255564286688,0.0160696058955829,0.0215475686937563,0.0285102305220978,0.037223104426415,0.0479549960374576,0.0609627479662844,0.0764723819368925,0.0946572988275371,0.115614685867945,0.139341764837855,0.165713930009679,0.194467098341135,0.22518665931084,0.257305215333912,0.290110820183939,0.322766662556805,0.354342155944772,0.383854275067035,0.410316845527414,0.432794486603365,0.450457164667846,0.46263095148214,0.468840670298449,0.468840670298449,0.46263095148214,0.450457164667846,0.432794486603365,0.410316845527413,0.383854275067035,0.354342155944772,0.322766662556805,0.290110820183939,0.257305215333912,0.22518665931084,0.194467098341135,0.165713930009679,0.139341764837855,0.115614685867945,0.094657298827537,0.0764723819368925,0.0609627479662843,0.0479549960374577,0.0372231044264149,0.0285102305220978,0.0215475686937563,0.0160696058955829,0.0118255564286688],[0.00388424899097364,0.00553041572239962,0.00776994424400036,0.0107717789637017,0.0147355506920122,0.0198909128860954,0.0264943010518157,0.0348224741954725,0.0451623144762508,0.0577965829382958,0.0729856603688545,0.0909457330521832,0.111824385623646,0.135675090712337,0.162432574343936,0.191891413089782,0.223690407961599,0.257305215333912,0.292051353575983,0.327099036366501,0.361500342322826,0.394228091889034,0.424224581583097,0.450457164667846,0.471976716639932,0.487974421026207,0.497832158229716,0.501162126846763,0.497832158229716,0.487974421026207,0.471976716639932,0.450457164667846,0.424224581583097,0.394228091889034,0.361500342322826,0.327099036366501,0.292051353575983,0.257305215333912,0.223690407961599,0.191891413089782,0.162432574343936,0.135675090712337,0.111824385623646,0.0909457330521832,0.0729856603688544,0.0577965829382958,0.0451623144762508,0.0348224741954725,0.0264943010518157,0.0198909128860954,0.0147355506920122],[0.00342212939880444,0.0049050387277381,0.00693741797248113,0.0096819467204102,0.0133332776414984,0.0181184300533911,0.0242948158614006,0.0321451951514675,0.0419689330467932,0.054069107049735,0.0687353063956126,0.0862223698666218,0.106725806407056,0.130355194312447,0.157107401148196,0.186841934348759,0.219261041115406,0.253897249390649,0.290110820183939,0.327099036366501,0.36391839582758,0.399519665209578,0.432794486603365,0.46263095148214,0.487974421026207,0.507889034934368,0.521614941239284,0.528616379653865,0.528616379653865,0.521614941239283,0.507889034934368,0.487974421026207,0.46263095148214,0.432794486603365,0.399519665209578,0.36391839582758,0.327099036366501,0.290110820183939,0.253897249390649,0.219261041115406,0.186841934348759,0.157107401148195,0.130355194312447,0.106725806407056,0.0862223698666217,0.0687353063956126,0.0540691070497349,0.0419689330467932,0.0321451951514675,0.0242948158614005,0.0181184300533911],[0.00297505637545101,0.00429275902338875,0.00611205462126438,0.00858711641353845,0.0119046568466222,0.0162853020866414,0.0219828584556888,0.0292807310172078,0.0384847857762759,0.0499120765521392,0.0638751026275517,0.0806616292762043,0.100510579292344,0.12358505892293,0.14994416456255,0.179515757313079,0.212072809175268,0.247216133531637,0.284366241056231,0.322766662556805,0.361500342322826,0.399519665209578,0.435689422244215,0.468840670298449,0.497832158229716,0.521614941239284,0.539295138882965,0.550189623358317,0.553869807831981,0.550189623358317,0.539295138882965,0.521614941239284,0.497832158229716,0.468840670298449,0.435689422244214,0.399519665209578,0.361500342322826,0.322766662556805,0.284366241056231,0.247216133531637,0.212072809175268,0.179515757313079,0.14994416456255,0.12358505892293,0.100510579292344,0.0806616292762043,0.0638751026275517,0.0499120765521392,0.0384847857762759,0.0292807310172078,0.0219828584556888],[0.00255213344688908,0.00370714852163513,0.00531356502020352,0.00751521517264462,0.0104883276721383,0.0144437672387548,0.0196274609634119,0.0263182598341997,0.0348224741954725,0.0454644024137073,0.0585723644277108,0.074460068511043,0.0934035782207984,0.115614685867945,0.14121209620293,0.170192415899862,0.202403451179384,0.237522650285267,0.275043606783134,0.314273299697836,0.354342155944772,0.394228091889034,0.432794486603365,0.468840670298449,0.501162126846763,0.528616379653865,0.550189623358317,0.565058720150549,0.572643287840188,0.572643287840188,0.565058720150549,0.550189623358317,0.528616379653865,0.501162126846763,0.468840670298449,0.432794486603365,0.394228091889034,0.354342155944772,0.314273299697836,0.275043606783134,0.237522650285267,0.202403451179384,0.170192415899862,0.14121209620293,0.115614685867945,0.0934035782207984,0.074460068511043,0.0585723644277108,0.0454644024137072,0.0348224741954725,0.0263182598341997],[0.00216033432227369,0.00315902358748666,0.00455820841654654,0.00649000297133997,0.00911811403733465,0.0126407997138249,0.0172923289271273,0.023342202505784,0.0310913503036355,0.0408645520232188,0.0529984669970311,0.0678249183842699,0.0856494658693669,0.106725806407056,0.131227132171329,0.159216193873727,0.190616391795795,0.22518665931084,0.26250312547953,0.301950467357234,0.342725438309289,0.383854275067035,0.424224581583097,0.46263095148214,0.497832158229716,0.528616379653865,0.553869807831981,0.572643287840188,0.584211449611887,0.588119203984053,0.584211449611887,0.572643287840188,0.553869807831981,0.528616379653865,0.497832158229716,0.46263095148214,0.424224581583097,0.383854275067035,0.342725438309289,0.301950467357234,0.26250312547953,0.22518665931084,0.190616391795795,0.159216193873727,0.131227132171329,0.106725806407056,0.0856494658693668,0.0678249183842699,0.0529984669970311,0.0408645520232188,0.0310913503036355],[0.00180446290608457,0.00265628798869891,0.00385844012284923,0.00553041572239961,0.00782191692206263,0.0109163644444218,0.0150332285620231,0.0204284728407596,0.0273923283934298,0.0362436063266842,0.047319839874405,0.0609627479662844,0.077498841553397,0.0972154505603284,0.120333010706226,0.146975071174301,0.177138100154409,0.210663692721226,0.247216133531637,0.286268349312621,0.327099036366501,0.368803133657878,0.410316845527414,0.450457164667846,0.487974421026207,0.521614941239284,0.550189623358317,0.572643287840188,0.588119203984053,0.596013303753021,0.596013303753021,0.588119203984053,0.572643287840188,0.550189623358316,0.521614941239283,0.487974421026207,0.450457164667846,0.410316845527413,0.368803133657878,0.327099036366501,0.286268349312621,0.247216133531636,0.210663692721226,0.177138100154409,0.146975071174301,0.120333010706226,0.0972154505603284,0.0774988415533969,0.0609627479662843,0.0473198398744049,0.0362436063266843],[0.00148725130599982,0.00220397597048942,0.00322284009713001,0.00465029033468202,0.00662110972813867,0.0093023121594056,0.012896160807054,0.0176416571436387,0.0238137462755767,0.0317194372430102,0.0416900707336809,0.054069107049735,0.0691950726228376,0.087379699852504,0.108881810711915,0.133878096089058,0.162432574343936,0.194467098341135,0.229735731585067,0.267806040375428,0.308050271419555,0.349648951424394,0.391608645798388,0.432794486603365,0.471976716639932,0.507889034934368,0.539295138882965,0.565058720150549,0.584211449611887,0.596013303753021,0.6,0.596013303753021,0.584211449611887,0.565058720150549,0.539295138882965,0.507889034934368,0.471976716639932,0.432794486603365,0.391608645798388,0.349648951424394,0.308050271419555,0.267806040375427,0.229735731585067,0.194467098341135,0.162432574343936,0.133878096089058,0.108881810711915,0.087379699852504,0.0691950726228374,0.0540691070497349,0.0416900707336809],[0.00120956765827622,0.00180446290608457,0.00265628798869891,0.00385844012284923,0.00553041572239961,0.00782191692206264,0.0109163644444218,0.0150332285620231,0.0204284728407596,0.0273923283934298,0.0362436063266842,0.047319839874405,0.0609627479662844,0.0774988415533969,0.0972154505603285,0.120333010706226,0.146975071174301,0.177138100154409,0.210663692721226,0.247216133531637,0.286268349312621,0.327099036366501,0.368803133657878,0.410316845527414,0.450457164667846,0.487974421026207,0.521614941239284,0.550189623358317,0.572643287840188,0.588119203984053,0.596013303753021,0.596013303753021,0.588119203984053,0.572643287840188,0.550189623358316,0.521614941239284,0.487974421026207,0.450457164667846,0.410316845527413,0.368803133657878,0.327099036366501,0.28626834931262,0.247216133531637,0.210663692721226,0.177138100154408,0.146975071174301,0.120333010706226,0.0972154505603285,0.0774988415533968,0.0609627479662844,0.047319839874405],[0.000970700783174117,0.00145780175701476,0.00216033432227369,0.00315902358748665,0.00455820841654654,0.00649000297133997,0.00911811403733466,0.0126407997138249,0.0172923289271272,0.023342202505784,0.0310913503036355,0.0408645520232188,0.0529984669970311,0.0678249183842699,0.0856494658693669,0.106725806407056,0.131227132171329,0.159216193873727,0.190616391795795,0.22518665931084,0.26250312547953,0.301950467357234,0.342725438309289,0.383854275067035,0.424224581583097,0.46263095148214,0.497832158229716,0.528616379653865,0.553869807831981,0.572643287840188,0.584211449611887,0.588119203984053,0.584211449611887,0.572643287840188,0.553869807831981,0.528616379653865,0.497832158229716,0.46263095148214,0.424224581583097,0.383854275067035,0.342725438309289,0.301950467357234,0.26250312547953,0.22518665931084,0.190616391795795,0.159216193873727,0.131227132171329,0.106725806407056,0.0856494658693668,0.0678249183842699,0.0529984669970311],[0.000768687822230526,0.00116213983201199,0.00173370890350817,0.00255213344688907,0.00370714852163513,0.00531356502020352,0.00751521517264462,0.0104883276721383,0.0144437672387548,0.0196274609634119,0.0263182598341997,0.0348224741954725,0.0454644024137073,0.0585723644277108,0.074460068511043,0.0934035782207984,0.115614685867945,0.14121209620293,0.170192415899862,0.202403451179384,0.237522650285267,0.275043606783134,0.314273299697836,0.354342155944772,0.394228091889034,0.432794486603365,0.468840670298449,0.501162126846763,0.528616379653865,0.550189623358317,0.565058720150549,0.572643287840188,0.572643287840188,0.565058720150549,0.550189623358316,0.528616379653865,0.501162126846763,0.468840670298449,0.432794486603365,0.394228091889034,0.354342155944772,0.314273299697836,0.275043606783134,0.237522650285267,0.202403451179384,0.170192415899862,0.14121209620293,0.115614685867945,0.0934035782207983,0.074460068511043,0.0585723644277108],[0.000600653523041358,0.000914171570315839,0.0013729059917533,0.00203452624540213,0.002975056375451,0.00429275902338875,0.00611205462126438,0.00858711641353846,0.0119046568466222,0.0162853020866414,0.0219828584556887,0.0292807310172078,0.0384847857762759,0.0499120765521392,0.0638751026275517,0.0806616292762043,0.100510579292344,0.12358505892293,0.14994416456255,0.179515757313079,0.212072809175268,0.247216133531637,0.284366241056231,0.322766662556805,0.361500342322826,0.399519665209578,0.435689422244214,0.468840670298449,0.497832158229716,0.521614941239283,0.539295138882965,0.550189623358316,0.553869807831981,0.550189623358316,0.539295138882965,0.521614941239283,0.497832158229716,0.468840670298449,0.435689422244214,0.399519665209578,0.361500342322826,0.322766662556804,0.284366241056231,0.247216133531637,0.212072809175268,0.179515757313079,0.14994416456255,0.12358505892293,0.100510579292344,0.0806616292762043,0.0638751026275517],[0.000463134851459538,0.000709588293969345,0.00107279027571729,0.00160041502870444,0.00235591610298337,0.00342212939880444,0.0049050387277381,0.00693741797248112,0.00968194672041019,0.0133332776414984,0.0181184300533911,0.0242948158614006,0.0321451951514675,0.0419689330467932,0.054069107049735,0.0687353063956126,0.0862223698666218,0.106725806407056,0.130355194312447,0.157107401148196,0.186841934348759,0.219261041115406,0.253897249390649,0.290110820183939,0.327099036366501,0.36391839582758,0.399519665209578,0.432794486603365,0.46263095148214,0.487974421026207,0.507889034934368,0.521614941239284,0.528616379653865,0.528616379653865,0.521614941239283,0.507889034934368,0.487974421026207,0.46263095148214,0.432794486603365,0.399519665209578,0.36391839582758,0.327099036366501,0.290110820183939,0.253897249390649,0.219261041115406,0.186841934348759,0.157107401148195,0.130355194312447,0.106725806407056,0.0862223698666217,0.0687353063956126],[0.000352371118527594,0.000543493782922945,0.000827176643326463,0.00124225671278416,0.00184091547481606,0.00269194232927451,0.00388424899097365,0.00553041572239962,0.00776994424400035,0.0107717789637017,0.0147355506920122,0.0198909128860954,0.0264943010518157,0.0348224741954725,0.0451623144762508,0.0577965829382958,0.0729856603688545,0.0909457330521832,0.111824385623646,0.135675090712337,0.162432574343936,0.191891413089782,0.223690407961599,0.257305215333912,0.292051353575983,0.327099036366501,0.361500342322826,0.394228091889034,0.424224581583097,0.450457164667846,0.471976716639932,0.487974421026207,0.497832158229716,0.501162126846763,0.497832158229716,0.487974421026207,0.471976716639932,0.450457164667846,0.424224581583097,0.394228091889034,0.361500342322826,0.327099036366501,0.292051353575983,0.257305215333912,0.223690407961599,0.191891413089782,0.162432574343936,0.135675090712337,0.111824385623646,0.0909457330521831,0.0729856603688545],[0.000264546824937963,0.00041076376471543,0.000629348359576674,0.000951479619845099,0.00141944079618724,0.002089510138927,0.00303515650089026,0.00435037909052005,0.00615293777784241,0.00858711641353846,0.0118255564286688,0.0160696058955829,0.0215475686937563,0.0285102305220978,0.037223104426415,0.0479549960374576,0.0609627479662844,0.0764723819368924,0.094657298827537,0.115614685867945,0.139341764837855,0.165713930009679,0.194467098341135,0.22518665931084,0.257305215333912,0.290110820183939,0.322766662556805,0.354342155944772,0.383854275067035,0.410316845527414,0.432794486603365,0.450457164667846,0.46263095148214,0.468840670298449,0.468840670298449,0.46263095148214,0.450457164667846,0.432794486603365,0.410316845527413,0.383854275067035,0.354342155944772,0.322766662556804,0.290110820183939,0.257305215333912,0.22518665931084,0.194467098341135,0.165713930009679,0.139341764837855,0.115614685867945,0.094657298827537,0.0764723819368925],[0.000195981108137539,0.000306336733808653,0.000472490796072114,0.000719112827005399,0.00107996610373835,0.00160041502870444,0.00234026223317344,0.00337680384864244,0.00480791245344713,0.00675486501711125,0.0093645347519897,0.0128104723472374,0.0172923289271273,0.0230330388105312,0.0302732047187965,0.0392622300195993,0.0502459353553176,0.0634506849076839,0.0790644220032968,0.0972154505603285,0.11795025744925,0.14121209620293,0.166822380271916,0.194467098341135,0.223690407961599,0.253897249390649,0.284366241056231,0.314273299697836,0.342725438309289,0.368803133657878,0.391608645798388,0.410316845527413,0.424224581583097,0.432794486603365,0.435689422244214,0.432794486603365,0.424224581583097,0.410316845527413,0.391608645798388,0.368803133657878,0.342725438309289,0.314273299697836,0.284366241056231,0.253897249390649,0.223690407961599,0.194467098341135,0.166822380271916,0.14121209620293,0.11795025744925,0.0972154505603284,0.0790644220032968],[0.000143263405762178,0.000225431933761616,0.000350029790834632,0.000536295295709335,0.000810797448378931,0.00120956765827621,0.00178056308686857,0.00258638976077405,0.00370714852163513,0.00524318771128397,0.00731745791692661,0.0100770744617596,0.0136936148550558,0.0183616268297072,0.0242948158614006,0.0317194372430102,0.0408645520232188,0.0519490270377818,0.0651654652949747,0.0806616292762043,0.098520336475649,0.118739219450169,0.14121209620293,0.165713930009679,0.191891413089782,0.219261041115406,0.247216133531637,0.275043606783134,0.301950467357234,0.327099036366501,0.349648951424394,0.368803133657878,0.383854275067035,0.394228091889034,0.399519665209578,0.399519665209578,0.394228091889034,0.383854275067035,0.368803133657878,0.349648951424394,0.327099036366501,0.301950467357234,0.275043606783134,0.247216133531637,0.219261041115406,0.191891413089782,0.165713930009679,0.14121209620293,0.118739219450169,0.098520336475649,0.0806616292762043],[0.000103339353576486,0.000163697181626658,0.000255873948411392,0.000394657487125034,0.000600653523041358,0.000902063515786544,0.00133677899937174,0.00195475133026059,0.00282054366454997,0.00401590605143768,0.00564213753089713,0.00782191692206264,0.0107002059457553,0.0144437672387548,0.0192388111967165,0.0252863061055658,0.0327945711689488,0.0419689330467932,0.0529984669970311,0.0660401564818038,0.0812011699419676,0.0985203364756491,0.11795025744925,0.139341764837855,0.162432574343936,0.186841934348759,0.212072809175268,0.237522650285267,0.26250312547953,0.286268349312621,0.308050271419555,0.327099036366501,0.342725438309289,0.354342155944772,0.361500342322826,0.36391839582758,0.361500342322826,0.354342155944772,0.342725438309289,0.327099036366501,0.308050271419555,0.28626834931262,0.26250312547953,0.237522650285267,0.212072809175268,0.186841934348759,0.162432574343936,0.139341764837855,0.11795025744925,0.098520336475649,0.0812011699419676],[7.3553885049215e-05,0.000117294156088185,0.000184568056896474,0.000286580154249766,0.000439081451328283,0.00066382480550499,0.000990310239759257,0.00145780175701476,0.00211755683659172,0.00303515650089026,0.00429275902338875,0.00599102783094177,0.00825041076289934,0.0112113836026397,0.0150332285620231,0.0198909128860954,0.0259696787411795,0.0334570654521643,0.0425322660283115,0.0533529704756318,0.0660401564818038,0.0806616292762043,0.0972154505603285,0.115614685867945,0.135675090712337,0.157107401148196,0.179515757313079,0.202403451179384,0.22518665931084,0.247216133531637,0.267806040375428,0.286268349312621,0.301950467357234,0.314273299697836,0.322766662556805,0.327099036366501,0.327099036366501,0.322766662556805,0.314273299697836,0.301950467357234,0.286268349312621,0.267806040375428,0.247216133531637,0.22518665931084,0.202403451179384,0.179515757313079,0.157107401148196,0.135675090712337,0.115614685867945,0.0972154505603284,0.0806616292762043],[5.16600592298042e-05,8.29317738144211e-05,0.000131370065428871,0.000205343653509023,0.000316720052174475,0.000482035743303079,0.000723922928374824,0.00107279027571729,0.00156872468787629,0.00226353931127532,0.00322284009713001,0.00452792142923455,0.00627723536605609,0.00858711641353846,0.0115913947224954,0.015439507635816,0.02029273598075,0.0263182598341997,0.0336808577004802,0.0425322660283115,0.0529984669970311,0.0651654652949748,0.0790644220032968,0.0946572988275371,0.111824385623646,0.130355194312447,0.14994416456255,0.170192415899862,0.190616391795795,0.210663692721226,0.229735731585067,0.247216133531637,0.26250312547953,0.275043606783134,0.284366241056231,0.290110820183939,0.292051353575983,0.290110820183939,0.284366241056231,0.275043606783134,0.26250312547953,0.247216133531637,0.229735731585067,0.210663692721226,0.190616391795795,0.170192415899862,0.14994416456255,0.130355194312447,0.111824385623646,0.094657298827537,0.0790644220032968],[3.58025194823257e-05,5.7859535269399e-05,9.22668511192552e-05,0.000145186375817683,0.000225431933761616,0.000345393702892365,0.000522183086877311,0.00077900562569413,0.00114674747797958,0.001665729205055,0.00238753866629755,0.00337680384864244,0.00471270940824392,0.00649000297133997,0.00881918670294107,0.0118255564286688,0.0156467462582503,0.0204284728407596,0.0263182598341997,0.0334570654521643,0.0419689330467932,0.0519490270377818,0.0634506849076839,0.0764723819368924,0.0909457330521832,0.106725806407056,0.12358505892293,0.14121209620293,0.159216193873727,0.177138100154408,0.194467098341135,0.210663692721226,0.22518665931084,0.237522650285267,0.247216133531637,0.253897249390649,0.257305215333912,0.257305215333912,0.253897249390649,0.247216133531637,0.237522650285267,0.22518665931084,0.210663692721226,0.194467098341135,0.177138100154408,0.159216193873727,0.14121209620293,0.12358505892293,0.106725806407056,0.0909457330521831,0.0764723819368924],[2.44839614265361e-05,3.98325705918466e-05,6.39446757131121e-05,0.000101293097276049,0.00015833054967283,0.000244207498484917,0.00037167442428784,0.000558181921922969,0.000827176643326461,0.00120956765827621,0.00174530557548755,0.00248497596768341,0.00349126099840525,0.00484007228345976,0.00662110972813866,0.00893756328245512,0.0119046568466222,0.0156467462582504,0.02029273598075,0.0259696787411795,0.0327945711689487,0.0408645520232188,0.0502459353553176,0.0609627479662844,0.0729856603688545,0.0862223698666217,0.100510579292344,0.115614685867945,0.131227132171329,0.146975071174301,0.162432574343936,0.177138100154408,0.190616391795795,0.202403451179384,0.212072809175268,0.219261041115406,0.223690407961599,0.22518665931084,0.223690407961599,0.219261041115406,0.212072809175268,0.202403451179384,0.190616391795795,0.177138100154408,0.162432574343936,0.146975071174301,0.131227132171329,0.115614685867945,0.100510579292344,0.0862223698666217,0.0729856603688545],[1.65218696098483e-05,2.70589621278937e-05,4.37292960447961e-05,6.97337859512493e-05,0.000109729502323886,0.000170377790341954,0.00026104294504724,0.000394657487125035,0.000588759786402167,0.000866693099412327,0.00125893105084854,0.00180446290608457,0.00255213344688908,0.00356178914895618,0.0049050387277381,0.00666539792294538,0.00893756328245514,0.0118255564286688,0.015439507635816,0.0198909128860954,0.0252863061055658,0.0317194372430102,0.0392622300195994,0.0479549960374577,0.0577965829382958,0.0687353063956126,0.0806616292762043,0.0934035782207984,0.106725806407056,0.120333010706226,0.133878096089058,0.146975071174301,0.159216193873727,0.170192415899862,0.179515757313079,0.186841934348759,0.191891413089782,0.194467098341135,0.194467098341135,0.191891413089782,0.186841934348759,0.179515757313079,0.170192415899862,0.159216193873727,0.146975071174301,0.133878096089058,0.120333010706226,0.106725806407056,0.0934035782207983,0.0806616292762043,0.0687353063956126],[1.10013530252715e-05,1.81381647392463e-05,2.95086940710308e-05,4.73713808838571e-05,7.50397720913829e-05,0.000117294156088185,0.00018091336450473,0.00027534318567382,0.000413511338215675,0.000612787529098586,0.000896069760398338,0.00129295417097857,0.00184091547481606,0.00258638976077405,0.00358561373700357,0.0049050387277381,0.00662110972813867,0.00881918670294107,0.0115913947224954,0.0150332285620231,0.0192388111967165,0.0242948158614006,0.0302732047187965,0.037223104426415,0.0451623144762508,0.0540691070497349,0.0638751026275517,0.074460068511043,0.0856494658693669,0.0972154505603285,0.108881810711915,0.120333010706226,0.131227132171329,0.14121209620293,0.14994416456255,0.157107401148196,0.162432574343936,0.165713930009679,0.166822380271916,0.165713930009679,0.162432574343936,0.157107401148196,0.14994416456255,0.14121209620293,0.131227132171329,0.120333010706226,0.108881810711915,0.0972154505603284,0.0856494658693668,0.074460068511043,0.0638751026275517],[7.22840401891739e-06,1.19973397078484e-05,1.96488392933835e-05,3.17539862148389e-05,5.06371215200025e-05,7.96799724510649e-05,0.000123719668395553,0.000189556083180934,0.000286580154249766,0.000427527351963646,0.000629348359576674,0.000914171570315839,0.00131030900168851,0.00185322924494206,0.00258638976077405,0.00356178914895618,0.00484007228345977,0.00649000297133998,0.00858711641353846,0.0112113836026397,0.0144437672387548,0.0183616268297072,0.0230330388105312,0.0285102305220978,0.0348224741954726,0.0419689330467932,0.0499120765521392,0.0585723644277108,0.0678249183842699,0.077498841553397,0.087379699852504,0.0972154505603285,0.106725806407056,0.115614685867945,0.12358505892293,0.130355194312447,0.135675090712337,0.139341764837855,0.14121209620293,0.14121209620293,0.139341764837855,0.135675090712337,0.130355194312447,0.12358505892293,0.115614685867945,0.106725806407056,0.0972154505603284,0.087379699852504,0.0774988415533969,0.0678249183842699,0.0585723644277108],[4.68649484013765e-06,7.83043659362291e-06,1.29102073707747e-05,2.10034127032407e-05,3.37175430614134e-05,5.34110828015164e-05,8.34864993370552e-05,0.000128768763845601,0.000195981108137539,0.00029432509866777,0.000436163977371402,0.000637795862576539,0.000920286407594678,0.00131030900168851,0.00184091547481606,0.00255213344688907,0.00349126099840525,0.00471270940824391,0.00627723536605608,0.00825041076289933,0.0107002059457552,0.0136936148550558,0.0172923289271272,0.0215475686937563,0.0264943010518157,0.0321451951514675,0.0384847857762759,0.0454644024137073,0.0529984669970311,0.0609627479662844,0.0691950726228375,0.0774988415533969,0.0856494658693669,0.0934035782207984,0.100510579292344,0.106725806407056,0.111824385623646,0.115614685867945,0.11795025744925,0.118739219450169,0.11795025744925,0.115614685867945,0.111824385623646,0.106725806407056,0.100510579292344,0.0934035782207984,0.0856494658693668,0.0774988415533969,0.0691950726228374,0.0609627479662844,0.0529984669970311],[2.9982184657774e-06,5.04308635927536e-06,8.37025988999777e-06,1.37085300097679e-05,2.2154004440479e-05,3.53283209048876e-05,5.55908304410999e-05,8.63162837089357e-05,0.000132248791698088,0.000199940189141846,0.000298275712212873,0.000439081451328283,0.00063779586257654,0.000914171570315838,0.00129295417097857,0.00180446290608457,0.00248497596768341,0.00337680384864244,0.00452792142923454,0.00599102783094176,0.00782191692206263,0.0100770744617596,0.0128104723472374,0.0160696058955829,0.0198909128860954,0.0242948158614005,0.0292807310172078,0.0348224741954725,0.0408645520232188,0.0473198398744049,0.0540691070497349,0.0609627479662844,0.0678249183842699,0.074460068511043,0.0806616292762043,0.0862223698666217,0.0909457330521831,0.094657298827537,0.0972154505603284,0.098520336475649,0.098520336475649,0.0972154505603284,0.094657298827537,0.0909457330521831,0.0862223698666217,0.0806616292762043,0.0744600685110429,0.0678249183842699,0.0609627479662843,0.0540691070497349,0.047319839874405],[1.89272628310214e-06,3.20491296564298e-06,5.35493340366297e-06,8.82879260580174e-06,1.43634234095069e-05,2.30581265131566e-05,3.65257783522028e-05,5.70931950868451e-05,8.80599883059858e-05,0.000134023916789937,0.000201277576741507,0.000298275712212874,0.000436163977371403,0.000629348359576674,0.000896069760398339,0.00125893105084854,0.00174530557548755,0.00238753866629755,0.00322284009713001,0.00429275902338875,0.00564213753089713,0.00731745791692661,0.00936453475198971,0.0118255564286688,0.0147355506920122,0.0181184300533911,0.0219828584556888,0.0263182598341997,0.0310913503036355,0.0362436063266843,0.0416900707336809,0.047319839874405,0.0529984669970311,0.0585723644277108,0.0638751026275517,0.0687353063956126,0.0729856603688545,0.0764723819368925,0.0790644220032968,0.0806616292762043,0.0812011699419676,0.0806616292762043,0.0790644220032968,0.0764723819368925,0.0729856603688545,0.0687353063956126,0.0638751026275517,0.0585723644277108,0.0529984669970311,0.047319839874405,0.0416900707336809],[1.17902159080384e-06,2.00976594000481e-06,3.38048188051117e-06,5.61075299479357e-06,9.18910246722853e-06,1.48502732764156e-05,2.36812816953261e-05,3.72636480204375e-05,5.78595352693989e-05,8.864901613922e-05,0.000134023916789937,0.000199940189141846,0.00029432509866777,0.000427527351963646,0.000612787529098585,0.000866693099412324,0.00120956765827621,0.001665729205055,0.00226353931127532,0.00303515650089026,0.00401590605143768,0.00524318771128397,0.00675486501711125,0.00858711641353845,0.0107717789637017,0.0133332776414983,0.0162853020866414,0.0196274609634119,0.023342202505784,0.0273923283934298,0.0317194372430102,0.0362436063266842,0.0408645520232188,0.0454644024137072,0.0499120765521392,0.0540691070497349,0.0577965829382957,0.0609627479662843,0.0634506849076838,0.0651654652949747,0.0660401564818037,0.0660401564818037,0.0651654652949747,0.0634506849076838,0.0609627479662843,0.0577965829382958,0.0540691070497349,0.0499120765521392,0.0454644024137072,0.0408645520232188,0.0362436063266842],[7.24711428897925e-07,1.24360978509263e-06,2.10577787185976e-06,3.51844196319816e-06,5.80093010444873e-06,9.43744167287364e-06,1.51502686963628e-05,2.39991465075845e-05,3.75129022648921e-05,5.7859535269399e-05,8.80599883059858e-05,0.000132248791698088,0.00019598110813754,0.000286580154249766,0.000413511338215676,0.000588759786402167,0.000827176643326462,0.00114674747797958,0.00156872468787629,0.00211755683659172,0.00282054366454997,0.00370714852163514,0.00480791245344714,0.00615293777784241,0.00776994424400035,0.00968194672041019,0.0119046568466222,0.0144437672387548,0.0172923289271272,0.0204284728407596,0.0238137462755767,0.0273923283934298,0.0310913503036355,0.0348224741954725,0.0384847857762759,0.0419689330467932,0.0451623144762508,0.0479549960374576,0.0502459353553176,0.0519490270377818,0.0529984669970311,0.0533529704756318,0.0529984669970311,0.0519490270377817,0.0502459353553176,0.0479549960374577,0.0451623144762508,0.0419689330467932,0.0384847857762759,0.0348224741954725,0.0310913503036355],[4.39559701070744e-07,7.59332845721063e-07,1.2943624632155e-06,2.17715345186082e-06,3.61352927834837e-06,5.91811666596016e-06,9.56411685304937e-06,1.52516079097196e-05,2.39991465075845e-05,3.72636480204375e-05,5.70931950868451e-05,8.63162837089357e-05,0.000128768763845601,0.000189556083180934,0.00027534318567382,0.000394657487125035,0.00055818192192297,0.000779005625694131,0.00107279027571729,0.00145780175701476,0.00195475133026059,0.00258638976077406,0.00337680384864244,0.00435037909052005,0.00553041572239962,0.00693741797248112,0.00858711641353847,0.0104883276721384,0.0126407997138249,0.0150332285620231,0.0176416571436387,0.0204284728407596,0.023342202505784,0.0263182598341997,0.0292807310172078,0.0321451951514675,0.0348224741954725,0.037223104426415,0.0392622300195994,0.0408645520232188,0.0419689330467932,0.0425322660283115,0.0425322660283115,0.0419689330467932,0.0408645520232188,0.0392622300195994,0.037223104426415,0.0348224741954726,0.0321451951514675,0.0292807310172078,0.0263182598341997],[2.63075276374519e-07,4.57498472775216e-07,7.85070518721362e-07,1.32934313157544e-06,2.22113486903228e-06,3.66203230366726e-06,5.95770258351065e-06,9.56411685304934e-06,1.51502686963627e-05,2.36812816953261e-05,3.65257783522028e-05,5.55908304411e-05,8.34864993370552e-05,0.000123719668395553,0.00018091336450473,0.000261042945047239,0.000371674424287839,0.000522183086877311,0.000723922928374824,0.000990310239759256,0.00133677899937174,0.00178056308686857,0.00234026223317344,0.00303515650089026,0.00388424899097364,0.0049050387277381,0.00611205462126437,0.00751521517264463,0.00911811403733465,0.0109163644444218,0.0128961608070539,0.0150332285620231,0.0172923289271272,0.0196274609634119,0.0219828584556888,0.0242948158614006,0.0264943010518157,0.0285102305220978,0.0302732047187964,0.0317194372430102,0.0327945711689487,0.0334570654521643,0.0336808577004802,0.0334570654521643,0.0327945711689487,0.0317194372430102,0.0302732047187964,0.0285102305220978,0.0264943010518157,0.0242948158614005,0.0219828584556888],[1.55364434343856e-07,2.71992242729848e-07,4.69862553785088e-07,8.00929995215034e-07,1.34718639742489e-06,2.2359919032472e-06,3.66203230366728e-06,5.91811666596016e-06,9.43744167287364e-06,1.48502732764156e-05,2.30581265131566e-05,3.53283209048877e-05,5.34110828015165e-05,7.96799724510649e-05,0.000117294156088185,0.000170377790341954,0.000244207498484917,0.000345393702892366,0.000482035743303079,0.000663824805504991,0.000902063515786544,0.00120956765827622,0.00160041502870444,0.002089510138927,0.00269194232927451,0.00342212939880444,0.00429275902338875,0.00531356502020352,0.00649000297133997,0.00782191692206265,0.0093023121594056,0.0109163644444218,0.0126407997138249,0.0144437672387548,0.0162853020866414,0.0181184300533911,0.0198909128860954,0.0215475686937563,0.0230330388105312,0.0242948158614006,0.0252863061055658,0.0259696787411795,0.0263182598341997,0.0263182598341997,0.0259696787411795,0.0252863061055659,0.0242948158614006,0.0230330388105312,0.0215475686937563,0.0198909128860954,0.0181184300533911],[9.05383525949556e-08,1.5956322093352e-07,2.77486850509873e-07,4.76169339641005e-07,8.0628736657891e-07,1.34718639742488e-06,2.22113486903228e-06,3.61352927834837e-06,5.80093010444872e-06,9.18910246722853e-06,1.43634234095068e-05,2.21540044404789e-05,3.37175430614135e-05,5.06371215200023e-05,7.50397720913829e-05,0.000109729502323886,0.00015833054967283,0.000225431933761616,0.000316720052174474,0.000439081451328283,0.000600653523041357,0.000810797448378931,0.00107996610373835,0.00141944079618724,0.00184091547481605,0.00235591610298337,0.002975056375451,0.00370714852163513,0.00455820841654654,0.00553041572239961,0.00662110972813865,0.00782191692206263,0.00911811403733464,0.0104883276721383,0.0119046568466222,0.0133332776414983,0.0147355506920122,0.0160696058955829,0.0172923289271272,0.0183616268297072,0.0192388111967164,0.0198909128860954,0.02029273598075,0.0204284728407596,0.02029273598075,0.0198909128860954,0.0192388111967164,0.0183616268297072,0.0172923289271272,0.0160696058955829,0.0147355506920122],[5.20622549555798e-08,9.23673486411887e-08,1.61704977191391e-07,2.79342942946985e-07,4.76169339641007e-07,8.00929995215034e-07,1.32934313157545e-06,2.17715345186082e-06,3.51844196319816e-06,5.61075299479358e-06,8.82879260580174e-06,1.37085300097679e-05,2.10034127032407e-05,3.17539862148389e-05,4.73713808838572e-05,6.97337859512493e-05,0.000101293097276049,0.000145186375817683,0.000205343653509023,0.000286580154249766,0.000394657487125034,0.000536295295709336,0.000719112827005401,0.000951479619845099,0.00124225671278416,0.00160041502870444,0.00203452624540213,0.00255213344688908,0.00315902358748665,0.00385844012284923,0.00465029033468202,0.00553041572239961,0.00649000297133997,0.00751521517264463,0.00858711641353847,0.0096819467204102,0.0107717789637017,0.0118255564286688,0.0128104723472374,0.0136936148550558,0.0144437672387548,0.0150332285620231,0.015439507635816,0.0156467462582504,0.0156467462582504,0.015439507635816,0.0150332285620231,0.0144437672387548,0.0136936148550558,0.0128104723472374,0.0118255564286688],[2.95408350901903e-08,5.27610667475303e-08,9.29851881421736e-08,1.61704977191391e-07,2.77486850509872e-07,4.69862553785087e-07,7.85070518721361e-07,1.29436246321549e-06,2.10577787185975e-06,3.38048188051115e-06,5.35493340366295e-06,8.37025988999775e-06,1.29102073707747e-05,1.96488392933834e-05,2.95086940710307e-05,4.37292960447957e-05,6.39446757131119e-05,9.22668511192552e-05,0.000131370065428871,0.000184568056896474,0.000255873948411391,0.000350029790834631,0.000472490796072112,0.000629348359576672,0.00082717664332646,0.00107279027571729,0.0013729059917533,0.00173370890350817,0.00216033432227369,0.0026562879886989,0.00322284009713,0.00385844012284922,0.00455820841654653,0.00531356502020351,0.00611205462126436,0.0069374179724811,0.00776994424400033,0.00858711641353845,0.00936453475198968,0.0100770744617596,0.0107002059457552,0.0112113836026396,0.0115913947224953,0.0118255564286688,0.0119046568466221,0.0118255564286688,0.0115913947224953,0.0112113836026396,0.0107002059457552,0.0100770744617596,0.00936453475198968],[1.65398653139756e-08,2.9738431915035e-08,5.27610667475305e-08,9.23673486411884e-08,1.5956322093352e-07,2.71992242729848e-07,4.57498472775216e-07,7.5933284572106e-07,1.24360978509263e-06,2.00976594000481e-06,3.20491296564297e-06,5.04308635927536e-06,7.83043659362291e-06,1.19973397078484e-05,1.81381647392463e-05,2.70589621278937e-05,3.98325705918466e-05,5.78595352693991e-05,8.29317738144211e-05,0.000117294156088185,0.000163697181626658,0.000225431933761616,0.000306336733808653,0.000410763764715429,0.000543493782922945,0.000709588293969343,0.000914171570315837,0.00116213983201199,0.00145780175701476,0.00180446290608457,0.00220397597048941,0.0026562879886989,0.00315902358748665,0.00370714852163513,0.00429275902338875,0.00490503872773809,0.0055304157223996,0.0061529377778424,0.00675486501711125,0.0073174579169266,0.00782191692206263,0.00825041076289932,0.00858711641353845,0.00881918670294106,0.00893756328245513,0.00893756328245512,0.00881918670294106,0.00858711641353845,0.00825041076289932,0.00782191692206263,0.0073174579169266],[9.13798784682758e-09,1.65398653139756e-08,2.95408350901905e-08,5.20622549555798e-08,9.05383525949556e-08,1.55364434343856e-07,2.6307527637452e-07,4.39559701070744e-07,7.24711428897925e-07,1.17902159080385e-06,1.89272628310214e-06,2.9982184657774e-06,4.68649484013767e-06,7.22840401891739e-06,1.10013530252715e-05,1.65218696098483e-05,2.44839614265362e-05,3.58025194823259e-05,5.16600592298042e-05,7.3553885049215e-05,0.000103339353576486,0.000143263405762178,0.00019598110813754,0.000264546824937963,0.000352371118527595,0.000463134851459538,0.000600653523041358,0.000768687822230527,0.000970700783174119,0.00120956765827622,0.00148725130599982,0.00180446290608458,0.00216033432227369,0.00255213344688908,0.00297505637545101,0.00342212939880444,0.00388424899097364,0.00435037909052005,0.00480791245344714,0.00524318771128398,0.00564213753089712,0.00599102783094176,0.00627723536605608,0.00649000297133997,0.00662110972813867,0.00666539792294538,0.00662110972813867,0.00649000297133997,0.00627723536605608,0.00599102783094177,0.00564213753089713]],"type":"surface","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"paper_bgcolor":"transparent","scene":{"xaxis":{"title":"X1"},"yaxis":{"title":"X2"},"zaxis":{"title":"Log Normal Densities"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.333333333333333}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"","ticklen":2,"len":0.333333333333333,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgb(255,112,183)"],["1","rgb(128,0,64)"]],"showscale":true,"x":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"y":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"z":[[0.00444359861529692,0.00441407315209244,0.00432666864755998,0.00418482357737072,0.00399401855396117,0.00376142502059808,0.00349545847418932,0.00320527496896476,0.00290025272701336,0.00258949932731576,0.00228141959920296,0.00198337091696734,0.00170142229792605,0.00144022288151579,0.00120297527072305,0.000991500870666543,0.000806378438850811,0.000647133855449412,0.000512458548153684,0.000400435682027572,0.000308756567639692,0.000234914079018396,0.000176364549958642,0.000130654072091693,9.55089371747855e-05,6.88929023843241e-05,4.90359233661433e-05,3.44400394865361e-05,2.38683463215505e-05,1.63226409510241e-05,1.10145797398989e-05,7.334235350181e-06,4.8189360126116e-06,3.1243298934251e-06,1.99881231051827e-06,1.26181752206809e-06,7.86014393869231e-07,4.83140952598617e-07,2.93039800713828e-07,1.75383517583013e-07,1.03576289562571e-07,6.03589017299704e-08,3.47081699703865e-08,1.9693890060127e-08,1.10265768759837e-08,6.09199189788505e-09,3.32114113225121e-09,1.78658939856142e-09,9.48356402924291e-10,4.96738493394313e-10,2.56739953204248e-10],[0.00595837552163676,0.00595837552163676,0.00587945780196071,0.00572474427569231,0.0055002738419329,0.00521461128137509,0.00487830527795108,0.00450324334474084,0.00410195851856161,0.00368694381493308,0.00327002581849207,0.00286183934892583,0.00247143234775675,0.0021060157249911,0.0017708586591326,0.00146931731365961,0.00120297527072305,0.000971867838009839,0.000774759888007996,0.000609447713543893,0.00047305886264623,0.000362329188615297,0.000273842509810286,0.000204224489205769,0.000150287955841078,0.000109131454417772,7.819610405879e-05,5.52878492096141e-05,3.85730235129327e-05,2.65550470612311e-05,1.80393080852625e-05,1.20921098261642e-05,7.99822647189894e-06,5.22029106241527e-06,3.36205757285024e-06,2.13660864376199e-06,1.33984396000321e-06,8.29073190061756e-07,5.06221897147374e-07,3.04998981850144e-07,1.81328161819899e-07,1.06375480622347e-07,6.15782324274592e-08,3.51740444983538e-08,1.982562127669e-08,1.10265768759837e-08,6.05151369582517e-09,3.2771531553391e-09,1.75121255821382e-09,9.23401114835194e-10,4.80453468540155e-10],[0.00788370428577923,0.00793643789774812,0.00788370428577923,0.00772759648166358,0.00747425573509311,0.0071334706305035,0.00671804964117307,0.00624302316799314,0.00572474427569231,0.00517996282933357,0.00462494531498742,0.00407470308084292,0.00354237668013568,0.00303880561103103,0.00257229341523282,0.00214856006475334,0.0017708586591326,0.0014402228815158,0.00115580593567211,0.000915270661168869,0.000715193517144861,0.000551451095550976,0.000419565573051116,0.000314993864048076,0.000233353193889754,0.000170582632274262,0.000123045371264316,8.75800436192475e-05,6.15112340795035e-05,4.26297838087414e-05,2.9152864029864e-05,1.96724627140205e-05,1.3099226195589e-05,8.60680491384983e-06,5.58017325999851e-06,3.56995560244198e-06,2.25365458700744e-06,1.40385191457317e-06,8.62908308810329e-07,5.23380345814241e-07,3.13241702523392e-07,1.84991233673249e-07,1.07803318127594e-07,6.1990125428116e-08,3.51740444983537e-08,1.9693890060127e-08,1.08805316494408e-08,5.93168569614551e-09,3.19091732580136e-09,1.69380098167069e-09,8.8719403923514e-10],[0.010293005090544,0.0104311641721669,0.0104311641721669,0.010293005090544,0.010022152374682,0.00962917815916989,0.00912907657003718,0.00854031489815827,0.00788370428577922,0.00718118597580114,0.00645463114694013,0.0057247442756923,0.00501014344842975,0.00432666864755998,0.00368694381493307,0.00310019355645468,0.00257229341523282,0.0021060157249911,0.00170142229792605,0.00135635083026808,0.00106694335246963,0.000828171141856104,0.000634319746563399,0.000479408551336933,0.000357530197139557,0.000263104991416689,0.000191053436166511,0.000136895769006016,9.67909172117888e-05,6.75287315173662e-05,4.64891906341662e-05,3.15809205892381e-05,2.11693241432259e-05,1.40022751354938e-05,9.13902000651191e-06,5.88586173720116e-06,3.74050199652905e-06,2.34562797546544e-06,1.45143563457388e-06,8.86228754383629e-07,5.33953330143356e-07,3.17446226427337e-07,1.86228628631323e-07,1.07803318127594e-07,6.1578232427459e-08,3.47081699703865e-08,1.93039249091855e-08,1.05942186124464e-08,5.73722190831072e-09,3.06579966783812e-09,1.61657293848473e-09],[0.0132606085907303,0.0135284906538333,0.0136189818938397,0.0135284906538333,0.0132606085907303,0.0128258741311443,0.0122410845531381,0.0115282192847515,0.0107130705970553,0.00982370046134149,0.0088888517609989,0.00793643789774811,0.00699221844809223,0.00607874269155644,0.00521461128137509,0.00441407315209244,0.00368694381493307,0.00303880561103102,0.00247143234775675,0.00198337091696734,0.00157061073532225,0.0012272769832107,0.00094629386412483,0.000719977402492236,0.000540531632252621,0.000400435682027572,0.000292720967552189,0.00021114670144965,0.000150287955841077,0.000105553699781887,7.31530015492573e-05,5.00265147275886e-05,3.3758081013335e-05,2.24783620409423e-05,1.47693362936527e-05,9.57561560633791e-06,6.12606831148571e-06,3.86728673629915e-06,2.40901951889891e-06,1.48075657935485e-06,8.98124264949923e-07,5.37524911052607e-07,3.17446226427338e-07,1.84991233673249e-07,1.06375480622347e-07,6.03589017299704e-08,3.3794850486292e-08,1.86710669746599e-08,1.01788133589094e-08,5.47563602773968e-09,2.90657410772236e-09],[0.0168575374037106,0.0173131191607864,0.0175455065561331,0.0175455065561331,0.0173131191607864,0.0168575374037106,0.0161965439076004,0.0153553592070208,0.0143650457958376,0.0132606085907303,0.0120789533689274,0.0108568680577609,0.00962917815916989,0.00842719980921658,0.00727757629628117,0.00620154143960373,0.00521461128137509,0.00432666864755998,0.00354237668013568,0.00286183934892583,0.00228141959920296,0.00179462821951634,0.00139300675928467,0.00106694335246963,0.00080637843885081,0.00060137567719103,0.00044254987033666,0.000321357162202053,0.00023026246859491,0.000162804998989945,0.000113585193561303,7.819610405879e-05,5.31199816340433e-05,3.56073885343443e-05,2.35522139365917e-05,1.53720843421044e-05,9.9001821842771e-06,6.29162778191576e-06,3.94541111064009e-06,2.44135486911151e-06,1.49066126883147e-06,8.98124264949922e-07,5.33953330143356e-07,3.13241702523392e-07,1.81328161819899e-07,1.03576289562571e-07,5.83801015606385e-08,3.24697354449611e-08,1.78197686330893e-08,9.65016588223688e-09,5.1567598097414e-09],[0.0211462914953401,0.0218630474459658,0.0223047103014429,0.0224539051336535,0.0223047103014429,0.0218630474459658,0.0211462914953402,0.0201821364791977,0.0190068203480652,0.0176628673678771,0.0161965439076004,0.0146552389704592,0.0130849739756079,0.0115282192847515,0.0100221523746821,0.00859744053803597,0.00727757629628117,0.00607874269155644,0.00501014344842975,0.00407470308084292,0.00327002581849207,0.00258949932731576,0.00202343766726017,0.00156017482211562,0.00118704205791238,0.000891185999581158,0.000660206826506172,0.000482615285583216,0.000348122057918207,0.000247782949525226,0.000174028630031493,0.00012060890966982,8.24797789303687e-05,5.56576662247035e-05,3.70605536273999e-05,2.43505189014685e-05,1.57875211302174e-05,1.01001791309085e-05,6.37607790203289e-06,3.97180172234043e-06,2.44135486911152e-06,1.48075657935485e-06,8.8622875438363e-07,5.23380345814242e-07,3.04998981850144e-07,1.75383517583013e-07,9.9515005158301e-08,5.57182888368804e-08,3.07833852677884e-08,1.67820260953206e-08,9.02779758954641e-09],[0.0261748200130662,0.0272430346821459,0.0279792886978622,0.0283548440188743,0.0283548440188744,0.0279792886978622,0.0272430346821459,0.0261748200130662,0.0248154029509433,0.0232149827969817,0.0214301301009784,0.0195204873448052,0.0175455065561331,0.0155614683371894,0.0136189818938397,0.0117611047624258,0.0100221523746821,0.00842719980921658,0.00699221844809223,0.00572474427569231,0.00462494531498741,0.00368694381493308,0.00290025272701337,0.00225120256576163,0.00172425984051604,0.00130316755350706,0.000971867838009839,0.000715193517144861,0.000519337083796087,0.00037212128128198,0.00026310499141669,0.000183562123782546,0.000126370722120622,8.5845842563734e-05,5.75441891392905e-05,3.80621300578968e-05,2.4842432013625e-05,1.59994310050563e-05,1.01677386064797e-05,6.37607790203289e-06,3.94541111064011e-06,2.40901951889891e-06,1.45143563457388e-06,8.62908308810332e-07,5.06221897147374e-07,2.93039800713829e-07,1.67386990236189e-07,9.43465877295696e-08,5.2473508298577e-08,2.87980719409356e-08,1.55953860591983e-08],[0.0319699973583051,0.0334972902368784,0.0346326846918545,0.0353323113313541,0.0355686469837545,0.0353323113313541,0.0346326846918545,0.0334972902368784,0.0319699973583051,0.0301082096508339,0.0279792886978621,0.0256565238508506,0.0232149827969817,0.0207275668690903,0.0182615522622865,0.0158758308503845,0.0136189818938397,0.0115282192847515,0.00962917815916988,0.00793643789774811,0.00645463114694013,0.00517996282933357,0.00410195851856161,0.00320527496896476,0.00247143234775675,0.00188036244303331,0.00141170455772781,0.00104581645858419,0.000764498318653054,0.000551451095550974,0.000392506524268112,0.000275674225477117,0.000191053436166511,0.000130654072091693,8.81658611320587e-05,5.87066588706572e-05,3.85730235129327e-05,2.50086015099281e-05,1.59994310050563e-05,1.01001791309085e-05,6.29162778191576e-06,3.86728673629915e-06,2.34562797546544e-06,1.40385191457317e-06,8.29073190061753e-07,4.83140952598617e-07,2.77820467600072e-07,1.57639130787856e-07,8.82618918349101e-08,4.87631597727661e-08,2.65839709530959e-08],[0.0385310552921972,0.0406418319775229,0.0423004566051226,0.0434436435299832,0.0440267709878692,0.0440267709878692,0.0434436435299832,0.0423004566051226,0.0406418319775229,0.0385310552921972,0.03604607136649,0.0332747177014261,0.0303096016091382,0.0272430346821459,0.0241624042177895,0.0211462914953401,0.0182615522622865,0.0155614683371893,0.0130849739756079,0.0108568680577609,0.0088888517609989,0.00718118597580115,0.00572474427569231,0.00450324334474084,0.00349545847418932,0.00267727070095846,0.00202343766726017,0.00150902620751688,0.00111048613670333,0.000806378438850809,0.000577795399608218,0.000408525019399057,0.000285018234642431,0.00019621673244518,0.000133293459427897,8.93492778599579e-05,5.90993440928136e-05,3.85730235129327e-05,2.4842432013625e-05,1.57875211302174e-05,9.9001821842771e-06,6.12606831148569e-06,3.74050199652905e-06,2.25365458700744e-06,1.3398439600032e-06,7.86014393869231e-07,4.55005013879485e-07,2.59902988024618e-07,1.46492627809615e-07,8.14759951158149e-08,4.47149741818993e-08],[0.0458235375970751,0.0486571069125697,0.0509815879579283,0.0527096146688646,0.0537744195174696,0.0541341132946451,0.0537744195174696,0.0527096146688646,0.0509815879579283,0.0486571069125697,0.0458235375970751,0.0425834017517011,0.0390482429518072,0.0353323113313541,0.03154655991627,0.0277933804891206,0.0241624042177895,0.0207275668690903,0.0175455065561331,0.0146552389704592,0.0120789533689274,0.00982370046134149,0.00788370428577922,0.00624302316799313,0.00487830527795107,0.00376142502059809,0.00286183934892583,0.00214856006475334,0.00159169244419837,0.00116353705032503,0.000839287367232362,0.000597379840265559,0.000419565573051116,0.000290775984914268,0.000198850474808582,0.000134185051161005,8.93492778599579e-05,5.87066588706572e-05,3.80621300578966e-05,2.43505189014685e-05,1.53720843421044e-05,9.57561560633789e-06,5.88586173720116e-06,3.56995560244198e-06,2.13660864376198e-06,1.26181752206809e-06,7.35321955800059e-07,4.22832071051303e-07,2.39920696720254e-07,1.34331206198904e-07,7.42156545046393e-08],[0.0537744195174695,0.0574815799110812,0.0606304887014555,0.0631048658850247,0.0648103003735523,0.0656802243170994,0.0656802243170994,0.0648103003735523,0.0631048658850247,0.0606304887014555,0.0574815799110812,0.0537744195174695,0.0496400456740287,0.04521661225618,0.0406418319775229,0.03604607136649,0.03154655991627,0.0272430346821459,0.0232149827969817,0.0195204873448052,0.0161965439076004,0.0132606085907303,0.0107130705970553,0.00854031489815828,0.00671804964117306,0.00521461128137509,0.00399401855396118,0.00301861428615636,0.00225120256576162,0.00165665064512227,0.00120297527072305,0.000861969447319046,0.000609447713543893,0.000425197241717693,0.000292720967552189,0.000198850474808582,0.000133293459427898,8.81658611320587e-05,5.75441891392904e-05,3.70605536274e-05,2.35522139365918e-05,1.47693362936526e-05,9.13902000651192e-06,5.58017325999852e-06,3.36205757285024e-06,1.99881231051827e-06,1.17259568547565e-06,6.78787717256027e-07,3.87729696525888e-07,2.18541320188754e-07,1.21547901778672e-07],[0.062269052147199,0.0670070528615627,0.0711505376047041,0.074549590415764,0.0770764572452968,0.0786335049661667,0.0791594796334459,0.0786335049661668,0.0770764572452968,0.074549590415764,0.0711505376047041,0.0670070528615627,0.0622690521471989,0.0570996439129113,0.0516658943689313,0.046130048415225,0.0406418319775229,0.0353323113313541,0.0303096016091382,0.0256565238508507,0.0214301301009784,0.0176628673678772,0.0143650457958376,0.0115282192847515,0.00912907657003719,0.00713347063050351,0.00550027384193289,0.00418482357737072,0.00314180627216261,0.00232750733227017,0.00170142229792605,0.00122727698321071,0.000873539334459006,0.000613524271729785,0.000425197241717693,0.000290775984914269,0.000196216732445181,0.000130654072091693,8.58458425637342e-05,5.56576662247035e-05,3.56073885343443e-05,2.24783620409423e-05,1.40022751354938e-05,8.60680491384984e-06,5.22029106241527e-06,3.12432989342511e-06,1.84513631719825e-06,1.07524996423906e-06,6.18300821416875e-07,3.50832337500352e-07,1.96430452704769e-07],[0.0711505376047041,0.0770764572452968,0.0823900392819534,0.0869034628749644,0.0904500604748912,0.0928945098919035,0.0941413974686199,0.0941413974686199,0.0928945098919035,0.0904500604748912,0.0869034628749645,0.0823900392819534,0.0770764572452968,0.0711505376047041,0.0648103003735523,0.0582531332350027,0.0516658943689313,0.0452166122561799,0.0390482429518072,0.0332747177014261,0.0279792886978621,0.0232149827969817,0.0190068203480652,0.0153553592070208,0.0122410845531381,0.00962917815916989,0.00747425573509311,0.00572474427569231,0.00432666864755998,0.00322671485563984,0.00237452609930412,0.00172425984051604,0.00123548616329471,0.000873539334459005,0.000609447713543892,0.000419565573051116,0.000285018234642431,0.000191053436166511,0.000126370722120622,8.24797789303686e-05,5.31199816340433e-05,3.37580810133349e-05,2.11693241432259e-05,1.3099226195589e-05,7.99822647189894e-06,4.8189360126116e-06,2.86495647885114e-06,1.6807158873788e-06,9.72926401385327e-07,5.55744378929111e-07,3.13241702523392e-07],[0.0802220071374839,0.087484754780886,0.0941413974686199,0.0999627763750334,0.10473826743213,0.108288382895958,0.110475953339786,0.111214920181278,0.110475953339786,0.108288382895958,0.10473826743213,0.0999627763750334,0.0941413974686199,0.087484754780886,0.0802220071374839,0.0725878738079432,0.0648103003735523,0.0570996439129113,0.0496400456740287,0.0425834017517012,0.03604607136649,0.0301082096508339,0.0248154029509433,0.0201821364791977,0.0161965439076004,0.0128258741311443,0.0100221523746821,0.00772759648166358,0.00587945780196071,0.00441407315209244,0.00327002581849207,0.00239040915800238,0.00172425984051604,0.0012272769832107,0.000861969447319045,0.000597379840265559,0.000408525019399058,0.000275674225477117,0.000183562123782546,0.00012060890966982,7.819610405879e-05,5.00265147275886e-05,3.15809205892381e-05,1.96724627140206e-05,1.20921098261642e-05,7.334235350181e-06,4.38951975589341e-06,2.59231982310907e-06,1.51066984651126e-06,8.68680249293133e-07,4.92901047262913e-07],[0.0892520640593719,0.0979833807828676,0.106144129249152,0.113461610599908,0.119677171542053,0.124561289565839,0.127927608726522,0.129644732227423,0.129644732227423,0.127927608726522,0.124561289565839,0.119677171542053,0.113461610599908,0.106144129249152,0.0979833807828676,0.0892520640593719,0.0802220071374838,0.0711505376047041,0.0622690521471989,0.0537744195174695,0.0458235375970751,0.0385310552921972,0.0319699973583051,0.0261748200130662,0.0211462914953401,0.0168575374037106,0.0132606085907303,0.010293005090544,0.00788370428577922,0.00595837552163675,0.00444359861529692,0.00327002581849207,0.00237452609930412,0.00170142229792605,0.00120297527072305,0.000839287367232362,0.000577795399608218,0.000392506524268112,0.00026310499141669,0.000174028630031493,0.000113585193561303,7.3153001549257e-05,4.64891906341662e-05,2.9152864029864e-05,1.80393080852624e-05,1.10145797398989e-05,6.63629057752583e-06,3.94541111064009e-06,2.31456051246988e-06,1.3398439600032e-06,7.65331014096921e-07],[0.0979833807828676,0.108288382895958,0.118092066769606,0.12707759453053,0.134935634119589,0.141381872783512,0.146174027410271,0.149126938641066,0.15012443954056,0.149126938641066,0.146174027410271,0.141381872783512,0.134935634119589,0.12707759453053,0.118092066769606,0.108288382895958,0.0979833807828676,0.0874847547808859,0.0770764572452968,0.0670070528615627,0.0574815799110812,0.0486571069125697,0.0406418319775229,0.0334972902368784,0.0272430346821459,0.0218630474459658,0.0173131191607864,0.0135284906538333,0.0104311641721669,0.00793643789774811,0.00595837552163676,0.00441407315209245,0.00322671485563985,0.00232750733227017,0.00165665064512228,0.00116353705032503,0.00080637843885081,0.000551451095550975,0.00037212128128198,0.000247782949525226,0.000162804998989945,0.000105553699781887,6.75287315173662e-05,4.26297838087415e-05,2.65550470612311e-05,1.63226409510241e-05,9.90018218427707e-06,5.92523190352158e-06,3.49926574527763e-06,2.03919099765271e-06,1.17259568547565e-06],[0.106144129249152,0.118092066769606,0.129644732227423,0.140442461814151,0.15012443954056,0.158348433523511,0.164810755687758,0.1692648329271,0.171536810222608,0.171536810222608,0.1692648329271,0.164810755687758,0.158348433523511,0.15012443954056,0.140442461814151,0.129644732227423,0.118092066769606,0.106144129249152,0.0941413974686199,0.0823900392819534,0.0711505376047041,0.0606304887014555,0.0509815879579283,0.0423004566051226,0.0346326846918545,0.0279792886978622,0.0223047103014429,0.0175455065561331,0.0136189818938397,0.0104311641721669,0.00788370428577923,0.00587945780196071,0.00432666864755999,0.00314180627216261,0.00225120256576162,0.00159169244419837,0.00111048613670334,0.000764498318653055,0.000519337083796086,0.000348122057918207,0.000230262468594911,0.000150287955841077,9.67909172117889e-05,6.15112340795036e-05,3.85730235129327e-05,2.38683463215506e-05,1.45737188242281e-05,8.78067390645846e-06,5.22029106241526e-06,3.06246401650793e-06,1.7727874873605e-06],[0.113461610599908,0.12707759453053,0.140442461814151,0.153157154390045,0.164810755687758,0.17500208365302,0.183362404522089,0.189577494037488,0.193407213455959,0.194700902383989,0.193407213455959,0.189577494037488,0.183362404522089,0.17500208365302,0.164810755687758,0.153157154390045,0.140442461814151,0.12707759453053,0.113461610599908,0.0999627763750334,0.0869034628749645,0.074549590415764,0.0631048658850247,0.0527096146688645,0.0434436435299832,0.0353323113313541,0.0283548440188743,0.0224539051336535,0.0175455065561331,0.0135284906538333,0.010293005090544,0.00772759648166358,0.00572474427569231,0.00418482357737072,0.00301861428615636,0.00214856006475334,0.00150902620751688,0.00104581645858419,0.00071519351714486,0.000482615285583216,0.000321357162202053,0.00021114670144965,0.000136895769006016,8.75800436192475e-05,5.52878492096141e-05,3.44400394865361e-05,2.11693241432259e-05,1.28398441382614e-05,7.68461152614904e-06,4.5383030262895e-06,2.6446881573138e-06],[0.119677171542053,0.134935634119589,0.15012443954056,0.164810755687758,0.178537360250285,0.190845566208414,0.201300311571489,0.209515533131891,0.21517777503787,0.218066024244334,0.218066024244334,0.21517777503787,0.209515533131891,0.201300311571489,0.190845566208414,0.178537360250285,0.164810755687758,0.15012443954056,0.134935634119589,0.119677171542053,0.10473826743213,0.0904500604748913,0.0770764572452968,0.0648103003735523,0.0537744195174695,0.0440267709878692,0.0355686469837545,0.0283548440188743,0.0223047103014429,0.0173131191607864,0.0132606085907303,0.0100221523746821,0.00747425573509311,0.00550027384193289,0.00399401855396117,0.00286183934892583,0.00202343766726017,0.00141170455772781,0.000971867838009839,0.000660206826506171,0.00044254987033666,0.000292720967552189,0.000191053436166511,0.000123045371264316,7.81961040587898e-05,4.90359233661433e-05,3.03426149808481e-05,1.8526827672323e-05,1.11624242428346e-05,6.63629057752583e-06,3.89315477887558e-06],[0.124561289565839,0.141381872783512,0.158348433523511,0.17500208365302,0.190845566208414,0.205366847613037,0.218066024244334,0.228483625539526,0.236228103963181,0.241000228215217,0.242612263885053,0.241000228215217,0.236228103963181,0.228483625539526,0.218066024244334,0.205366847613037,0.190845566208414,0.17500208365302,0.158348433523511,0.141381872783512,0.124561289565839,0.108288382895958,0.0928945098919035,0.0786335049661667,0.0656802243170993,0.0541341132946451,0.0440267709878692,0.0353323113313541,0.0279792886978621,0.0218630474459658,0.0168575374037106,0.0128258741311443,0.00962917815916989,0.00713347063050349,0.00521461128137509,0.00376142502059809,0.00267727070095846,0.00188036244303331,0.00130316755350706,0.000891185999581157,0.00060137567719103,0.000400435682027572,0.000263104991416689,0.000170582632274262,0.000109131454417772,6.88929023843241e-05,4.29149318046831e-05,2.63786022171354e-05,1.59994310050563e-05,9.57561560633791e-06,5.65507379741364e-06],[0.127927608726522,0.146174027410271,0.164810755687758,0.183362404522089,0.201300311571489,0.218066024244334,0.233099300949596,0.245868755771919,0.25590285004469,0.262818727926023,0.266346443473052,0.266346443473052,0.262818727926023,0.25590285004469,0.245868755771919,0.233099300949596,0.218066024244334,0.201300311571489,0.183362404522089,0.164810755687758,0.146174027410271,0.127927608726522,0.110475953339786,0.0941413974686199,0.0791594796334459,0.0656802243170994,0.0537744195174696,0.0434436435299832,0.0346326846918545,0.0272430346821459,0.0211462914953402,0.0161965439076004,0.0122410845531382,0.00912907657003719,0.00671804964117306,0.00487830527795108,0.00349545847418932,0.00247143234775676,0.00172425984051603,0.00118704205791238,0.000806378438850812,0.000540531632252621,0.000357530197139557,0.000233353193889755,0.000150287955841078,9.55089371747857e-05,5.9892612048338e-05,3.70605536274e-05,2.26287184189335e-05,1.36338157579522e-05,8.10558361534242e-06],[0.129644732227423,0.149126938641066,0.1692648329271,0.189577494037488,0.209515533131891,0.228483625539526,0.245868755771919,0.261072430532258,0.273544563684942,0.282816387722065,0.288529657735577,0.290459614829476,0.288529657735577,0.282816387722065,0.273544563684942,0.261072430532258,0.245868755771919,0.228483625539526,0.209515533131891,0.189577494037488,0.1692648329271,0.149126938641066,0.129644732227423,0.111214920181278,0.0941413974686199,0.0786335049661668,0.0648103003735523,0.0527096146688645,0.0423004566051226,0.0334972902368784,0.0261748200130662,0.0201821364791977,0.0153553592070208,0.0115282192847515,0.00854031489815827,0.00624302316799314,0.00450324334474084,0.00320527496896476,0.00225120256576163,0.00156017482211562,0.00106694335246963,0.000719977402492236,0.000479408551336934,0.000314993864048076,0.000204224489205768,0.000130654072091693,8.24797789303686e-05,5.13785014937552e-05,3.1580920589238e-05,1.91547965993229e-05,1.1464093266488e-05],[0.129644732227423,0.15012443954056,0.171536810222608,0.193407213455959,0.21517777503787,0.236228103963181,0.25590285004469,0.273544563684942,0.288529657735577,0.300304776445231,0.308420634321427,0.312560446865633,0.312560446865633,0.308420634321427,0.300304776445231,0.288529657735577,0.273544563684942,0.25590285004469,0.236228103963181,0.21517777503787,0.193407213455959,0.171536810222608,0.15012443954056,0.129644732227423,0.110475953339786,0.0928945098919036,0.0770764572452968,0.0631048658850247,0.0509815879579283,0.0406418319775229,0.0319699973583051,0.0248154029509433,0.0190068203480652,0.0143650457958375,0.0107130705970553,0.00788370428577923,0.00572474427569231,0.00410195851856161,0.00290025272701337,0.00202343766726017,0.00139300675928467,0.00094629386412483,0.0006343197465634,0.000419565573051116,0.000273842509810286,0.000176364549958642,0.000112080776051485,7.02846313308199e-05,4.34909625667288e-05,2.65550470612311e-05,1.59994310050564e-05],[0.127927608726522,0.149126938641066,0.171536810222608,0.194700902383989,0.218066024244334,0.241000228215217,0.262818727926023,0.282816387722065,0.300304776445231,0.314651144426621,0.325316280684138,0.331888105486478,0.334108084564509,0.331888105486478,0.325316280684138,0.314651144426621,0.300304776445231,0.282816387722065,0.262818727926023,0.241000228215217,0.218066024244334,0.194700902383989,0.171536810222608,0.149126938641066,0.127927608726522,0.108288382895958,0.0904500604748913,0.074549590415764,0.0606304887014555,0.0486571069125697,0.0385310552921972,0.0301082096508339,0.0232149827969817,0.0176628673678771,0.0132606085907303,0.0098237004613415,0.00718118597580115,0.00517996282933357,0.00368694381493308,0.00258949932731576,0.00179462821951634,0.0012272769832107,0.000828171141856104,0.000551451095550976,0.000362329188615297,0.000234914079018397,0.000150287955841078,9.48743286391396e-05,5.90993440928135e-05,3.63267054975913e-05,2.20332606506407e-05],[0.124561289565839,0.146174027410271,0.1692648329271,0.193407213455959,0.218066024244334,0.242612263885053,0.266346443473052,0.288529657735577,0.308420634321427,0.325316280684138,0.338592689956246,0.347743294159522,0.352410919769243,0.352410919769243,0.347743294159522,0.338592689956246,0.325316280684138,0.308420634321427,0.288529657735577,0.266346443473052,0.242612263885053,0.218066024244334,0.193407213455959,0.169264832927099,0.146174027410271,0.124561289565839,0.10473826743213,0.0869034628749644,0.0711505376047041,0.0574815799110811,0.0458235375970751,0.03604607136649,0.0279792886978621,0.0214301301009783,0.0161965439076004,0.0120789533689274,0.0088888517609989,0.00645463114694013,0.00462494531498741,0.00327002581849206,0.00228141959920296,0.00157061073532225,0.00106694335246963,0.000715193517144861,0.000473058862646229,0.000308756567639692,0.000198850474808582,0.000126370722120622,7.92457005399165e-05,4.90359233661433e-05,2.99407319550803e-05],[0.119677171542053,0.141381872783512,0.164810755687758,0.189577494037488,0.21517777503787,0.241000228215217,0.266346443473052,0.290459614829476,0.312560446865632,0.331888105486478,0.347743294159522,0.359530092588644,0.366793082238878,0.369246538554654,0.366793082238878,0.359530092588644,0.347743294159522,0.331888105486478,0.312560446865632,0.290459614829476,0.266346443473052,0.241000228215217,0.21517777503787,0.189577494037488,0.164810755687758,0.141381872783512,0.119677171542053,0.0999627763750334,0.0823900392819534,0.0670070528615627,0.0537744195174696,0.0425834017517011,0.0332747177014262,0.0256565238508506,0.0195204873448052,0.0146552389704592,0.0108568680577609,0.00793643789774811,0.00572474427569231,0.00407470308084291,0.00286183934892583,0.00198337091696733,0.00135635083026808,0.000915270661168869,0.000609447713543891,0.000400435682027572,0.000259620208377656,0.000166093878132596,0.000104852348883928,6.53148763687232e-05,4.01472184654367e-05],[0.113461610599908,0.134935634119589,0.158348433523511,0.183362404522089,0.209515533131891,0.236228103963181,0.262818727926023,0.288529657735577,0.312560446865632,0.334108084564509,0.352410919769243,0.366793082238878,0.3767058134337,0.381762191893459,0.381762191893459,0.3767058134337,0.366793082238878,0.352410919769243,0.334108084564509,0.312560446865632,0.288529657735577,0.262818727926023,0.236228103963181,0.209515533131891,0.183362404522089,0.158348433523511,0.134935634119589,0.113461610599908,0.0941413974686199,0.0770764572452967,0.0622690521471989,0.0496400456740287,0.0390482429518072,0.0303096016091382,0.0232149827969817,0.0175455065561331,0.0130849739756079,0.00962917815916989,0.00699221844809223,0.00501014344842975,0.00354237668013568,0.00247143234775675,0.00170142229792605,0.00115580593567212,0.000774759888007995,0.000512458548153685,0.000334471996783786,0.000215412147761162,0.000136895769006016,8.5845842563734e-05,5.31199816340434e-05],[0.106144129249151,0.12707759453053,0.15012443954056,0.17500208365302,0.201300311571489,0.228483625539526,0.25590285004469,0.282816387722065,0.308420634321427,0.331888105486478,0.352410919769243,0.369246538554654,0.381762191893459,0.389474299741258,0.392079469322702,0.389474299741258,0.381762191893459,0.369246538554654,0.352410919769243,0.331888105486478,0.308420634321427,0.282816387722065,0.25590285004469,0.228483625539526,0.201300311571489,0.17500208365302,0.15012443954056,0.12707759453053,0.106144129249152,0.0874847547808859,0.0711505376047041,0.0570996439129113,0.04521661225618,0.0353323113313541,0.0272430346821459,0.0207275668690903,0.0155614683371893,0.0115282192847515,0.00842719980921657,0.00607874269155644,0.00432666864755998,0.00303880561103102,0.0021060157249911,0.0014402228815158,0.000971867838009838,0.000647133855449413,0.000425197241717693,0.000275674225477118,0.000176364549958642,0.00011133605603608,6.93537227614023e-05],[0.0979833807828676,0.118092066769606,0.140442461814151,0.164810755687758,0.190845566208414,0.218066024244334,0.245868755771919,0.273544563684942,0.30030477644523,0.325316280684138,0.347743294159522,0.366793082238878,0.381762191893459,0.392079469322702,0.397342202502014,0.397342202502014,0.392079469322702,0.381762191893459,0.366793082238878,0.347743294159522,0.325316280684138,0.300304776445231,0.273544563684942,0.245868755771919,0.218066024244334,0.190845566208414,0.164810755687758,0.140442461814151,0.118092066769606,0.0979833807828675,0.0802220071374839,0.0648103003735523,0.0516658943689313,0.0406418319775229,0.03154655991627,0.0241624042177895,0.0182615522622866,0.0136189818938397,0.0100221523746821,0.00727757629628117,0.0052146112813751,0.00368694381493307,0.00257229341523282,0.00177085865913261,0.00120297527072305,0.000806378438850811,0.000533372378245343,0.000348122057918207,0.00022420328428174,0.000142482591322814,8.93492778599581e-05],[0.0892520640593719,0.108288382895958,0.129644732227423,0.153157154390045,0.178537360250285,0.205366847613037,0.233099300949596,0.261072430532258,0.288529657735577,0.314651144426621,0.338592689956246,0.359530092588644,0.3767058134337,0.389474299741258,0.397342202502014,0.4,0.397342202502014,0.389474299741258,0.3767058134337,0.359530092588644,0.338592689956246,0.314651144426621,0.288529657735577,0.261072430532258,0.233099300949596,0.205366847613037,0.178537360250285,0.153157154390045,0.129644732227423,0.108288382895957,0.0892520640593719,0.0725878738079432,0.0582531332350026,0.046130048415225,0.0360460713664899,0.0277933804891206,0.0211462914953401,0.0158758308503845,0.0117611047624258,0.00859744053803596,0.00620154143960373,0.00441407315209244,0.00310019355645468,0.00214856006475334,0.00146931731365961,0.000991500870666543,0.00066020682650617,0.000433786795776071,0.000281243214242592,0.000179927350201022,0.000113585193561303],[0.0802220071374838,0.0979833807828676,0.118092066769606,0.14044246181415,0.164810755687758,0.190845566208414,0.218066024244334,0.245868755771919,0.273544563684942,0.30030477644523,0.325316280684138,0.347743294159522,0.366793082238878,0.381762191893459,0.392079469322702,0.397342202502014,0.397342202502014,0.392079469322702,0.381762191893459,0.366793082238878,0.347743294159522,0.325316280684138,0.30030477644523,0.273544563684942,0.245868755771918,0.218066024244334,0.190845566208414,0.164810755687758,0.14044246181415,0.118092066769606,0.0979833807828676,0.0802220071374838,0.0648103003735523,0.0516658943689313,0.0406418319775229,0.03154655991627,0.0241624042177895,0.0182615522622865,0.0136189818938397,0.010022152374682,0.00727757629628118,0.00521461128137509,0.00368694381493307,0.00257229341523282,0.0017708586591326,0.00120297527072305,0.000806378438850809,0.000533372378245343,0.000348122057918207,0.00022420328428174,0.000142482591322814],[0.0711505376047041,0.0874847547808859,0.106144129249152,0.12707759453053,0.15012443954056,0.17500208365302,0.201300311571489,0.228483625539526,0.25590285004469,0.282816387722065,0.308420634321427,0.331888105486478,0.352410919769243,0.369246538554654,0.381762191893459,0.389474299741258,0.392079469322702,0.389474299741258,0.381762191893459,0.369246538554654,0.352410919769243,0.331888105486478,0.308420634321426,0.282816387722065,0.25590285004469,0.228483625539526,0.201300311571489,0.17500208365302,0.15012443954056,0.12707759453053,0.106144129249152,0.0874847547808859,0.0711505376047041,0.0570996439129112,0.0452166122561799,0.0353323113313541,0.0272430346821459,0.0207275668690903,0.0155614683371893,0.0115282192847515,0.00842719980921658,0.00607874269155643,0.00432666864755998,0.00303880561103103,0.0021060157249911,0.0014402228815158,0.000971867838009837,0.000647133855449412,0.000425197241717692,0.000275674225477117,0.000176364549958642],[0.0622690521471989,0.0770764572452968,0.0941413974686199,0.113461610599908,0.134935634119589,0.158348433523511,0.183362404522089,0.209515533131891,0.236228103963181,0.262818727926023,0.288529657735577,0.312560446865632,0.334108084564509,0.352410919769243,0.366793082238878,0.376705813433699,0.381762191893459,0.381762191893459,0.376705813433699,0.366793082238878,0.352410919769243,0.334108084564509,0.312560446865632,0.288529657735577,0.262818727926023,0.236228103963181,0.209515533131891,0.183362404522089,0.158348433523511,0.134935634119589,0.113461610599908,0.0941413974686199,0.0770764572452968,0.0622690521471989,0.0496400456740287,0.0390482429518072,0.0303096016091382,0.0232149827969817,0.0175455065561331,0.0130849739756079,0.0096291781591699,0.00699221844809222,0.00501014344842975,0.00354237668013568,0.00247143234775675,0.00170142229792605,0.00115580593567211,0.000774759888007996,0.000512458548153684,0.000334471996783787,0.000215412147761162],[0.0537744195174695,0.0670070528615627,0.0823900392819534,0.0999627763750333,0.119677171542053,0.141381872783512,0.164810755687758,0.189577494037488,0.21517777503787,0.241000228215217,0.266346443473052,0.290459614829476,0.312560446865632,0.331888105486477,0.347743294159522,0.359530092588643,0.366793082238878,0.369246538554654,0.366793082238878,0.359530092588644,0.347743294159522,0.331888105486478,0.312560446865632,0.290459614829476,0.266346443473052,0.241000228215217,0.21517777503787,0.189577494037488,0.164810755687758,0.141381872783512,0.119677171542053,0.0999627763750334,0.0823900392819534,0.0670070528615627,0.0537744195174695,0.0425834017517012,0.0332747177014262,0.0256565238508506,0.0195204873448052,0.0146552389704592,0.0108568680577609,0.00793643789774811,0.00572474427569231,0.00407470308084292,0.00286183934892583,0.00198337091696734,0.00135635083026808,0.000915270661168869,0.000609447713543891,0.000400435682027572,0.000259620208377657],[0.0458235375970751,0.0574815799110812,0.0711505376047041,0.0869034628749644,0.10473826743213,0.124561289565839,0.146174027410271,0.1692648329271,0.193407213455959,0.218066024244334,0.242612263885053,0.266346443473052,0.288529657735577,0.308420634321427,0.325316280684138,0.338592689956246,0.347743294159522,0.352410919769243,0.352410919769243,0.347743294159522,0.338592689956246,0.325316280684138,0.308420634321426,0.288529657735577,0.266346443473052,0.242612263885053,0.218066024244334,0.193407213455959,0.169264832927099,0.146174027410271,0.124561289565839,0.10473826743213,0.0869034628749644,0.0711505376047041,0.0574815799110811,0.0458235375970751,0.03604607136649,0.0279792886978621,0.0214301301009783,0.0161965439076004,0.0120789533689274,0.00888885176099889,0.00645463114694013,0.00462494531498742,0.00327002581849206,0.00228141959920296,0.00157061073532225,0.00106694335246963,0.000715193517144859,0.000473058862646229,0.000308756567639692],[0.0385310552921972,0.0486571069125697,0.0606304887014555,0.074549590415764,0.0904500604748912,0.108288382895958,0.127927608726522,0.149126938641066,0.171536810222608,0.194700902383989,0.218066024244334,0.241000228215217,0.262818727926023,0.282816387722065,0.300304776445231,0.314651144426621,0.325316280684138,0.331888105486478,0.334108084564509,0.331888105486478,0.325316280684138,0.314651144426621,0.30030477644523,0.282816387722065,0.262818727926023,0.241000228215217,0.218066024244334,0.194700902383989,0.171536810222608,0.149126938641066,0.127927608726522,0.108288382895958,0.0904500604748912,0.0745495904157639,0.0606304887014554,0.0486571069125697,0.0385310552921972,0.0301082096508339,0.0232149827969817,0.0176628673678771,0.0132606085907303,0.00982370046134148,0.00718118597580114,0.00517996282933357,0.00368694381493307,0.00258949932731576,0.00179462821951633,0.0012272769832107,0.000828171141856103,0.000551451095550975,0.000362329188615298],[0.0319699973583051,0.0406418319775229,0.0509815879579283,0.0631048658850247,0.0770764572452968,0.0928945098919035,0.110475953339786,0.129644732227423,0.15012443954056,0.171536810222608,0.193407213455959,0.21517777503787,0.236228103963181,0.25590285004469,0.273544563684942,0.288529657735577,0.30030477644523,0.308420634321426,0.312560446865632,0.312560446865632,0.308420634321426,0.30030477644523,0.288529657735577,0.273544563684942,0.25590285004469,0.236228103963181,0.21517777503787,0.193407213455959,0.171536810222608,0.15012443954056,0.129644732227423,0.110475953339786,0.0928945098919035,0.0770764572452967,0.0631048658850247,0.0509815879579283,0.0406418319775229,0.0319699973583051,0.0248154029509433,0.0190068203480652,0.0143650457958376,0.0107130705970553,0.00788370428577922,0.00572474427569231,0.0041019585185616,0.00290025272701336,0.00202343766726017,0.00139300675928467,0.000946293864124829,0.000634319746563399,0.000419565573051116],[0.0261748200130662,0.0334972902368784,0.0423004566051226,0.0527096146688645,0.0648103003735523,0.0786335049661667,0.0941413974686199,0.111214920181278,0.129644732227423,0.149126938641066,0.169264832927099,0.189577494037488,0.209515533131891,0.228483625539526,0.245868755771918,0.261072430532258,0.273544563684942,0.282816387722065,0.288529657735577,0.290459614829476,0.288529657735577,0.282816387722065,0.273544563684942,0.261072430532258,0.245868755771918,0.228483625539526,0.209515533131891,0.189577494037487,0.169264832927099,0.149126938641066,0.129644732227423,0.111214920181278,0.0941413974686199,0.0786335049661667,0.0648103003735523,0.0527096146688646,0.0423004566051226,0.0334972902368784,0.0261748200130662,0.0201821364791976,0.0153553592070208,0.0115282192847515,0.00854031489815828,0.00624302316799314,0.00450324334474083,0.00320527496896476,0.00225120256576162,0.00156017482211562,0.00106694335246963,0.000719977402492236,0.000479408551336934],[0.0211462914953401,0.0272430346821459,0.0346326846918545,0.0434436435299832,0.0537744195174695,0.0656802243170993,0.0791594796334459,0.0941413974686199,0.110475953339786,0.127927608726522,0.146174027410271,0.164810755687758,0.183362404522089,0.201300311571489,0.218066024244334,0.233099300949596,0.245868755771918,0.25590285004469,0.262818727926023,0.266346443473052,0.266346443473052,0.262818727926023,0.25590285004469,0.245868755771918,0.233099300949596,0.218066024244334,0.201300311571489,0.183362404522089,0.164810755687758,0.146174027410271,0.127927608726522,0.110475953339786,0.0941413974686199,0.0791594796334458,0.0656802243170993,0.0537744195174695,0.0434436435299832,0.0346326846918545,0.0272430346821459,0.0211462914953401,0.0161965439076004,0.0122410845531381,0.00912907657003718,0.00671804964117307,0.00487830527795107,0.00349545847418932,0.00247143234775675,0.00172425984051603,0.00118704205791238,0.00080637843885081,0.000540531632252623],[0.0168575374037106,0.0218630474459658,0.0279792886978622,0.0353323113313541,0.0440267709878692,0.0541341132946451,0.0656802243170994,0.0786335049661668,0.0928945098919035,0.108288382895958,0.124561289565839,0.141381872783512,0.158348433523511,0.17500208365302,0.190845566208414,0.205366847613037,0.218066024244334,0.228483625539526,0.236228103963181,0.241000228215217,0.242612263885053,0.241000228215217,0.236228103963181,0.228483625539526,0.218066024244334,0.205366847613037,0.190845566208414,0.17500208365302,0.158348433523511,0.141381872783512,0.124561289565839,0.108288382895958,0.0928945098919035,0.0786335049661667,0.0656802243170993,0.0541341132946451,0.0440267709878692,0.0353323113313541,0.0279792886978621,0.0218630474459658,0.0168575374037106,0.0128258741311443,0.00962917815916989,0.0071334706305035,0.00521461128137509,0.00376142502059808,0.00267727070095845,0.00188036244303331,0.00130316755350706,0.000891185999581157,0.00060137567719103],[0.0132606085907303,0.0173131191607864,0.0223047103014429,0.0283548440188743,0.0355686469837545,0.0440267709878691,0.0537744195174695,0.0648103003735523,0.0770764572452967,0.0904500604748911,0.10473826743213,0.119677171542053,0.134935634119589,0.15012443954056,0.164810755687758,0.178537360250285,0.190845566208414,0.201300311571489,0.209515533131891,0.21517777503787,0.218066024244334,0.218066024244334,0.21517777503787,0.209515533131891,0.201300311571489,0.190845566208414,0.178537360250285,0.164810755687758,0.15012443954056,0.134935634119589,0.119677171542053,0.10473826743213,0.0904500604748912,0.0770764572452967,0.0648103003735523,0.0537744195174695,0.0440267709878692,0.0355686469837545,0.0283548440188743,0.0223047103014429,0.0173131191607864,0.0132606085907302,0.0100221523746821,0.00747425573509312,0.00550027384193288,0.00399401855396117,0.00286183934892583,0.00202343766726017,0.00141170455772781,0.000971867838009838,0.000660206826506172],[0.010293005090544,0.0135284906538333,0.0175455065561331,0.0224539051336535,0.0283548440188743,0.0353323113313541,0.0434436435299832,0.0527096146688645,0.0631048658850247,0.074549590415764,0.0869034628749644,0.0999627763750334,0.113461610599908,0.12707759453053,0.14044246181415,0.153157154390045,0.164810755687758,0.17500208365302,0.183362404522089,0.189577494037488,0.193407213455959,0.194700902383989,0.193407213455959,0.189577494037487,0.183362404522089,0.17500208365302,0.164810755687758,0.153157154390045,0.14044246181415,0.12707759453053,0.113461610599908,0.0999627763750334,0.0869034628749644,0.074549590415764,0.0631048658850247,0.0527096146688646,0.0434436435299832,0.0353323113313541,0.0283548440188743,0.0224539051336535,0.0175455065561331,0.0135284906538333,0.010293005090544,0.00772759648166358,0.0057247442756923,0.00418482357737072,0.00301861428615636,0.00214856006475334,0.00150902620751688,0.00104581645858419,0.000715193517144861],[0.00788370428577923,0.0104311641721669,0.0136189818938398,0.0175455065561331,0.0223047103014429,0.0279792886978622,0.0346326846918545,0.0423004566051226,0.0509815879579283,0.0606304887014555,0.0711505376047041,0.0823900392819534,0.09414139746862,0.106144129249152,0.118092066769606,0.129644732227423,0.140442461814151,0.15012443954056,0.158348433523511,0.164810755687758,0.1692648329271,0.171536810222608,0.171536810222608,0.1692648329271,0.164810755687758,0.158348433523511,0.15012443954056,0.140442461814151,0.129644732227423,0.118092066769606,0.106144129249152,0.0941413974686199,0.0823900392819534,0.0711505376047041,0.0606304887014554,0.0509815879579283,0.0423004566051226,0.0346326846918545,0.0279792886978621,0.0223047103014429,0.0175455065561331,0.0136189818938397,0.0104311641721669,0.00788370428577923,0.00587945780196071,0.00432666864755998,0.0031418062721626,0.00225120256576162,0.00159169244419836,0.00111048613670333,0.000764498318653055],[0.00595837552163675,0.00793643789774811,0.0104311641721669,0.0135284906538333,0.0173131191607863,0.0218630474459658,0.0272430346821459,0.0334972902368784,0.0406418319775229,0.0486571069125696,0.0574815799110811,0.0670070528615627,0.0770764572452967,0.0874847547808859,0.0979833807828675,0.108288382895957,0.118092066769606,0.12707759453053,0.134935634119589,0.141381872783512,0.146174027410271,0.149126938641066,0.15012443954056,0.149126938641066,0.146174027410271,0.141381872783512,0.134935634119589,0.12707759453053,0.118092066769606,0.108288382895957,0.0979833807828676,0.0874847547808859,0.0770764572452967,0.0670070528615627,0.0574815799110811,0.0486571069125697,0.0406418319775229,0.0334972902368784,0.0272430346821459,0.0218630474459658,0.0173131191607864,0.0135284906538333,0.0104311641721669,0.00793643789774812,0.00595837552163675,0.00441407315209244,0.00322671485563984,0.00232750733227017,0.00165665064512227,0.00116353705032503,0.000806378438850812],[0.00444359861529692,0.00595837552163676,0.00788370428577923,0.010293005090544,0.0132606085907303,0.0168575374037106,0.0211462914953402,0.0261748200130662,0.0319699973583051,0.0385310552921972,0.0458235375970751,0.0537744195174696,0.0622690521471989,0.0711505376047041,0.0802220071374839,0.0892520640593719,0.0979833807828676,0.106144129249152,0.113461610599908,0.119677171542053,0.124561289565839,0.127927608726522,0.129644732227423,0.129644732227423,0.127927608726522,0.124561289565839,0.119677171542053,0.113461610599908,0.106144129249151,0.0979833807828676,0.0892520640593719,0.0802220071374838,0.0711505376047041,0.0622690521471989,0.0537744195174695,0.0458235375970751,0.0385310552921972,0.0319699973583051,0.0261748200130662,0.0211462914953401,0.0168575374037106,0.0132606085907302,0.010293005090544,0.00788370428577923,0.00595837552163675,0.00444359861529692,0.00327002581849206,0.00237452609930412,0.00170142229792605,0.00120297527072305,0.000839287367232362],[0.00327002581849206,0.00441407315209244,0.00587945780196071,0.00772759648166356,0.010022152374682,0.0128258741311443,0.0161965439076004,0.0201821364791976,0.0248154029509433,0.0301082096508338,0.0360460713664899,0.0425834017517011,0.0496400456740287,0.0570996439129112,0.0648103003735523,0.0725878738079431,0.0802220071374838,0.0874847547808858,0.0941413974686198,0.0999627763750333,0.10473826743213,0.108288382895957,0.110475953339786,0.111214920181278,0.110475953339786,0.108288382895957,0.10473826743213,0.0999627763750333,0.0941413974686198,0.0874847547808859,0.0802220071374838,0.0725878738079432,0.0648103003735523,0.0570996439129112,0.0496400456740286,0.0425834017517011,0.03604607136649,0.0301082096508339,0.0248154029509433,0.0201821364791976,0.0161965439076004,0.0128258741311443,0.010022152374682,0.00772759648166358,0.00587945780196071,0.00441407315209244,0.00327002581849206,0.00239040915800238,0.00172425984051603,0.0012272769832107,0.000861969447319046],[0.00237452609930412,0.00322671485563985,0.00432666864755999,0.00572474427569231,0.00747425573509311,0.00962917815916989,0.0122410845531382,0.0153553592070208,0.0190068203480652,0.0232149827969817,0.0279792886978621,0.0332747177014262,0.0390482429518072,0.0452166122561799,0.0516658943689313,0.0582531332350026,0.0648103003735523,0.0711505376047041,0.0770764572452968,0.0823900392819534,0.0869034628749644,0.0904500604748912,0.0928945098919035,0.0941413974686199,0.0941413974686199,0.0928945098919035,0.0904500604748912,0.0869034628749644,0.0823900392819534,0.0770764572452967,0.0711505376047041,0.0648103003735523,0.0582531332350026,0.0516658943689313,0.0452166122561799,0.0390482429518072,0.0332747177014261,0.0279792886978621,0.0232149827969817,0.0190068203480652,0.0153553592070208,0.0122410845531381,0.00962917815916989,0.00747425573509311,0.0057247442756923,0.00432666864755998,0.00322671485563984,0.00237452609930412,0.00172425984051603,0.00123548616329471,0.000873539334459006],[0.00170142229792605,0.00232750733227016,0.0031418062721626,0.00418482357737071,0.00550027384193288,0.00713347063050349,0.00912907657003717,0.0115282192847515,0.0143650457958375,0.0176628673678771,0.0214301301009783,0.0256565238508506,0.0303096016091382,0.035332311331354,0.0406418319775229,0.0461300484152249,0.0516658943689312,0.0570996439129112,0.0622690521471988,0.0670070528615626,0.071150537604704,0.0745495904157639,0.0770764572452967,0.0786335049661666,0.0791594796334458,0.0786335049661666,0.0770764572452967,0.0745495904157639,0.071150537604704,0.0670070528615626,0.0622690521471989,0.0570996439129112,0.0516658943689313,0.046130048415225,0.0406418319775229,0.0353323113313541,0.0303096016091382,0.0256565238508506,0.0214301301009783,0.0176628673678771,0.0143650457958375,0.0115282192847515,0.00912907657003719,0.0071334706305035,0.00550027384193288,0.00418482357737072,0.0031418062721626,0.00232750733227016,0.00170142229792605,0.0012272769832107,0.000873539334459005],[0.00120297527072305,0.00165665064512228,0.00225120256576162,0.00301861428615636,0.00399401855396117,0.00521461128137509,0.00671804964117306,0.00854031489815827,0.0107130705970553,0.0132606085907302,0.0161965439076004,0.0195204873448052,0.0232149827969817,0.0272430346821459,0.03154655991627,0.0360460713664899,0.0406418319775229,0.0452166122561799,0.0496400456740287,0.0537744195174695,0.0574815799110811,0.0606304887014554,0.0631048658850247,0.0648103003735523,0.0656802243170993,0.0656802243170993,0.0648103003735523,0.0631048658850247,0.0606304887014554,0.0574815799110811,0.0537744195174695,0.0496400456740287,0.0452166122561799,0.0406418319775229,0.0360460713664899,0.03154655991627,0.0272430346821459,0.0232149827969817,0.0195204873448052,0.0161965439076004,0.0132606085907303,0.0107130705970553,0.00854031489815827,0.00671804964117307,0.00521461128137509,0.00399401855396118,0.00301861428615636,0.00225120256576162,0.00165665064512227,0.00120297527072305,0.000861969447319046],[0.000839287367232362,0.00116353705032503,0.00159169244419837,0.00214856006475334,0.00286183934892583,0.00376142502059809,0.00487830527795108,0.00624302316799314,0.00788370428577922,0.00982370046134149,0.0120789533689274,0.0146552389704592,0.0175455065561331,0.0207275668690903,0.0241624042177895,0.0277933804891206,0.03154655991627,0.0353323113313541,0.0390482429518072,0.0425834017517011,0.0458235375970751,0.0486571069125697,0.0509815879579283,0.0527096146688646,0.0537744195174695,0.0541341132946451,0.0537744195174696,0.0527096146688646,0.0509815879579283,0.0486571069125697,0.0458235375970751,0.0425834017517011,0.0390482429518072,0.0353323113313541,0.03154655991627,0.0277933804891206,0.0241624042177895,0.0207275668690903,0.0175455065561331,0.0146552389704592,0.0120789533689274,0.00982370046134149,0.00788370428577922,0.00624302316799314,0.00487830527795107,0.00376142502059809,0.00286183934892583,0.00214856006475334,0.00159169244419836,0.00116353705032503,0.000839287367232362]],"type":"surface","frame":null},{"colorbar":{"title":"","ticklen":2,"len":0.333333333333333,"lenmode":"fraction","y":0.666666666666667,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"y":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"z":[[0.00564213753089713,0.00731745791692661,0.00936453475198971,0.0118255564286688,0.0147355506920122,0.0181184300533911,0.0219828584556888,0.0263182598341997,0.0310913503036355,0.0362436063266842,0.0416900707336809,0.047319839874405,0.0529984669970311,0.0585723644277108,0.0638751026275517,0.0687353063956126,0.0729856603688545,0.0764723819368925,0.0790644220032968,0.0806616292762043,0.0812011699419676,0.0806616292762043,0.0790644220032968,0.0764723819368924,0.0729856603688545,0.0687353063956126,0.0638751026275517,0.0585723644277108,0.0529984669970311,0.0473198398744049,0.0416900707336809,0.0362436063266842,0.0310913503036355,0.0263182598341997,0.0219828584556887,0.0181184300533911,0.0147355506920122,0.0118255564286688,0.0093645347519897,0.00731745791692661,0.00564213753089713,0.00429275902338874,0.00322284009713001,0.00238753866629755,0.00174530557548755,0.00125893105084854,0.000896069760398337,0.000629348359576674,0.000436163977371401,0.000298275712212873,0.000201277576741507],[0.00599102783094177,0.00782191692206264,0.0100770744617596,0.0128104723472374,0.0160696058955829,0.0198909128860954,0.0242948158614006,0.0292807310172078,0.0348224741954725,0.0408645520232188,0.047319839874405,0.054069107049735,0.0609627479662844,0.0678249183842699,0.0744600685110431,0.0806616292762043,0.0862223698666218,0.0909457330521832,0.094657298827537,0.0972154505603285,0.0985203364756491,0.0985203364756491,0.0972154505603285,0.094657298827537,0.0909457330521832,0.0862223698666218,0.0806616292762043,0.074460068511043,0.0678249183842699,0.0609627479662844,0.054069107049735,0.047319839874405,0.0408645520232188,0.0348224741954725,0.0292807310172078,0.0242948158614006,0.0198909128860954,0.0160696058955829,0.0128104723472374,0.0100770744617596,0.00782191692206264,0.00599102783094176,0.00452792142923455,0.00337680384864244,0.00248497596768341,0.00180446290608457,0.00129295417097857,0.000914171570315839,0.000637795862576538,0.000439081451328283,0.000298275712212874],[0.00627723536605608,0.00825041076289934,0.0107002059457552,0.0136936148550558,0.0172923289271272,0.0215475686937563,0.0264943010518157,0.0321451951514675,0.0384847857762759,0.0454644024137073,0.0529984669970311,0.0609627479662844,0.0691950726228376,0.077498841553397,0.0856494658693669,0.0934035782207984,0.100510579292344,0.106725806407056,0.111824385623646,0.115614685867945,0.11795025744925,0.118739219450169,0.11795025744925,0.115614685867945,0.111824385623646,0.106725806407056,0.100510579292344,0.0934035782207984,0.0856494658693669,0.077498841553397,0.0691950726228376,0.0609627479662844,0.0529984669970311,0.0454644024137073,0.0384847857762759,0.0321451951514675,0.0264943010518157,0.0215475686937563,0.0172923289271273,0.0136936148550558,0.0107002059457553,0.00825041076289933,0.00627723536605609,0.00471270940824392,0.00349126099840525,0.00255213344688908,0.00184091547481606,0.00131030900168851,0.000920286407594678,0.00063779586257654,0.000436163977371403],[0.00649000297133997,0.00858711641353847,0.0112113836026397,0.0144437672387548,0.0183616268297072,0.0230330388105312,0.0285102305220978,0.0348224741954726,0.0419689330467932,0.0499120765521392,0.0585723644277108,0.0678249183842699,0.077498841553397,0.087379699852504,0.0972154505603285,0.106725806407056,0.115614685867945,0.12358505892293,0.130355194312447,0.135675090712337,0.139341764837855,0.14121209620293,0.14121209620293,0.139341764837855,0.135675090712337,0.130355194312447,0.12358505892293,0.115614685867945,0.106725806407056,0.0972154505603284,0.087379699852504,0.0774988415533969,0.0678249183842699,0.0585723644277108,0.0499120765521392,0.0419689330467932,0.0348224741954725,0.0285102305220978,0.0230330388105312,0.0183616268297072,0.0144437672387548,0.0112113836026396,0.00858711641353846,0.00649000297133998,0.00484007228345976,0.00356178914895618,0.00258638976077405,0.00185322924494206,0.00131030900168851,0.000914171570315837,0.000629348359576674],[0.00662110972813867,0.00881918670294107,0.0115913947224954,0.0150332285620231,0.0192388111967165,0.0242948158614006,0.0302732047187965,0.037223104426415,0.0451623144762508,0.054069107049735,0.0638751026275517,0.074460068511043,0.0856494658693669,0.0972154505603284,0.108881810711915,0.120333010706226,0.131227132171329,0.14121209620293,0.14994416456255,0.157107401148196,0.162432574343936,0.165713930009679,0.166822380271916,0.165713930009679,0.162432574343936,0.157107401148196,0.14994416456255,0.14121209620293,0.131227132171329,0.120333010706226,0.108881810711915,0.0972154505603284,0.0856494658693669,0.074460068511043,0.0638751026275517,0.0540691070497349,0.0451623144762508,0.037223104426415,0.0302732047187964,0.0242948158614005,0.0192388111967165,0.015033228562023,0.0115913947224954,0.00881918670294107,0.00662110972813866,0.0049050387277381,0.00358561373700356,0.00258638976077405,0.00184091547481605,0.00129295417097857,0.000896069760398338],[0.00666539792294538,0.00893756328245514,0.0118255564286688,0.015439507635816,0.0198909128860954,0.0252863061055659,0.0317194372430102,0.0392622300195994,0.0479549960374577,0.0577965829382958,0.0687353063956126,0.0806616292762043,0.0934035782207984,0.106725806407056,0.120333010706226,0.133878096089058,0.146975071174301,0.159216193873727,0.170192415899862,0.179515757313079,0.186841934348759,0.191891413089782,0.194467098341135,0.194467098341135,0.191891413089782,0.186841934348759,0.179515757313079,0.170192415899862,0.159216193873727,0.146975071174301,0.133878096089058,0.120333010706226,0.106725806407056,0.0934035782207984,0.0806616292762043,0.0687353063956126,0.0577965829382958,0.0479549960374576,0.0392622300195993,0.0317194372430102,0.0252863061055658,0.0198909128860954,0.015439507635816,0.0118255564286688,0.00893756328245512,0.00666539792294538,0.00490503872773809,0.00356178914895618,0.00255213344688907,0.00180446290608457,0.00125893105084854],[0.00662110972813867,0.00893756328245514,0.0119046568466222,0.0156467462582504,0.02029273598075,0.0259696787411795,0.0327945711689488,0.0408645520232188,0.0502459353553176,0.0609627479662844,0.0729856603688545,0.0862223698666218,0.100510579292344,0.115614685867945,0.131227132171329,0.146975071174301,0.162432574343936,0.177138100154409,0.190616391795795,0.202403451179384,0.212072809175268,0.219261041115406,0.223690407961599,0.22518665931084,0.223690407961599,0.219261041115406,0.212072809175268,0.202403451179384,0.190616391795795,0.177138100154408,0.162432574343936,0.146975071174301,0.131227132171329,0.115614685867945,0.100510579292344,0.0862223698666218,0.0729856603688545,0.0609627479662844,0.0502459353553176,0.0408645520232188,0.0327945711689488,0.0259696787411795,0.02029273598075,0.0156467462582504,0.0119046568466222,0.00893756328245514,0.00662110972813866,0.00484007228345977,0.00349126099840524,0.00248497596768341,0.00174530557548755],[0.00649000297133997,0.00881918670294107,0.0118255564286688,0.0156467462582504,0.0204284728407596,0.0263182598341997,0.0334570654521643,0.0419689330467932,0.0519490270377817,0.0634506849076839,0.0764723819368925,0.0909457330521832,0.106725806407056,0.12358505892293,0.14121209620293,0.159216193873727,0.177138100154409,0.194467098341135,0.210663692721226,0.22518665931084,0.237522650285267,0.247216133531637,0.253897249390649,0.257305215333912,0.257305215333912,0.253897249390649,0.247216133531637,0.237522650285267,0.22518665931084,0.210663692721226,0.194467098341135,0.177138100154409,0.159216193873727,0.14121209620293,0.12358505892293,0.106725806407056,0.0909457330521832,0.0764723819368924,0.0634506849076839,0.0519490270377818,0.0419689330467932,0.0334570654521643,0.0263182598341997,0.0204284728407596,0.0156467462582504,0.0118255564286688,0.00881918670294106,0.00649000297133998,0.0047127094082439,0.00337680384864244,0.00238753866629755],[0.00627723536605608,0.00858711641353847,0.0115913947224954,0.015439507635816,0.02029273598075,0.0263182598341997,0.0336808577004802,0.0425322660283115,0.0529984669970311,0.0651654652949748,0.0790644220032968,0.094657298827537,0.111824385623646,0.130355194312447,0.14994416456255,0.170192415899862,0.190616391795795,0.210663692721226,0.229735731585067,0.247216133531637,0.26250312547953,0.275043606783134,0.284366241056231,0.290110820183939,0.292051353575983,0.290110820183939,0.284366241056231,0.275043606783134,0.26250312547953,0.247216133531637,0.229735731585067,0.210663692721226,0.190616391795795,0.170192415899862,0.14994416456255,0.130355194312447,0.111824385623646,0.094657298827537,0.0790644220032968,0.0651654652949747,0.0529984669970311,0.0425322660283115,0.0336808577004802,0.0263182598341997,0.02029273598075,0.015439507635816,0.0115913947224953,0.00858711641353846,0.00627723536605607,0.00452792142923454,0.00322284009713001],[0.00599102783094176,0.00825041076289934,0.0112113836026397,0.0150332285620231,0.0198909128860954,0.0259696787411795,0.0334570654521643,0.0425322660283115,0.0533529704756318,0.0660401564818038,0.0806616292762043,0.0972154505603285,0.115614685867945,0.135675090712337,0.157107401148196,0.179515757313079,0.202403451179384,0.22518665931084,0.247216133531637,0.267806040375428,0.286268349312621,0.301950467357234,0.314273299697836,0.322766662556805,0.327099036366501,0.327099036366501,0.322766662556805,0.314273299697836,0.301950467357234,0.286268349312621,0.267806040375428,0.247216133531637,0.22518665931084,0.202403451179384,0.179515757313079,0.157107401148196,0.135675090712337,0.115614685867945,0.0972154505603284,0.0806616292762042,0.0660401564818038,0.0533529704756318,0.0425322660283115,0.0334570654521643,0.0259696787411795,0.0198909128860954,0.0150332285620231,0.0112113836026397,0.00825041076289931,0.00599102783094176,0.00429275902338875],[0.00564213753089712,0.00782191692206264,0.0107002059457552,0.0144437672387548,0.0192388111967165,0.0252863061055658,0.0327945711689488,0.0419689330467932,0.0529984669970311,0.0660401564818037,0.0812011699419676,0.0985203364756491,0.11795025744925,0.139341764837855,0.162432574343936,0.186841934348759,0.212072809175268,0.237522650285267,0.26250312547953,0.286268349312621,0.308050271419555,0.327099036366501,0.342725438309289,0.354342155944772,0.361500342322826,0.36391839582758,0.361500342322826,0.354342155944772,0.342725438309289,0.327099036366501,0.308050271419555,0.286268349312621,0.26250312547953,0.237522650285267,0.212072809175268,0.186841934348759,0.162432574343936,0.139341764837855,0.11795025744925,0.098520336475649,0.0812011699419676,0.0660401564818037,0.0529984669970311,0.0419689330467932,0.0327945711689487,0.0252863061055658,0.0192388111967164,0.0144437672387548,0.0107002059457552,0.00782191692206263,0.00564213753089713],[0.00524318771128397,0.00731745791692661,0.0100770744617596,0.0136936148550558,0.0183616268297072,0.0242948158614006,0.0317194372430102,0.0408645520232188,0.0519490270377817,0.0651654652949747,0.0806616292762043,0.0985203364756491,0.118739219450169,0.14121209620293,0.165713930009679,0.191891413089782,0.219261041115406,0.247216133531637,0.275043606783134,0.301950467357234,0.327099036366501,0.349648951424394,0.368803133657878,0.383854275067035,0.394228091889034,0.399519665209578,0.399519665209578,0.394228091889034,0.383854275067035,0.368803133657878,0.349648951424394,0.327099036366501,0.301950467357234,0.275043606783134,0.247216133531637,0.219261041115406,0.191891413089782,0.165713930009679,0.14121209620293,0.118739219450169,0.0985203364756491,0.0806616292762042,0.0651654652949748,0.0519490270377818,0.0408645520232188,0.0317194372430102,0.0242948158614006,0.0183616268297072,0.0136936148550558,0.0100770744617596,0.00731745791692661],[0.00480791245344714,0.00675486501711126,0.00936453475198971,0.0128104723472374,0.0172923289271272,0.0230330388105312,0.0302732047187965,0.0392622300195994,0.0502459353553176,0.0634506849076839,0.0790644220032968,0.0972154505603285,0.11795025744925,0.14121209620293,0.166822380271917,0.194467098341135,0.223690407961599,0.253897249390649,0.284366241056231,0.314273299697836,0.342725438309289,0.368803133657878,0.391608645798388,0.410316845527414,0.424224581583097,0.432794486603365,0.435689422244215,0.432794486603365,0.424224581583097,0.410316845527414,0.391608645798388,0.368803133657878,0.342725438309289,0.314273299697836,0.284366241056231,0.253897249390649,0.223690407961599,0.194467098341135,0.166822380271916,0.14121209620293,0.11795025744925,0.0972154505603284,0.0790644220032968,0.0634506849076839,0.0502459353553176,0.0392622300195994,0.0302732047187964,0.0230330388105312,0.0172923289271272,0.0128104723472374,0.00936453475198971],[0.00435037909052005,0.00615293777784242,0.00858711641353847,0.0118255564286688,0.0160696058955829,0.0215475686937563,0.0285102305220978,0.037223104426415,0.0479549960374576,0.0609627479662844,0.0764723819368925,0.0946572988275371,0.115614685867945,0.139341764837855,0.165713930009679,0.194467098341135,0.22518665931084,0.257305215333912,0.290110820183939,0.322766662556805,0.354342155944772,0.383854275067035,0.410316845527414,0.432794486603365,0.450457164667846,0.46263095148214,0.468840670298449,0.468840670298449,0.46263095148214,0.450457164667846,0.432794486603365,0.410316845527413,0.383854275067035,0.354342155944772,0.322766662556805,0.290110820183939,0.257305215333912,0.22518665931084,0.194467098341135,0.165713930009679,0.139341764837855,0.115614685867945,0.094657298827537,0.0764723819368925,0.0609627479662843,0.0479549960374577,0.0372231044264149,0.0285102305220978,0.0215475686937563,0.0160696058955829,0.0118255564286688],[0.00388424899097364,0.00553041572239962,0.00776994424400036,0.0107717789637017,0.0147355506920122,0.0198909128860954,0.0264943010518157,0.0348224741954725,0.0451623144762508,0.0577965829382958,0.0729856603688545,0.0909457330521832,0.111824385623646,0.135675090712337,0.162432574343936,0.191891413089782,0.223690407961599,0.257305215333912,0.292051353575983,0.327099036366501,0.361500342322826,0.394228091889034,0.424224581583097,0.450457164667846,0.471976716639932,0.487974421026207,0.497832158229716,0.501162126846763,0.497832158229716,0.487974421026207,0.471976716639932,0.450457164667846,0.424224581583097,0.394228091889034,0.361500342322826,0.327099036366501,0.292051353575983,0.257305215333912,0.223690407961599,0.191891413089782,0.162432574343936,0.135675090712337,0.111824385623646,0.0909457330521832,0.0729856603688544,0.0577965829382958,0.0451623144762508,0.0348224741954725,0.0264943010518157,0.0198909128860954,0.0147355506920122],[0.00342212939880444,0.0049050387277381,0.00693741797248113,0.0096819467204102,0.0133332776414984,0.0181184300533911,0.0242948158614006,0.0321451951514675,0.0419689330467932,0.054069107049735,0.0687353063956126,0.0862223698666218,0.106725806407056,0.130355194312447,0.157107401148196,0.186841934348759,0.219261041115406,0.253897249390649,0.290110820183939,0.327099036366501,0.36391839582758,0.399519665209578,0.432794486603365,0.46263095148214,0.487974421026207,0.507889034934368,0.521614941239284,0.528616379653865,0.528616379653865,0.521614941239283,0.507889034934368,0.487974421026207,0.46263095148214,0.432794486603365,0.399519665209578,0.36391839582758,0.327099036366501,0.290110820183939,0.253897249390649,0.219261041115406,0.186841934348759,0.157107401148195,0.130355194312447,0.106725806407056,0.0862223698666217,0.0687353063956126,0.0540691070497349,0.0419689330467932,0.0321451951514675,0.0242948158614005,0.0181184300533911],[0.00297505637545101,0.00429275902338875,0.00611205462126438,0.00858711641353845,0.0119046568466222,0.0162853020866414,0.0219828584556888,0.0292807310172078,0.0384847857762759,0.0499120765521392,0.0638751026275517,0.0806616292762043,0.100510579292344,0.12358505892293,0.14994416456255,0.179515757313079,0.212072809175268,0.247216133531637,0.284366241056231,0.322766662556805,0.361500342322826,0.399519665209578,0.435689422244215,0.468840670298449,0.497832158229716,0.521614941239284,0.539295138882965,0.550189623358317,0.553869807831981,0.550189623358317,0.539295138882965,0.521614941239284,0.497832158229716,0.468840670298449,0.435689422244214,0.399519665209578,0.361500342322826,0.322766662556805,0.284366241056231,0.247216133531637,0.212072809175268,0.179515757313079,0.14994416456255,0.12358505892293,0.100510579292344,0.0806616292762043,0.0638751026275517,0.0499120765521392,0.0384847857762759,0.0292807310172078,0.0219828584556888],[0.00255213344688908,0.00370714852163513,0.00531356502020352,0.00751521517264462,0.0104883276721383,0.0144437672387548,0.0196274609634119,0.0263182598341997,0.0348224741954725,0.0454644024137073,0.0585723644277108,0.074460068511043,0.0934035782207984,0.115614685867945,0.14121209620293,0.170192415899862,0.202403451179384,0.237522650285267,0.275043606783134,0.314273299697836,0.354342155944772,0.394228091889034,0.432794486603365,0.468840670298449,0.501162126846763,0.528616379653865,0.550189623358317,0.565058720150549,0.572643287840188,0.572643287840188,0.565058720150549,0.550189623358317,0.528616379653865,0.501162126846763,0.468840670298449,0.432794486603365,0.394228091889034,0.354342155944772,0.314273299697836,0.275043606783134,0.237522650285267,0.202403451179384,0.170192415899862,0.14121209620293,0.115614685867945,0.0934035782207984,0.074460068511043,0.0585723644277108,0.0454644024137072,0.0348224741954725,0.0263182598341997],[0.00216033432227369,0.00315902358748666,0.00455820841654654,0.00649000297133997,0.00911811403733465,0.0126407997138249,0.0172923289271273,0.023342202505784,0.0310913503036355,0.0408645520232188,0.0529984669970311,0.0678249183842699,0.0856494658693669,0.106725806407056,0.131227132171329,0.159216193873727,0.190616391795795,0.22518665931084,0.26250312547953,0.301950467357234,0.342725438309289,0.383854275067035,0.424224581583097,0.46263095148214,0.497832158229716,0.528616379653865,0.553869807831981,0.572643287840188,0.584211449611887,0.588119203984053,0.584211449611887,0.572643287840188,0.553869807831981,0.528616379653865,0.497832158229716,0.46263095148214,0.424224581583097,0.383854275067035,0.342725438309289,0.301950467357234,0.26250312547953,0.22518665931084,0.190616391795795,0.159216193873727,0.131227132171329,0.106725806407056,0.0856494658693668,0.0678249183842699,0.0529984669970311,0.0408645520232188,0.0310913503036355],[0.00180446290608457,0.00265628798869891,0.00385844012284923,0.00553041572239961,0.00782191692206263,0.0109163644444218,0.0150332285620231,0.0204284728407596,0.0273923283934298,0.0362436063266842,0.047319839874405,0.0609627479662844,0.077498841553397,0.0972154505603284,0.120333010706226,0.146975071174301,0.177138100154409,0.210663692721226,0.247216133531637,0.286268349312621,0.327099036366501,0.368803133657878,0.410316845527414,0.450457164667846,0.487974421026207,0.521614941239284,0.550189623358317,0.572643287840188,0.588119203984053,0.596013303753021,0.596013303753021,0.588119203984053,0.572643287840188,0.550189623358316,0.521614941239283,0.487974421026207,0.450457164667846,0.410316845527413,0.368803133657878,0.327099036366501,0.286268349312621,0.247216133531636,0.210663692721226,0.177138100154409,0.146975071174301,0.120333010706226,0.0972154505603284,0.0774988415533969,0.0609627479662843,0.0473198398744049,0.0362436063266843],[0.00148725130599982,0.00220397597048942,0.00322284009713001,0.00465029033468202,0.00662110972813867,0.0093023121594056,0.012896160807054,0.0176416571436387,0.0238137462755767,0.0317194372430102,0.0416900707336809,0.054069107049735,0.0691950726228376,0.087379699852504,0.108881810711915,0.133878096089058,0.162432574343936,0.194467098341135,0.229735731585067,0.267806040375428,0.308050271419555,0.349648951424394,0.391608645798388,0.432794486603365,0.471976716639932,0.507889034934368,0.539295138882965,0.565058720150549,0.584211449611887,0.596013303753021,0.6,0.596013303753021,0.584211449611887,0.565058720150549,0.539295138882965,0.507889034934368,0.471976716639932,0.432794486603365,0.391608645798388,0.349648951424394,0.308050271419555,0.267806040375427,0.229735731585067,0.194467098341135,0.162432574343936,0.133878096089058,0.108881810711915,0.087379699852504,0.0691950726228374,0.0540691070497349,0.0416900707336809],[0.00120956765827622,0.00180446290608457,0.00265628798869891,0.00385844012284923,0.00553041572239961,0.00782191692206264,0.0109163644444218,0.0150332285620231,0.0204284728407596,0.0273923283934298,0.0362436063266842,0.047319839874405,0.0609627479662844,0.0774988415533969,0.0972154505603285,0.120333010706226,0.146975071174301,0.177138100154409,0.210663692721226,0.247216133531637,0.286268349312621,0.327099036366501,0.368803133657878,0.410316845527414,0.450457164667846,0.487974421026207,0.521614941239284,0.550189623358317,0.572643287840188,0.588119203984053,0.596013303753021,0.596013303753021,0.588119203984053,0.572643287840188,0.550189623358316,0.521614941239284,0.487974421026207,0.450457164667846,0.410316845527413,0.368803133657878,0.327099036366501,0.28626834931262,0.247216133531637,0.210663692721226,0.177138100154408,0.146975071174301,0.120333010706226,0.0972154505603285,0.0774988415533968,0.0609627479662844,0.047319839874405],[0.000970700783174117,0.00145780175701476,0.00216033432227369,0.00315902358748665,0.00455820841654654,0.00649000297133997,0.00911811403733466,0.0126407997138249,0.0172923289271272,0.023342202505784,0.0310913503036355,0.0408645520232188,0.0529984669970311,0.0678249183842699,0.0856494658693669,0.106725806407056,0.131227132171329,0.159216193873727,0.190616391795795,0.22518665931084,0.26250312547953,0.301950467357234,0.342725438309289,0.383854275067035,0.424224581583097,0.46263095148214,0.497832158229716,0.528616379653865,0.553869807831981,0.572643287840188,0.584211449611887,0.588119203984053,0.584211449611887,0.572643287840188,0.553869807831981,0.528616379653865,0.497832158229716,0.46263095148214,0.424224581583097,0.383854275067035,0.342725438309289,0.301950467357234,0.26250312547953,0.22518665931084,0.190616391795795,0.159216193873727,0.131227132171329,0.106725806407056,0.0856494658693668,0.0678249183842699,0.0529984669970311],[0.000768687822230526,0.00116213983201199,0.00173370890350817,0.00255213344688907,0.00370714852163513,0.00531356502020352,0.00751521517264462,0.0104883276721383,0.0144437672387548,0.0196274609634119,0.0263182598341997,0.0348224741954725,0.0454644024137073,0.0585723644277108,0.074460068511043,0.0934035782207984,0.115614685867945,0.14121209620293,0.170192415899862,0.202403451179384,0.237522650285267,0.275043606783134,0.314273299697836,0.354342155944772,0.394228091889034,0.432794486603365,0.468840670298449,0.501162126846763,0.528616379653865,0.550189623358317,0.565058720150549,0.572643287840188,0.572643287840188,0.565058720150549,0.550189623358316,0.528616379653865,0.501162126846763,0.468840670298449,0.432794486603365,0.394228091889034,0.354342155944772,0.314273299697836,0.275043606783134,0.237522650285267,0.202403451179384,0.170192415899862,0.14121209620293,0.115614685867945,0.0934035782207983,0.074460068511043,0.0585723644277108],[0.000600653523041358,0.000914171570315839,0.0013729059917533,0.00203452624540213,0.002975056375451,0.00429275902338875,0.00611205462126438,0.00858711641353846,0.0119046568466222,0.0162853020866414,0.0219828584556887,0.0292807310172078,0.0384847857762759,0.0499120765521392,0.0638751026275517,0.0806616292762043,0.100510579292344,0.12358505892293,0.14994416456255,0.179515757313079,0.212072809175268,0.247216133531637,0.284366241056231,0.322766662556805,0.361500342322826,0.399519665209578,0.435689422244214,0.468840670298449,0.497832158229716,0.521614941239283,0.539295138882965,0.550189623358316,0.553869807831981,0.550189623358316,0.539295138882965,0.521614941239283,0.497832158229716,0.468840670298449,0.435689422244214,0.399519665209578,0.361500342322826,0.322766662556804,0.284366241056231,0.247216133531637,0.212072809175268,0.179515757313079,0.14994416456255,0.12358505892293,0.100510579292344,0.0806616292762043,0.0638751026275517],[0.000463134851459538,0.000709588293969345,0.00107279027571729,0.00160041502870444,0.00235591610298337,0.00342212939880444,0.0049050387277381,0.00693741797248112,0.00968194672041019,0.0133332776414984,0.0181184300533911,0.0242948158614006,0.0321451951514675,0.0419689330467932,0.054069107049735,0.0687353063956126,0.0862223698666218,0.106725806407056,0.130355194312447,0.157107401148196,0.186841934348759,0.219261041115406,0.253897249390649,0.290110820183939,0.327099036366501,0.36391839582758,0.399519665209578,0.432794486603365,0.46263095148214,0.487974421026207,0.507889034934368,0.521614941239284,0.528616379653865,0.528616379653865,0.521614941239283,0.507889034934368,0.487974421026207,0.46263095148214,0.432794486603365,0.399519665209578,0.36391839582758,0.327099036366501,0.290110820183939,0.253897249390649,0.219261041115406,0.186841934348759,0.157107401148195,0.130355194312447,0.106725806407056,0.0862223698666217,0.0687353063956126],[0.000352371118527594,0.000543493782922945,0.000827176643326463,0.00124225671278416,0.00184091547481606,0.00269194232927451,0.00388424899097365,0.00553041572239962,0.00776994424400035,0.0107717789637017,0.0147355506920122,0.0198909128860954,0.0264943010518157,0.0348224741954725,0.0451623144762508,0.0577965829382958,0.0729856603688545,0.0909457330521832,0.111824385623646,0.135675090712337,0.162432574343936,0.191891413089782,0.223690407961599,0.257305215333912,0.292051353575983,0.327099036366501,0.361500342322826,0.394228091889034,0.424224581583097,0.450457164667846,0.471976716639932,0.487974421026207,0.497832158229716,0.501162126846763,0.497832158229716,0.487974421026207,0.471976716639932,0.450457164667846,0.424224581583097,0.394228091889034,0.361500342322826,0.327099036366501,0.292051353575983,0.257305215333912,0.223690407961599,0.191891413089782,0.162432574343936,0.135675090712337,0.111824385623646,0.0909457330521831,0.0729856603688545],[0.000264546824937963,0.00041076376471543,0.000629348359576674,0.000951479619845099,0.00141944079618724,0.002089510138927,0.00303515650089026,0.00435037909052005,0.00615293777784241,0.00858711641353846,0.0118255564286688,0.0160696058955829,0.0215475686937563,0.0285102305220978,0.037223104426415,0.0479549960374576,0.0609627479662844,0.0764723819368924,0.094657298827537,0.115614685867945,0.139341764837855,0.165713930009679,0.194467098341135,0.22518665931084,0.257305215333912,0.290110820183939,0.322766662556805,0.354342155944772,0.383854275067035,0.410316845527414,0.432794486603365,0.450457164667846,0.46263095148214,0.468840670298449,0.468840670298449,0.46263095148214,0.450457164667846,0.432794486603365,0.410316845527413,0.383854275067035,0.354342155944772,0.322766662556804,0.290110820183939,0.257305215333912,0.22518665931084,0.194467098341135,0.165713930009679,0.139341764837855,0.115614685867945,0.094657298827537,0.0764723819368925],[0.000195981108137539,0.000306336733808653,0.000472490796072114,0.000719112827005399,0.00107996610373835,0.00160041502870444,0.00234026223317344,0.00337680384864244,0.00480791245344713,0.00675486501711125,0.0093645347519897,0.0128104723472374,0.0172923289271273,0.0230330388105312,0.0302732047187965,0.0392622300195993,0.0502459353553176,0.0634506849076839,0.0790644220032968,0.0972154505603285,0.11795025744925,0.14121209620293,0.166822380271916,0.194467098341135,0.223690407961599,0.253897249390649,0.284366241056231,0.314273299697836,0.342725438309289,0.368803133657878,0.391608645798388,0.410316845527413,0.424224581583097,0.432794486603365,0.435689422244214,0.432794486603365,0.424224581583097,0.410316845527413,0.391608645798388,0.368803133657878,0.342725438309289,0.314273299697836,0.284366241056231,0.253897249390649,0.223690407961599,0.194467098341135,0.166822380271916,0.14121209620293,0.11795025744925,0.0972154505603284,0.0790644220032968],[0.000143263405762178,0.000225431933761616,0.000350029790834632,0.000536295295709335,0.000810797448378931,0.00120956765827621,0.00178056308686857,0.00258638976077405,0.00370714852163513,0.00524318771128397,0.00731745791692661,0.0100770744617596,0.0136936148550558,0.0183616268297072,0.0242948158614006,0.0317194372430102,0.0408645520232188,0.0519490270377818,0.0651654652949747,0.0806616292762043,0.098520336475649,0.118739219450169,0.14121209620293,0.165713930009679,0.191891413089782,0.219261041115406,0.247216133531637,0.275043606783134,0.301950467357234,0.327099036366501,0.349648951424394,0.368803133657878,0.383854275067035,0.394228091889034,0.399519665209578,0.399519665209578,0.394228091889034,0.383854275067035,0.368803133657878,0.349648951424394,0.327099036366501,0.301950467357234,0.275043606783134,0.247216133531637,0.219261041115406,0.191891413089782,0.165713930009679,0.14121209620293,0.118739219450169,0.098520336475649,0.0806616292762043],[0.000103339353576486,0.000163697181626658,0.000255873948411392,0.000394657487125034,0.000600653523041358,0.000902063515786544,0.00133677899937174,0.00195475133026059,0.00282054366454997,0.00401590605143768,0.00564213753089713,0.00782191692206264,0.0107002059457553,0.0144437672387548,0.0192388111967165,0.0252863061055658,0.0327945711689488,0.0419689330467932,0.0529984669970311,0.0660401564818038,0.0812011699419676,0.0985203364756491,0.11795025744925,0.139341764837855,0.162432574343936,0.186841934348759,0.212072809175268,0.237522650285267,0.26250312547953,0.286268349312621,0.308050271419555,0.327099036366501,0.342725438309289,0.354342155944772,0.361500342322826,0.36391839582758,0.361500342322826,0.354342155944772,0.342725438309289,0.327099036366501,0.308050271419555,0.28626834931262,0.26250312547953,0.237522650285267,0.212072809175268,0.186841934348759,0.162432574343936,0.139341764837855,0.11795025744925,0.098520336475649,0.0812011699419676],[7.3553885049215e-05,0.000117294156088185,0.000184568056896474,0.000286580154249766,0.000439081451328283,0.00066382480550499,0.000990310239759257,0.00145780175701476,0.00211755683659172,0.00303515650089026,0.00429275902338875,0.00599102783094177,0.00825041076289934,0.0112113836026397,0.0150332285620231,0.0198909128860954,0.0259696787411795,0.0334570654521643,0.0425322660283115,0.0533529704756318,0.0660401564818038,0.0806616292762043,0.0972154505603285,0.115614685867945,0.135675090712337,0.157107401148196,0.179515757313079,0.202403451179384,0.22518665931084,0.247216133531637,0.267806040375428,0.286268349312621,0.301950467357234,0.314273299697836,0.322766662556805,0.327099036366501,0.327099036366501,0.322766662556805,0.314273299697836,0.301950467357234,0.286268349312621,0.267806040375428,0.247216133531637,0.22518665931084,0.202403451179384,0.179515757313079,0.157107401148196,0.135675090712337,0.115614685867945,0.0972154505603284,0.0806616292762043],[5.16600592298042e-05,8.29317738144211e-05,0.000131370065428871,0.000205343653509023,0.000316720052174475,0.000482035743303079,0.000723922928374824,0.00107279027571729,0.00156872468787629,0.00226353931127532,0.00322284009713001,0.00452792142923455,0.00627723536605609,0.00858711641353846,0.0115913947224954,0.015439507635816,0.02029273598075,0.0263182598341997,0.0336808577004802,0.0425322660283115,0.0529984669970311,0.0651654652949748,0.0790644220032968,0.0946572988275371,0.111824385623646,0.130355194312447,0.14994416456255,0.170192415899862,0.190616391795795,0.210663692721226,0.229735731585067,0.247216133531637,0.26250312547953,0.275043606783134,0.284366241056231,0.290110820183939,0.292051353575983,0.290110820183939,0.284366241056231,0.275043606783134,0.26250312547953,0.247216133531637,0.229735731585067,0.210663692721226,0.190616391795795,0.170192415899862,0.14994416456255,0.130355194312447,0.111824385623646,0.094657298827537,0.0790644220032968],[3.58025194823257e-05,5.7859535269399e-05,9.22668511192552e-05,0.000145186375817683,0.000225431933761616,0.000345393702892365,0.000522183086877311,0.00077900562569413,0.00114674747797958,0.001665729205055,0.00238753866629755,0.00337680384864244,0.00471270940824392,0.00649000297133997,0.00881918670294107,0.0118255564286688,0.0156467462582503,0.0204284728407596,0.0263182598341997,0.0334570654521643,0.0419689330467932,0.0519490270377818,0.0634506849076839,0.0764723819368924,0.0909457330521832,0.106725806407056,0.12358505892293,0.14121209620293,0.159216193873727,0.177138100154408,0.194467098341135,0.210663692721226,0.22518665931084,0.237522650285267,0.247216133531637,0.253897249390649,0.257305215333912,0.257305215333912,0.253897249390649,0.247216133531637,0.237522650285267,0.22518665931084,0.210663692721226,0.194467098341135,0.177138100154408,0.159216193873727,0.14121209620293,0.12358505892293,0.106725806407056,0.0909457330521831,0.0764723819368924],[2.44839614265361e-05,3.98325705918466e-05,6.39446757131121e-05,0.000101293097276049,0.00015833054967283,0.000244207498484917,0.00037167442428784,0.000558181921922969,0.000827176643326461,0.00120956765827621,0.00174530557548755,0.00248497596768341,0.00349126099840525,0.00484007228345976,0.00662110972813866,0.00893756328245512,0.0119046568466222,0.0156467462582504,0.02029273598075,0.0259696787411795,0.0327945711689487,0.0408645520232188,0.0502459353553176,0.0609627479662844,0.0729856603688545,0.0862223698666217,0.100510579292344,0.115614685867945,0.131227132171329,0.146975071174301,0.162432574343936,0.177138100154408,0.190616391795795,0.202403451179384,0.212072809175268,0.219261041115406,0.223690407961599,0.22518665931084,0.223690407961599,0.219261041115406,0.212072809175268,0.202403451179384,0.190616391795795,0.177138100154408,0.162432574343936,0.146975071174301,0.131227132171329,0.115614685867945,0.100510579292344,0.0862223698666217,0.0729856603688545],[1.65218696098483e-05,2.70589621278937e-05,4.37292960447961e-05,6.97337859512493e-05,0.000109729502323886,0.000170377790341954,0.00026104294504724,0.000394657487125035,0.000588759786402167,0.000866693099412327,0.00125893105084854,0.00180446290608457,0.00255213344688908,0.00356178914895618,0.0049050387277381,0.00666539792294538,0.00893756328245514,0.0118255564286688,0.015439507635816,0.0198909128860954,0.0252863061055658,0.0317194372430102,0.0392622300195994,0.0479549960374577,0.0577965829382958,0.0687353063956126,0.0806616292762043,0.0934035782207984,0.106725806407056,0.120333010706226,0.133878096089058,0.146975071174301,0.159216193873727,0.170192415899862,0.179515757313079,0.186841934348759,0.191891413089782,0.194467098341135,0.194467098341135,0.191891413089782,0.186841934348759,0.179515757313079,0.170192415899862,0.159216193873727,0.146975071174301,0.133878096089058,0.120333010706226,0.106725806407056,0.0934035782207983,0.0806616292762043,0.0687353063956126],[1.10013530252715e-05,1.81381647392463e-05,2.95086940710308e-05,4.73713808838571e-05,7.50397720913829e-05,0.000117294156088185,0.00018091336450473,0.00027534318567382,0.000413511338215675,0.000612787529098586,0.000896069760398338,0.00129295417097857,0.00184091547481606,0.00258638976077405,0.00358561373700357,0.0049050387277381,0.00662110972813867,0.00881918670294107,0.0115913947224954,0.0150332285620231,0.0192388111967165,0.0242948158614006,0.0302732047187965,0.037223104426415,0.0451623144762508,0.0540691070497349,0.0638751026275517,0.074460068511043,0.0856494658693669,0.0972154505603285,0.108881810711915,0.120333010706226,0.131227132171329,0.14121209620293,0.14994416456255,0.157107401148196,0.162432574343936,0.165713930009679,0.166822380271916,0.165713930009679,0.162432574343936,0.157107401148196,0.14994416456255,0.14121209620293,0.131227132171329,0.120333010706226,0.108881810711915,0.0972154505603284,0.0856494658693668,0.074460068511043,0.0638751026275517],[7.22840401891739e-06,1.19973397078484e-05,1.96488392933835e-05,3.17539862148389e-05,5.06371215200025e-05,7.96799724510649e-05,0.000123719668395553,0.000189556083180934,0.000286580154249766,0.000427527351963646,0.000629348359576674,0.000914171570315839,0.00131030900168851,0.00185322924494206,0.00258638976077405,0.00356178914895618,0.00484007228345977,0.00649000297133998,0.00858711641353846,0.0112113836026397,0.0144437672387548,0.0183616268297072,0.0230330388105312,0.0285102305220978,0.0348224741954726,0.0419689330467932,0.0499120765521392,0.0585723644277108,0.0678249183842699,0.077498841553397,0.087379699852504,0.0972154505603285,0.106725806407056,0.115614685867945,0.12358505892293,0.130355194312447,0.135675090712337,0.139341764837855,0.14121209620293,0.14121209620293,0.139341764837855,0.135675090712337,0.130355194312447,0.12358505892293,0.115614685867945,0.106725806407056,0.0972154505603284,0.087379699852504,0.0774988415533969,0.0678249183842699,0.0585723644277108],[4.68649484013765e-06,7.83043659362291e-06,1.29102073707747e-05,2.10034127032407e-05,3.37175430614134e-05,5.34110828015164e-05,8.34864993370552e-05,0.000128768763845601,0.000195981108137539,0.00029432509866777,0.000436163977371402,0.000637795862576539,0.000920286407594678,0.00131030900168851,0.00184091547481606,0.00255213344688907,0.00349126099840525,0.00471270940824391,0.00627723536605608,0.00825041076289933,0.0107002059457552,0.0136936148550558,0.0172923289271272,0.0215475686937563,0.0264943010518157,0.0321451951514675,0.0384847857762759,0.0454644024137073,0.0529984669970311,0.0609627479662844,0.0691950726228375,0.0774988415533969,0.0856494658693669,0.0934035782207984,0.100510579292344,0.106725806407056,0.111824385623646,0.115614685867945,0.11795025744925,0.118739219450169,0.11795025744925,0.115614685867945,0.111824385623646,0.106725806407056,0.100510579292344,0.0934035782207984,0.0856494658693668,0.0774988415533969,0.0691950726228374,0.0609627479662844,0.0529984669970311],[2.9982184657774e-06,5.04308635927536e-06,8.37025988999777e-06,1.37085300097679e-05,2.2154004440479e-05,3.53283209048876e-05,5.55908304410999e-05,8.63162837089357e-05,0.000132248791698088,0.000199940189141846,0.000298275712212873,0.000439081451328283,0.00063779586257654,0.000914171570315838,0.00129295417097857,0.00180446290608457,0.00248497596768341,0.00337680384864244,0.00452792142923454,0.00599102783094176,0.00782191692206263,0.0100770744617596,0.0128104723472374,0.0160696058955829,0.0198909128860954,0.0242948158614005,0.0292807310172078,0.0348224741954725,0.0408645520232188,0.0473198398744049,0.0540691070497349,0.0609627479662844,0.0678249183842699,0.074460068511043,0.0806616292762043,0.0862223698666217,0.0909457330521831,0.094657298827537,0.0972154505603284,0.098520336475649,0.098520336475649,0.0972154505603284,0.094657298827537,0.0909457330521831,0.0862223698666217,0.0806616292762043,0.0744600685110429,0.0678249183842699,0.0609627479662843,0.0540691070497349,0.047319839874405],[1.89272628310214e-06,3.20491296564298e-06,5.35493340366297e-06,8.82879260580174e-06,1.43634234095069e-05,2.30581265131566e-05,3.65257783522028e-05,5.70931950868451e-05,8.80599883059858e-05,0.000134023916789937,0.000201277576741507,0.000298275712212874,0.000436163977371403,0.000629348359576674,0.000896069760398339,0.00125893105084854,0.00174530557548755,0.00238753866629755,0.00322284009713001,0.00429275902338875,0.00564213753089713,0.00731745791692661,0.00936453475198971,0.0118255564286688,0.0147355506920122,0.0181184300533911,0.0219828584556888,0.0263182598341997,0.0310913503036355,0.0362436063266843,0.0416900707336809,0.047319839874405,0.0529984669970311,0.0585723644277108,0.0638751026275517,0.0687353063956126,0.0729856603688545,0.0764723819368925,0.0790644220032968,0.0806616292762043,0.0812011699419676,0.0806616292762043,0.0790644220032968,0.0764723819368925,0.0729856603688545,0.0687353063956126,0.0638751026275517,0.0585723644277108,0.0529984669970311,0.047319839874405,0.0416900707336809],[1.17902159080384e-06,2.00976594000481e-06,3.38048188051117e-06,5.61075299479357e-06,9.18910246722853e-06,1.48502732764156e-05,2.36812816953261e-05,3.72636480204375e-05,5.78595352693989e-05,8.864901613922e-05,0.000134023916789937,0.000199940189141846,0.00029432509866777,0.000427527351963646,0.000612787529098585,0.000866693099412324,0.00120956765827621,0.001665729205055,0.00226353931127532,0.00303515650089026,0.00401590605143768,0.00524318771128397,0.00675486501711125,0.00858711641353845,0.0107717789637017,0.0133332776414983,0.0162853020866414,0.0196274609634119,0.023342202505784,0.0273923283934298,0.0317194372430102,0.0362436063266842,0.0408645520232188,0.0454644024137072,0.0499120765521392,0.0540691070497349,0.0577965829382957,0.0609627479662843,0.0634506849076838,0.0651654652949747,0.0660401564818037,0.0660401564818037,0.0651654652949747,0.0634506849076838,0.0609627479662843,0.0577965829382958,0.0540691070497349,0.0499120765521392,0.0454644024137072,0.0408645520232188,0.0362436063266842],[7.24711428897925e-07,1.24360978509263e-06,2.10577787185976e-06,3.51844196319816e-06,5.80093010444873e-06,9.43744167287364e-06,1.51502686963628e-05,2.39991465075845e-05,3.75129022648921e-05,5.7859535269399e-05,8.80599883059858e-05,0.000132248791698088,0.00019598110813754,0.000286580154249766,0.000413511338215676,0.000588759786402167,0.000827176643326462,0.00114674747797958,0.00156872468787629,0.00211755683659172,0.00282054366454997,0.00370714852163514,0.00480791245344714,0.00615293777784241,0.00776994424400035,0.00968194672041019,0.0119046568466222,0.0144437672387548,0.0172923289271272,0.0204284728407596,0.0238137462755767,0.0273923283934298,0.0310913503036355,0.0348224741954725,0.0384847857762759,0.0419689330467932,0.0451623144762508,0.0479549960374576,0.0502459353553176,0.0519490270377818,0.0529984669970311,0.0533529704756318,0.0529984669970311,0.0519490270377817,0.0502459353553176,0.0479549960374577,0.0451623144762508,0.0419689330467932,0.0384847857762759,0.0348224741954725,0.0310913503036355],[4.39559701070744e-07,7.59332845721063e-07,1.2943624632155e-06,2.17715345186082e-06,3.61352927834837e-06,5.91811666596016e-06,9.56411685304937e-06,1.52516079097196e-05,2.39991465075845e-05,3.72636480204375e-05,5.70931950868451e-05,8.63162837089357e-05,0.000128768763845601,0.000189556083180934,0.00027534318567382,0.000394657487125035,0.00055818192192297,0.000779005625694131,0.00107279027571729,0.00145780175701476,0.00195475133026059,0.00258638976077406,0.00337680384864244,0.00435037909052005,0.00553041572239962,0.00693741797248112,0.00858711641353847,0.0104883276721384,0.0126407997138249,0.0150332285620231,0.0176416571436387,0.0204284728407596,0.023342202505784,0.0263182598341997,0.0292807310172078,0.0321451951514675,0.0348224741954725,0.037223104426415,0.0392622300195994,0.0408645520232188,0.0419689330467932,0.0425322660283115,0.0425322660283115,0.0419689330467932,0.0408645520232188,0.0392622300195994,0.037223104426415,0.0348224741954726,0.0321451951514675,0.0292807310172078,0.0263182598341997],[2.63075276374519e-07,4.57498472775216e-07,7.85070518721362e-07,1.32934313157544e-06,2.22113486903228e-06,3.66203230366726e-06,5.95770258351065e-06,9.56411685304934e-06,1.51502686963627e-05,2.36812816953261e-05,3.65257783522028e-05,5.55908304411e-05,8.34864993370552e-05,0.000123719668395553,0.00018091336450473,0.000261042945047239,0.000371674424287839,0.000522183086877311,0.000723922928374824,0.000990310239759256,0.00133677899937174,0.00178056308686857,0.00234026223317344,0.00303515650089026,0.00388424899097364,0.0049050387277381,0.00611205462126437,0.00751521517264463,0.00911811403733465,0.0109163644444218,0.0128961608070539,0.0150332285620231,0.0172923289271272,0.0196274609634119,0.0219828584556888,0.0242948158614006,0.0264943010518157,0.0285102305220978,0.0302732047187964,0.0317194372430102,0.0327945711689487,0.0334570654521643,0.0336808577004802,0.0334570654521643,0.0327945711689487,0.0317194372430102,0.0302732047187964,0.0285102305220978,0.0264943010518157,0.0242948158614005,0.0219828584556888],[1.55364434343856e-07,2.71992242729848e-07,4.69862553785088e-07,8.00929995215034e-07,1.34718639742489e-06,2.2359919032472e-06,3.66203230366728e-06,5.91811666596016e-06,9.43744167287364e-06,1.48502732764156e-05,2.30581265131566e-05,3.53283209048877e-05,5.34110828015165e-05,7.96799724510649e-05,0.000117294156088185,0.000170377790341954,0.000244207498484917,0.000345393702892366,0.000482035743303079,0.000663824805504991,0.000902063515786544,0.00120956765827622,0.00160041502870444,0.002089510138927,0.00269194232927451,0.00342212939880444,0.00429275902338875,0.00531356502020352,0.00649000297133997,0.00782191692206265,0.0093023121594056,0.0109163644444218,0.0126407997138249,0.0144437672387548,0.0162853020866414,0.0181184300533911,0.0198909128860954,0.0215475686937563,0.0230330388105312,0.0242948158614006,0.0252863061055658,0.0259696787411795,0.0263182598341997,0.0263182598341997,0.0259696787411795,0.0252863061055659,0.0242948158614006,0.0230330388105312,0.0215475686937563,0.0198909128860954,0.0181184300533911],[9.05383525949556e-08,1.5956322093352e-07,2.77486850509873e-07,4.76169339641005e-07,8.0628736657891e-07,1.34718639742488e-06,2.22113486903228e-06,3.61352927834837e-06,5.80093010444872e-06,9.18910246722853e-06,1.43634234095068e-05,2.21540044404789e-05,3.37175430614135e-05,5.06371215200023e-05,7.50397720913829e-05,0.000109729502323886,0.00015833054967283,0.000225431933761616,0.000316720052174474,0.000439081451328283,0.000600653523041357,0.000810797448378931,0.00107996610373835,0.00141944079618724,0.00184091547481605,0.00235591610298337,0.002975056375451,0.00370714852163513,0.00455820841654654,0.00553041572239961,0.00662110972813865,0.00782191692206263,0.00911811403733464,0.0104883276721383,0.0119046568466222,0.0133332776414983,0.0147355506920122,0.0160696058955829,0.0172923289271272,0.0183616268297072,0.0192388111967164,0.0198909128860954,0.02029273598075,0.0204284728407596,0.02029273598075,0.0198909128860954,0.0192388111967164,0.0183616268297072,0.0172923289271272,0.0160696058955829,0.0147355506920122],[5.20622549555798e-08,9.23673486411887e-08,1.61704977191391e-07,2.79342942946985e-07,4.76169339641007e-07,8.00929995215034e-07,1.32934313157545e-06,2.17715345186082e-06,3.51844196319816e-06,5.61075299479358e-06,8.82879260580174e-06,1.37085300097679e-05,2.10034127032407e-05,3.17539862148389e-05,4.73713808838572e-05,6.97337859512493e-05,0.000101293097276049,0.000145186375817683,0.000205343653509023,0.000286580154249766,0.000394657487125034,0.000536295295709336,0.000719112827005401,0.000951479619845099,0.00124225671278416,0.00160041502870444,0.00203452624540213,0.00255213344688908,0.00315902358748665,0.00385844012284923,0.00465029033468202,0.00553041572239961,0.00649000297133997,0.00751521517264463,0.00858711641353847,0.0096819467204102,0.0107717789637017,0.0118255564286688,0.0128104723472374,0.0136936148550558,0.0144437672387548,0.0150332285620231,0.015439507635816,0.0156467462582504,0.0156467462582504,0.015439507635816,0.0150332285620231,0.0144437672387548,0.0136936148550558,0.0128104723472374,0.0118255564286688],[2.95408350901903e-08,5.27610667475303e-08,9.29851881421736e-08,1.61704977191391e-07,2.77486850509872e-07,4.69862553785087e-07,7.85070518721361e-07,1.29436246321549e-06,2.10577787185975e-06,3.38048188051115e-06,5.35493340366295e-06,8.37025988999775e-06,1.29102073707747e-05,1.96488392933834e-05,2.95086940710307e-05,4.37292960447957e-05,6.39446757131119e-05,9.22668511192552e-05,0.000131370065428871,0.000184568056896474,0.000255873948411391,0.000350029790834631,0.000472490796072112,0.000629348359576672,0.00082717664332646,0.00107279027571729,0.0013729059917533,0.00173370890350817,0.00216033432227369,0.0026562879886989,0.00322284009713,0.00385844012284922,0.00455820841654653,0.00531356502020351,0.00611205462126436,0.0069374179724811,0.00776994424400033,0.00858711641353845,0.00936453475198968,0.0100770744617596,0.0107002059457552,0.0112113836026396,0.0115913947224953,0.0118255564286688,0.0119046568466221,0.0118255564286688,0.0115913947224953,0.0112113836026396,0.0107002059457552,0.0100770744617596,0.00936453475198968],[1.65398653139756e-08,2.9738431915035e-08,5.27610667475305e-08,9.23673486411884e-08,1.5956322093352e-07,2.71992242729848e-07,4.57498472775216e-07,7.5933284572106e-07,1.24360978509263e-06,2.00976594000481e-06,3.20491296564297e-06,5.04308635927536e-06,7.83043659362291e-06,1.19973397078484e-05,1.81381647392463e-05,2.70589621278937e-05,3.98325705918466e-05,5.78595352693991e-05,8.29317738144211e-05,0.000117294156088185,0.000163697181626658,0.000225431933761616,0.000306336733808653,0.000410763764715429,0.000543493782922945,0.000709588293969343,0.000914171570315837,0.00116213983201199,0.00145780175701476,0.00180446290608457,0.00220397597048941,0.0026562879886989,0.00315902358748665,0.00370714852163513,0.00429275902338875,0.00490503872773809,0.0055304157223996,0.0061529377778424,0.00675486501711125,0.0073174579169266,0.00782191692206263,0.00825041076289932,0.00858711641353845,0.00881918670294106,0.00893756328245513,0.00893756328245512,0.00881918670294106,0.00858711641353845,0.00825041076289932,0.00782191692206263,0.0073174579169266],[9.13798784682758e-09,1.65398653139756e-08,2.95408350901905e-08,5.20622549555798e-08,9.05383525949556e-08,1.55364434343856e-07,2.6307527637452e-07,4.39559701070744e-07,7.24711428897925e-07,1.17902159080385e-06,1.89272628310214e-06,2.9982184657774e-06,4.68649484013767e-06,7.22840401891739e-06,1.10013530252715e-05,1.65218696098483e-05,2.44839614265362e-05,3.58025194823259e-05,5.16600592298042e-05,7.3553885049215e-05,0.000103339353576486,0.000143263405762178,0.00019598110813754,0.000264546824937963,0.000352371118527595,0.000463134851459538,0.000600653523041358,0.000768687822230527,0.000970700783174119,0.00120956765827622,0.00148725130599982,0.00180446290608458,0.00216033432227369,0.00255213344688908,0.00297505637545101,0.00342212939880444,0.00388424899097364,0.00435037909052005,0.00480791245344714,0.00524318771128398,0.00564213753089712,0.00599102783094176,0.00627723536605608,0.00649000297133997,0.00662110972813867,0.00666539792294538,0.00662110972813867,0.00649000297133997,0.00627723536605608,0.00599102783094177,0.00564213753089713]],"type":"surface","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
<div id="linear-discriminant-analysis" class="section level2 hasAnchor" number="12.3">
<h2 class="hasAnchor"><span class="header-section-number">12.3</span> Linear Discriminant Analysis<a href="#linear-discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As we demonstrated earlier using the Bayes rule, the conditional probability can be formulated using Bayes Theorem. For this time, we will assume in generate that there are <span class="math inline">\(K\)</span> classes instead of just two. However, the notation are similar to the previous case:</p>
<p><span class="math display">\[\begin{align}
\text{P}(Y = k | X = \mathbf{x}) =&amp;~ \frac{\text{P}(X = \mathbf{x}| Y = k)\text{P}(Y = k)}{\text{P}(X = \mathbf{x})}\\
                     =&amp;~ \frac{\text{P}(X = \mathbf{x}| Y = k)\text{P}(Y = k)}{\sum_{l=1}^K \text{P}(X = \mathbf{x}| Y = l) \text{P}(Y = l)}\\
                     =&amp;~ \frac{\pi_k f_k(\mathbf{x})}{\sum_{l=1}^K \pi_l f_l(\mathbf{x})}
\end{align}\]</span></p>
<p>Given any target point <span class="math inline">\(\mathbf{x}\)</span>, the best prediction is simply picking the one that maximizes the posterior</p>
<p><span class="math display">\[\underset{k}{\arg\max} \,\, \pi_k f_k(x)\]</span>
Both LDA and QDA model <span class="math inline">\(f_k\)</span> as a normal density function. Suppose we model each class density as multivariate Gaussian <span class="math inline">\({\cal N}(\boldsymbol \mu_k, \Sigma_k)\)</span>, and . Then</p>
<p><span class="math display">\[f_k(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp\left[ -\frac{1}{2} (\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma^{-1} (\mathbf{x}- \boldsymbol \mu_k) \right].\]</span>
The log-likelihood function for the conditional distribution is</p>
<p><span class="math display">\[\begin{align}
\log f_k(\mathbf{x}) =&amp;~ -\log \big((2\pi)^{p/2} |\Sigma|^{1/2} \big) - \frac{1}{2} (\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma^{-1} (\mathbf{x}- \boldsymbol \mu_k) \\
    =&amp;~ - \frac{1}{2} (\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma^{-1} (\mathbf{x}- \boldsymbol \mu_k) + \text{Constant}
\end{align}\]</span></p>
<p>The <strong>maximum a posteriori</strong> probability (MAP) estimate is simply</p>
<p><span class="math display">\[\begin{align}
\widehat f(\mathbf{x}) =&amp; ~\underset{k}{\arg\max} \,\, \log \big( \pi_k f_k(\mathbf{x}) \big) \\
    =&amp; ~\underset{k}{\arg\max} \,\, - \frac{1}{2} (\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma^{-1} (\mathbf{x}- \boldsymbol \mu_k) + \log(\pi_k)
\end{align}\]</span></p>
<p>The term <span class="math inline">\((\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma^{-1} (\mathbf{x}- \boldsymbol \mu_k)\)</span> is simply the between <span class="math inline">\(x\)</span> and the centroid <span class="math inline">\(\boldsymbol \mu_k\)</span> for class <span class="math inline">\(k\)</span>. Hence, this is essentially classifying <span class="math inline">\(x\)</span> to the class label with the closest centroid (after adjusting the for prior). This sets a connection with the <span class="math inline">\(k\)</span>NN algorithm. <strong>A special case</strong> is that when <span class="math inline">\(\Sigma = \mathbf{I}\)</span>, i.e., only Euclidean distance is needed, and we have</p>
<p><span class="math display">\[\underset{k}{\arg\max} \,\, - \frac{1}{2} \lVert x - \boldsymbol \mu_k \rVert^2 + \log(\pi_k)\]</span></p>
<p>The decision boundary of LDA, as its name suggests, is a linear function of <span class="math inline">\(\mathbf{x}\)</span>. To see this, let’s look at the terms in the MAP. Note that anything that does not depends on the class index <span class="math inline">\(k\)</span> is irrelevant to the decision.</p>
<p><span class="math display">\[\begin{align}
&amp; - \frac{1}{2} (\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma^{-1} (\mathbf{x}- \boldsymbol \mu_k) + \log(\pi_k)\\
=&amp;~ \mathbf{x}^\text{T}\Sigma^{-1} \boldsymbol \mu_k - \frac{1}{2}\boldsymbol \mu_k^\text{T}\Sigma^{-1} \boldsymbol \mu_k + \log(\pi_k) \text{irrelevant terms} \\
=&amp;~ \mathbf{x}^\text{T}\mathbf{w}_k + b_k + \text{irrelevant terms}
\end{align}\]</span></p>
<p>Then, the decision boundary between two classes, <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> is</p>
<p><span class="math display">\[\begin{align}
\mathbf{x}^\text{T}\mathbf{w}_k + b_k &amp;= \mathbf{x}^\text{T}\mathbf{w}_l + b_l \\
\Leftrightarrow \quad \mathbf{x}^\text{T}(\mathbf{w}_k - \mathbf{w}_l) + (b_k - b_l) &amp;= 0, \\
\end{align}\]</span></p>
<p>which is a linear function of <span class="math inline">\(\mathbf{x}\)</span>. The previous density plot already showed this effect. Estimating the parameters in LDA is very simple:</p>
<ul>
<li>Prior probabilities: <span class="math inline">\(\widehat{\pi}_k = n_k / n = n^{-1} \sum_k \mathbf{1}\{y_i = k\}\)</span>, where <span class="math inline">\(n_k\)</span> is the number of observations in class <span class="math inline">\(k\)</span>.</li>
<li>Centroids: <span class="math inline">\(\widehat{\boldsymbol \mu}_k = n_k^{-1} \sum_{i: \,y_i = k} x_i\)</span></li>
<li>Pooled covariance matrix:
<span class="math display">\[\widehat \Sigma = \frac{1}{n-K} \sum_{k=1}^K \sum_{i : \, y_i = k} (\mathbf{x}_i - \widehat{\boldsymbol \mu}_k) (\mathbf{x}_i - \widehat{\boldsymbol \mu}_k)^\text{T}\]</span></li>
</ul>
</div>
<div id="example-quadratic-discriminant-analysis-qda" class="section level2 hasAnchor" number="12.4">
<h2 class="hasAnchor"><span class="header-section-number">12.4</span> Example: Quadratic Discriminant Analysis (QDA)<a href="#example-quadratic-discriminant-analysis-qda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we assume that each class has its own covariance structure, the decision boundary will not be linear anymore. Let’s visualize this by creating two density functions that use different covariance matrices.</p>
<div id="htmlwidget-ee5160df473437e9e6c6" style="width:75%;height:576px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-ee5160df473437e9e6c6">{"x":{"visdat":{"46f42d910a1":["function () ","plotlyVisDat"]},"cur_data":"46f42d910a1","attrs":{"46f42d910a1":{"x":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"y":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[0.00513102571342465,0.00509693264516655,0.00499600661672614,0.00483221803781215,0.00461189537455569,0.00434331949635779,0.00403620778202872,0.00370113273231715,0.00334892338531822,0.00299009626738422,0.00263435643946864,0.00229019946562793,0.00196463324342567,0.00166302613667241,0.00138907619276083,0.00114488658916215,0.000931125617478452,0.000747245811290876,0.000591736161450109,0.0004623832976235,0.000356521374881682,0.000271255413515408,0.000203648240788258,0.000150866327385719,0.000110284221175755,7.95506714736882e-05,5.66218071108094e-05,3.9767932136906e-05,2.75607923477168e-05,1.88477622939187e-05,1.27185411556824e-05,8.46884550745414e-06,5.56442800817777e-06,3.60766541001235e-06,2.30802965107452e-06,1.4570213720684e-06,9.07611243774642e-07,5.57883118078687e-07,3.38373215650806e-07,2.02515442175952e-07,1.19599597321226e-07,6.96964563235772e-08,4.00775425509639e-08,2.27405454552105e-08,1.27323942551786e-08,7.03442632428991e-09,3.83492345344395e-09,2.06297574038154e-09,1.09506764903209e-09,5.73584205756112e-10,2.96457762188409e-10],[0.00688013942269972,0.00688013942269972,0.00678901308930213,0.00661036529722545,0.00635116916651323,0.00602131445404234,0.00563298173082845,0.00519989751462503,0.00473653370979245,0.00425731600807728,0.00377590057312685,0.00330456743695959,0.00285376426252262,0.0024318174914824,0.00204481144709398,0.00169662149313271,0.00138907619276083,0.00112221631578344,0.000894615659797482,0.000703729602943137,0.000546241323382678,0.000418381709164601,0.000316206093509063,0.000235818127636129,0.00017353758352161,0.000126014149170312,9.02930834558448e-05,6.38409092481723e-05,4.45402910172989e-05,3.06631271382899e-05,2.08299987580418e-05,1.39627657264129e-05,9.23555641318089e-06,6.02787290026731e-06,3.88216968943221e-06,2.46714315125773e-06,1.54711854195989e-06,9.57331258920113e-07,5.8453469717544e-07,3.52182488547485e-07,2.09379726076757e-07,1.22831824744976e-07,7.11044181364298e-08,4.06154881192249e-08,2.28926555618971e-08,1.27323942551786e-08,6.98768612257874e-09,3.78413051282132e-09,2.022126083786e-09,1.0662517644402e-09,5.54779878789496e-10],[0.00910331758321209,0.00916420911334325,0.00910331758321209,0.00892305981775455,0.00863052712129623,0.00823702237755497,0.00775733553752108,0.00720882221319582,0.00661036529722545,0.00598130586781599,0.00534042684519057,0.00470506184118492,0.00409038425969477,0.00350891047508742,0.00297022859143874,0.00248094346351084,0.00204481144709398,0.00166302613667241,0.00133460973618253,0.00105686352521443,0.000825834339292521,0.00063676087692254,0.000484472593087523,0.000363723584402474,0.000269453058617017,0.00019697185732524,0.000142080556443981,0.000101128723518423,7.10270551079748e-05,4.92246343149448e-05,3.36628277905811e-05,2.27158032871252e-05,1.51256835403982e-05,9.93828226774759e-06,6.44342906756979e-06,4.12222965612978e-06,2.60229616493837e-06,1.62102856156239e-06,9.96400688755216e-07,6.04347567088824e-07,3.61700362546594e-07,2.13609477117942e-07,1.24480549481003e-07,7.15800312060429e-08,4.06154881192247e-08,2.27405454552105e-08,1.25637557534617e-08,6.84932066683573e-09,3.68455395402651e-09,1.95583290544245e-09,1.02444343475168e-09],[0.0118853385195915,0.0120448708855235,0.0120448708855235,0.0118853385195915,0.0115725847427643,0.0111188172045432,0.0105413496303273,0.0098615062108317,0.00910331758321207,0.00829211931245911,0.00745316606041126,0.00661036529722544,0.00578521533725911,0.00499600661672614,0.00425731600807727,0.0035797951687181,0.00297022859143874,0.00243181749148239,0.00196463324342567,0.0015661790339417,0.00123200006351684,0.000956289663371402,0.000732449352861347,0.000553573312332373,0.000412840311123886,0.00030380747523912,0.000220609505600676,0.000158073618173088,0.000111764524214674,7.79754626391706e-05,5.36810934540873e-05,3.64665060069056e-05,2.44442299853079e-05,1.61684346374891e-05,1.05528313217781e-05,6.79640771677201e-06,4.31915966920076e-06,2.70849788610738e-06,1.67597350866529e-06,1.02332881981395e-06,6.16556197785927e-07,3.66555328562108e-07,2.15038297742219e-07,1.24480549481003e-07,7.11044181364296e-08,4.00775425509639e-08,2.22902524854691e-08,1.22331499354993e-08,6.62477322632763e-09,3.54008052701561e-09,1.86665764239765e-09],[0.0153120318789528,0.01562135544144,0.015725845724994,0.01562135544144,0.0153120318789528,0.0148100437644168,0.0141347869238546,0.0133116410146566,0.0123703883861147,0.0113434322115875,0.0102639619139988,0.00916420911334325,0.00807391840647743,0.00701912745860916,0.00602131445404234,0.00509693264516655,0.00425731600807727,0.00350891047508742,0.00285376426252262,0.00229019946562793,0.00181358506166083,0.00141713739325386,0.00109268603436992,0.000831358294278681,0.000624152166773117,0.0004623832976235,0.000338005058827408,0.000243811209840914,0.000173537583521609,0.000121882913966067,8.44698102729856e-05,5.77656434891841e-05,3.89804743207483e-05,2.59557767505598e-05,1.70541605697849e-05,1.10569684959511e-05,7.07377437742062e-06,4.46555807647157e-06,2.7816961354387e-06,1.70983041938967e-06,1.03706457226915e-06,6.20680304184704e-07,3.66555328562109e-07,2.13609477117943e-07,1.22831824744976e-07,6.96964563235772e-08,3.90229320509677e-08,2.15594910877548e-08,1.17534812655946e-08,6.32271986919984e-09,3.3562226870262e-09],[0.019465407515813,0.0199914680159841,0.0202598058665036,0.0202598058665036,0.0199914680159842,0.019465407515813,0.0187021579699893,0.0177308415433538,0.0165873261142962,0.0153120318789528,0.0139475739581584,0.0125364313914091,0.0111188172045432,0.00973089215673191,0.00840342126741195,0.00716092323909167,0.00602131445404234,0.00499600661672614,0.00409038425969478,0.00330456743695959,0.00263435643946864,0.00207225817126612,0.00160850565491194,0.00123200006351684,0.000931125617478451,0.000694408818220669,0.000511012573537409,0.000371071288206739,0.000265884196455078,0.000187991019984523,0.000131156884157148,9.02930834558448e-05,6.13376713915257e-05,4.11158707108865e-05,2.71957541126058e-05,1.77501540658392e-05,1.14317456982374e-05,7.26494598705999e-06,4.5557683335836e-06,2.81903378173787e-06,1.7212673696608e-06,1.03706457226914e-06,6.16556197785927e-07,3.61700362546594e-07,2.09379726076757e-07,1.19599597321226e-07,6.74115347027047e-08,3.74928209993285e-08,2.05764964344219e-08,1.1143051739668e-08,5.95451332860088e-09],[0.0244176341743939,0.0252452726564679,0.0257552609934694,0.0259275363465463,0.0257552609934693,0.0252452726564679,0.0244176341743939,0.0233043238581731,0.0219471856887886,0.0203953224590091,0.0187021579699894,0.0169224123292658,0.015109226494313,0.0133116410146566,0.0115725847427643,0.00992746921795374,0.00840342126741195,0.00701912745860916,0.00578521533725912,0.00470506184118492,0.00377590057312685,0.00299009626738422,0.00233646456376218,0.00180153470706267,0.00137067810335024,0.00102905295351241,0.000762341178008333,0.000557276130092996,0.000401976727699847,0.000286115105217978,0.000200950952790769,0.000139267172929075,9.52394451362984e-05,6.42679371545978e-05,4.27938412261918e-05,2.81175572853399e-05,1.82298591487359e-05,1.16626822801869e-05,7.36246058622543e-06,4.58624158712214e-06,2.81903378173788e-06,1.70983041938967e-06,1.02332881981395e-06,6.04347567088825e-07,3.52182488547485e-07,2.02515442175952e-07,1.14910030033104e-07,6.43379381175164e-08,3.55455915418512e-08,1.93782145673613e-08,1.04244027370281e-08],[0.0302240787610676,0.0314575468145585,0.0323076997228899,0.0327413536542539,0.0327413536542539,0.0323076997228899,0.0314575468145585,0.0302240787610676,0.0286543591475523,0.0268063531341398,0.0247453827651371,0.0225403172464719,0.0202598058665036,0.0179688358669242,0.015725845724994,0.0135805540011078,0.0115725847427643,0.00973089215673191,0.00807391840647743,0.00661036529722545,0.00534042684519057,0.00425731600807728,0.00334892338531823,0.0025994648146857,0.00199100376614959,0.00150476827563298,0.00112221631578344,0.000825834339292521,0.000599678810259652,0.000429688643838679,0.00030380747523912,0.000211959283157745,0.000145920340868057,9.91262406259641e-05,6.64463061797362e-05,4.39503620697145e-05,2.86855696207828e-05,1.84745515953002e-05,1.17406932430016e-05,7.36246058622543e-06,4.55576833358361e-06,2.7816961354387e-06,1.67597350866529e-06,9.9640068875522e-07,5.84534697175438e-07,3.38373215650807e-07,1.93281847743411e-07,1.08942055645579e-07,6.0591188283015e-08,3.32531491744827e-08,1.80080006787885e-08],[0.0369157731616182,0.0386793390707695,0.0399903796592033,0.0407982389164979,0.0410711358215629,0.0407982389164979,0.0399903796592033,0.0386793390707695,0.0369157731616181,0.0347659658934533,0.0323076997228899,0.0296256019035173,0.0268063531341398,0.0239341326230305,0.0210866242289031,0.0183318304301569,0.015725845724994,0.0133116410146566,0.0111188172045432,0.00916420911334324,0.00745316606041125,0.00598130586781599,0.00473653370979244,0.00370113273231714,0.00285376426252262,0.00217125552531869,0.00163009601284075,0.00120760482777305,0.000882766620138714,0.000636760876922538,0.000453227494889757,0.00031832117657571,0.000220609505600676,0.000150866327385719,0.000101805167315859,6.7788610604395e-05,4.45402910172989e-05,2.88774456276261e-05,1.84745515953002e-05,1.16626822801869e-05,7.26494598705999e-06,4.46555807647156e-06,2.70849788610738e-06,1.62102856156239e-06,9.5733125892011e-07,5.57883118078687e-07,3.20799443510579e-07,1.82025989190375e-07,1.01916054020142e-07,5.63068468426864e-08,3.06965255717982e-08],[0.0444918302902208,0.0469291452651648,0.0488443600156232,0.0501643985732279,0.0508377361627926,0.0508377361627925,0.0501643985732279,0.0488443600156232,0.0469291452651648,0.0444918302902208,0.0416224180133429,0.0384223344442544,0.0349985132961325,0.0314575468145585,0.0279003411588186,0.0244176341743939,0.0210866242289031,0.0179688358669242,0.015109226494313,0.0125364313914091,0.0102639619139988,0.00829211931245912,0.00661036529722545,0.00519989751462503,0.00403620778202872,0.00309144591978373,0.00233646456376218,0.00174247337424814,0.00128227893991404,0.00093112561747845,0.000667180659000664,0.000471724059841485,0.000329110708989519,0.000226571566593467,0.000153914029363826,0.000103171659248691,6.82420444418325e-05,4.45402910172989e-05,2.86855696207828e-05,1.82298591487359e-05,1.14317456982374e-05,7.0737743774206e-06,4.31915966920076e-06,2.60229616493837e-06,1.54711854195989e-06,9.07611243774642e-07,5.25394534491901e-07,3.00110120198403e-07,1.69155116200354e-07,9.40803754252168e-08,5.16324047614535e-08],[0.0529124635337845,0.056184387547921,0.058868467062449,0.0608638204359008,0.0620933511678538,0.0625086897660101,0.0620933511678538,0.0608638204359008,0.058868467062449,0.056184387547921,0.0529124635337845,0.0491710769287093,0.0450890271592156,0.0407982389164979,0.0364268297193303,0.032093031414167,0.0279003411588186,0.0239341326230306,0.0202598058665036,0.0169224123292658,0.0139475739581583,0.0113434322115875,0.00910331758321208,0.00720882221319581,0.00563298173082845,0.0043433194963578,0.00330456743695959,0.00248094346351084,0.00183792812225004,0.00134353685843452,0.000969125574798113,0.000689794823171552,0.000484472593087523,0.000335759186328262,0.000229612750318439,0.00015494355081806,0.000103171659248691,6.7788610604395e-05,4.39503620697143e-05,2.81175572853399e-05,1.77501540658392e-05,1.10569684959511e-05,6.79640771677201e-06,4.12222965612978e-06,2.46714315125772e-06,1.4570213720684e-06,8.49076658244412e-07,4.88244420086954e-07,2.77036557671203e-07,1.55112316119009e-07,8.56968562126755e-08],[0.0620933511678538,0.0663740112702154,0.0700100579457678,0.0728672226117885,0.0748364887338619,0.0758409903798247,0.0758409903798247,0.0748364887338619,0.0728672226117885,0.0700100579457677,0.0663740112702154,0.0620933511678538,0.0573193874649716,0.0522116465158969,0.0469291452651648,0.0416224180133429,0.0364268297193303,0.0314575468145585,0.0268063531341398,0.0225403172464719,0.0187021579699893,0.0153120318789528,0.0123703883861147,0.00986150621083171,0.00775733553752107,0.00602131445404234,0.00461189537455569,0.00348559554138405,0.0025994648146857,0.00191293539182902,0.00138907619276083,0.000995316584885769,0.000703729602943137,0.000490975483928793,0.000338005058827408,0.00022961275031844,0.000153914029363826,0.000101805167315859,6.64463061797361e-05,4.27938412261919e-05,2.71957541126059e-05,1.70541605697849e-05,1.05528313217781e-05,6.4434290675698e-06,3.88216968943221e-06,2.30802965107452e-06,1.35399686931992e-06,7.83796542560758e-07,4.47711689324066e-07,2.52349780080067e-07,1.40351427622702e-07],[0.0719021080387363,0.0773730800144535,0.0821575640781251,0.0860824521890354,0.0890002266775096,0.0907981505190803,0.0914054937505614,0.0907981505190803,0.0890002266775096,0.0860824521890354,0.0821575640781251,0.0773730800144535,0.0719021080387363,0.0659329895675022,0.0596586360436505,0.0532663917405213,0.0469291452651648,0.0407982389164979,0.0349985132961325,0.0296256019035174,0.0247453827651371,0.0203953224590091,0.0165873261142962,0.0133116410146566,0.0105413496303274,0.00823702237755498,0.00635116916651322,0.00483221803781216,0.00362784539394948,0.00268757396965402,0.00196463324342568,0.00141713739325387,0.00100867633979527,0.000708436806875121,0.000490975483928793,0.000335759186328263,0.000226571566593468,0.00015086632738572,9.91262406259643e-05,6.42679371545978e-05,4.11158707108866e-05,2.59557767505599e-05,1.61684346374891e-05,9.93828226774761e-06,6.02787290026731e-06,3.60766541001237e-06,2.13057989885193e-06,1.24159171259911e-06,7.139522913704e-07,4.05106288992507e-07,2.26818349492277e-07],[0.082157564078125,0.0890002266775096,0.0951358227159593,0.100347475368743,0.104442733526793,0.107265340584658,0.108705122340791,0.108705122340791,0.107265340584658,0.104442733526793,0.100347475368743,0.0951358227159593,0.0890002266775095,0.082157564078125,0.0748364887338619,0.0672649243087358,0.0596586360436505,0.0522116465158969,0.0450890271592156,0.0384223344442544,0.0323076997228899,0.0268063531341398,0.0219471856887886,0.0177308415433537,0.0141347869238546,0.0111188172045432,0.00863052712129622,0.00661036529722545,0.00499600661672614,0.00372588938100366,0.00274186656526205,0.00199100376614959,0.00142661653791652,0.00100867633979527,0.000703729602943136,0.000484472593087523,0.000329110708989519,0.000220609505600676,0.000145920340868057,9.52394451362982e-05,6.13376713915257e-05,3.89804743207482e-05,2.44442299853079e-05,1.51256835403982e-05,9.23555641318089e-06,5.56442800817777e-06,3.30816678856254e-06,1.94072354001886e-06,1.12343863948302e-06,6.41718333550687e-07,3.61700362546594e-07],[0.0926323948315168,0.101018693445466,0.108705122340791,0.115427071698136,0.120941333792791,0.125040654030181,0.127566642799412,0.128419928209127,0.127566642799412,0.125040654030181,0.120941333792791,0.115427071698136,0.108705122340791,0.101018693445466,0.0926323948315168,0.0838172569658373,0.0748364887338619,0.0659329895675022,0.0573193874649716,0.0491710769287093,0.0416224180133429,0.0347659658934533,0.0286543591475523,0.0233043238581731,0.0187021579699893,0.0148100437644169,0.0115725847427643,0.00892305981775455,0.00678901308930213,0.00509693264516654,0.00377590057312685,0.00276020674169204,0.00199100376614959,0.00141713739325386,0.000995316584885768,0.000689794823171553,0.000471724059841486,0.000318321176575711,0.000211959283157745,0.000139267172929075,9.02930834558448e-05,5.77656434891841e-05,3.64665060069057e-05,2.27158032871253e-05,1.39627657264129e-05,8.46884550745414e-06,5.06858082535648e-06,2.99335309539525e-06,1.74437128507985e-06,1.0030655515382e-06,5.69153104642183e-07],[0.103059406420816,0.113141462542196,0.122564683189792,0.131014182845091,0.138191294411314,0.143830988122888,0.147718078670085,0.14970084210104,0.14970084210104,0.147718078670085,0.143830988122888,0.138191294411314,0.131014182845091,0.122564683189792,0.113141462542196,0.103059406420816,0.0926323948315168,0.082157564078125,0.0719021080387363,0.0620933511678538,0.0529124635337845,0.0444918302902208,0.0369157731616181,0.0302240787610676,0.0244176341743938,0.019465407515813,0.0153120318789528,0.0118853385195915,0.00910331758321208,0.00688013942269971,0.00513102571342465,0.00377590057312684,0.00274186656526205,0.00196463324342567,0.00138907619276083,0.000969125574798113,0.000667180659000664,0.000453227494889757,0.00030380747523912,0.000200950952790768,0.000131156884157148,8.44698102729853e-05,5.36810934540873e-05,3.36628277905811e-05,2.08299987580417e-05,1.27185411556824e-05,7.66292830271023e-06,4.5557683335836e-06,2.67262426986033e-06,1.54711854195989e-06,8.83728134016053e-07],[0.113141462542196,0.125040654030181,0.136360973077182,0.146736566820343,0.155810249364435,0.163253724620188,0.168787228147703,0.172196956335689,0.173348771161368,0.172196956335689,0.168787228147703,0.163253724620188,0.155810249364435,0.146736566820343,0.136360973077182,0.125040654030181,0.113141462542196,0.101018693445466,0.0890002266775096,0.0773730800144534,0.0663740112702154,0.056184387547921,0.0469291452651648,0.0386793390707695,0.0314575468145585,0.0252452726564679,0.0199914680159842,0.01562135544144,0.0120448708855235,0.00916420911334324,0.00688013942269972,0.00509693264516655,0.00372588938100366,0.00268757396965402,0.00191293539182903,0.00134353685843452,0.000931125617478451,0.000636760876922538,0.000429688643838679,0.000286115105217978,0.000187991019984523,0.000121882913966067,7.79754626391706e-05,4.92246343149449e-05,3.06631271382899e-05,1.88477622939188e-05,1.14317456982374e-05,6.84186846901829e-06,4.04060404000416e-06,2.35465494284771e-06,1.35399686931993e-06],[0.122564683189792,0.136360973077182,0.14970084210104,0.162168986268107,0.173348771161368,0.18284502144111,0.190307068323345,0.195450193709596,0.198073647115905,0.198073647115905,0.195450193709596,0.190307068323345,0.18284502144111,0.173348771161368,0.162168986268107,0.14970084210104,0.136360973077182,0.122564683189792,0.108705122340791,0.0951358227159593,0.082157564078125,0.0700100579457678,0.058868467062449,0.0488443600156232,0.0399903796592033,0.03230769972289,0.0257552609934694,0.0202598058665036,0.015725845724994,0.0120448708855235,0.00910331758321209,0.00678901308930213,0.00499600661672615,0.00362784539394947,0.0025994648146857,0.00183792812225004,0.00128227893991404,0.000882766620138716,0.000599678810259651,0.000401976727699847,0.000265884196455079,0.000173537583521609,0.000111764524214674,7.1027055107975e-05,4.4540291017299e-05,2.75607923477169e-05,1.6828280972524e-05,1.01390488871202e-05,6.0278729002673e-06,3.53622884862879e-06,2.04703866608716e-06],[0.131014182845091,0.146736566820343,0.162168986268107,0.176850648630819,0.190307068323345,0.202075000211633,0.211728667220171,0.218905234429676,0.223327413504027,0.224821236805718,0.223327413504027,0.218905234429676,0.211728667220171,0.202075000211633,0.190307068323345,0.176850648630819,0.162168986268107,0.146736566820343,0.131014182845091,0.115427071698136,0.100347475368743,0.0860824521890354,0.0728672226117885,0.0608638204359008,0.0501643985732279,0.0407982389164979,0.0327413536542539,0.0259275363465463,0.0202598058665036,0.01562135544144,0.0118853385195915,0.00892305981775454,0.00661036529722545,0.00483221803781215,0.00348559554138405,0.00248094346351084,0.00174247337424814,0.00120760482777305,0.000825834339292521,0.000557276130092996,0.000371071288206739,0.000243811209840913,0.000158073618173088,0.000101128723518423,6.38409092481723e-05,3.9767932136906e-05,2.44442299853079e-05,1.48261749391561e-05,8.8734250664797e-06,5.24038094778467e-06,3.05382283909547e-06],[0.138191294411314,0.155810249364435,0.173348771161368,0.190307068323345,0.206157186001815,0.220369478048149,0.23244157814751,0.24192769890621,0.248465892683478,0.251800955597156,0.251800955597156,0.248465892683478,0.24192769890621,0.23244157814751,0.220369478048149,0.206157186001815,0.190307068323345,0.173348771161368,0.155810249364435,0.138191294411314,0.120941333792791,0.104442733526793,0.0890002266775096,0.0748364887338619,0.0620933511678538,0.0508377361627926,0.0410711358215629,0.0327413536542539,0.0257552609934693,0.0199914680159841,0.0153120318789528,0.0115725847427643,0.00863052712129622,0.00635116916651322,0.00461189537455569,0.00330456743695959,0.00233646456376218,0.00163009601284075,0.00112221631578344,0.000762341178008333,0.00051101257353741,0.000338005058827408,0.000220609505600676,0.000142080556443981,9.02930834558446e-05,5.66218071108094e-05,3.50366338542196e-05,2.1392937887691e-05,1.28892572828188e-05,7.66292830271023e-06,4.49542791916139e-06],[0.143830988122888,0.163253724620188,0.18284502144111,0.202075000211633,0.220369478048148,0.237137209504023,0.251800955597156,0.263830165421334,0.272772718826595,0.278283093269634,0.28014451172548,0.278283093269634,0.272772718826595,0.263830165421334,0.251800955597156,0.237137209504023,0.220369478048148,0.202075000211633,0.18284502144111,0.163253724620188,0.143830988122888,0.125040654030181,0.107265340584658,0.0907981505190802,0.0758409903798246,0.0625086897660101,0.0508377361627926,0.0407982389164979,0.0323076997228899,0.0252452726564679,0.019465407515813,0.0148100437644169,0.0111188172045432,0.00823702237755496,0.00602131445404234,0.0043433194963578,0.00309144591978373,0.00217125552531869,0.00150476827563298,0.00102905295351241,0.000694408818220669,0.0004623832976235,0.00030380747523912,0.00019697185732524,0.000126014149170312,7.95506714736882e-05,4.95538948593765e-05,3.04593861818184e-05,1.84745515953002e-05,1.10569684959511e-05,6.52991675844793e-06],[0.147718078670085,0.168787228147703,0.190307068323345,0.211728667220171,0.23244157814751,0.251800955597156,0.269159888302326,0.283904784660471,0.295491158719388,0.303476926632329,0.307550381673732,0.307550381673732,0.303476926632329,0.295491158719388,0.283904784660471,0.269159888302326,0.251800955597156,0.23244157814751,0.211728667220171,0.190307068323345,0.168787228147703,0.147718078670085,0.127566642799412,0.10870512234079,0.0914054937505614,0.0758409903798247,0.0620933511678538,0.0501643985732279,0.0399903796592033,0.0314575468145585,0.0244176341743939,0.0187021579699893,0.0141347869238546,0.0105413496303274,0.00775733553752107,0.00563298173082845,0.00403620778202872,0.00285376426252263,0.00199100376614959,0.00137067810335024,0.000931125617478453,0.000624152166773117,0.000412840311123887,0.000269453058617018,0.00017353758352161,0.000110284221175755,6.91580313771556e-05,4.2793841226192e-05,2.6129393341175e-05,1.57429743958709e-05,9.35952176451393e-06],[0.14970084210104,0.172196956335689,0.195450193709596,0.218905234429676,0.24192769890621,0.263830165421334,0.283904784660471,0.301460476091579,0.31586205495772,0.326568235165143,0.333165351125652,0.335393873621027,0.333165351125652,0.326568235165143,0.31586205495772,0.301460476091579,0.283904784660471,0.263830165421334,0.24192769890621,0.218905234429676,0.195450193709596,0.172196956335689,0.14970084210104,0.128419928209127,0.10870512234079,0.0907981505190803,0.0748364887338619,0.0608638204359008,0.0488443600156232,0.0386793390707695,0.0302240787610676,0.0233043238581731,0.0177308415433538,0.0133116410146566,0.0098615062108317,0.00720882221319582,0.00519989751462503,0.00370113273231715,0.0025994648146857,0.00180153470706267,0.00123200006351684,0.000831358294278681,0.000553573312332375,0.000363723584402474,0.000235818127636128,0.00015086632738572,9.52394451362982e-05,5.93267833359583e-05,3.64665060069055e-05,2.21180539457832e-05,1.32375946668436e-05],[0.149700842101039,0.173348771161368,0.198073647115905,0.223327413504027,0.248465892683478,0.272772718826595,0.295491158719388,0.315862054957721,0.333165351125652,0.346762087039169,0.356133472498222,0.360913716271805,0.360913716271805,0.356133472498222,0.346762087039169,0.333165351125652,0.315862054957721,0.295491158719388,0.272772718826595,0.248465892683478,0.223327413504027,0.198073647115905,0.173348771161368,0.14970084210104,0.127566642799412,0.107265340584658,0.0890002266775096,0.0728672226117885,0.058868467062449,0.0469291452651648,0.0369157731616182,0.0286543591475523,0.0219471856887886,0.0165873261142962,0.0123703883861147,0.00910331758321209,0.00661036529722545,0.00473653370979245,0.00334892338531823,0.00233646456376218,0.00160850565491194,0.00109268603436992,0.000732449352861348,0.000484472593087523,0.000316206093509063,0.000203648240788258,0.000129419732448614,8.11577016374849e-05,5.02190378904336e-05,3.066312713829e-05,1.84745515953003e-05],[0.147718078670085,0.172196956335689,0.198073647115905,0.224821236805718,0.251800955597156,0.278283093269634,0.303476926632329,0.326568235165143,0.346762087039169,0.363327845871067,0.37564288444951,0.383231374086906,0.385794785123499,0.383231374086906,0.37564288444951,0.363327845871067,0.346762087039169,0.326568235165143,0.303476926632329,0.278283093269634,0.251800955597156,0.224821236805718,0.198073647115905,0.172196956335689,0.147718078670085,0.125040654030181,0.104442733526793,0.0860824521890354,0.0700100579457677,0.056184387547921,0.0444918302902209,0.0347659658934533,0.0268063531341398,0.0203953224590091,0.0153120318789528,0.0113434322115875,0.00829211931245912,0.00598130586781599,0.00425731600807728,0.00299009626738422,0.00207225817126612,0.00141713739325386,0.000956289663371403,0.00063676087692254,0.000418381709164601,0.000271255413515409,0.00017353758352161,0.000109551438357984,6.82420444418324e-05,4.19464663956132e-05,2.54418179355451e-05],[0.143830988122888,0.168787228147703,0.195450193709596,0.223327413504027,0.251800955597156,0.28014451172548,0.307550381673732,0.333165351125652,0.356133472498222,0.37564288444951,0.390973161383756,0.401539368983775,0.406929078788273,0.406929078788273,0.401539368983775,0.390973161383756,0.37564288444951,0.356133472498221,0.333165351125652,0.307550381673732,0.28014451172548,0.251800955597156,0.223327413504027,0.195450193709596,0.168787228147703,0.143830988122888,0.120941333792791,0.100347475368743,0.082157564078125,0.0663740112702153,0.0529124635337845,0.0416224180133429,0.0323076997228899,0.0247453827651371,0.0187021579699893,0.0139475739581583,0.0102639619139988,0.00745316606041125,0.00534042684519056,0.00377590057312684,0.00263435643946864,0.00181358506166083,0.00123200006351684,0.000825834339292521,0.000546241323382677,0.000356521374881682,0.000229612750318439,0.000145920340868057,9.15050530776825e-05,5.66218071108094e-05,3.45725793080001e-05],[0.138191294411314,0.163253724620188,0.190307068323345,0.218905234429676,0.248465892683478,0.278283093269634,0.307550381673732,0.335393873621027,0.360913716271805,0.383231374086905,0.401539368983775,0.415149591475649,0.423536169535017,0.426369176863734,0.423536169535017,0.415149591475649,0.401539368983775,0.383231374086905,0.360913716271805,0.335393873621026,0.307550381673732,0.278283093269634,0.248465892683478,0.218905234429676,0.190307068323345,0.163253724620188,0.138191294411314,0.115427071698136,0.0951358227159593,0.0773730800144534,0.0620933511678538,0.0491710769287093,0.0384223344442544,0.0296256019035173,0.0225403172464719,0.0169224123292658,0.0125364313914091,0.00916420911334324,0.00661036529722545,0.00470506184118491,0.00330456743695959,0.00229019946562793,0.0015661790339417,0.00105686352521443,0.000703729602943135,0.0004623832976235,0.000299783594387813,0.000191788690501207,0.000121073063706601,7.5419122907139e-05,4.63580147764693e-05],[0.131014182845091,0.155810249364435,0.18284502144111,0.211728667220171,0.24192769890621,0.272772718826595,0.303476926632329,0.333165351125652,0.360913716271805,0.385794785123499,0.406929078788273,0.423536169535017,0.434982405582487,0.44082100851222,0.44082100851222,0.434982405582487,0.423536169535017,0.406929078788273,0.385794785123499,0.360913716271805,0.333165351125652,0.303476926632329,0.272772718826595,0.24192769890621,0.211728667220171,0.18284502144111,0.155810249364435,0.131014182845091,0.10870512234079,0.0890002266775095,0.0719021080387363,0.0573193874649716,0.0450890271592156,0.0349985132961325,0.0268063531341398,0.0202598058665036,0.015109226494313,0.0111188172045432,0.00807391840647743,0.00578521533725912,0.00409038425969478,0.00285376426252262,0.00196463324342567,0.00133460973618253,0.000894615659797481,0.000591736161450109,0.000386214994759022,0.000248736522993245,0.000158073618173088,9.91262406259641e-05,6.13376713915258e-05],[0.122564683189792,0.146736566820343,0.173348771161368,0.202075000211633,0.23244157814751,0.263830165421334,0.295491158719388,0.326568235165143,0.356133472498222,0.383231374086905,0.406929078788273,0.426369176863734,0.44082100851222,0.449726183596113,0.452734374314375,0.449726183596113,0.44082100851222,0.426369176863734,0.406929078788273,0.383231374086905,0.356133472498222,0.326568235165143,0.295491158719388,0.263830165421334,0.23244157814751,0.202075000211633,0.173348771161368,0.146736566820343,0.122564683189792,0.101018693445466,0.082157564078125,0.0659329895675022,0.0522116465158969,0.0407982389164979,0.0314575468145585,0.0239341326230306,0.0179688358669242,0.0133116410146566,0.0097308921567319,0.00701912745860916,0.00499600661672614,0.00350891047508742,0.0024318174914824,0.00166302613667241,0.00112221631578344,0.000747245811290878,0.000490975483928793,0.000318321176575711,0.000203648240788258,0.000128559803845884,8.00827810111967e-05],[0.113141462542196,0.136360973077182,0.162168986268107,0.190307068323345,0.220369478048148,0.251800955597156,0.283904784660471,0.31586205495772,0.346762087039168,0.375642884449509,0.401539368983775,0.423536169535017,0.44082100851222,0.452734374314375,0.458811255149873,0.458811255149873,0.452734374314375,0.44082100851222,0.423536169535017,0.401539368983775,0.37564288444951,0.346762087039169,0.31586205495772,0.283904784660471,0.251800955597156,0.220369478048149,0.190307068323345,0.162168986268107,0.136360973077182,0.113141462542196,0.0926323948315168,0.0748364887338619,0.0596586360436505,0.0469291452651648,0.0364268297193302,0.0279003411588186,0.0210866242289031,0.015725845724994,0.0115725847427643,0.00840342126741195,0.00602131445404235,0.00425731600807727,0.00297022859143874,0.00204481144709398,0.00138907619276083,0.000931125617478452,0.00061588537231652,0.000401976727699847,0.000258887653066522,0.000164524724910124,0.000103171659248691],[0.103059406420816,0.125040654030181,0.14970084210104,0.176850648630819,0.206157186001815,0.237137209504023,0.269159888302326,0.301460476091579,0.333165351125652,0.363327845871067,0.390973161383756,0.415149591475649,0.434982405582487,0.449726183596113,0.458811255149873,0.461880215351701,0.458811255149873,0.449726183596113,0.434982405582487,0.415149591475649,0.390973161383756,0.363327845871067,0.333165351125652,0.301460476091579,0.269159888302325,0.237137209504023,0.206157186001815,0.176850648630819,0.149700842101039,0.125040654030181,0.103059406420816,0.0838172569658372,0.0672649243087358,0.0532663917405212,0.0416224180133429,0.032093031414167,0.0244176341743939,0.0183318304301569,0.0135805540011078,0.00992746921795373,0.00716092323909167,0.00509693264516654,0.0035797951687181,0.00248094346351084,0.0016966214931327,0.00114488658916215,0.000762341178008332,0.000500893846624439,0.000324751690901433,0.000207762208146272,0.000131156884157148],[0.0926323948315168,0.113141462542196,0.136360973077182,0.162168986268107,0.190307068323345,0.220369478048148,0.251800955597156,0.283904784660471,0.31586205495772,0.346762087039168,0.37564288444951,0.401539368983775,0.423536169535017,0.44082100851222,0.452734374314375,0.458811255149873,0.458811255149873,0.452734374314375,0.44082100851222,0.423536169535017,0.401539368983775,0.37564288444951,0.346762087039168,0.31586205495772,0.283904784660471,0.251800955597156,0.220369478048148,0.190307068323345,0.162168986268107,0.136360973077182,0.113141462542196,0.0926323948315168,0.0748364887338619,0.0596586360436505,0.0469291452651648,0.0364268297193303,0.0279003411588186,0.0210866242289031,0.015725845724994,0.0115725847427642,0.00840342126741196,0.00602131445404233,0.00425731600807727,0.00297022859143875,0.00204481144709398,0.00138907619276083,0.000931125617478449,0.00061588537231652,0.000401976727699846,0.000258887653066522,0.000164524724910124],[0.082157564078125,0.101018693445466,0.122564683189792,0.146736566820343,0.173348771161368,0.202075000211633,0.23244157814751,0.263830165421334,0.295491158719388,0.326568235165143,0.356133472498221,0.383231374086905,0.406929078788273,0.426369176863734,0.44082100851222,0.449726183596113,0.452734374314375,0.449726183596113,0.44082100851222,0.426369176863734,0.406929078788273,0.383231374086905,0.356133472498221,0.326568235165143,0.295491158719388,0.263830165421334,0.23244157814751,0.202075000211633,0.173348771161368,0.146736566820343,0.122564683189792,0.101018693445466,0.082157564078125,0.0659329895675022,0.0522116465158968,0.0407982389164979,0.0314575468145585,0.0239341326230305,0.0179688358669242,0.0133116410146566,0.00973089215673191,0.00701912745860915,0.00499600661672614,0.00350891047508742,0.00243181749148239,0.00166302613667241,0.00112221631578344,0.000747245811290877,0.000490975483928792,0.000318321176575711,0.000203648240788259],[0.0719021080387363,0.0890002266775096,0.108705122340791,0.131014182845091,0.155810249364435,0.18284502144111,0.211728667220171,0.24192769890621,0.272772718826595,0.303476926632328,0.333165351125652,0.360913716271805,0.385794785123499,0.406929078788272,0.423536169535017,0.434982405582487,0.44082100851222,0.44082100851222,0.434982405582487,0.423536169535017,0.406929078788273,0.385794785123499,0.360913716271805,0.333165351125652,0.303476926632328,0.272772718826595,0.24192769890621,0.211728667220171,0.18284502144111,0.155810249364435,0.131014182845091,0.108705122340791,0.0890002266775096,0.0719021080387363,0.0573193874649715,0.0450890271592156,0.0349985132961325,0.0268063531341398,0.0202598058665036,0.015109226494313,0.0111188172045432,0.00807391840647743,0.00578521533725912,0.00409038425969478,0.00285376426252262,0.00196463324342568,0.00133460973618252,0.000894615659797481,0.000591736161450109,0.000386214994759022,0.000248736522993245],[0.0620933511678538,0.0773730800144534,0.0951358227159593,0.115427071698136,0.138191294411314,0.163253724620188,0.190307068323345,0.218905234429676,0.248465892683478,0.278283093269634,0.307550381673732,0.335393873621026,0.360913716271805,0.383231374086905,0.401539368983775,0.415149591475649,0.423536169535017,0.426369176863734,0.423536169535017,0.415149591475649,0.401539368983775,0.383231374086905,0.360913716271805,0.335393873621026,0.307550381673732,0.278283093269634,0.248465892683478,0.218905234429676,0.190307068323345,0.163253724620188,0.138191294411314,0.115427071698136,0.0951358227159593,0.0773730800144534,0.0620933511678538,0.0491710769287093,0.0384223344442544,0.0296256019035173,0.0225403172464719,0.0169224123292658,0.0125364313914091,0.00916420911334324,0.00661036529722545,0.00470506184118492,0.00330456743695959,0.00229019946562794,0.0015661790339417,0.00105686352521443,0.000703729602943135,0.0004623832976235,0.000299783594387814],[0.0529124635337845,0.0663740112702154,0.082157564078125,0.100347475368743,0.120941333792791,0.143830988122888,0.168787228147703,0.195450193709596,0.223327413504027,0.251800955597156,0.28014451172548,0.307550381673732,0.333165351125652,0.356133472498222,0.37564288444951,0.390973161383756,0.401539368983775,0.406929078788273,0.406929078788273,0.401539368983775,0.390973161383756,0.37564288444951,0.356133472498221,0.333165351125652,0.307550381673732,0.28014451172548,0.251800955597156,0.223327413504027,0.195450193709596,0.168787228147703,0.143830988122888,0.120941333792791,0.100347475368743,0.082157564078125,0.0663740112702154,0.0529124635337845,0.0416224180133429,0.0323076997228899,0.0247453827651371,0.0187021579699893,0.0139475739581584,0.0102639619139988,0.00745316606041126,0.00534042684519057,0.00377590057312684,0.00263435643946864,0.00181358506166083,0.00123200006351684,0.000825834339292519,0.000546241323382677,0.000356521374881683],[0.0444918302902208,0.056184387547921,0.0700100579457678,0.0860824521890354,0.104442733526793,0.125040654030181,0.147718078670085,0.172196956335689,0.198073647115905,0.224821236805718,0.251800955597156,0.278283093269634,0.303476926632329,0.326568235165143,0.346762087039168,0.363327845871067,0.37564288444951,0.383231374086905,0.385794785123499,0.383231374086905,0.37564288444951,0.363327845871067,0.346762087039168,0.326568235165143,0.303476926632328,0.278283093269634,0.251800955597156,0.224821236805718,0.198073647115905,0.172196956335689,0.147718078670085,0.125040654030181,0.104442733526793,0.0860824521890353,0.0700100579457677,0.056184387547921,0.0444918302902208,0.0347659658934533,0.0268063531341398,0.020395322459009,0.0153120318789528,0.0113434322115875,0.00829211931245911,0.00598130586781599,0.00425731600807727,0.00299009626738422,0.00207225817126611,0.00141713739325386,0.000956289663371401,0.000636760876922539,0.000418381709164602],[0.0369157731616181,0.0469291452651648,0.058868467062449,0.0728672226117884,0.0890002266775095,0.107265340584658,0.127566642799412,0.14970084210104,0.173348771161368,0.198073647115905,0.223327413504027,0.248465892683478,0.272772718826595,0.295491158719388,0.31586205495772,0.333165351125652,0.346762087039168,0.356133472498221,0.360913716271805,0.360913716271805,0.356133472498221,0.346762087039168,0.333165351125652,0.31586205495772,0.295491158719388,0.272772718826595,0.248465892683478,0.223327413504027,0.198073647115905,0.173348771161368,0.14970084210104,0.127566642799412,0.107265340584658,0.0890002266775095,0.0728672226117884,0.058868467062449,0.0469291452651648,0.0369157731616181,0.0286543591475523,0.0219471856887886,0.0165873261142962,0.0123703883861146,0.00910331758321207,0.00661036529722545,0.00473653370979244,0.00334892338531822,0.00233646456376218,0.00160850565491194,0.00109268603436992,0.000732449352861347,0.000484472593087523],[0.0302240787610676,0.0386793390707695,0.0488443600156232,0.0608638204359008,0.0748364887338618,0.0907981505190802,0.10870512234079,0.128419928209127,0.149700842101039,0.172196956335689,0.195450193709596,0.218905234429676,0.24192769890621,0.263830165421334,0.283904784660471,0.301460476091579,0.31586205495772,0.326568235165143,0.333165351125652,0.335393873621026,0.333165351125652,0.326568235165143,0.31586205495772,0.301460476091579,0.283904784660471,0.263830165421334,0.24192769890621,0.218905234429676,0.195450193709596,0.172196956335689,0.14970084210104,0.128419928209127,0.10870512234079,0.0907981505190802,0.0748364887338618,0.0608638204359008,0.0488443600156232,0.0386793390707695,0.0302240787610676,0.023304323858173,0.0177308415433537,0.0133116410146566,0.00986150621083171,0.00720882221319582,0.00519989751462502,0.00370113273231715,0.0025994648146857,0.00180153470706267,0.00123200006351684,0.000831358294278681,0.000553573312332375],[0.0244176341743938,0.0314575468145585,0.0399903796592033,0.0501643985732279,0.0620933511678538,0.0758409903798246,0.0914054937505614,0.10870512234079,0.127566642799412,0.147718078670085,0.168787228147703,0.190307068323345,0.211728667220171,0.23244157814751,0.251800955597155,0.269159888302325,0.283904784660471,0.295491158719388,0.303476926632328,0.307550381673732,0.307550381673732,0.303476926632328,0.295491158719388,0.283904784660471,0.269159888302325,0.251800955597156,0.23244157814751,0.211728667220171,0.190307068323345,0.168787228147703,0.147718078670085,0.127566642799412,0.10870512234079,0.0914054937505613,0.0758409903798246,0.0620933511678538,0.0501643985732279,0.0399903796592033,0.0314575468145585,0.0244176341743938,0.0187021579699894,0.0141347869238546,0.0105413496303273,0.00775733553752108,0.00563298173082844,0.00403620778202872,0.00285376426252262,0.00199100376614959,0.00137067810335024,0.000931125617478451,0.00062415216677312],[0.019465407515813,0.0252452726564679,0.03230769972289,0.0407982389164979,0.0508377361627926,0.0625086897660101,0.0758409903798247,0.0907981505190803,0.107265340584658,0.125040654030181,0.143830988122888,0.163253724620188,0.18284502144111,0.202075000211633,0.220369478048149,0.237137209504023,0.251800955597156,0.263830165421334,0.272772718826595,0.278283093269634,0.28014451172548,0.278283093269634,0.272772718826595,0.263830165421334,0.251800955597156,0.237137209504023,0.220369478048148,0.202075000211633,0.18284502144111,0.163253724620188,0.143830988122888,0.125040654030181,0.107265340584658,0.0907981505190802,0.0758409903798246,0.0625086897660101,0.0508377361627925,0.0407982389164979,0.0323076997228899,0.0252452726564678,0.019465407515813,0.0148100437644168,0.0111188172045432,0.00823702237755497,0.00602131445404233,0.00434331949635779,0.00309144591978372,0.00217125552531869,0.00150476827563297,0.00102905295351241,0.000694408818220669],[0.0153120318789528,0.0199914680159841,0.0257552609934693,0.0327413536542539,0.0410711358215629,0.0508377361627925,0.0620933511678538,0.0748364887338618,0.0890002266775095,0.104442733526793,0.120941333792791,0.138191294411314,0.155810249364435,0.173348771161368,0.190307068323345,0.206157186001815,0.220369478048148,0.23244157814751,0.24192769890621,0.248465892683478,0.251800955597155,0.251800955597155,0.248465892683478,0.24192769890621,0.23244157814751,0.220369478048148,0.206157186001815,0.190307068323345,0.173348771161368,0.155810249364435,0.138191294411314,0.120941333792791,0.104442733526793,0.0890002266775095,0.0748364887338618,0.0620933511678538,0.0508377361627925,0.0410711358215629,0.0327413536542539,0.0257552609934693,0.0199914680159842,0.0153120318789527,0.0115725847427643,0.00863052712129623,0.00635116916651321,0.00461189537455569,0.00330456743695958,0.00233646456376218,0.00163009601284075,0.00112221631578344,0.000762341178008334],[0.0118853385195915,0.01562135544144,0.0202598058665036,0.0259275363465463,0.0327413536542539,0.0407982389164979,0.0501643985732279,0.0608638204359008,0.0728672226117885,0.0860824521890354,0.100347475368743,0.115427071698136,0.131014182845091,0.146736566820343,0.162168986268107,0.176850648630819,0.190307068323345,0.202075000211633,0.211728667220171,0.218905234429676,0.223327413504027,0.224821236805718,0.223327413504027,0.218905234429676,0.211728667220171,0.202075000211633,0.190307068323345,0.176850648630819,0.162168986268107,0.146736566820343,0.131014182845091,0.115427071698136,0.100347475368743,0.0860824521890353,0.0728672226117884,0.0608638204359008,0.0501643985732279,0.0407982389164979,0.0327413536542539,0.0259275363465463,0.0202598058665036,0.01562135544144,0.0118853385195915,0.00892305981775455,0.00661036529722544,0.00483221803781215,0.00348559554138405,0.00248094346351084,0.00174247337424814,0.00120760482777305,0.000825834339292521],[0.00910331758321209,0.0120448708855235,0.015725845724994,0.0202598058665036,0.0257552609934694,0.03230769972289,0.0399903796592033,0.0488443600156233,0.058868467062449,0.0700100579457678,0.082157564078125,0.0951358227159593,0.108705122340791,0.122564683189792,0.136360973077182,0.14970084210104,0.162168986268107,0.173348771161368,0.18284502144111,0.190307068323345,0.195450193709596,0.198073647115905,0.198073647115905,0.195450193709596,0.190307068323345,0.18284502144111,0.173348771161368,0.162168986268107,0.14970084210104,0.136360973077182,0.122564683189792,0.108705122340791,0.0951358227159593,0.082157564078125,0.0700100579457677,0.058868467062449,0.0488443600156232,0.0399903796592033,0.0323076997228899,0.0257552609934693,0.0202598058665036,0.015725845724994,0.0120448708855235,0.00910331758321209,0.00678901308930212,0.00499600661672614,0.00362784539394947,0.0025994648146857,0.00183792812225004,0.00128227893991404,0.000882766620138716],[0.00688013942269971,0.00916420911334324,0.0120448708855235,0.01562135544144,0.0199914680159841,0.0252452726564679,0.0314575468145585,0.0386793390707695,0.0469291452651648,0.056184387547921,0.0663740112702153,0.0773730800144534,0.0890002266775095,0.101018693445466,0.113141462542196,0.125040654030181,0.136360973077182,0.146736566820343,0.155810249364435,0.163253724620188,0.168787228147703,0.172196956335689,0.173348771161368,0.172196956335689,0.168787228147703,0.163253724620188,0.155810249364435,0.146736566820343,0.136360973077182,0.125040654030181,0.113141462542196,0.101018693445466,0.0890002266775095,0.0773730800144534,0.0663740112702153,0.056184387547921,0.0469291452651648,0.0386793390707695,0.0314575468145585,0.0252452726564679,0.0199914680159842,0.01562135544144,0.0120448708855235,0.00916420911334325,0.00688013942269972,0.00509693264516655,0.00372588938100366,0.00268757396965402,0.00191293539182902,0.00134353685843452,0.000931125617478453],[0.00513102571342465,0.00688013942269972,0.00910331758321209,0.0118853385195915,0.0153120318789528,0.019465407515813,0.0244176341743939,0.0302240787610676,0.0369157731616182,0.0444918302902208,0.0529124635337845,0.0620933511678538,0.0719021080387363,0.082157564078125,0.0926323948315168,0.103059406420816,0.113141462542196,0.122564683189792,0.131014182845091,0.138191294411314,0.143830988122888,0.147718078670085,0.14970084210104,0.14970084210104,0.147718078670085,0.143830988122888,0.138191294411314,0.131014182845091,0.122564683189792,0.113141462542196,0.103059406420816,0.0926323948315168,0.082157564078125,0.0719021080387363,0.0620933511678538,0.0529124635337845,0.0444918302902208,0.0369157731616182,0.0302240787610676,0.0244176341743938,0.019465407515813,0.0153120318789527,0.0118853385195915,0.00910331758321209,0.00688013942269971,0.00513102571342465,0.00377590057312684,0.00274186656526205,0.00196463324342567,0.00138907619276083,0.000969125574798113],[0.00377590057312684,0.00509693264516654,0.00678901308930212,0.00892305981775453,0.0115725847427642,0.0148100437644168,0.0187021579699893,0.023304323858173,0.0286543591475523,0.0347659658934532,0.0416224180133428,0.0491710769287092,0.0573193874649715,0.0659329895675021,0.0748364887338618,0.0838172569658371,0.0926323948315167,0.101018693445466,0.10870512234079,0.115427071698136,0.120941333792791,0.125040654030181,0.127566642799412,0.128419928209127,0.127566642799412,0.125040654030181,0.120941333792791,0.115427071698136,0.10870512234079,0.101018693445466,0.0926323948315167,0.0838172569658372,0.0748364887338618,0.0659329895675021,0.0573193874649715,0.0491710769287093,0.0416224180133429,0.0347659658934532,0.0286543591475523,0.023304323858173,0.0187021579699893,0.0148100437644168,0.0115725847427643,0.00892305981775455,0.00678901308930212,0.00509693264516655,0.00377590057312684,0.00276020674169204,0.00199100376614958,0.00141713739325386,0.000995316584885769],[0.00274186656526205,0.00372588938100366,0.00499600661672615,0.00661036529722545,0.00863052712129622,0.0111188172045432,0.0141347869238546,0.0177308415433538,0.0219471856887886,0.0268063531341398,0.0323076997228899,0.0384223344442544,0.0450890271592156,0.0522116465158969,0.0596586360436505,0.0672649243087358,0.0748364887338619,0.082157564078125,0.0890002266775096,0.0951358227159593,0.100347475368743,0.104442733526793,0.107265340584658,0.10870512234079,0.10870512234079,0.107265340584658,0.104442733526793,0.100347475368743,0.0951358227159592,0.0890002266775095,0.082157564078125,0.0748364887338619,0.0672649243087358,0.0596586360436505,0.0522116465158968,0.0450890271592156,0.0384223344442544,0.0323076997228899,0.0268063531341398,0.0219471856887886,0.0177308415433538,0.0141347869238546,0.0111188172045432,0.00863052712129623,0.00661036529722544,0.00499600661672614,0.00372588938100366,0.00274186656526205,0.00199100376614959,0.00142661653791651,0.00100867633979527],[0.00196463324342567,0.00268757396965401,0.00362784539394947,0.00483221803781215,0.00635116916651321,0.00823702237755496,0.0105413496303273,0.0133116410146566,0.0165873261142962,0.020395322459009,0.0247453827651371,0.0296256019035173,0.0349985132961325,0.0407982389164978,0.0469291452651647,0.0532663917405212,0.0596586360436504,0.0659329895675021,0.0719021080387362,0.0773730800144533,0.0821575640781249,0.0860824521890353,0.0890002266775095,0.0907981505190802,0.0914054937505612,0.0907981505190802,0.0890002266775095,0.0860824521890353,0.0821575640781249,0.0773730800144533,0.0719021080387362,0.0659329895675021,0.0596586360436505,0.0532663917405212,0.0469291452651647,0.0407982389164979,0.0349985132961325,0.0296256019035173,0.0247453827651371,0.020395322459009,0.0165873261142962,0.0133116410146566,0.0105413496303274,0.00823702237755497,0.00635116916651321,0.00483221803781215,0.00362784539394947,0.00268757396965401,0.00196463324342567,0.00141713739325386,0.00100867633979527],[0.00138907619276083,0.00191293539182903,0.0025994648146857,0.00348559554138405,0.00461189537455569,0.00602131445404234,0.00775733553752107,0.0098615062108317,0.0123703883861146,0.0153120318789527,0.0187021579699893,0.0225403172464719,0.0268063531341398,0.0314575468145585,0.0364268297193302,0.0416224180133429,0.0469291452651648,0.0522116465158968,0.0573193874649715,0.0620933511678538,0.0663740112702154,0.0700100579457677,0.0728672226117884,0.0748364887338618,0.0758409903798246,0.0758409903798246,0.0748364887338618,0.0728672226117884,0.0700100579457677,0.0663740112702153,0.0620933511678538,0.0573193874649715,0.0522116465158968,0.0469291452651648,0.0416224180133429,0.0364268297193303,0.0314575468145585,0.0268063531341398,0.0225403172464719,0.0187021579699893,0.0153120318789528,0.0123703883861146,0.0098615062108317,0.00775733553752108,0.00602131445404233,0.00461189537455569,0.00348559554138405,0.0025994648146857,0.00191293539182902,0.00138907619276083,0.000995316584885769],[0.000969125574798113,0.00134353685843452,0.00183792812225004,0.00248094346351084,0.00330456743695959,0.0043433194963578,0.00563298173082845,0.00720882221319582,0.00910331758321208,0.0113434322115875,0.0139475739581583,0.0169224123292658,0.0202598058665036,0.0239341326230306,0.0279003411588186,0.032093031414167,0.0364268297193303,0.0407982389164979,0.0450890271592156,0.0491710769287093,0.0529124635337845,0.056184387547921,0.058868467062449,0.0608638204359008,0.0620933511678538,0.0625086897660101,0.0620933511678538,0.0608638204359008,0.058868467062449,0.056184387547921,0.0529124635337845,0.0491710769287093,0.0450890271592156,0.0407982389164979,0.0364268297193303,0.032093031414167,0.0279003411588186,0.0239341326230306,0.0202598058665036,0.0169224123292658,0.0139475739581584,0.0113434322115875,0.00910331758321208,0.00720882221319582,0.00563298173082844,0.0043433194963578,0.00330456743695958,0.00248094346351084,0.00183792812225004,0.00134353685843452,0.000969125574798113]],"type":"surface","colorscale":[["0","rgb(107,184,214)"],["1","rgb(0,90,124)"]],"inherit":true},"46f42d910a1.1":{"x":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"y":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[2.1855320581026e-06,3.70071472688659e-06,6.18334448419467e-06,1.0194611575158e-05,1.65854527439267e-05,2.66252310987588e-05,4.21763359280098e-05,6.59255431045717e-05,0.000101682915906592,0.000154757488873036,0.000232415326227089,0.00034441912547766,0.000503638779492395,0.000726708889631284,0.00103469223475733,0.00145368836219717,0.00201530528765178,0.00275689218337506,0.00372141519526626,0.00495685115543938,0.0065149792445367,0.00844947259624268,0.0108132333197937,0.0136549763748181,0.0170151483173813,0.0209213609372375,0.0253836184938987,0.0303897087997554,0.0359011989345458,0.041850511738228,0.0481395471212505,0.0546402445789954,0.0611973583747469,0.0676335407388234,0.0737566153930639,0.0793686953006767,0.0842765813218815,0.0883027005936735,0.0912957306538512,0.0931400267517807,0.0937630346490151,0.0931400267517807,0.0912957306538512,0.0883027005936735,0.0842765813218815,0.0793686953006767,0.0737566153930639,0.0676335407388234,0.0611973583747468,0.0546402445789954,0.0481395471212505],[3.46204447661178e-06,5.82325453414832e-06,9.6651436013547e-06,1.58292469826671e-05,2.55812408546774e-05,4.07936311689088e-05,6.41907618392879e-05,9.96694592696043e-05,0.000152707750973788,0.00023087104404574,0.00034441912547766,0.000507007588241112,0.000736463225893191,0.00105559440441471,0.00149297487732865,0.00208361428914125,0.00286940308774354,0.00389919722202856,0.00522839331207609,0.00691784306183354,0.00903197168106351,0.0116360033062816,0.0147922593162476,0.018555582579172,0.0229680478184292,0.028053236954984,0.0338104758697079,0.0402095297012098,0.0471863202218377,0.0546402445789954,0.0624336270200144,0.0703937178977472,0.0783174697738453,0.0859790811974574,0.0931400267517807,0.0995610169053231,0.105015086918652,0.109300833917683,0.112254733100793,0.113761485569737,0.113761485569737,0.112254733100793,0.109300833917683,0.105015086918652,0.0995610169053231,0.0931400267517807,0.0859790811974573,0.0783174697738453,0.0703937178977472,0.0624336270200143,0.0546402445789954],[5.41149811501855e-06,9.04180935040098e-06,1.49074234016214e-05,2.42526519562336e-05,3.89336651258399e-05,6.16738060663299e-05,9.64019057318968e-05,0.000148689360938946,0.000226299491078579,0.000339857349890201,0.000503638779492395,0.000736463225893191,0.00106265521031268,0.0015130145096929,0.0021257060898808,0.00294694986513851,0.00403136095448103,0.00544176809092422,0.00724832705671824,0.00952675374976984,0.0123555335663325,0.0158120244454911,0.019967461521985,0.0248809891714444,0.0305929836885136,0.0371180741477057,0.044438402855276,0.0524977699441988,0.0611973583747469,0.0703937178977473,0.0798995876107819,0.0894879540654759,0.0988994843512533,0.107853162058105,0.11605962002168,0.123236346117188,0.129123678283553,0.133500340016264,0.13619722577862,0.137108240625842,0.13619722577862,0.133500340016264,0.129123678283553,0.123236346117188,0.11605962002168,0.107853162058104,0.0988994843512533,0.0894879540654758,0.0798995876107819,0.0703937178977472,0.0611973583747469],[8.34664201226666e-06,1.38533346197713e-05,2.26885253105973e-05,3.66663449779618e-05,5.84707114811225e-05,9.20065070872886e-05,0.000142859167704448,0.000218880511302086,0.000330914258401014,0.000493666063484278,0.000726708889631284,0.00105559440441471,0.0015130145096929,0.00213992480687477,0.00298650564922438,0.00411279984789308,0.00558883407150549,0.00749400992508922,0.00991554794583817,0.0129457906819443,0.0166782258068148,0.0212021803857819,0.0265962623150306,0.032920778533183,0.0402095297012098,0.0484615495843349,0.0576335016663816,0.0676335407388234,0.0783174697738453,0.0894879540654758,0.100897386463104,0.112254733100793,0.123236346117187,0.133500340016264,0.142703734073939,0.150521213053114,0.156664100290189,0.160898010876987,0.163057683511186,0.163057683511186,0.160898010876987,0.156664100290189,0.150521213053114,0.142703734073939,0.133500340016264,0.123236346117188,0.112254733100793,0.100897386463104,0.0894879540654757,0.0783174697738452,0.0676335407388234],[1.27032682611812e-05,2.09441485896193e-05,3.40737049306879e-05,5.46997590103584e-05,8.66484652337762e-05,0.000135439625183767,0.000208900759393613,0.000317938924736618,0.000477481764863565,0.000707586089762228,0.00103469223475733,0.00149297487732865,0.0021257060898808,0.00298650564922438,0.00414031011253806,0.00566385085969026,0.00764539896774982,0.0101835196339532,0.0133845897266318,0.0173588771141464,0.0222150656466253,0.028053236954984,0.0349564857872596,0.0429815387213285,0.0521489488401799,0.0624336270200143,0.0737566153930639,0.0859790811974574,0.0988994843512533,0.112254733100793,0.125725885448756,0.138948592247275,0.151528040168199,0.163057683511186,0.173140607547204,0.181412000689187,0.187560981045271,0.191349964199118,0.19262989231369,0.191349964199118,0.187560981045271,0.181412000689186,0.173140607547204,0.163057683511186,0.151528040168199,0.138948592247275,0.125725885448756,0.112254733100793,0.0988994843512532,0.0859790811974573,0.0737566153930639],[1.90778117335236e-05,3.12449981370626e-05,5.04942416858717e-05,8.0521640181131e-05,0.000126704715409478,0.000196735326235722,0.000301426429186153,0.00045571121285868,0.000679841242334636,0.001000770988501,0.00145368836219717,0.00208361428914125,0.00294694986513851,0.00411279984789308,0.00566385085969027,0.00769653857013698,0.0103202091340496,0.0136549763748181,0.0178280077793873,0.0229680478184291,0.0291981112737194,0.0366264512615908,0.0453361181416014,0.0553736597424273,0.0667377454353313,0.0793686953006767,0.0931400267517807,0.107853162058104,0.123236346117188,0.138948592247275,0.154589109631224,0.169712193813295,0.183847024784688,0.196521274267636,0.207286941616971,0.215746482184332,0.221577118005127,0.224551263151559,0.224551263151559,0.221577118005127,0.215746482184332,0.207286941616971,0.196521274267636,0.183847024784688,0.169712193813295,0.154589109631224,0.138948592247275,0.123236346117188,0.107853162058104,0.0931400267517807,0.0793686953006767],[2.82716434408781e-05,4.59946907074349e-05,7.38369514724173e-05,0.000116963193958756,0.000182824370949101,0.000281986529976784,0.000429172657826968,0.000644532965758019,0.000955141315383808,0.00139668842621768,0.00201530528765178,0.00286940308774354,0.00403136095448103,0.00558883407150549,0.00764539896774982,0.0103202091340496,0.0137463136700149,0.0180673063282852,0.02343203316216,0.0299872020239762,0.0378679089847018,0.0471863202218377,0.0580190086061543,0.0703937178977472,0.0842765813218816,0.0995610169053231,0.11605962002168,0.133500340016264,0.151528040168199,0.169712193813295,0.187560981045271,0.204541459615773,0.220104850230515,0.233715374046653,0.244880586930282,0.253180842221555,0.258295434503533,0.260023156742052,0.258295434503533,0.253180842221555,0.244880586930282,0.233715374046653,0.220104850230514,0.204541459615773,0.187560981045271,0.169712193813295,0.151528040168199,0.133500340016264,0.11605962002168,0.0995610169053231,0.0842765813218815],[4.13411885215753e-05,6.68104365259486e-05,0.000106540582661962,0.000167646786322011,0.000260306375282415,0.000398826294682618,0.000602965091549771,0.000899518215389479,0.00132414993020807,0.00192341840987106,0.00275689218337506,0.00389919722202856,0.00544176809092422,0.00749400992508922,0.0101835196339532,0.0136549763748181,0.0180673063282852,0.0235887685874911,0.0303897087997554,0.038632891490204,0.0484615495843349,0.059985569488805,0.0732665400234349,0.0883027005936736,0.105015086918652,0.123236346117188,0.142703734073939,0.163057683511186,0.183847024784688,0.204541459615773,0.224551263151559,0.243253479402161,0.260023156742052,0.274267532161665,0.285460602485018,0.293175290564394,0.297110470673857,0.297110470673857,0.293175290564394,0.285460602485018,0.274267532161665,0.260023156742052,0.243253479402161,0.224551263151559,0.204541459615773,0.183847024784688,0.163057683511186,0.142703734073939,0.123236346117187,0.105015086918652,0.0883027005936735],[5.96518982053589e-05,9.57613638722586e-05,0.000151693085277635,0.000237110427259632,0.000365716814761371,0.000556606932310108,0.000835914195139495,0.00123875150893878,0.00181140724165957,0.00261371006137222,0.00372141519526626,0.00522839331207609,0.00724832705671824,0.00991554794583818,0.0133845897266318,0.0178280077793873,0.02343203316216,0.0303897087997554,0.0388913045198195,0.0491120304813809,0.0611973583747468,0.0752465978598418,0.0912957306538512,0.109300833917683,0.129123678283553,0.150521213053114,0.173140607547204,0.196521274267636,0.220104850230515,0.243253479402161,0.265275972946228,0.285460602485018,0.303112500317449,0.317593000830256,0.328357851644514,0.33499112025604,0.337231855208577,0.33499112025604,0.328357851644514,0.317593000830256,0.303112500317449,0.285460602485018,0.265275972946228,0.243253479402161,0.220104850230514,0.196521274267636,0.173140607547204,0.150521213053114,0.129123678283553,0.109300833917683,0.0912957306538512],[8.49327106662141e-05,0.000135439625183767,0.000213120834665971,0.000330914258401014,0.000507007588241112,0.000766518860306114,0.0011435117670125,0.00168332447367516,0.00244514401926113,0.00350469684564327,0.00495685115543938,0.00691784306183354,0.00952675374976984,0.0129457906819443,0.0173588771141464,0.0229680478184291,0.0299872020239762,0.038632891490204,0.0491120304813809,0.0616067037323444,0.0762566042441888,0.0931400267517807,0.112254733100793,0.133500340016264,0.156664100290189,0.181412000689187,0.207286941616971,0.233715374046653,0.260023156742052,0.285460602485018,0.309235779002722,0.330554217072223,0.348662367221265,0.362891548359315,0.372698839025217,0.377701433395733,0.377701433395733,0.372698839025217,0.362891548359315,0.348662367221265,0.330554217072223,0.309235779002722,0.285460602485018,0.260023156742052,0.233715374046653,0.207286941616971,0.181412000689186,0.156664100290189,0.133500340016264,0.112254733100793,0.0931400267517807],[0.000119326007210532,0.000189021223755468,0.00029545778598786,0.00045571121285868,0.00069357494643525,0.001041613227331,0.00154357943026862,0.00225715241344947,0.00325688328797803,0.00463716887967559,0.0065149792445367,0.00903197168106351,0.0123555335663325,0.0166782258068148,0.0222150656466253,0.0291981112737194,0.0378679089847018,0.0484615495843349,0.0611973583747468,0.0762566042441889,0.0937630346490151,0.113761485569737,0.13619722577862,0.160898010876987,0.187560981045271,0.215746482184332,0.244880586930282,0.274267532161665,0.303112500317449,0.330554217072223,0.355705814256035,0.377701433395733,0.395745248132001,0.409159078239893,0.417424639904451,0.42021676758822,0.417424639904451,0.409159078239893,0.395745248132001,0.377701433395733,0.355705814256035,0.330554217072223,0.303112500317449,0.274267532161665,0.244880586930282,0.215746482184332,0.187560981045271,0.160898010876987,0.13619722577862,0.113761485569737,0.0937630346490151],[0.000165426331763633,0.000260306375282415,0.000404179587925526,0.00061926046668583,0.000936228250159679,0.00139668842621768,0.00205601715502536,0.00298650564922439,0.00428064639378394,0.00605431167304309,0.00844947259624268,0.0116360033062816,0.0158120244454911,0.0212021803857819,0.0280532369549841,0.0366264512615908,0.0471863202218377,0.059985569488805,0.0752465978598418,0.0931400267517807,0.113761485569737,0.137108240625842,0.163057683511186,0.191349964199119,0.221577118005127,0.253180842221555,0.285460602485018,0.317593000830256,0.348662367221265,0.377701433395734,0.403739832453488,0.425857176990707,0.443236738079082,0.455215389948493,0.461325572510598,0.461325572510598,0.455215389948493,0.443236738079082,0.425857176990707,0.403739832453488,0.377701433395733,0.348662367221265,0.317593000830256,0.285460602485018,0.253180842221555,0.221577118005127,0.191349964199118,0.163057683511186,0.137108240625842,0.113761485569737,0.0931400267517807],[0.000226299491078579,0.000353727191454194,0.000545585376603712,0.000830359968498562,0.00124703744141802,0.00184800009527527,0.002702302060594,0.00389919722202855,0.00555169909847572,0.00779984627193754,0.0108132333197937,0.0147922593162476,0.019967461521985,0.0265962623150306,0.0349564857872596,0.0453361181416014,0.0580190086061543,0.0732665400234349,0.0912957306538512,0.112254733100793,0.13619722577862,0.163057683511186,0.19262989231369,0.224551263151559,0.258295434503533,0.293175290564394,0.328357851644514,0.362891548359315,0.395745248132001,0.425857176990707,0.452190714137368,0.473793082436581,0.489852352747715,0.499748026688478,0.50309081043154,0.499748026688478,0.489852352747715,0.473793082436581,0.452190714137368,0.425857176990707,0.395745248132001,0.362891548359315,0.328357851644514,0.293175290564394,0.258295434503533,0.224551263151559,0.19262989231369,0.163057683511186,0.13619722577862,0.112254733100793,0.0912957306538512],[0.000305472361182387,0.000474309140263595,0.000726708889631284,0.00109867402929202,0.00163902905155488,0.00241275848236791,0.00350469684564327,0.00502338507797734,0.00710480056468866,0.00991554794583817,0.0136549763748181,0.018555582579172,0.0248809891714444,0.0329207785331829,0.0429815387213285,0.0553736597424273,0.0703937178977472,0.0883027005936735,0.109300833917683,0.133500340016264,0.160898010876987,0.191349964199119,0.224551263151559,0.260023156742052,0.297110470673857,0.33499112025604,0.372698839025217,0.409159078239893,0.443236738079082,0.473793082436581,0.499748026688478,0.520143130558753,0.534200208747332,0.541370574407708,0.541370574407708,0.534200208747332,0.520143130558753,0.499748026688478,0.473793082436581,0.443236738079082,0.409159078239893,0.372698839025216,0.33499112025604,0.297110470673857,0.260023156742052,0.224551263151559,0.191349964199118,0.160898010876987,0.133500340016264,0.109300833917683,0.0883027005936735],[0.000406883120273112,0.000627572563746903,0.000955141315383809,0.0014344344950571,0.0021257060898808,0.00310838725689917,0.00448514440107633,0.00638597401211592,0.00897195880172398,0.0124381789686887,0.0170151483173813,0.0229680478184292,0.0305929836885136,0.0402095297012097,0.0521489488401799,0.0667377454353312,0.0842765813218815,0.105015086918652,0.129123678283553,0.156664100290189,0.187560981045271,0.221577118005127,0.258295434503533,0.297110470673857,0.337231855208577,0.377701433395733,0.417424639904451,0.455215389948493,0.489852352747715,0.520143130558753,0.544991768806601,0.563464326674264,0.574847061130358,0.578692177685248,0.574847061130358,0.563464326674264,0.544991768806601,0.520143130558753,0.489852352747715,0.455215389948493,0.417424639904451,0.377701433395733,0.337231855208577,0.297110470673857,0.258295434503533,0.221577118005127,0.187560981045271,0.156664100290189,0.129123678283553,0.105015086918652,0.0842765813218815],[0.000534782062322523,0.000819361985074017,0.00123875150893878,0.00184800009527526,0.00272037759249125,0.00395153465920296,0.00566385085969027,0.00801064026778585,0.0111797490906169,0.0153959428709982,0.0209213609372375,0.028053236954984,0.0371180741477057,0.0484615495843349,0.0624336270200144,0.0793686953006767,0.0995610169053231,0.123236346117188,0.150521213053114,0.181412000689187,0.215746482184332,0.253180842221555,0.293175290564394,0.33499112025604,0.377701433395734,0.42021676758822,0.461325572510598,0.499748026688478,0.534200208747332,0.563464326674264,0.586459742075634,0.602309053475662,0.610393618182409,0.610393618182409,0.602309053475662,0.586459742075634,0.563464326674264,0.534200208747332,0.499748026688478,0.461325572510598,0.42021676758822,0.377701433395733,0.33499112025604,0.293175290564394,0.253180842221555,0.215746482184332,0.181412000689186,0.150521213053114,0.123236346117187,0.0995610169053231,0.0793686953006767],[0.00069357494643525,0.00105559440441471,0.00158529528782164,0.00234926855091255,0.0034352991984419,0.00495685115543938,0.00705759276177737,0.00991554794583818,0.0137463136700149,0.0188046470871136,0.0253836184938987,0.0338104758697079,0.044438402855276,0.0576335016663816,0.0737566153930639,0.0931400267517807,0.11605962002168,0.142703734073939,0.173140607547204,0.207286941616971,0.244880586930282,0.285460602485018,0.328357851644514,0.372698839025217,0.417424639904451,0.461325572510598,0.50309081043154,0.541370574407708,0.574847061130358,0.602309053475662,0.622724387213473,0.635304254302526,0.639553765295602,0.635304254302526,0.622724387213473,0.602309053475662,0.574847061130358,0.541370574407708,0.50309081043154,0.461325572510598,0.417424639904451,0.372698839025216,0.328357851644514,0.285460602485018,0.244880586930282,0.207286941616971,0.173140607547204,0.142703734073939,0.11605962002168,0.0931400267517807,0.0737566153930639],[0.000887604242175164,0.00134192348969622,0.00200191460427379,0.00294694986513851,0.00428064639378394,0.00613557638954216,0.00867782300588868,0.0121108776097162,0.0166782258068148,0.0226638397414695,0.0303897087997554,0.0402095297012098,0.0524977699441988,0.0676335407388234,0.0859790811974574,0.107853162058104,0.133500340016264,0.163057683511186,0.196521274267636,0.233715374046653,0.274267532161665,0.317593000830256,0.362891548359315,0.409159078239893,0.455215389948493,0.499748026688478,0.541370574407708,0.578692177685248,0.610393618182409,0.635304254302526,0.65247360837373,0.66123151276833,0.66123151276833,0.65247360837373,0.635304254302526,0.610393618182409,0.578692177685248,0.541370574407708,0.499748026688478,0.455215389948493,0.409159078239893,0.362891548359315,0.317593000830256,0.274267532161665,0.233715374046653,0.196521274267636,0.163057683511186,0.133500340016264,0.107853162058104,0.0859790811974573,0.0676335407388234],[0.00112086871693632,0.00168332447367516,0.00249453920500861,0.00364772623722359,0.00526336571263112,0.00749400992508921,0.0105286911879137,0.0145963382350979,0.0199674615219849,0.0269532538003863,0.0359011989345458,0.0471863202218377,0.0611973583747469,0.0783174697738453,0.0988994843512533,0.123236346117187,0.151528040168199,0.183847024784688,0.220104850230515,0.260023156742052,0.303112500317449,0.348662367221265,0.395745248132001,0.443236738079082,0.489852352747715,0.534200208747332,0.574847061130358,0.610393618182409,0.639553765295602,0.66123151276833,0.674589275394169,0.679101561471563,0.674589275394169,0.66123151276833,0.639553765295601,0.610393618182409,0.574847061130358,0.534200208747332,0.489852352747715,0.443236738079082,0.395745248132001,0.348662367221264,0.303112500317449,0.260023156742052,0.220104850230514,0.183847024784688,0.151528040168199,0.123236346117187,0.0988994843512531,0.0783174697738452,0.0611973583747468],[0.00139668842621768,0.00208361428914125,0.00306721717064097,0.00445534288715811,0.00638597401211591,0.00903197168106352,0.0126051319011179,0.0173588771141464,0.023588768587491,0.0316299363433547,0.041850511738228,0.0546402445789954,0.0703937178977473,0.0894879540654759,0.112254733100793,0.138948592247275,0.169712193813295,0.204541459615773,0.243253479402161,0.285460602485018,0.330554217072223,0.377701433395733,0.425857176990707,0.473793082436581,0.520143130558753,0.563464326674264,0.602309053475662,0.635304254302526,0.66123151276833,0.679101561471563,0.688216882724809,0.688216882724809,0.679101561471563,0.66123151276833,0.635304254302526,0.602309053475662,0.563464326674264,0.520143130558752,0.473793082436581,0.425857176990706,0.377701433395733,0.330554217072222,0.285460602485018,0.243253479402161,0.204541459615773,0.169712193813295,0.138948592247275,0.112254733100793,0.0894879540654756,0.0703937178977471,0.0546402445789954],[0.00171732988374323,0.00254493223969906,0.00372141519526627,0.00536969275307715,0.00764539896774982,0.0107413848586375,0.0148912038269306,0.0203708310016617,0.0274977456452354,0.0366264512615908,0.0481395471212505,0.0624336270200144,0.0798995876107819,0.100897386463104,0.125725885448756,0.154589109631224,0.187560981045271,0.224551263151559,0.265275972946228,0.309235779002722,0.355705814256035,0.403739832453488,0.452190714137368,0.499748026688478,0.544991768806601,0.586459742075634,0.622724387213473,0.65247360837373,0.674589275394169,0.688216882724809,0.692820323027551,0.688216882724809,0.674589275394169,0.65247360837373,0.622724387213473,0.586459742075634,0.544991768806601,0.499748026688478,0.452190714137368,0.403739832453488,0.355705814256035,0.309235779002722,0.265275972946228,0.224551263151559,0.187560981045271,0.154589109631224,0.125725885448756,0.100897386463104,0.0798995876107818,0.0624336270200143,0.0481395471212505],[0.00208361428914125,0.00306721717064097,0.00445534288715812,0.00638597401211591,0.00903197168106351,0.0126051319011179,0.0173588771141464,0.023588768587491,0.0316299363433547,0.041850511738228,0.0546402445789954,0.0703937178977472,0.0894879540654759,0.112254733100793,0.138948592247275,0.169712193813295,0.204541459615773,0.243253479402161,0.285460602485018,0.330554217072223,0.377701433395733,0.425857176990707,0.473793082436581,0.520143130558753,0.563464326674264,0.602309053475662,0.635304254302526,0.66123151276833,0.679101561471563,0.688216882724809,0.688216882724809,0.679101561471563,0.66123151276833,0.635304254302526,0.602309053475662,0.563464326674264,0.520143130558753,0.473793082436581,0.425857176990707,0.377701433395733,0.330554217072223,0.285460602485018,0.243253479402161,0.204541459615773,0.169712193813295,0.138948592247275,0.112254733100793,0.0894879540654758,0.0703937178977471,0.0546402445789953,0.0418505117382279],[0.00249453920500861,0.0036477262372236,0.00526336571263113,0.0074940099250892,0.0105286911879137,0.0145963382350979,0.019967461521985,0.0269532538003863,0.0359011989345458,0.0471863202218377,0.0611973583747469,0.0783174697738453,0.0988994843512533,0.123236346117188,0.151528040168199,0.183847024784688,0.220104850230515,0.260023156742052,0.303112500317449,0.348662367221265,0.395745248132001,0.443236738079082,0.489852352747715,0.534200208747332,0.574847061130358,0.610393618182409,0.639553765295602,0.66123151276833,0.674589275394169,0.679101561471563,0.674589275394169,0.66123151276833,0.639553765295601,0.610393618182409,0.574847061130358,0.534200208747332,0.489852352747715,0.443236738079082,0.395745248132001,0.348662367221264,0.303112500317449,0.260023156742052,0.220104850230514,0.183847024784688,0.151528040168199,0.123236346117187,0.0988994843512531,0.0783174697738453,0.0611973583747467,0.0471863202218377,0.0359011989345458],[0.00294694986513851,0.00428064639378394,0.00613557638954216,0.00867782300588868,0.0121108776097161,0.0166782258068148,0.0226638397414695,0.0303897087997554,0.0402095297012097,0.0524977699441988,0.0676335407388234,0.0859790811974574,0.107853162058105,0.133500340016264,0.163057683511186,0.196521274267636,0.233715374046653,0.274267532161665,0.317593000830256,0.362891548359315,0.409159078239893,0.455215389948493,0.499748026688478,0.541370574407708,0.578692177685248,0.610393618182409,0.635304254302526,0.65247360837373,0.66123151276833,0.66123151276833,0.65247360837373,0.635304254302526,0.610393618182409,0.578692177685248,0.541370574407708,0.499748026688478,0.455215389948493,0.409159078239893,0.362891548359315,0.317593000830256,0.274267532161664,0.233715374046653,0.196521274267636,0.163057683511186,0.133500340016264,0.107853162058104,0.0859790811974572,0.0676335407388233,0.0524977699441987,0.0402095297012097,0.0303897087997554],[0.00343529919844191,0.00495685115543939,0.00705759276177738,0.00991554794583818,0.0137463136700149,0.0188046470871136,0.0253836184938987,0.0338104758697079,0.044438402855276,0.0576335016663816,0.0737566153930639,0.0931400267517807,0.11605962002168,0.142703734073939,0.173140607547204,0.207286941616971,0.244880586930282,0.285460602485018,0.328357851644514,0.372698839025217,0.417424639904451,0.461325572510598,0.50309081043154,0.541370574407708,0.574847061130358,0.602309053475662,0.622724387213473,0.635304254302526,0.639553765295601,0.635304254302526,0.622724387213473,0.602309053475662,0.574847061130358,0.541370574407708,0.503090810431539,0.461325572510598,0.417424639904451,0.372698839025216,0.328357851644514,0.285460602485018,0.244880586930282,0.207286941616971,0.173140607547204,0.142703734073939,0.11605962002168,0.0931400267517807,0.0737566153930638,0.0576335016663815,0.0444384028552759,0.0338104758697078,0.0253836184938987],[0.00395153465920295,0.00566385085969027,0.00801064026778585,0.0111797490906169,0.0153959428709982,0.0209213609372375,0.028053236954984,0.0371180741477057,0.0484615495843349,0.0624336270200143,0.0793686953006767,0.0995610169053231,0.123236346117188,0.150521213053114,0.181412000689187,0.215746482184332,0.253180842221555,0.293175290564394,0.33499112025604,0.377701433395733,0.42021676758822,0.461325572510598,0.499748026688478,0.534200208747332,0.563464326674264,0.586459742075634,0.602309053475662,0.610393618182409,0.610393618182409,0.602309053475662,0.586459742075634,0.563464326674264,0.534200208747332,0.499748026688478,0.461325572510598,0.42021676758822,0.377701433395733,0.33499112025604,0.293175290564394,0.253180842221555,0.215746482184332,0.181412000689186,0.150521213053114,0.123236346117188,0.099561016905323,0.0793686953006767,0.0624336270200143,0.0484615495843349,0.0371180741477056,0.028053236954984,0.0209213609372375],[0.00448514440107633,0.00638597401211591,0.00897195880172399,0.0124381789686887,0.0170151483173813,0.0229680478184291,0.0305929836885136,0.0402095297012097,0.0521489488401799,0.0667377454353312,0.0842765813218815,0.105015086918652,0.129123678283553,0.156664100290189,0.187560981045271,0.221577118005127,0.258295434503533,0.297110470673857,0.337231855208577,0.377701433395733,0.417424639904451,0.455215389948493,0.489852352747715,0.520143130558753,0.544991768806601,0.563464326674264,0.574847061130358,0.578692177685248,0.574847061130358,0.563464326674264,0.544991768806601,0.520143130558753,0.489852352747715,0.455215389948493,0.417424639904451,0.377701433395733,0.337231855208577,0.297110470673857,0.258295434503533,0.221577118005127,0.187560981045271,0.156664100290189,0.129123678283553,0.105015086918652,0.0842765813218814,0.0667377454353312,0.0521489488401798,0.0402095297012097,0.0305929836885135,0.0229680478184291,0.0170151483173813],[0.00502338507797734,0.00710480056468868,0.00991554794583818,0.0136549763748181,0.018555582579172,0.0248809891714443,0.032920778533183,0.0429815387213285,0.0553736597424272,0.0703937178977472,0.0883027005936735,0.109300833917683,0.133500340016264,0.160898010876987,0.191349964199119,0.224551263151559,0.260023156742052,0.297110470673857,0.33499112025604,0.372698839025217,0.409159078239893,0.443236738079082,0.473793082436581,0.499748026688478,0.520143130558753,0.534200208747332,0.541370574407708,0.541370574407708,0.534200208747332,0.520143130558752,0.499748026688478,0.473793082436581,0.443236738079082,0.409159078239893,0.372698839025216,0.33499112025604,0.297110470673857,0.260023156742052,0.224551263151559,0.191349964199118,0.160898010876987,0.133500340016264,0.109300833917683,0.0883027005936735,0.0703937178977471,0.0553736597424272,0.0429815387213284,0.0329207785331829,0.0248809891714443,0.018555582579172,0.0136549763748181],[0.00555169909847572,0.00779984627193754,0.0108132333197937,0.0147922593162476,0.0199674615219849,0.0265962623150306,0.0349564857872596,0.0453361181416014,0.0580190086061542,0.0732665400234349,0.0912957306538512,0.112254733100793,0.13619722577862,0.163057683511186,0.19262989231369,0.224551263151559,0.258295434503533,0.293175290564394,0.328357851644514,0.362891548359315,0.395745248132001,0.425857176990707,0.452190714137368,0.473793082436581,0.489852352747715,0.499748026688478,0.50309081043154,0.499748026688478,0.489852352747715,0.47379308243658,0.452190714137368,0.425857176990707,0.395745248132001,0.362891548359315,0.328357851644514,0.293175290564394,0.258295434503533,0.224551263151559,0.19262989231369,0.163057683511186,0.13619722577862,0.112254733100793,0.0912957306538511,0.0732665400234348,0.0580190086061542,0.0453361181416014,0.0349564857872595,0.0265962623150306,0.0199674615219849,0.0147922593162475,0.0108132333197937],[0.00605431167304309,0.00844947259624269,0.0116360033062816,0.015812024445491,0.0212021803857818,0.028053236954984,0.0366264512615908,0.0471863202218377,0.0599855694888049,0.0752465978598418,0.0931400267517807,0.113761485569737,0.137108240625842,0.163057683511186,0.191349964199119,0.221577118005127,0.253180842221555,0.285460602485018,0.317593000830256,0.348662367221265,0.377701433395733,0.403739832453488,0.425857176990707,0.443236738079082,0.455215389948493,0.461325572510598,0.461325572510598,0.455215389948493,0.443236738079082,0.425857176990706,0.403739832453488,0.377701433395733,0.348662367221264,0.317593000830256,0.285460602485018,0.253180842221555,0.221577118005127,0.191349964199118,0.163057683511186,0.137108240625842,0.113761485569737,0.0931400267517806,0.0752465978598418,0.0599855694888049,0.0471863202218377,0.0366264512615908,0.028053236954984,0.0212021803857818,0.015812024445491,0.0116360033062816,0.00844947259624267],[0.00651497924453669,0.00903197168106351,0.0123555335663325,0.0166782258068148,0.0222150656466253,0.0291981112737194,0.0378679089847018,0.0484615495843349,0.0611973583747468,0.0762566042441888,0.0937630346490151,0.113761485569737,0.13619722577862,0.160898010876987,0.187560981045271,0.215746482184332,0.244880586930282,0.274267532161665,0.303112500317449,0.330554217072223,0.355705814256035,0.377701433395733,0.395745248132001,0.409159078239893,0.417424639904451,0.42021676758822,0.417424639904451,0.409159078239893,0.395745248132001,0.377701433395733,0.355705814256035,0.330554217072223,0.303112500317449,0.274267532161664,0.244880586930282,0.215746482184332,0.187560981045271,0.160898010876987,0.13619722577862,0.113761485569737,0.0937630346490151,0.0762566042441887,0.0611973583747468,0.0484615495843349,0.0378679089847018,0.0291981112737194,0.0222150656466252,0.0166782258068148,0.0123555335663324,0.0090319716810635,0.0065149792445367],[0.00691784306183353,0.00952675374976984,0.0129457906819443,0.0173588771141464,0.0229680478184291,0.0299872020239762,0.038632891490204,0.0491120304813809,0.0616067037323443,0.0762566042441888,0.0931400267517807,0.112254733100793,0.133500340016264,0.156664100290189,0.181412000689187,0.207286941616971,0.233715374046653,0.260023156742052,0.285460602485018,0.309235779002722,0.330554217072223,0.348662367221265,0.362891548359315,0.372698839025217,0.377701433395733,0.377701433395733,0.372698839025217,0.362891548359315,0.348662367221265,0.330554217072223,0.309235779002722,0.285460602485018,0.260023156742052,0.233715374046653,0.207286941616971,0.181412000689187,0.156664100290189,0.133500340016264,0.112254733100793,0.0931400267517807,0.0762566042441888,0.0616067037323443,0.0491120304813808,0.038632891490204,0.0299872020239762,0.0229680478184291,0.0173588771141464,0.0129457906819443,0.00952675374976981,0.00691784306183353,0.00495685115543938],[0.00724832705671823,0.00991554794583818,0.0133845897266318,0.0178280077793873,0.02343203316216,0.0303897087997554,0.0388913045198195,0.0491120304813809,0.0611973583747468,0.0752465978598418,0.0912957306538512,0.109300833917683,0.129123678283553,0.150521213053114,0.173140607547204,0.196521274267636,0.220104850230514,0.243253479402161,0.265275972946228,0.285460602485018,0.303112500317449,0.317593000830256,0.328357851644514,0.33499112025604,0.337231855208577,0.33499112025604,0.328357851644514,0.317593000830256,0.303112500317449,0.285460602485018,0.265275972946228,0.243253479402161,0.220104850230514,0.196521274267636,0.173140607547204,0.150521213053114,0.129123678283553,0.109300833917683,0.0912957306538511,0.0752465978598418,0.0611973583747468,0.0491120304813808,0.0388913045198195,0.0303897087997554,0.02343203316216,0.0178280077793873,0.0133845897266318,0.00991554794583817,0.00724832705671822,0.00522839331207607,0.00372141519526626],[0.0074940099250892,0.0101835196339532,0.0136549763748181,0.0180673063282852,0.023588768587491,0.0303897087997554,0.038632891490204,0.0484615495843349,0.0599855694888049,0.0732665400234349,0.0883027005936735,0.105015086918652,0.123236346117188,0.142703734073939,0.163057683511186,0.183847024784688,0.204541459615773,0.224551263151559,0.243253479402161,0.260023156742052,0.274267532161665,0.285460602485018,0.293175290564394,0.297110470673857,0.297110470673857,0.293175290564394,0.285460602485018,0.274267532161664,0.260023156742052,0.24325347940216,0.224551263151559,0.204541459615773,0.183847024784688,0.163057683511186,0.142703734073939,0.123236346117187,0.105015086918652,0.0883027005936734,0.0732665400234348,0.0599855694888048,0.0484615495843349,0.0386328914902039,0.0303897087997554,0.023588768587491,0.0180673063282852,0.0136549763748181,0.0101835196339532,0.0074940099250892,0.0054417680909242,0.00389919722202855,0.00275689218337506],[0.00764539896774982,0.0103202091340496,0.0137463136700149,0.0180673063282852,0.02343203316216,0.0299872020239762,0.0378679089847018,0.0471863202218377,0.0580190086061542,0.0703937178977472,0.0842765813218815,0.0995610169053231,0.11605962002168,0.133500340016264,0.151528040168199,0.169712193813295,0.187560981045271,0.204541459615773,0.220104850230514,0.233715374046653,0.244880586930282,0.253180842221555,0.258295434503533,0.260023156742052,0.258295434503533,0.253180842221555,0.244880586930282,0.233715374046653,0.220104850230514,0.204541459615773,0.187560981045271,0.169712193813295,0.151528040168199,0.133500340016264,0.11605962002168,0.099561016905323,0.0842765813218814,0.0703937178977471,0.0580190086061542,0.0471863202218377,0.0378679089847018,0.0299872020239762,0.02343203316216,0.0180673063282852,0.0137463136700148,0.0103202091340496,0.00764539896774981,0.00558883407150548,0.00403136095448101,0.00286940308774354,0.00201530528765178],[0.00769653857013698,0.0103202091340496,0.0136549763748181,0.0178280077793873,0.0229680478184291,0.0291981112737195,0.0366264512615908,0.0453361181416014,0.0553736597424272,0.0667377454353312,0.0793686953006767,0.0931400267517807,0.107853162058104,0.123236346117188,0.138948592247275,0.154589109631224,0.169712193813295,0.183847024784688,0.196521274267636,0.207286941616971,0.215746482184332,0.221577118005127,0.224551263151559,0.224551263151559,0.221577118005127,0.215746482184332,0.207286941616971,0.196521274267636,0.183847024784688,0.169712193813295,0.154589109631224,0.138948592247275,0.123236346117187,0.107853162058104,0.0931400267517807,0.0793686953006767,0.0667377454353312,0.0553736597424272,0.0453361181416014,0.0366264512615908,0.0291981112737194,0.0229680478184291,0.0178280077793873,0.0136549763748181,0.0103202091340496,0.00769653857013698,0.00566385085969025,0.00411279984789308,0.0029469498651385,0.00208361428914124,0.00145368836219717],[0.00764539896774982,0.0101835196339532,0.0133845897266318,0.0173588771141464,0.0222150656466253,0.028053236954984,0.0349564857872596,0.0429815387213285,0.0521489488401799,0.0624336270200143,0.0737566153930639,0.0859790811974574,0.0988994843512533,0.112254733100793,0.125725885448756,0.138948592247275,0.151528040168199,0.163057683511186,0.173140607547204,0.181412000689187,0.187560981045271,0.191349964199118,0.19262989231369,0.191349964199118,0.187560981045271,0.181412000689187,0.173140607547204,0.163057683511186,0.151528040168199,0.138948592247275,0.125725885448756,0.112254733100793,0.0988994843512532,0.0859790811974573,0.0737566153930638,0.0624336270200143,0.0521489488401799,0.0429815387213285,0.0349564857872595,0.028053236954984,0.0222150656466253,0.0173588771141464,0.0133845897266318,0.0101835196339532,0.00764539896774981,0.00566385085969026,0.00414031011253805,0.00298650564922438,0.00212570608988079,0.00149297487732865,0.00103469223475733],[0.00749400992508921,0.00991554794583818,0.0129457906819443,0.0166782258068148,0.0212021803857818,0.0265962623150306,0.0329207785331829,0.0402095297012098,0.0484615495843349,0.0576335016663816,0.0676335407388234,0.0783174697738453,0.0894879540654758,0.100897386463104,0.112254733100793,0.123236346117188,0.133500340016264,0.142703734073939,0.150521213053114,0.156664100290189,0.160898010876987,0.163057683511186,0.163057683511186,0.160898010876987,0.156664100290189,0.150521213053114,0.142703734073939,0.133500340016264,0.123236346117187,0.112254733100793,0.100897386463104,0.0894879540654758,0.0783174697738453,0.0676335407388233,0.0576335016663815,0.0484615495843349,0.0402095297012097,0.0329207785331829,0.0265962623150306,0.0212021803857818,0.0166782258068148,0.0129457906819443,0.00991554794583817,0.00749400992508922,0.00558883407150548,0.00411279984789308,0.00298650564922438,0.00213992480687477,0.0015130145096929,0.0010555944044147,0.000726708889631284],[0.00724832705671824,0.00952675374976984,0.0123555335663325,0.015812024445491,0.0199674615219849,0.0248809891714443,0.0305929836885136,0.0371180741477057,0.044438402855276,0.0524977699441988,0.0611973583747468,0.0703937178977472,0.0798995876107819,0.0894879540654758,0.0988994843512533,0.107853162058104,0.11605962002168,0.123236346117187,0.129123678283553,0.133500340016264,0.13619722577862,0.137108240625842,0.13619722577862,0.133500340016264,0.129123678283553,0.123236346117187,0.11605962002168,0.107853162058104,0.0988994843512532,0.0894879540654757,0.0798995876107818,0.0703937178977471,0.0611973583747468,0.0524977699441987,0.0444384028552759,0.0371180741477056,0.0305929836885136,0.0248809891714443,0.0199674615219849,0.015812024445491,0.0123555335663324,0.00952675374976981,0.00724832705671822,0.00544176809092421,0.00403136095448102,0.00294694986513851,0.00212570608988079,0.0015130145096929,0.00106265521031268,0.000736463225893188,0.000503638779492394],[0.00691784306183354,0.00903197168106351,0.0116360033062816,0.0147922593162475,0.018555582579172,0.0229680478184291,0.028053236954984,0.0338104758697079,0.0402095297012097,0.0471863202218377,0.0546402445789954,0.0624336270200143,0.0703937178977472,0.0783174697738453,0.0859790811974573,0.0931400267517807,0.0995610169053231,0.105015086918652,0.109300833917683,0.112254733100793,0.113761485569737,0.113761485569737,0.112254733100793,0.109300833917683,0.105015086918652,0.0995610169053231,0.0931400267517807,0.0859790811974573,0.0783174697738452,0.0703937178977471,0.0624336270200143,0.0546402445789953,0.0471863202218377,0.0402095297012097,0.0338104758697078,0.028053236954984,0.0229680478184291,0.018555582579172,0.0147922593162475,0.0116360033062816,0.0090319716810635,0.00691784306183352,0.00522839331207607,0.00389919722202855,0.00286940308774354,0.00208361428914124,0.00149297487732865,0.0010555944044147,0.000736463225893186,0.000507007588241111,0.000344419125477659],[0.0065149792445367,0.00844947259624268,0.0108132333197937,0.0136549763748181,0.0170151483173813,0.0209213609372375,0.0253836184938987,0.0303897087997554,0.0359011989345458,0.0418505117382279,0.0481395471212505,0.0546402445789954,0.0611973583747469,0.0676335407388234,0.0737566153930639,0.0793686953006767,0.0842765813218815,0.0883027005936735,0.0912957306538512,0.0931400267517807,0.0937630346490151,0.0931400267517807,0.0912957306538512,0.0883027005936735,0.0842765813218815,0.0793686953006767,0.0737566153930639,0.0676335407388234,0.0611973583747468,0.0546402445789954,0.0481395471212505,0.0418505117382279,0.0359011989345458,0.0303897087997554,0.0253836184938987,0.0209213609372375,0.0170151483173813,0.0136549763748181,0.0108132333197937,0.00844947259624267,0.0065149792445367,0.00495685115543937,0.00372141519526626,0.00275689218337506,0.00201530528765178,0.00145368836219717,0.00103469223475733,0.000726708889631284,0.000503638779492393,0.000344419125477659,0.000232415326227089],[0.00605431167304308,0.00779984627193754,0.00991554794583818,0.0124381789686887,0.0153959428709982,0.0188046470871136,0.0226638397414694,0.0269532538003863,0.0316299363433546,0.0366264512615908,0.0418505117382279,0.0471863202218377,0.0524977699441987,0.0576335016663815,0.0624336270200143,0.0667377454353312,0.0703937178977471,0.0732665400234348,0.0752465978598417,0.0762566042441888,0.0762566042441887,0.0752465978598417,0.0732665400234348,0.0703937178977471,0.0667377454353312,0.0624336270200143,0.0576335016663815,0.0524977699441987,0.0471863202218376,0.0418505117382279,0.0366264512615907,0.0316299363433546,0.0269532538003863,0.0226638397414694,0.0188046470871135,0.0153959428709982,0.0124381789686887,0.00991554794583815,0.00779984627193753,0.00605431167304307,0.00463716887967558,0.00350469684564326,0.00261371006137221,0.00192341840987105,0.00139668842621767,0.00100077098850099,0.000707586089762225,0.000493666063484277,0.000339857349890199,0.000230871044045739,0.000154757488873036],[0.00555169909847572,0.00710480056468867,0.00897195880172398,0.0111797490906169,0.0137463136700149,0.0166782258068148,0.0199674615219849,0.023588768587491,0.0274977456452354,0.0316299363433547,0.0359011989345458,0.0402095297012097,0.044438402855276,0.0484615495843349,0.0521489488401799,0.0553736597424272,0.0580190086061542,0.0599855694888049,0.0611973583747468,0.0616067037323444,0.0611973583747468,0.0599855694888049,0.0580190086061542,0.0553736597424272,0.0521489488401799,0.0484615495843349,0.044438402855276,0.0402095297012097,0.0359011989345458,0.0316299363433546,0.0274977456452354,0.023588768587491,0.0199674615219849,0.0166782258068148,0.0137463136700148,0.0111797490906169,0.00897195880172398,0.00710480056468866,0.00555169909847571,0.00428064639378393,0.00325688328797803,0.00244514401926112,0.00181140724165957,0.00132414993020807,0.000955141315383807,0.000679841242334636,0.000477481764863565,0.000330914258401014,0.000226299491078578,0.000152707750973788,0.000101682915906592],[0.00502338507797734,0.00638597401211592,0.00801064026778585,0.00991554794583817,0.0121108776097161,0.0145963382350979,0.0173588771141464,0.0203708310016617,0.023588768587491,0.0269532538003863,0.0303897087997554,0.0338104758697079,0.0371180741477057,0.0402095297012098,0.0429815387213285,0.0453361181416014,0.0471863202218377,0.0484615495843349,0.0491120304813809,0.0491120304813809,0.0484615495843349,0.0471863202218377,0.0453361181416014,0.0429815387213285,0.0402095297012098,0.0371180741477057,0.0338104758697079,0.0303897087997554,0.0269532538003863,0.023588768587491,0.0203708310016617,0.0173588771141464,0.0145963382350979,0.0121108776097161,0.00991554794583817,0.00801064026778585,0.00638597401211592,0.00502338507797734,0.00389919722202855,0.00298650564922438,0.00225715241344947,0.00168332447367516,0.00123875150893878,0.000899518215389479,0.000644532965758018,0.00045571121285868,0.000317938924736617,0.000218880511302086,0.000148689360938946,9.96694592696043e-05,6.59255431045717e-05],[0.00448514440107633,0.00566385085969026,0.00705759276177737,0.00867782300588867,0.0105286911879137,0.0126051319011179,0.0148912038269306,0.0173588771141464,0.0199674615219849,0.0226638397414694,0.0253836184938987,0.028053236954984,0.0305929836885136,0.0329207785331829,0.0349564857872596,0.0366264512615908,0.0378679089847018,0.038632891490204,0.0388913045198195,0.038632891490204,0.0378679089847018,0.0366264512615908,0.0349564857872596,0.0329207785331829,0.0305929836885136,0.028053236954984,0.0253836184938987,0.0226638397414694,0.0199674615219849,0.0173588771141464,0.0148912038269306,0.0126051319011179,0.0105286911879137,0.00867782300588866,0.00705759276177736,0.00566385085969026,0.00448514440107633,0.00350469684564326,0.002702302060594,0.00205601715502535,0.00154357943026862,0.0011435117670125,0.000835914195139493,0.000602965091549771,0.000429172657826966,0.000301426429186152,0.000208900759393613,0.000142859167704447,9.64019057318965e-05,6.41907618392877e-05,4.21763359280098e-05],[0.00395153465920296,0.00495685115543938,0.00613557638954216,0.00749400992508921,0.00903197168106351,0.0107413848586375,0.0126051319011179,0.0145963382350979,0.0166782258068148,0.0188046470871136,0.0209213609372375,0.0229680478184291,0.0248809891714443,0.0265962623150306,0.028053236954984,0.0291981112737195,0.0299872020239762,0.0303897087997554,0.0303897087997554,0.0299872020239762,0.0291981112737194,0.028053236954984,0.0265962623150306,0.0248809891714443,0.0229680478184291,0.0209213609372375,0.0188046470871136,0.0166782258068148,0.0145963382350979,0.0126051319011179,0.0107413848586375,0.00903197168106351,0.00749400992508921,0.00613557638954216,0.00495685115543938,0.00395153465920296,0.00310838725689917,0.00241275848236791,0.00184800009527526,0.00139668842621768,0.001041613227331,0.000766518860306113,0.000556606932310108,0.000398826294682618,0.000281986529976784,0.000196735326235722,0.000135439625183767,9.20065070872886e-05,6.16738060663296e-05,4.07936311689087e-05,2.66252310987588e-05],[0.0034352991984419,0.00428064639378393,0.00526336571263112,0.00638597401211591,0.00764539896774981,0.0090319716810635,0.0105286911879137,0.0121108776097161,0.0137463136700149,0.0153959428709982,0.0170151483173813,0.018555582579172,0.0199674615219849,0.0212021803857818,0.0222150656466252,0.0229680478184291,0.02343203316216,0.023588768587491,0.02343203316216,0.0229680478184291,0.0222150656466252,0.0212021803857818,0.0199674615219849,0.0185555825791719,0.0170151483173813,0.0153959428709982,0.0137463136700148,0.0121108776097161,0.0105286911879137,0.0090319716810635,0.00764539896774981,0.0063859740121159,0.00526336571263112,0.00428064639378393,0.00343529919844189,0.00272037759249124,0.00212570608988079,0.00163902905155488,0.00124703744141802,0.000936228250159675,0.00069357494643525,0.00050700758824111,0.00036571681476137,0.000260306375282414,0.0001828243709491,0.000126704715409478,8.6648465233776e-05,5.84707114811223e-05,3.89336651258396e-05,2.55812408546773e-05,1.65854527439267e-05],[0.00294694986513851,0.00364772623722359,0.00445534288715811,0.00536969275307715,0.00638597401211591,0.00749400992508921,0.00867782300588868,0.00991554794583817,0.0111797490906169,0.0124381789686887,0.0136549763748181,0.0147922593162476,0.015812024445491,0.0166782258068148,0.0173588771141464,0.0178280077793873,0.0180673063282852,0.0180673063282852,0.0178280077793873,0.0173588771141464,0.0166782258068148,0.015812024445491,0.0147922593162475,0.0136549763748181,0.0124381789686887,0.0111797490906169,0.00991554794583816,0.00867782300588867,0.0074940099250892,0.00638597401211591,0.00536969275307715,0.00445534288715811,0.00364772623722359,0.00294694986513851,0.00234926855091255,0.00184800009527526,0.0014344344950571,0.00109867402929202,0.00083035996849856,0.000619260466685829,0.00045571121285868,0.000330914258401013,0.000237110427259632,0.000167646786322011,0.000116963193958756,8.0521640181131e-05,5.46997590103583e-05,3.66663449779618e-05,2.42526519562335e-05,1.58292469826671e-05,1.0194611575158e-05],[0.00249453920500861,0.00306721717064097,0.00372141519526626,0.0044553428871581,0.00526336571263112,0.00613557638954215,0.00705759276177736,0.00801064026778584,0.00897195880172396,0.00991554794583816,0.0108132333197937,0.0116360033062816,0.0123555335663324,0.0129457906819443,0.0133845897266318,0.0136549763748181,0.0137463136700148,0.0136549763748181,0.0133845897266318,0.0129457906819443,0.0123555335663324,0.0116360033062816,0.0108132333197937,0.00991554794583815,0.00897195880172395,0.00801064026778583,0.00705759276177735,0.00613557638954214,0.00526336571263111,0.0044553428871581,0.00372141519526625,0.00306721717064096,0.0024945392050086,0.00200191460427378,0.00158529528782163,0.00123875150893878,0.000955141315383805,0.000726708889631281,0.000545585376603709,0.000404179587925524,0.000295457785987859,0.00021312083466597,0.000151693085277635,0.000106540582661962,7.38369514724168e-05,5.04942416858713e-05,3.40737049306877e-05,2.26885253105972e-05,1.49074234016213e-05,9.66514360135466e-06,6.18334448419464e-06],[0.00208361428914125,0.00254493223969906,0.00306721717064097,0.00364772623722359,0.00428064639378393,0.00495685115543938,0.00566385085969026,0.00638597401211591,0.00710480056468866,0.00779984627193754,0.00844947259624266,0.0090319716810635,0.00952675374976983,0.00991554794583816,0.0101835196339532,0.0103202091340496,0.0103202091340496,0.0101835196339532,0.00991554794583816,0.00952675374976982,0.0090319716810635,0.00844947259624266,0.00779984627193753,0.00710480056468866,0.0063859740121159,0.00566385085969025,0.00495685115543937,0.00428064639378393,0.00364772623722359,0.00306721717064096,0.00254493223969906,0.00208361428914124,0.00168332447367516,0.00134192348969622,0.0010555944044147,0.000819361985074015,0.0006275725637469,0.000474309140263594,0.000353727191454193,0.000260306375282414,0.000189021223755468,0.000135439625183767,9.57613638722582e-05,6.68104365259484e-05,4.59946907074348e-05,3.12449981370626e-05,2.09441485896192e-05,1.38533346197713e-05,9.04180935040093e-06,5.8232545341483e-06,3.70071472688658e-06],[0.00171732988374323,0.00208361428914125,0.00249453920500861,0.00294694986513851,0.0034352991984419,0.00395153465920296,0.00448514440107633,0.00502338507797734,0.00555169909847572,0.00605431167304308,0.0065149792445367,0.00691784306183354,0.00724832705671824,0.00749400992508921,0.00764539896774982,0.00769653857013698,0.00764539896774982,0.0074940099250892,0.00724832705671823,0.00691784306183354,0.00651497924453669,0.00605431167304308,0.00555169909847572,0.00502338507797733,0.00448514440107633,0.00395153465920295,0.0034352991984419,0.00294694986513851,0.00249453920500861,0.00208361428914125,0.00171732988374323,0.00139668842621768,0.00112086871693631,0.000887604242175163,0.00069357494643525,0.000534782062322523,0.000406883120273112,0.000305472361182387,0.000226299491078579,0.000165426331763632,0.000119326007210532,8.4932710666214e-05,5.96518982053589e-05,4.13411885215753e-05,2.8271643440878e-05,1.90778117335236e-05,1.27032682611812e-05,8.34664201226666e-06,5.41149811501853e-06,3.46204447661178e-06,2.1855320581026e-06]],"type":"surface","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"paper_bgcolor":"transparent","scene":{"xaxis":{"title":"X1"},"yaxis":{"title":"X2"},"zaxis":{"title":"Log Normal Densities"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.333333333333333}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"","ticklen":2,"len":0.333333333333333,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgb(107,184,214)"],["1","rgb(0,90,124)"]],"showscale":true,"x":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"y":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"z":[[0.00513102571342465,0.00509693264516655,0.00499600661672614,0.00483221803781215,0.00461189537455569,0.00434331949635779,0.00403620778202872,0.00370113273231715,0.00334892338531822,0.00299009626738422,0.00263435643946864,0.00229019946562793,0.00196463324342567,0.00166302613667241,0.00138907619276083,0.00114488658916215,0.000931125617478452,0.000747245811290876,0.000591736161450109,0.0004623832976235,0.000356521374881682,0.000271255413515408,0.000203648240788258,0.000150866327385719,0.000110284221175755,7.95506714736882e-05,5.66218071108094e-05,3.9767932136906e-05,2.75607923477168e-05,1.88477622939187e-05,1.27185411556824e-05,8.46884550745414e-06,5.56442800817777e-06,3.60766541001235e-06,2.30802965107452e-06,1.4570213720684e-06,9.07611243774642e-07,5.57883118078687e-07,3.38373215650806e-07,2.02515442175952e-07,1.19599597321226e-07,6.96964563235772e-08,4.00775425509639e-08,2.27405454552105e-08,1.27323942551786e-08,7.03442632428991e-09,3.83492345344395e-09,2.06297574038154e-09,1.09506764903209e-09,5.73584205756112e-10,2.96457762188409e-10],[0.00688013942269972,0.00688013942269972,0.00678901308930213,0.00661036529722545,0.00635116916651323,0.00602131445404234,0.00563298173082845,0.00519989751462503,0.00473653370979245,0.00425731600807728,0.00377590057312685,0.00330456743695959,0.00285376426252262,0.0024318174914824,0.00204481144709398,0.00169662149313271,0.00138907619276083,0.00112221631578344,0.000894615659797482,0.000703729602943137,0.000546241323382678,0.000418381709164601,0.000316206093509063,0.000235818127636129,0.00017353758352161,0.000126014149170312,9.02930834558448e-05,6.38409092481723e-05,4.45402910172989e-05,3.06631271382899e-05,2.08299987580418e-05,1.39627657264129e-05,9.23555641318089e-06,6.02787290026731e-06,3.88216968943221e-06,2.46714315125773e-06,1.54711854195989e-06,9.57331258920113e-07,5.8453469717544e-07,3.52182488547485e-07,2.09379726076757e-07,1.22831824744976e-07,7.11044181364298e-08,4.06154881192249e-08,2.28926555618971e-08,1.27323942551786e-08,6.98768612257874e-09,3.78413051282132e-09,2.022126083786e-09,1.0662517644402e-09,5.54779878789496e-10],[0.00910331758321209,0.00916420911334325,0.00910331758321209,0.00892305981775455,0.00863052712129623,0.00823702237755497,0.00775733553752108,0.00720882221319582,0.00661036529722545,0.00598130586781599,0.00534042684519057,0.00470506184118492,0.00409038425969477,0.00350891047508742,0.00297022859143874,0.00248094346351084,0.00204481144709398,0.00166302613667241,0.00133460973618253,0.00105686352521443,0.000825834339292521,0.00063676087692254,0.000484472593087523,0.000363723584402474,0.000269453058617017,0.00019697185732524,0.000142080556443981,0.000101128723518423,7.10270551079748e-05,4.92246343149448e-05,3.36628277905811e-05,2.27158032871252e-05,1.51256835403982e-05,9.93828226774759e-06,6.44342906756979e-06,4.12222965612978e-06,2.60229616493837e-06,1.62102856156239e-06,9.96400688755216e-07,6.04347567088824e-07,3.61700362546594e-07,2.13609477117942e-07,1.24480549481003e-07,7.15800312060429e-08,4.06154881192247e-08,2.27405454552105e-08,1.25637557534617e-08,6.84932066683573e-09,3.68455395402651e-09,1.95583290544245e-09,1.02444343475168e-09],[0.0118853385195915,0.0120448708855235,0.0120448708855235,0.0118853385195915,0.0115725847427643,0.0111188172045432,0.0105413496303273,0.0098615062108317,0.00910331758321207,0.00829211931245911,0.00745316606041126,0.00661036529722544,0.00578521533725911,0.00499600661672614,0.00425731600807727,0.0035797951687181,0.00297022859143874,0.00243181749148239,0.00196463324342567,0.0015661790339417,0.00123200006351684,0.000956289663371402,0.000732449352861347,0.000553573312332373,0.000412840311123886,0.00030380747523912,0.000220609505600676,0.000158073618173088,0.000111764524214674,7.79754626391706e-05,5.36810934540873e-05,3.64665060069056e-05,2.44442299853079e-05,1.61684346374891e-05,1.05528313217781e-05,6.79640771677201e-06,4.31915966920076e-06,2.70849788610738e-06,1.67597350866529e-06,1.02332881981395e-06,6.16556197785927e-07,3.66555328562108e-07,2.15038297742219e-07,1.24480549481003e-07,7.11044181364296e-08,4.00775425509639e-08,2.22902524854691e-08,1.22331499354993e-08,6.62477322632763e-09,3.54008052701561e-09,1.86665764239765e-09],[0.0153120318789528,0.01562135544144,0.015725845724994,0.01562135544144,0.0153120318789528,0.0148100437644168,0.0141347869238546,0.0133116410146566,0.0123703883861147,0.0113434322115875,0.0102639619139988,0.00916420911334325,0.00807391840647743,0.00701912745860916,0.00602131445404234,0.00509693264516655,0.00425731600807727,0.00350891047508742,0.00285376426252262,0.00229019946562793,0.00181358506166083,0.00141713739325386,0.00109268603436992,0.000831358294278681,0.000624152166773117,0.0004623832976235,0.000338005058827408,0.000243811209840914,0.000173537583521609,0.000121882913966067,8.44698102729856e-05,5.77656434891841e-05,3.89804743207483e-05,2.59557767505598e-05,1.70541605697849e-05,1.10569684959511e-05,7.07377437742062e-06,4.46555807647157e-06,2.7816961354387e-06,1.70983041938967e-06,1.03706457226915e-06,6.20680304184704e-07,3.66555328562109e-07,2.13609477117943e-07,1.22831824744976e-07,6.96964563235772e-08,3.90229320509677e-08,2.15594910877548e-08,1.17534812655946e-08,6.32271986919984e-09,3.3562226870262e-09],[0.019465407515813,0.0199914680159841,0.0202598058665036,0.0202598058665036,0.0199914680159842,0.019465407515813,0.0187021579699893,0.0177308415433538,0.0165873261142962,0.0153120318789528,0.0139475739581584,0.0125364313914091,0.0111188172045432,0.00973089215673191,0.00840342126741195,0.00716092323909167,0.00602131445404234,0.00499600661672614,0.00409038425969478,0.00330456743695959,0.00263435643946864,0.00207225817126612,0.00160850565491194,0.00123200006351684,0.000931125617478451,0.000694408818220669,0.000511012573537409,0.000371071288206739,0.000265884196455078,0.000187991019984523,0.000131156884157148,9.02930834558448e-05,6.13376713915257e-05,4.11158707108865e-05,2.71957541126058e-05,1.77501540658392e-05,1.14317456982374e-05,7.26494598705999e-06,4.5557683335836e-06,2.81903378173787e-06,1.7212673696608e-06,1.03706457226914e-06,6.16556197785927e-07,3.61700362546594e-07,2.09379726076757e-07,1.19599597321226e-07,6.74115347027047e-08,3.74928209993285e-08,2.05764964344219e-08,1.1143051739668e-08,5.95451332860088e-09],[0.0244176341743939,0.0252452726564679,0.0257552609934694,0.0259275363465463,0.0257552609934693,0.0252452726564679,0.0244176341743939,0.0233043238581731,0.0219471856887886,0.0203953224590091,0.0187021579699894,0.0169224123292658,0.015109226494313,0.0133116410146566,0.0115725847427643,0.00992746921795374,0.00840342126741195,0.00701912745860916,0.00578521533725912,0.00470506184118492,0.00377590057312685,0.00299009626738422,0.00233646456376218,0.00180153470706267,0.00137067810335024,0.00102905295351241,0.000762341178008333,0.000557276130092996,0.000401976727699847,0.000286115105217978,0.000200950952790769,0.000139267172929075,9.52394451362984e-05,6.42679371545978e-05,4.27938412261918e-05,2.81175572853399e-05,1.82298591487359e-05,1.16626822801869e-05,7.36246058622543e-06,4.58624158712214e-06,2.81903378173788e-06,1.70983041938967e-06,1.02332881981395e-06,6.04347567088825e-07,3.52182488547485e-07,2.02515442175952e-07,1.14910030033104e-07,6.43379381175164e-08,3.55455915418512e-08,1.93782145673613e-08,1.04244027370281e-08],[0.0302240787610676,0.0314575468145585,0.0323076997228899,0.0327413536542539,0.0327413536542539,0.0323076997228899,0.0314575468145585,0.0302240787610676,0.0286543591475523,0.0268063531341398,0.0247453827651371,0.0225403172464719,0.0202598058665036,0.0179688358669242,0.015725845724994,0.0135805540011078,0.0115725847427643,0.00973089215673191,0.00807391840647743,0.00661036529722545,0.00534042684519057,0.00425731600807728,0.00334892338531823,0.0025994648146857,0.00199100376614959,0.00150476827563298,0.00112221631578344,0.000825834339292521,0.000599678810259652,0.000429688643838679,0.00030380747523912,0.000211959283157745,0.000145920340868057,9.91262406259641e-05,6.64463061797362e-05,4.39503620697145e-05,2.86855696207828e-05,1.84745515953002e-05,1.17406932430016e-05,7.36246058622543e-06,4.55576833358361e-06,2.7816961354387e-06,1.67597350866529e-06,9.9640068875522e-07,5.84534697175438e-07,3.38373215650807e-07,1.93281847743411e-07,1.08942055645579e-07,6.0591188283015e-08,3.32531491744827e-08,1.80080006787885e-08],[0.0369157731616182,0.0386793390707695,0.0399903796592033,0.0407982389164979,0.0410711358215629,0.0407982389164979,0.0399903796592033,0.0386793390707695,0.0369157731616181,0.0347659658934533,0.0323076997228899,0.0296256019035173,0.0268063531341398,0.0239341326230305,0.0210866242289031,0.0183318304301569,0.015725845724994,0.0133116410146566,0.0111188172045432,0.00916420911334324,0.00745316606041125,0.00598130586781599,0.00473653370979244,0.00370113273231714,0.00285376426252262,0.00217125552531869,0.00163009601284075,0.00120760482777305,0.000882766620138714,0.000636760876922538,0.000453227494889757,0.00031832117657571,0.000220609505600676,0.000150866327385719,0.000101805167315859,6.7788610604395e-05,4.45402910172989e-05,2.88774456276261e-05,1.84745515953002e-05,1.16626822801869e-05,7.26494598705999e-06,4.46555807647156e-06,2.70849788610738e-06,1.62102856156239e-06,9.5733125892011e-07,5.57883118078687e-07,3.20799443510579e-07,1.82025989190375e-07,1.01916054020142e-07,5.63068468426864e-08,3.06965255717982e-08],[0.0444918302902208,0.0469291452651648,0.0488443600156232,0.0501643985732279,0.0508377361627926,0.0508377361627925,0.0501643985732279,0.0488443600156232,0.0469291452651648,0.0444918302902208,0.0416224180133429,0.0384223344442544,0.0349985132961325,0.0314575468145585,0.0279003411588186,0.0244176341743939,0.0210866242289031,0.0179688358669242,0.015109226494313,0.0125364313914091,0.0102639619139988,0.00829211931245912,0.00661036529722545,0.00519989751462503,0.00403620778202872,0.00309144591978373,0.00233646456376218,0.00174247337424814,0.00128227893991404,0.00093112561747845,0.000667180659000664,0.000471724059841485,0.000329110708989519,0.000226571566593467,0.000153914029363826,0.000103171659248691,6.82420444418325e-05,4.45402910172989e-05,2.86855696207828e-05,1.82298591487359e-05,1.14317456982374e-05,7.0737743774206e-06,4.31915966920076e-06,2.60229616493837e-06,1.54711854195989e-06,9.07611243774642e-07,5.25394534491901e-07,3.00110120198403e-07,1.69155116200354e-07,9.40803754252168e-08,5.16324047614535e-08],[0.0529124635337845,0.056184387547921,0.058868467062449,0.0608638204359008,0.0620933511678538,0.0625086897660101,0.0620933511678538,0.0608638204359008,0.058868467062449,0.056184387547921,0.0529124635337845,0.0491710769287093,0.0450890271592156,0.0407982389164979,0.0364268297193303,0.032093031414167,0.0279003411588186,0.0239341326230306,0.0202598058665036,0.0169224123292658,0.0139475739581583,0.0113434322115875,0.00910331758321208,0.00720882221319581,0.00563298173082845,0.0043433194963578,0.00330456743695959,0.00248094346351084,0.00183792812225004,0.00134353685843452,0.000969125574798113,0.000689794823171552,0.000484472593087523,0.000335759186328262,0.000229612750318439,0.00015494355081806,0.000103171659248691,6.7788610604395e-05,4.39503620697143e-05,2.81175572853399e-05,1.77501540658392e-05,1.10569684959511e-05,6.79640771677201e-06,4.12222965612978e-06,2.46714315125772e-06,1.4570213720684e-06,8.49076658244412e-07,4.88244420086954e-07,2.77036557671203e-07,1.55112316119009e-07,8.56968562126755e-08],[0.0620933511678538,0.0663740112702154,0.0700100579457678,0.0728672226117885,0.0748364887338619,0.0758409903798247,0.0758409903798247,0.0748364887338619,0.0728672226117885,0.0700100579457677,0.0663740112702154,0.0620933511678538,0.0573193874649716,0.0522116465158969,0.0469291452651648,0.0416224180133429,0.0364268297193303,0.0314575468145585,0.0268063531341398,0.0225403172464719,0.0187021579699893,0.0153120318789528,0.0123703883861147,0.00986150621083171,0.00775733553752107,0.00602131445404234,0.00461189537455569,0.00348559554138405,0.0025994648146857,0.00191293539182902,0.00138907619276083,0.000995316584885769,0.000703729602943137,0.000490975483928793,0.000338005058827408,0.00022961275031844,0.000153914029363826,0.000101805167315859,6.64463061797361e-05,4.27938412261919e-05,2.71957541126059e-05,1.70541605697849e-05,1.05528313217781e-05,6.4434290675698e-06,3.88216968943221e-06,2.30802965107452e-06,1.35399686931992e-06,7.83796542560758e-07,4.47711689324066e-07,2.52349780080067e-07,1.40351427622702e-07],[0.0719021080387363,0.0773730800144535,0.0821575640781251,0.0860824521890354,0.0890002266775096,0.0907981505190803,0.0914054937505614,0.0907981505190803,0.0890002266775096,0.0860824521890354,0.0821575640781251,0.0773730800144535,0.0719021080387363,0.0659329895675022,0.0596586360436505,0.0532663917405213,0.0469291452651648,0.0407982389164979,0.0349985132961325,0.0296256019035174,0.0247453827651371,0.0203953224590091,0.0165873261142962,0.0133116410146566,0.0105413496303274,0.00823702237755498,0.00635116916651322,0.00483221803781216,0.00362784539394948,0.00268757396965402,0.00196463324342568,0.00141713739325387,0.00100867633979527,0.000708436806875121,0.000490975483928793,0.000335759186328263,0.000226571566593468,0.00015086632738572,9.91262406259643e-05,6.42679371545978e-05,4.11158707108866e-05,2.59557767505599e-05,1.61684346374891e-05,9.93828226774761e-06,6.02787290026731e-06,3.60766541001237e-06,2.13057989885193e-06,1.24159171259911e-06,7.139522913704e-07,4.05106288992507e-07,2.26818349492277e-07],[0.082157564078125,0.0890002266775096,0.0951358227159593,0.100347475368743,0.104442733526793,0.107265340584658,0.108705122340791,0.108705122340791,0.107265340584658,0.104442733526793,0.100347475368743,0.0951358227159593,0.0890002266775095,0.082157564078125,0.0748364887338619,0.0672649243087358,0.0596586360436505,0.0522116465158969,0.0450890271592156,0.0384223344442544,0.0323076997228899,0.0268063531341398,0.0219471856887886,0.0177308415433537,0.0141347869238546,0.0111188172045432,0.00863052712129622,0.00661036529722545,0.00499600661672614,0.00372588938100366,0.00274186656526205,0.00199100376614959,0.00142661653791652,0.00100867633979527,0.000703729602943136,0.000484472593087523,0.000329110708989519,0.000220609505600676,0.000145920340868057,9.52394451362982e-05,6.13376713915257e-05,3.89804743207482e-05,2.44442299853079e-05,1.51256835403982e-05,9.23555641318089e-06,5.56442800817777e-06,3.30816678856254e-06,1.94072354001886e-06,1.12343863948302e-06,6.41718333550687e-07,3.61700362546594e-07],[0.0926323948315168,0.101018693445466,0.108705122340791,0.115427071698136,0.120941333792791,0.125040654030181,0.127566642799412,0.128419928209127,0.127566642799412,0.125040654030181,0.120941333792791,0.115427071698136,0.108705122340791,0.101018693445466,0.0926323948315168,0.0838172569658373,0.0748364887338619,0.0659329895675022,0.0573193874649716,0.0491710769287093,0.0416224180133429,0.0347659658934533,0.0286543591475523,0.0233043238581731,0.0187021579699893,0.0148100437644169,0.0115725847427643,0.00892305981775455,0.00678901308930213,0.00509693264516654,0.00377590057312685,0.00276020674169204,0.00199100376614959,0.00141713739325386,0.000995316584885768,0.000689794823171553,0.000471724059841486,0.000318321176575711,0.000211959283157745,0.000139267172929075,9.02930834558448e-05,5.77656434891841e-05,3.64665060069057e-05,2.27158032871253e-05,1.39627657264129e-05,8.46884550745414e-06,5.06858082535648e-06,2.99335309539525e-06,1.74437128507985e-06,1.0030655515382e-06,5.69153104642183e-07],[0.103059406420816,0.113141462542196,0.122564683189792,0.131014182845091,0.138191294411314,0.143830988122888,0.147718078670085,0.14970084210104,0.14970084210104,0.147718078670085,0.143830988122888,0.138191294411314,0.131014182845091,0.122564683189792,0.113141462542196,0.103059406420816,0.0926323948315168,0.082157564078125,0.0719021080387363,0.0620933511678538,0.0529124635337845,0.0444918302902208,0.0369157731616181,0.0302240787610676,0.0244176341743938,0.019465407515813,0.0153120318789528,0.0118853385195915,0.00910331758321208,0.00688013942269971,0.00513102571342465,0.00377590057312684,0.00274186656526205,0.00196463324342567,0.00138907619276083,0.000969125574798113,0.000667180659000664,0.000453227494889757,0.00030380747523912,0.000200950952790768,0.000131156884157148,8.44698102729853e-05,5.36810934540873e-05,3.36628277905811e-05,2.08299987580417e-05,1.27185411556824e-05,7.66292830271023e-06,4.5557683335836e-06,2.67262426986033e-06,1.54711854195989e-06,8.83728134016053e-07],[0.113141462542196,0.125040654030181,0.136360973077182,0.146736566820343,0.155810249364435,0.163253724620188,0.168787228147703,0.172196956335689,0.173348771161368,0.172196956335689,0.168787228147703,0.163253724620188,0.155810249364435,0.146736566820343,0.136360973077182,0.125040654030181,0.113141462542196,0.101018693445466,0.0890002266775096,0.0773730800144534,0.0663740112702154,0.056184387547921,0.0469291452651648,0.0386793390707695,0.0314575468145585,0.0252452726564679,0.0199914680159842,0.01562135544144,0.0120448708855235,0.00916420911334324,0.00688013942269972,0.00509693264516655,0.00372588938100366,0.00268757396965402,0.00191293539182903,0.00134353685843452,0.000931125617478451,0.000636760876922538,0.000429688643838679,0.000286115105217978,0.000187991019984523,0.000121882913966067,7.79754626391706e-05,4.92246343149449e-05,3.06631271382899e-05,1.88477622939188e-05,1.14317456982374e-05,6.84186846901829e-06,4.04060404000416e-06,2.35465494284771e-06,1.35399686931993e-06],[0.122564683189792,0.136360973077182,0.14970084210104,0.162168986268107,0.173348771161368,0.18284502144111,0.190307068323345,0.195450193709596,0.198073647115905,0.198073647115905,0.195450193709596,0.190307068323345,0.18284502144111,0.173348771161368,0.162168986268107,0.14970084210104,0.136360973077182,0.122564683189792,0.108705122340791,0.0951358227159593,0.082157564078125,0.0700100579457678,0.058868467062449,0.0488443600156232,0.0399903796592033,0.03230769972289,0.0257552609934694,0.0202598058665036,0.015725845724994,0.0120448708855235,0.00910331758321209,0.00678901308930213,0.00499600661672615,0.00362784539394947,0.0025994648146857,0.00183792812225004,0.00128227893991404,0.000882766620138716,0.000599678810259651,0.000401976727699847,0.000265884196455079,0.000173537583521609,0.000111764524214674,7.1027055107975e-05,4.4540291017299e-05,2.75607923477169e-05,1.6828280972524e-05,1.01390488871202e-05,6.0278729002673e-06,3.53622884862879e-06,2.04703866608716e-06],[0.131014182845091,0.146736566820343,0.162168986268107,0.176850648630819,0.190307068323345,0.202075000211633,0.211728667220171,0.218905234429676,0.223327413504027,0.224821236805718,0.223327413504027,0.218905234429676,0.211728667220171,0.202075000211633,0.190307068323345,0.176850648630819,0.162168986268107,0.146736566820343,0.131014182845091,0.115427071698136,0.100347475368743,0.0860824521890354,0.0728672226117885,0.0608638204359008,0.0501643985732279,0.0407982389164979,0.0327413536542539,0.0259275363465463,0.0202598058665036,0.01562135544144,0.0118853385195915,0.00892305981775454,0.00661036529722545,0.00483221803781215,0.00348559554138405,0.00248094346351084,0.00174247337424814,0.00120760482777305,0.000825834339292521,0.000557276130092996,0.000371071288206739,0.000243811209840913,0.000158073618173088,0.000101128723518423,6.38409092481723e-05,3.9767932136906e-05,2.44442299853079e-05,1.48261749391561e-05,8.8734250664797e-06,5.24038094778467e-06,3.05382283909547e-06],[0.138191294411314,0.155810249364435,0.173348771161368,0.190307068323345,0.206157186001815,0.220369478048149,0.23244157814751,0.24192769890621,0.248465892683478,0.251800955597156,0.251800955597156,0.248465892683478,0.24192769890621,0.23244157814751,0.220369478048149,0.206157186001815,0.190307068323345,0.173348771161368,0.155810249364435,0.138191294411314,0.120941333792791,0.104442733526793,0.0890002266775096,0.0748364887338619,0.0620933511678538,0.0508377361627926,0.0410711358215629,0.0327413536542539,0.0257552609934693,0.0199914680159841,0.0153120318789528,0.0115725847427643,0.00863052712129622,0.00635116916651322,0.00461189537455569,0.00330456743695959,0.00233646456376218,0.00163009601284075,0.00112221631578344,0.000762341178008333,0.00051101257353741,0.000338005058827408,0.000220609505600676,0.000142080556443981,9.02930834558446e-05,5.66218071108094e-05,3.50366338542196e-05,2.1392937887691e-05,1.28892572828188e-05,7.66292830271023e-06,4.49542791916139e-06],[0.143830988122888,0.163253724620188,0.18284502144111,0.202075000211633,0.220369478048148,0.237137209504023,0.251800955597156,0.263830165421334,0.272772718826595,0.278283093269634,0.28014451172548,0.278283093269634,0.272772718826595,0.263830165421334,0.251800955597156,0.237137209504023,0.220369478048148,0.202075000211633,0.18284502144111,0.163253724620188,0.143830988122888,0.125040654030181,0.107265340584658,0.0907981505190802,0.0758409903798246,0.0625086897660101,0.0508377361627926,0.0407982389164979,0.0323076997228899,0.0252452726564679,0.019465407515813,0.0148100437644169,0.0111188172045432,0.00823702237755496,0.00602131445404234,0.0043433194963578,0.00309144591978373,0.00217125552531869,0.00150476827563298,0.00102905295351241,0.000694408818220669,0.0004623832976235,0.00030380747523912,0.00019697185732524,0.000126014149170312,7.95506714736882e-05,4.95538948593765e-05,3.04593861818184e-05,1.84745515953002e-05,1.10569684959511e-05,6.52991675844793e-06],[0.147718078670085,0.168787228147703,0.190307068323345,0.211728667220171,0.23244157814751,0.251800955597156,0.269159888302326,0.283904784660471,0.295491158719388,0.303476926632329,0.307550381673732,0.307550381673732,0.303476926632329,0.295491158719388,0.283904784660471,0.269159888302326,0.251800955597156,0.23244157814751,0.211728667220171,0.190307068323345,0.168787228147703,0.147718078670085,0.127566642799412,0.10870512234079,0.0914054937505614,0.0758409903798247,0.0620933511678538,0.0501643985732279,0.0399903796592033,0.0314575468145585,0.0244176341743939,0.0187021579699893,0.0141347869238546,0.0105413496303274,0.00775733553752107,0.00563298173082845,0.00403620778202872,0.00285376426252263,0.00199100376614959,0.00137067810335024,0.000931125617478453,0.000624152166773117,0.000412840311123887,0.000269453058617018,0.00017353758352161,0.000110284221175755,6.91580313771556e-05,4.2793841226192e-05,2.6129393341175e-05,1.57429743958709e-05,9.35952176451393e-06],[0.14970084210104,0.172196956335689,0.195450193709596,0.218905234429676,0.24192769890621,0.263830165421334,0.283904784660471,0.301460476091579,0.31586205495772,0.326568235165143,0.333165351125652,0.335393873621027,0.333165351125652,0.326568235165143,0.31586205495772,0.301460476091579,0.283904784660471,0.263830165421334,0.24192769890621,0.218905234429676,0.195450193709596,0.172196956335689,0.14970084210104,0.128419928209127,0.10870512234079,0.0907981505190803,0.0748364887338619,0.0608638204359008,0.0488443600156232,0.0386793390707695,0.0302240787610676,0.0233043238581731,0.0177308415433538,0.0133116410146566,0.0098615062108317,0.00720882221319582,0.00519989751462503,0.00370113273231715,0.0025994648146857,0.00180153470706267,0.00123200006351684,0.000831358294278681,0.000553573312332375,0.000363723584402474,0.000235818127636128,0.00015086632738572,9.52394451362982e-05,5.93267833359583e-05,3.64665060069055e-05,2.21180539457832e-05,1.32375946668436e-05],[0.149700842101039,0.173348771161368,0.198073647115905,0.223327413504027,0.248465892683478,0.272772718826595,0.295491158719388,0.315862054957721,0.333165351125652,0.346762087039169,0.356133472498222,0.360913716271805,0.360913716271805,0.356133472498222,0.346762087039169,0.333165351125652,0.315862054957721,0.295491158719388,0.272772718826595,0.248465892683478,0.223327413504027,0.198073647115905,0.173348771161368,0.14970084210104,0.127566642799412,0.107265340584658,0.0890002266775096,0.0728672226117885,0.058868467062449,0.0469291452651648,0.0369157731616182,0.0286543591475523,0.0219471856887886,0.0165873261142962,0.0123703883861147,0.00910331758321209,0.00661036529722545,0.00473653370979245,0.00334892338531823,0.00233646456376218,0.00160850565491194,0.00109268603436992,0.000732449352861348,0.000484472593087523,0.000316206093509063,0.000203648240788258,0.000129419732448614,8.11577016374849e-05,5.02190378904336e-05,3.066312713829e-05,1.84745515953003e-05],[0.147718078670085,0.172196956335689,0.198073647115905,0.224821236805718,0.251800955597156,0.278283093269634,0.303476926632329,0.326568235165143,0.346762087039169,0.363327845871067,0.37564288444951,0.383231374086906,0.385794785123499,0.383231374086906,0.37564288444951,0.363327845871067,0.346762087039169,0.326568235165143,0.303476926632329,0.278283093269634,0.251800955597156,0.224821236805718,0.198073647115905,0.172196956335689,0.147718078670085,0.125040654030181,0.104442733526793,0.0860824521890354,0.0700100579457677,0.056184387547921,0.0444918302902209,0.0347659658934533,0.0268063531341398,0.0203953224590091,0.0153120318789528,0.0113434322115875,0.00829211931245912,0.00598130586781599,0.00425731600807728,0.00299009626738422,0.00207225817126612,0.00141713739325386,0.000956289663371403,0.00063676087692254,0.000418381709164601,0.000271255413515409,0.00017353758352161,0.000109551438357984,6.82420444418324e-05,4.19464663956132e-05,2.54418179355451e-05],[0.143830988122888,0.168787228147703,0.195450193709596,0.223327413504027,0.251800955597156,0.28014451172548,0.307550381673732,0.333165351125652,0.356133472498222,0.37564288444951,0.390973161383756,0.401539368983775,0.406929078788273,0.406929078788273,0.401539368983775,0.390973161383756,0.37564288444951,0.356133472498221,0.333165351125652,0.307550381673732,0.28014451172548,0.251800955597156,0.223327413504027,0.195450193709596,0.168787228147703,0.143830988122888,0.120941333792791,0.100347475368743,0.082157564078125,0.0663740112702153,0.0529124635337845,0.0416224180133429,0.0323076997228899,0.0247453827651371,0.0187021579699893,0.0139475739581583,0.0102639619139988,0.00745316606041125,0.00534042684519056,0.00377590057312684,0.00263435643946864,0.00181358506166083,0.00123200006351684,0.000825834339292521,0.000546241323382677,0.000356521374881682,0.000229612750318439,0.000145920340868057,9.15050530776825e-05,5.66218071108094e-05,3.45725793080001e-05],[0.138191294411314,0.163253724620188,0.190307068323345,0.218905234429676,0.248465892683478,0.278283093269634,0.307550381673732,0.335393873621027,0.360913716271805,0.383231374086905,0.401539368983775,0.415149591475649,0.423536169535017,0.426369176863734,0.423536169535017,0.415149591475649,0.401539368983775,0.383231374086905,0.360913716271805,0.335393873621026,0.307550381673732,0.278283093269634,0.248465892683478,0.218905234429676,0.190307068323345,0.163253724620188,0.138191294411314,0.115427071698136,0.0951358227159593,0.0773730800144534,0.0620933511678538,0.0491710769287093,0.0384223344442544,0.0296256019035173,0.0225403172464719,0.0169224123292658,0.0125364313914091,0.00916420911334324,0.00661036529722545,0.00470506184118491,0.00330456743695959,0.00229019946562793,0.0015661790339417,0.00105686352521443,0.000703729602943135,0.0004623832976235,0.000299783594387813,0.000191788690501207,0.000121073063706601,7.5419122907139e-05,4.63580147764693e-05],[0.131014182845091,0.155810249364435,0.18284502144111,0.211728667220171,0.24192769890621,0.272772718826595,0.303476926632329,0.333165351125652,0.360913716271805,0.385794785123499,0.406929078788273,0.423536169535017,0.434982405582487,0.44082100851222,0.44082100851222,0.434982405582487,0.423536169535017,0.406929078788273,0.385794785123499,0.360913716271805,0.333165351125652,0.303476926632329,0.272772718826595,0.24192769890621,0.211728667220171,0.18284502144111,0.155810249364435,0.131014182845091,0.10870512234079,0.0890002266775095,0.0719021080387363,0.0573193874649716,0.0450890271592156,0.0349985132961325,0.0268063531341398,0.0202598058665036,0.015109226494313,0.0111188172045432,0.00807391840647743,0.00578521533725912,0.00409038425969478,0.00285376426252262,0.00196463324342567,0.00133460973618253,0.000894615659797481,0.000591736161450109,0.000386214994759022,0.000248736522993245,0.000158073618173088,9.91262406259641e-05,6.13376713915258e-05],[0.122564683189792,0.146736566820343,0.173348771161368,0.202075000211633,0.23244157814751,0.263830165421334,0.295491158719388,0.326568235165143,0.356133472498222,0.383231374086905,0.406929078788273,0.426369176863734,0.44082100851222,0.449726183596113,0.452734374314375,0.449726183596113,0.44082100851222,0.426369176863734,0.406929078788273,0.383231374086905,0.356133472498222,0.326568235165143,0.295491158719388,0.263830165421334,0.23244157814751,0.202075000211633,0.173348771161368,0.146736566820343,0.122564683189792,0.101018693445466,0.082157564078125,0.0659329895675022,0.0522116465158969,0.0407982389164979,0.0314575468145585,0.0239341326230306,0.0179688358669242,0.0133116410146566,0.0097308921567319,0.00701912745860916,0.00499600661672614,0.00350891047508742,0.0024318174914824,0.00166302613667241,0.00112221631578344,0.000747245811290878,0.000490975483928793,0.000318321176575711,0.000203648240788258,0.000128559803845884,8.00827810111967e-05],[0.113141462542196,0.136360973077182,0.162168986268107,0.190307068323345,0.220369478048148,0.251800955597156,0.283904784660471,0.31586205495772,0.346762087039168,0.375642884449509,0.401539368983775,0.423536169535017,0.44082100851222,0.452734374314375,0.458811255149873,0.458811255149873,0.452734374314375,0.44082100851222,0.423536169535017,0.401539368983775,0.37564288444951,0.346762087039169,0.31586205495772,0.283904784660471,0.251800955597156,0.220369478048149,0.190307068323345,0.162168986268107,0.136360973077182,0.113141462542196,0.0926323948315168,0.0748364887338619,0.0596586360436505,0.0469291452651648,0.0364268297193302,0.0279003411588186,0.0210866242289031,0.015725845724994,0.0115725847427643,0.00840342126741195,0.00602131445404235,0.00425731600807727,0.00297022859143874,0.00204481144709398,0.00138907619276083,0.000931125617478452,0.00061588537231652,0.000401976727699847,0.000258887653066522,0.000164524724910124,0.000103171659248691],[0.103059406420816,0.125040654030181,0.14970084210104,0.176850648630819,0.206157186001815,0.237137209504023,0.269159888302326,0.301460476091579,0.333165351125652,0.363327845871067,0.390973161383756,0.415149591475649,0.434982405582487,0.449726183596113,0.458811255149873,0.461880215351701,0.458811255149873,0.449726183596113,0.434982405582487,0.415149591475649,0.390973161383756,0.363327845871067,0.333165351125652,0.301460476091579,0.269159888302325,0.237137209504023,0.206157186001815,0.176850648630819,0.149700842101039,0.125040654030181,0.103059406420816,0.0838172569658372,0.0672649243087358,0.0532663917405212,0.0416224180133429,0.032093031414167,0.0244176341743939,0.0183318304301569,0.0135805540011078,0.00992746921795373,0.00716092323909167,0.00509693264516654,0.0035797951687181,0.00248094346351084,0.0016966214931327,0.00114488658916215,0.000762341178008332,0.000500893846624439,0.000324751690901433,0.000207762208146272,0.000131156884157148],[0.0926323948315168,0.113141462542196,0.136360973077182,0.162168986268107,0.190307068323345,0.220369478048148,0.251800955597156,0.283904784660471,0.31586205495772,0.346762087039168,0.37564288444951,0.401539368983775,0.423536169535017,0.44082100851222,0.452734374314375,0.458811255149873,0.458811255149873,0.452734374314375,0.44082100851222,0.423536169535017,0.401539368983775,0.37564288444951,0.346762087039168,0.31586205495772,0.283904784660471,0.251800955597156,0.220369478048148,0.190307068323345,0.162168986268107,0.136360973077182,0.113141462542196,0.0926323948315168,0.0748364887338619,0.0596586360436505,0.0469291452651648,0.0364268297193303,0.0279003411588186,0.0210866242289031,0.015725845724994,0.0115725847427642,0.00840342126741196,0.00602131445404233,0.00425731600807727,0.00297022859143875,0.00204481144709398,0.00138907619276083,0.000931125617478449,0.00061588537231652,0.000401976727699846,0.000258887653066522,0.000164524724910124],[0.082157564078125,0.101018693445466,0.122564683189792,0.146736566820343,0.173348771161368,0.202075000211633,0.23244157814751,0.263830165421334,0.295491158719388,0.326568235165143,0.356133472498221,0.383231374086905,0.406929078788273,0.426369176863734,0.44082100851222,0.449726183596113,0.452734374314375,0.449726183596113,0.44082100851222,0.426369176863734,0.406929078788273,0.383231374086905,0.356133472498221,0.326568235165143,0.295491158719388,0.263830165421334,0.23244157814751,0.202075000211633,0.173348771161368,0.146736566820343,0.122564683189792,0.101018693445466,0.082157564078125,0.0659329895675022,0.0522116465158968,0.0407982389164979,0.0314575468145585,0.0239341326230305,0.0179688358669242,0.0133116410146566,0.00973089215673191,0.00701912745860915,0.00499600661672614,0.00350891047508742,0.00243181749148239,0.00166302613667241,0.00112221631578344,0.000747245811290877,0.000490975483928792,0.000318321176575711,0.000203648240788259],[0.0719021080387363,0.0890002266775096,0.108705122340791,0.131014182845091,0.155810249364435,0.18284502144111,0.211728667220171,0.24192769890621,0.272772718826595,0.303476926632328,0.333165351125652,0.360913716271805,0.385794785123499,0.406929078788272,0.423536169535017,0.434982405582487,0.44082100851222,0.44082100851222,0.434982405582487,0.423536169535017,0.406929078788273,0.385794785123499,0.360913716271805,0.333165351125652,0.303476926632328,0.272772718826595,0.24192769890621,0.211728667220171,0.18284502144111,0.155810249364435,0.131014182845091,0.108705122340791,0.0890002266775096,0.0719021080387363,0.0573193874649715,0.0450890271592156,0.0349985132961325,0.0268063531341398,0.0202598058665036,0.015109226494313,0.0111188172045432,0.00807391840647743,0.00578521533725912,0.00409038425969478,0.00285376426252262,0.00196463324342568,0.00133460973618252,0.000894615659797481,0.000591736161450109,0.000386214994759022,0.000248736522993245],[0.0620933511678538,0.0773730800144534,0.0951358227159593,0.115427071698136,0.138191294411314,0.163253724620188,0.190307068323345,0.218905234429676,0.248465892683478,0.278283093269634,0.307550381673732,0.335393873621026,0.360913716271805,0.383231374086905,0.401539368983775,0.415149591475649,0.423536169535017,0.426369176863734,0.423536169535017,0.415149591475649,0.401539368983775,0.383231374086905,0.360913716271805,0.335393873621026,0.307550381673732,0.278283093269634,0.248465892683478,0.218905234429676,0.190307068323345,0.163253724620188,0.138191294411314,0.115427071698136,0.0951358227159593,0.0773730800144534,0.0620933511678538,0.0491710769287093,0.0384223344442544,0.0296256019035173,0.0225403172464719,0.0169224123292658,0.0125364313914091,0.00916420911334324,0.00661036529722545,0.00470506184118492,0.00330456743695959,0.00229019946562794,0.0015661790339417,0.00105686352521443,0.000703729602943135,0.0004623832976235,0.000299783594387814],[0.0529124635337845,0.0663740112702154,0.082157564078125,0.100347475368743,0.120941333792791,0.143830988122888,0.168787228147703,0.195450193709596,0.223327413504027,0.251800955597156,0.28014451172548,0.307550381673732,0.333165351125652,0.356133472498222,0.37564288444951,0.390973161383756,0.401539368983775,0.406929078788273,0.406929078788273,0.401539368983775,0.390973161383756,0.37564288444951,0.356133472498221,0.333165351125652,0.307550381673732,0.28014451172548,0.251800955597156,0.223327413504027,0.195450193709596,0.168787228147703,0.143830988122888,0.120941333792791,0.100347475368743,0.082157564078125,0.0663740112702154,0.0529124635337845,0.0416224180133429,0.0323076997228899,0.0247453827651371,0.0187021579699893,0.0139475739581584,0.0102639619139988,0.00745316606041126,0.00534042684519057,0.00377590057312684,0.00263435643946864,0.00181358506166083,0.00123200006351684,0.000825834339292519,0.000546241323382677,0.000356521374881683],[0.0444918302902208,0.056184387547921,0.0700100579457678,0.0860824521890354,0.104442733526793,0.125040654030181,0.147718078670085,0.172196956335689,0.198073647115905,0.224821236805718,0.251800955597156,0.278283093269634,0.303476926632329,0.326568235165143,0.346762087039168,0.363327845871067,0.37564288444951,0.383231374086905,0.385794785123499,0.383231374086905,0.37564288444951,0.363327845871067,0.346762087039168,0.326568235165143,0.303476926632328,0.278283093269634,0.251800955597156,0.224821236805718,0.198073647115905,0.172196956335689,0.147718078670085,0.125040654030181,0.104442733526793,0.0860824521890353,0.0700100579457677,0.056184387547921,0.0444918302902208,0.0347659658934533,0.0268063531341398,0.020395322459009,0.0153120318789528,0.0113434322115875,0.00829211931245911,0.00598130586781599,0.00425731600807727,0.00299009626738422,0.00207225817126611,0.00141713739325386,0.000956289663371401,0.000636760876922539,0.000418381709164602],[0.0369157731616181,0.0469291452651648,0.058868467062449,0.0728672226117884,0.0890002266775095,0.107265340584658,0.127566642799412,0.14970084210104,0.173348771161368,0.198073647115905,0.223327413504027,0.248465892683478,0.272772718826595,0.295491158719388,0.31586205495772,0.333165351125652,0.346762087039168,0.356133472498221,0.360913716271805,0.360913716271805,0.356133472498221,0.346762087039168,0.333165351125652,0.31586205495772,0.295491158719388,0.272772718826595,0.248465892683478,0.223327413504027,0.198073647115905,0.173348771161368,0.14970084210104,0.127566642799412,0.107265340584658,0.0890002266775095,0.0728672226117884,0.058868467062449,0.0469291452651648,0.0369157731616181,0.0286543591475523,0.0219471856887886,0.0165873261142962,0.0123703883861146,0.00910331758321207,0.00661036529722545,0.00473653370979244,0.00334892338531822,0.00233646456376218,0.00160850565491194,0.00109268603436992,0.000732449352861347,0.000484472593087523],[0.0302240787610676,0.0386793390707695,0.0488443600156232,0.0608638204359008,0.0748364887338618,0.0907981505190802,0.10870512234079,0.128419928209127,0.149700842101039,0.172196956335689,0.195450193709596,0.218905234429676,0.24192769890621,0.263830165421334,0.283904784660471,0.301460476091579,0.31586205495772,0.326568235165143,0.333165351125652,0.335393873621026,0.333165351125652,0.326568235165143,0.31586205495772,0.301460476091579,0.283904784660471,0.263830165421334,0.24192769890621,0.218905234429676,0.195450193709596,0.172196956335689,0.14970084210104,0.128419928209127,0.10870512234079,0.0907981505190802,0.0748364887338618,0.0608638204359008,0.0488443600156232,0.0386793390707695,0.0302240787610676,0.023304323858173,0.0177308415433537,0.0133116410146566,0.00986150621083171,0.00720882221319582,0.00519989751462502,0.00370113273231715,0.0025994648146857,0.00180153470706267,0.00123200006351684,0.000831358294278681,0.000553573312332375],[0.0244176341743938,0.0314575468145585,0.0399903796592033,0.0501643985732279,0.0620933511678538,0.0758409903798246,0.0914054937505614,0.10870512234079,0.127566642799412,0.147718078670085,0.168787228147703,0.190307068323345,0.211728667220171,0.23244157814751,0.251800955597155,0.269159888302325,0.283904784660471,0.295491158719388,0.303476926632328,0.307550381673732,0.307550381673732,0.303476926632328,0.295491158719388,0.283904784660471,0.269159888302325,0.251800955597156,0.23244157814751,0.211728667220171,0.190307068323345,0.168787228147703,0.147718078670085,0.127566642799412,0.10870512234079,0.0914054937505613,0.0758409903798246,0.0620933511678538,0.0501643985732279,0.0399903796592033,0.0314575468145585,0.0244176341743938,0.0187021579699894,0.0141347869238546,0.0105413496303273,0.00775733553752108,0.00563298173082844,0.00403620778202872,0.00285376426252262,0.00199100376614959,0.00137067810335024,0.000931125617478451,0.00062415216677312],[0.019465407515813,0.0252452726564679,0.03230769972289,0.0407982389164979,0.0508377361627926,0.0625086897660101,0.0758409903798247,0.0907981505190803,0.107265340584658,0.125040654030181,0.143830988122888,0.163253724620188,0.18284502144111,0.202075000211633,0.220369478048149,0.237137209504023,0.251800955597156,0.263830165421334,0.272772718826595,0.278283093269634,0.28014451172548,0.278283093269634,0.272772718826595,0.263830165421334,0.251800955597156,0.237137209504023,0.220369478048148,0.202075000211633,0.18284502144111,0.163253724620188,0.143830988122888,0.125040654030181,0.107265340584658,0.0907981505190802,0.0758409903798246,0.0625086897660101,0.0508377361627925,0.0407982389164979,0.0323076997228899,0.0252452726564678,0.019465407515813,0.0148100437644168,0.0111188172045432,0.00823702237755497,0.00602131445404233,0.00434331949635779,0.00309144591978372,0.00217125552531869,0.00150476827563297,0.00102905295351241,0.000694408818220669],[0.0153120318789528,0.0199914680159841,0.0257552609934693,0.0327413536542539,0.0410711358215629,0.0508377361627925,0.0620933511678538,0.0748364887338618,0.0890002266775095,0.104442733526793,0.120941333792791,0.138191294411314,0.155810249364435,0.173348771161368,0.190307068323345,0.206157186001815,0.220369478048148,0.23244157814751,0.24192769890621,0.248465892683478,0.251800955597155,0.251800955597155,0.248465892683478,0.24192769890621,0.23244157814751,0.220369478048148,0.206157186001815,0.190307068323345,0.173348771161368,0.155810249364435,0.138191294411314,0.120941333792791,0.104442733526793,0.0890002266775095,0.0748364887338618,0.0620933511678538,0.0508377361627925,0.0410711358215629,0.0327413536542539,0.0257552609934693,0.0199914680159842,0.0153120318789527,0.0115725847427643,0.00863052712129623,0.00635116916651321,0.00461189537455569,0.00330456743695958,0.00233646456376218,0.00163009601284075,0.00112221631578344,0.000762341178008334],[0.0118853385195915,0.01562135544144,0.0202598058665036,0.0259275363465463,0.0327413536542539,0.0407982389164979,0.0501643985732279,0.0608638204359008,0.0728672226117885,0.0860824521890354,0.100347475368743,0.115427071698136,0.131014182845091,0.146736566820343,0.162168986268107,0.176850648630819,0.190307068323345,0.202075000211633,0.211728667220171,0.218905234429676,0.223327413504027,0.224821236805718,0.223327413504027,0.218905234429676,0.211728667220171,0.202075000211633,0.190307068323345,0.176850648630819,0.162168986268107,0.146736566820343,0.131014182845091,0.115427071698136,0.100347475368743,0.0860824521890353,0.0728672226117884,0.0608638204359008,0.0501643985732279,0.0407982389164979,0.0327413536542539,0.0259275363465463,0.0202598058665036,0.01562135544144,0.0118853385195915,0.00892305981775455,0.00661036529722544,0.00483221803781215,0.00348559554138405,0.00248094346351084,0.00174247337424814,0.00120760482777305,0.000825834339292521],[0.00910331758321209,0.0120448708855235,0.015725845724994,0.0202598058665036,0.0257552609934694,0.03230769972289,0.0399903796592033,0.0488443600156233,0.058868467062449,0.0700100579457678,0.082157564078125,0.0951358227159593,0.108705122340791,0.122564683189792,0.136360973077182,0.14970084210104,0.162168986268107,0.173348771161368,0.18284502144111,0.190307068323345,0.195450193709596,0.198073647115905,0.198073647115905,0.195450193709596,0.190307068323345,0.18284502144111,0.173348771161368,0.162168986268107,0.14970084210104,0.136360973077182,0.122564683189792,0.108705122340791,0.0951358227159593,0.082157564078125,0.0700100579457677,0.058868467062449,0.0488443600156232,0.0399903796592033,0.0323076997228899,0.0257552609934693,0.0202598058665036,0.015725845724994,0.0120448708855235,0.00910331758321209,0.00678901308930212,0.00499600661672614,0.00362784539394947,0.0025994648146857,0.00183792812225004,0.00128227893991404,0.000882766620138716],[0.00688013942269971,0.00916420911334324,0.0120448708855235,0.01562135544144,0.0199914680159841,0.0252452726564679,0.0314575468145585,0.0386793390707695,0.0469291452651648,0.056184387547921,0.0663740112702153,0.0773730800144534,0.0890002266775095,0.101018693445466,0.113141462542196,0.125040654030181,0.136360973077182,0.146736566820343,0.155810249364435,0.163253724620188,0.168787228147703,0.172196956335689,0.173348771161368,0.172196956335689,0.168787228147703,0.163253724620188,0.155810249364435,0.146736566820343,0.136360973077182,0.125040654030181,0.113141462542196,0.101018693445466,0.0890002266775095,0.0773730800144534,0.0663740112702153,0.056184387547921,0.0469291452651648,0.0386793390707695,0.0314575468145585,0.0252452726564679,0.0199914680159842,0.01562135544144,0.0120448708855235,0.00916420911334325,0.00688013942269972,0.00509693264516655,0.00372588938100366,0.00268757396965402,0.00191293539182902,0.00134353685843452,0.000931125617478453],[0.00513102571342465,0.00688013942269972,0.00910331758321209,0.0118853385195915,0.0153120318789528,0.019465407515813,0.0244176341743939,0.0302240787610676,0.0369157731616182,0.0444918302902208,0.0529124635337845,0.0620933511678538,0.0719021080387363,0.082157564078125,0.0926323948315168,0.103059406420816,0.113141462542196,0.122564683189792,0.131014182845091,0.138191294411314,0.143830988122888,0.147718078670085,0.14970084210104,0.14970084210104,0.147718078670085,0.143830988122888,0.138191294411314,0.131014182845091,0.122564683189792,0.113141462542196,0.103059406420816,0.0926323948315168,0.082157564078125,0.0719021080387363,0.0620933511678538,0.0529124635337845,0.0444918302902208,0.0369157731616182,0.0302240787610676,0.0244176341743938,0.019465407515813,0.0153120318789527,0.0118853385195915,0.00910331758321209,0.00688013942269971,0.00513102571342465,0.00377590057312684,0.00274186656526205,0.00196463324342567,0.00138907619276083,0.000969125574798113],[0.00377590057312684,0.00509693264516654,0.00678901308930212,0.00892305981775453,0.0115725847427642,0.0148100437644168,0.0187021579699893,0.023304323858173,0.0286543591475523,0.0347659658934532,0.0416224180133428,0.0491710769287092,0.0573193874649715,0.0659329895675021,0.0748364887338618,0.0838172569658371,0.0926323948315167,0.101018693445466,0.10870512234079,0.115427071698136,0.120941333792791,0.125040654030181,0.127566642799412,0.128419928209127,0.127566642799412,0.125040654030181,0.120941333792791,0.115427071698136,0.10870512234079,0.101018693445466,0.0926323948315167,0.0838172569658372,0.0748364887338618,0.0659329895675021,0.0573193874649715,0.0491710769287093,0.0416224180133429,0.0347659658934532,0.0286543591475523,0.023304323858173,0.0187021579699893,0.0148100437644168,0.0115725847427643,0.00892305981775455,0.00678901308930212,0.00509693264516655,0.00377590057312684,0.00276020674169204,0.00199100376614958,0.00141713739325386,0.000995316584885769],[0.00274186656526205,0.00372588938100366,0.00499600661672615,0.00661036529722545,0.00863052712129622,0.0111188172045432,0.0141347869238546,0.0177308415433538,0.0219471856887886,0.0268063531341398,0.0323076997228899,0.0384223344442544,0.0450890271592156,0.0522116465158969,0.0596586360436505,0.0672649243087358,0.0748364887338619,0.082157564078125,0.0890002266775096,0.0951358227159593,0.100347475368743,0.104442733526793,0.107265340584658,0.10870512234079,0.10870512234079,0.107265340584658,0.104442733526793,0.100347475368743,0.0951358227159592,0.0890002266775095,0.082157564078125,0.0748364887338619,0.0672649243087358,0.0596586360436505,0.0522116465158968,0.0450890271592156,0.0384223344442544,0.0323076997228899,0.0268063531341398,0.0219471856887886,0.0177308415433538,0.0141347869238546,0.0111188172045432,0.00863052712129623,0.00661036529722544,0.00499600661672614,0.00372588938100366,0.00274186656526205,0.00199100376614959,0.00142661653791651,0.00100867633979527],[0.00196463324342567,0.00268757396965401,0.00362784539394947,0.00483221803781215,0.00635116916651321,0.00823702237755496,0.0105413496303273,0.0133116410146566,0.0165873261142962,0.020395322459009,0.0247453827651371,0.0296256019035173,0.0349985132961325,0.0407982389164978,0.0469291452651647,0.0532663917405212,0.0596586360436504,0.0659329895675021,0.0719021080387362,0.0773730800144533,0.0821575640781249,0.0860824521890353,0.0890002266775095,0.0907981505190802,0.0914054937505612,0.0907981505190802,0.0890002266775095,0.0860824521890353,0.0821575640781249,0.0773730800144533,0.0719021080387362,0.0659329895675021,0.0596586360436505,0.0532663917405212,0.0469291452651647,0.0407982389164979,0.0349985132961325,0.0296256019035173,0.0247453827651371,0.020395322459009,0.0165873261142962,0.0133116410146566,0.0105413496303274,0.00823702237755497,0.00635116916651321,0.00483221803781215,0.00362784539394947,0.00268757396965401,0.00196463324342567,0.00141713739325386,0.00100867633979527],[0.00138907619276083,0.00191293539182903,0.0025994648146857,0.00348559554138405,0.00461189537455569,0.00602131445404234,0.00775733553752107,0.0098615062108317,0.0123703883861146,0.0153120318789527,0.0187021579699893,0.0225403172464719,0.0268063531341398,0.0314575468145585,0.0364268297193302,0.0416224180133429,0.0469291452651648,0.0522116465158968,0.0573193874649715,0.0620933511678538,0.0663740112702154,0.0700100579457677,0.0728672226117884,0.0748364887338618,0.0758409903798246,0.0758409903798246,0.0748364887338618,0.0728672226117884,0.0700100579457677,0.0663740112702153,0.0620933511678538,0.0573193874649715,0.0522116465158968,0.0469291452651648,0.0416224180133429,0.0364268297193303,0.0314575468145585,0.0268063531341398,0.0225403172464719,0.0187021579699893,0.0153120318789528,0.0123703883861146,0.0098615062108317,0.00775733553752108,0.00602131445404233,0.00461189537455569,0.00348559554138405,0.0025994648146857,0.00191293539182902,0.00138907619276083,0.000995316584885769],[0.000969125574798113,0.00134353685843452,0.00183792812225004,0.00248094346351084,0.00330456743695959,0.0043433194963578,0.00563298173082845,0.00720882221319582,0.00910331758321208,0.0113434322115875,0.0139475739581583,0.0169224123292658,0.0202598058665036,0.0239341326230306,0.0279003411588186,0.032093031414167,0.0364268297193303,0.0407982389164979,0.0450890271592156,0.0491710769287093,0.0529124635337845,0.056184387547921,0.058868467062449,0.0608638204359008,0.0620933511678538,0.0625086897660101,0.0620933511678538,0.0608638204359008,0.058868467062449,0.056184387547921,0.0529124635337845,0.0491710769287093,0.0450890271592156,0.0407982389164979,0.0364268297193303,0.032093031414167,0.0279003411588186,0.0239341326230306,0.0202598058665036,0.0169224123292658,0.0139475739581584,0.0113434322115875,0.00910331758321208,0.00720882221319582,0.00563298173082844,0.0043433194963578,0.00330456743695958,0.00248094346351084,0.00183792812225004,0.00134353685843452,0.000969125574798113]],"type":"surface","frame":null},{"colorbar":{"title":"","ticklen":2,"len":0.333333333333333,"lenmode":"fraction","y":0.666666666666667,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"y":[-2.5,-2.4,-2.3,-2.2,-2.1,-2,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.0999999999999996,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.4,2.5],"z":[[2.1855320581026e-06,3.70071472688659e-06,6.18334448419467e-06,1.0194611575158e-05,1.65854527439267e-05,2.66252310987588e-05,4.21763359280098e-05,6.59255431045717e-05,0.000101682915906592,0.000154757488873036,0.000232415326227089,0.00034441912547766,0.000503638779492395,0.000726708889631284,0.00103469223475733,0.00145368836219717,0.00201530528765178,0.00275689218337506,0.00372141519526626,0.00495685115543938,0.0065149792445367,0.00844947259624268,0.0108132333197937,0.0136549763748181,0.0170151483173813,0.0209213609372375,0.0253836184938987,0.0303897087997554,0.0359011989345458,0.041850511738228,0.0481395471212505,0.0546402445789954,0.0611973583747469,0.0676335407388234,0.0737566153930639,0.0793686953006767,0.0842765813218815,0.0883027005936735,0.0912957306538512,0.0931400267517807,0.0937630346490151,0.0931400267517807,0.0912957306538512,0.0883027005936735,0.0842765813218815,0.0793686953006767,0.0737566153930639,0.0676335407388234,0.0611973583747468,0.0546402445789954,0.0481395471212505],[3.46204447661178e-06,5.82325453414832e-06,9.6651436013547e-06,1.58292469826671e-05,2.55812408546774e-05,4.07936311689088e-05,6.41907618392879e-05,9.96694592696043e-05,0.000152707750973788,0.00023087104404574,0.00034441912547766,0.000507007588241112,0.000736463225893191,0.00105559440441471,0.00149297487732865,0.00208361428914125,0.00286940308774354,0.00389919722202856,0.00522839331207609,0.00691784306183354,0.00903197168106351,0.0116360033062816,0.0147922593162476,0.018555582579172,0.0229680478184292,0.028053236954984,0.0338104758697079,0.0402095297012098,0.0471863202218377,0.0546402445789954,0.0624336270200144,0.0703937178977472,0.0783174697738453,0.0859790811974574,0.0931400267517807,0.0995610169053231,0.105015086918652,0.109300833917683,0.112254733100793,0.113761485569737,0.113761485569737,0.112254733100793,0.109300833917683,0.105015086918652,0.0995610169053231,0.0931400267517807,0.0859790811974573,0.0783174697738453,0.0703937178977472,0.0624336270200143,0.0546402445789954],[5.41149811501855e-06,9.04180935040098e-06,1.49074234016214e-05,2.42526519562336e-05,3.89336651258399e-05,6.16738060663299e-05,9.64019057318968e-05,0.000148689360938946,0.000226299491078579,0.000339857349890201,0.000503638779492395,0.000736463225893191,0.00106265521031268,0.0015130145096929,0.0021257060898808,0.00294694986513851,0.00403136095448103,0.00544176809092422,0.00724832705671824,0.00952675374976984,0.0123555335663325,0.0158120244454911,0.019967461521985,0.0248809891714444,0.0305929836885136,0.0371180741477057,0.044438402855276,0.0524977699441988,0.0611973583747469,0.0703937178977473,0.0798995876107819,0.0894879540654759,0.0988994843512533,0.107853162058105,0.11605962002168,0.123236346117188,0.129123678283553,0.133500340016264,0.13619722577862,0.137108240625842,0.13619722577862,0.133500340016264,0.129123678283553,0.123236346117188,0.11605962002168,0.107853162058104,0.0988994843512533,0.0894879540654758,0.0798995876107819,0.0703937178977472,0.0611973583747469],[8.34664201226666e-06,1.38533346197713e-05,2.26885253105973e-05,3.66663449779618e-05,5.84707114811225e-05,9.20065070872886e-05,0.000142859167704448,0.000218880511302086,0.000330914258401014,0.000493666063484278,0.000726708889631284,0.00105559440441471,0.0015130145096929,0.00213992480687477,0.00298650564922438,0.00411279984789308,0.00558883407150549,0.00749400992508922,0.00991554794583817,0.0129457906819443,0.0166782258068148,0.0212021803857819,0.0265962623150306,0.032920778533183,0.0402095297012098,0.0484615495843349,0.0576335016663816,0.0676335407388234,0.0783174697738453,0.0894879540654758,0.100897386463104,0.112254733100793,0.123236346117187,0.133500340016264,0.142703734073939,0.150521213053114,0.156664100290189,0.160898010876987,0.163057683511186,0.163057683511186,0.160898010876987,0.156664100290189,0.150521213053114,0.142703734073939,0.133500340016264,0.123236346117188,0.112254733100793,0.100897386463104,0.0894879540654757,0.0783174697738452,0.0676335407388234],[1.27032682611812e-05,2.09441485896193e-05,3.40737049306879e-05,5.46997590103584e-05,8.66484652337762e-05,0.000135439625183767,0.000208900759393613,0.000317938924736618,0.000477481764863565,0.000707586089762228,0.00103469223475733,0.00149297487732865,0.0021257060898808,0.00298650564922438,0.00414031011253806,0.00566385085969026,0.00764539896774982,0.0101835196339532,0.0133845897266318,0.0173588771141464,0.0222150656466253,0.028053236954984,0.0349564857872596,0.0429815387213285,0.0521489488401799,0.0624336270200143,0.0737566153930639,0.0859790811974574,0.0988994843512533,0.112254733100793,0.125725885448756,0.138948592247275,0.151528040168199,0.163057683511186,0.173140607547204,0.181412000689187,0.187560981045271,0.191349964199118,0.19262989231369,0.191349964199118,0.187560981045271,0.181412000689186,0.173140607547204,0.163057683511186,0.151528040168199,0.138948592247275,0.125725885448756,0.112254733100793,0.0988994843512532,0.0859790811974573,0.0737566153930639],[1.90778117335236e-05,3.12449981370626e-05,5.04942416858717e-05,8.0521640181131e-05,0.000126704715409478,0.000196735326235722,0.000301426429186153,0.00045571121285868,0.000679841242334636,0.001000770988501,0.00145368836219717,0.00208361428914125,0.00294694986513851,0.00411279984789308,0.00566385085969027,0.00769653857013698,0.0103202091340496,0.0136549763748181,0.0178280077793873,0.0229680478184291,0.0291981112737194,0.0366264512615908,0.0453361181416014,0.0553736597424273,0.0667377454353313,0.0793686953006767,0.0931400267517807,0.107853162058104,0.123236346117188,0.138948592247275,0.154589109631224,0.169712193813295,0.183847024784688,0.196521274267636,0.207286941616971,0.215746482184332,0.221577118005127,0.224551263151559,0.224551263151559,0.221577118005127,0.215746482184332,0.207286941616971,0.196521274267636,0.183847024784688,0.169712193813295,0.154589109631224,0.138948592247275,0.123236346117188,0.107853162058104,0.0931400267517807,0.0793686953006767],[2.82716434408781e-05,4.59946907074349e-05,7.38369514724173e-05,0.000116963193958756,0.000182824370949101,0.000281986529976784,0.000429172657826968,0.000644532965758019,0.000955141315383808,0.00139668842621768,0.00201530528765178,0.00286940308774354,0.00403136095448103,0.00558883407150549,0.00764539896774982,0.0103202091340496,0.0137463136700149,0.0180673063282852,0.02343203316216,0.0299872020239762,0.0378679089847018,0.0471863202218377,0.0580190086061543,0.0703937178977472,0.0842765813218816,0.0995610169053231,0.11605962002168,0.133500340016264,0.151528040168199,0.169712193813295,0.187560981045271,0.204541459615773,0.220104850230515,0.233715374046653,0.244880586930282,0.253180842221555,0.258295434503533,0.260023156742052,0.258295434503533,0.253180842221555,0.244880586930282,0.233715374046653,0.220104850230514,0.204541459615773,0.187560981045271,0.169712193813295,0.151528040168199,0.133500340016264,0.11605962002168,0.0995610169053231,0.0842765813218815],[4.13411885215753e-05,6.68104365259486e-05,0.000106540582661962,0.000167646786322011,0.000260306375282415,0.000398826294682618,0.000602965091549771,0.000899518215389479,0.00132414993020807,0.00192341840987106,0.00275689218337506,0.00389919722202856,0.00544176809092422,0.00749400992508922,0.0101835196339532,0.0136549763748181,0.0180673063282852,0.0235887685874911,0.0303897087997554,0.038632891490204,0.0484615495843349,0.059985569488805,0.0732665400234349,0.0883027005936736,0.105015086918652,0.123236346117188,0.142703734073939,0.163057683511186,0.183847024784688,0.204541459615773,0.224551263151559,0.243253479402161,0.260023156742052,0.274267532161665,0.285460602485018,0.293175290564394,0.297110470673857,0.297110470673857,0.293175290564394,0.285460602485018,0.274267532161665,0.260023156742052,0.243253479402161,0.224551263151559,0.204541459615773,0.183847024784688,0.163057683511186,0.142703734073939,0.123236346117187,0.105015086918652,0.0883027005936735],[5.96518982053589e-05,9.57613638722586e-05,0.000151693085277635,0.000237110427259632,0.000365716814761371,0.000556606932310108,0.000835914195139495,0.00123875150893878,0.00181140724165957,0.00261371006137222,0.00372141519526626,0.00522839331207609,0.00724832705671824,0.00991554794583818,0.0133845897266318,0.0178280077793873,0.02343203316216,0.0303897087997554,0.0388913045198195,0.0491120304813809,0.0611973583747468,0.0752465978598418,0.0912957306538512,0.109300833917683,0.129123678283553,0.150521213053114,0.173140607547204,0.196521274267636,0.220104850230515,0.243253479402161,0.265275972946228,0.285460602485018,0.303112500317449,0.317593000830256,0.328357851644514,0.33499112025604,0.337231855208577,0.33499112025604,0.328357851644514,0.317593000830256,0.303112500317449,0.285460602485018,0.265275972946228,0.243253479402161,0.220104850230514,0.196521274267636,0.173140607547204,0.150521213053114,0.129123678283553,0.109300833917683,0.0912957306538512],[8.49327106662141e-05,0.000135439625183767,0.000213120834665971,0.000330914258401014,0.000507007588241112,0.000766518860306114,0.0011435117670125,0.00168332447367516,0.00244514401926113,0.00350469684564327,0.00495685115543938,0.00691784306183354,0.00952675374976984,0.0129457906819443,0.0173588771141464,0.0229680478184291,0.0299872020239762,0.038632891490204,0.0491120304813809,0.0616067037323444,0.0762566042441888,0.0931400267517807,0.112254733100793,0.133500340016264,0.156664100290189,0.181412000689187,0.207286941616971,0.233715374046653,0.260023156742052,0.285460602485018,0.309235779002722,0.330554217072223,0.348662367221265,0.362891548359315,0.372698839025217,0.377701433395733,0.377701433395733,0.372698839025217,0.362891548359315,0.348662367221265,0.330554217072223,0.309235779002722,0.285460602485018,0.260023156742052,0.233715374046653,0.207286941616971,0.181412000689186,0.156664100290189,0.133500340016264,0.112254733100793,0.0931400267517807],[0.000119326007210532,0.000189021223755468,0.00029545778598786,0.00045571121285868,0.00069357494643525,0.001041613227331,0.00154357943026862,0.00225715241344947,0.00325688328797803,0.00463716887967559,0.0065149792445367,0.00903197168106351,0.0123555335663325,0.0166782258068148,0.0222150656466253,0.0291981112737194,0.0378679089847018,0.0484615495843349,0.0611973583747468,0.0762566042441889,0.0937630346490151,0.113761485569737,0.13619722577862,0.160898010876987,0.187560981045271,0.215746482184332,0.244880586930282,0.274267532161665,0.303112500317449,0.330554217072223,0.355705814256035,0.377701433395733,0.395745248132001,0.409159078239893,0.417424639904451,0.42021676758822,0.417424639904451,0.409159078239893,0.395745248132001,0.377701433395733,0.355705814256035,0.330554217072223,0.303112500317449,0.274267532161665,0.244880586930282,0.215746482184332,0.187560981045271,0.160898010876987,0.13619722577862,0.113761485569737,0.0937630346490151],[0.000165426331763633,0.000260306375282415,0.000404179587925526,0.00061926046668583,0.000936228250159679,0.00139668842621768,0.00205601715502536,0.00298650564922439,0.00428064639378394,0.00605431167304309,0.00844947259624268,0.0116360033062816,0.0158120244454911,0.0212021803857819,0.0280532369549841,0.0366264512615908,0.0471863202218377,0.059985569488805,0.0752465978598418,0.0931400267517807,0.113761485569737,0.137108240625842,0.163057683511186,0.191349964199119,0.221577118005127,0.253180842221555,0.285460602485018,0.317593000830256,0.348662367221265,0.377701433395734,0.403739832453488,0.425857176990707,0.443236738079082,0.455215389948493,0.461325572510598,0.461325572510598,0.455215389948493,0.443236738079082,0.425857176990707,0.403739832453488,0.377701433395733,0.348662367221265,0.317593000830256,0.285460602485018,0.253180842221555,0.221577118005127,0.191349964199118,0.163057683511186,0.137108240625842,0.113761485569737,0.0931400267517807],[0.000226299491078579,0.000353727191454194,0.000545585376603712,0.000830359968498562,0.00124703744141802,0.00184800009527527,0.002702302060594,0.00389919722202855,0.00555169909847572,0.00779984627193754,0.0108132333197937,0.0147922593162476,0.019967461521985,0.0265962623150306,0.0349564857872596,0.0453361181416014,0.0580190086061543,0.0732665400234349,0.0912957306538512,0.112254733100793,0.13619722577862,0.163057683511186,0.19262989231369,0.224551263151559,0.258295434503533,0.293175290564394,0.328357851644514,0.362891548359315,0.395745248132001,0.425857176990707,0.452190714137368,0.473793082436581,0.489852352747715,0.499748026688478,0.50309081043154,0.499748026688478,0.489852352747715,0.473793082436581,0.452190714137368,0.425857176990707,0.395745248132001,0.362891548359315,0.328357851644514,0.293175290564394,0.258295434503533,0.224551263151559,0.19262989231369,0.163057683511186,0.13619722577862,0.112254733100793,0.0912957306538512],[0.000305472361182387,0.000474309140263595,0.000726708889631284,0.00109867402929202,0.00163902905155488,0.00241275848236791,0.00350469684564327,0.00502338507797734,0.00710480056468866,0.00991554794583817,0.0136549763748181,0.018555582579172,0.0248809891714444,0.0329207785331829,0.0429815387213285,0.0553736597424273,0.0703937178977472,0.0883027005936735,0.109300833917683,0.133500340016264,0.160898010876987,0.191349964199119,0.224551263151559,0.260023156742052,0.297110470673857,0.33499112025604,0.372698839025217,0.409159078239893,0.443236738079082,0.473793082436581,0.499748026688478,0.520143130558753,0.534200208747332,0.541370574407708,0.541370574407708,0.534200208747332,0.520143130558753,0.499748026688478,0.473793082436581,0.443236738079082,0.409159078239893,0.372698839025216,0.33499112025604,0.297110470673857,0.260023156742052,0.224551263151559,0.191349964199118,0.160898010876987,0.133500340016264,0.109300833917683,0.0883027005936735],[0.000406883120273112,0.000627572563746903,0.000955141315383809,0.0014344344950571,0.0021257060898808,0.00310838725689917,0.00448514440107633,0.00638597401211592,0.00897195880172398,0.0124381789686887,0.0170151483173813,0.0229680478184292,0.0305929836885136,0.0402095297012097,0.0521489488401799,0.0667377454353312,0.0842765813218815,0.105015086918652,0.129123678283553,0.156664100290189,0.187560981045271,0.221577118005127,0.258295434503533,0.297110470673857,0.337231855208577,0.377701433395733,0.417424639904451,0.455215389948493,0.489852352747715,0.520143130558753,0.544991768806601,0.563464326674264,0.574847061130358,0.578692177685248,0.574847061130358,0.563464326674264,0.544991768806601,0.520143130558753,0.489852352747715,0.455215389948493,0.417424639904451,0.377701433395733,0.337231855208577,0.297110470673857,0.258295434503533,0.221577118005127,0.187560981045271,0.156664100290189,0.129123678283553,0.105015086918652,0.0842765813218815],[0.000534782062322523,0.000819361985074017,0.00123875150893878,0.00184800009527526,0.00272037759249125,0.00395153465920296,0.00566385085969027,0.00801064026778585,0.0111797490906169,0.0153959428709982,0.0209213609372375,0.028053236954984,0.0371180741477057,0.0484615495843349,0.0624336270200144,0.0793686953006767,0.0995610169053231,0.123236346117188,0.150521213053114,0.181412000689187,0.215746482184332,0.253180842221555,0.293175290564394,0.33499112025604,0.377701433395734,0.42021676758822,0.461325572510598,0.499748026688478,0.534200208747332,0.563464326674264,0.586459742075634,0.602309053475662,0.610393618182409,0.610393618182409,0.602309053475662,0.586459742075634,0.563464326674264,0.534200208747332,0.499748026688478,0.461325572510598,0.42021676758822,0.377701433395733,0.33499112025604,0.293175290564394,0.253180842221555,0.215746482184332,0.181412000689186,0.150521213053114,0.123236346117187,0.0995610169053231,0.0793686953006767],[0.00069357494643525,0.00105559440441471,0.00158529528782164,0.00234926855091255,0.0034352991984419,0.00495685115543938,0.00705759276177737,0.00991554794583818,0.0137463136700149,0.0188046470871136,0.0253836184938987,0.0338104758697079,0.044438402855276,0.0576335016663816,0.0737566153930639,0.0931400267517807,0.11605962002168,0.142703734073939,0.173140607547204,0.207286941616971,0.244880586930282,0.285460602485018,0.328357851644514,0.372698839025217,0.417424639904451,0.461325572510598,0.50309081043154,0.541370574407708,0.574847061130358,0.602309053475662,0.622724387213473,0.635304254302526,0.639553765295602,0.635304254302526,0.622724387213473,0.602309053475662,0.574847061130358,0.541370574407708,0.50309081043154,0.461325572510598,0.417424639904451,0.372698839025216,0.328357851644514,0.285460602485018,0.244880586930282,0.207286941616971,0.173140607547204,0.142703734073939,0.11605962002168,0.0931400267517807,0.0737566153930639],[0.000887604242175164,0.00134192348969622,0.00200191460427379,0.00294694986513851,0.00428064639378394,0.00613557638954216,0.00867782300588868,0.0121108776097162,0.0166782258068148,0.0226638397414695,0.0303897087997554,0.0402095297012098,0.0524977699441988,0.0676335407388234,0.0859790811974574,0.107853162058104,0.133500340016264,0.163057683511186,0.196521274267636,0.233715374046653,0.274267532161665,0.317593000830256,0.362891548359315,0.409159078239893,0.455215389948493,0.499748026688478,0.541370574407708,0.578692177685248,0.610393618182409,0.635304254302526,0.65247360837373,0.66123151276833,0.66123151276833,0.65247360837373,0.635304254302526,0.610393618182409,0.578692177685248,0.541370574407708,0.499748026688478,0.455215389948493,0.409159078239893,0.362891548359315,0.317593000830256,0.274267532161665,0.233715374046653,0.196521274267636,0.163057683511186,0.133500340016264,0.107853162058104,0.0859790811974573,0.0676335407388234],[0.00112086871693632,0.00168332447367516,0.00249453920500861,0.00364772623722359,0.00526336571263112,0.00749400992508921,0.0105286911879137,0.0145963382350979,0.0199674615219849,0.0269532538003863,0.0359011989345458,0.0471863202218377,0.0611973583747469,0.0783174697738453,0.0988994843512533,0.123236346117187,0.151528040168199,0.183847024784688,0.220104850230515,0.260023156742052,0.303112500317449,0.348662367221265,0.395745248132001,0.443236738079082,0.489852352747715,0.534200208747332,0.574847061130358,0.610393618182409,0.639553765295602,0.66123151276833,0.674589275394169,0.679101561471563,0.674589275394169,0.66123151276833,0.639553765295601,0.610393618182409,0.574847061130358,0.534200208747332,0.489852352747715,0.443236738079082,0.395745248132001,0.348662367221264,0.303112500317449,0.260023156742052,0.220104850230514,0.183847024784688,0.151528040168199,0.123236346117187,0.0988994843512531,0.0783174697738452,0.0611973583747468],[0.00139668842621768,0.00208361428914125,0.00306721717064097,0.00445534288715811,0.00638597401211591,0.00903197168106352,0.0126051319011179,0.0173588771141464,0.023588768587491,0.0316299363433547,0.041850511738228,0.0546402445789954,0.0703937178977473,0.0894879540654759,0.112254733100793,0.138948592247275,0.169712193813295,0.204541459615773,0.243253479402161,0.285460602485018,0.330554217072223,0.377701433395733,0.425857176990707,0.473793082436581,0.520143130558753,0.563464326674264,0.602309053475662,0.635304254302526,0.66123151276833,0.679101561471563,0.688216882724809,0.688216882724809,0.679101561471563,0.66123151276833,0.635304254302526,0.602309053475662,0.563464326674264,0.520143130558752,0.473793082436581,0.425857176990706,0.377701433395733,0.330554217072222,0.285460602485018,0.243253479402161,0.204541459615773,0.169712193813295,0.138948592247275,0.112254733100793,0.0894879540654756,0.0703937178977471,0.0546402445789954],[0.00171732988374323,0.00254493223969906,0.00372141519526627,0.00536969275307715,0.00764539896774982,0.0107413848586375,0.0148912038269306,0.0203708310016617,0.0274977456452354,0.0366264512615908,0.0481395471212505,0.0624336270200144,0.0798995876107819,0.100897386463104,0.125725885448756,0.154589109631224,0.187560981045271,0.224551263151559,0.265275972946228,0.309235779002722,0.355705814256035,0.403739832453488,0.452190714137368,0.499748026688478,0.544991768806601,0.586459742075634,0.622724387213473,0.65247360837373,0.674589275394169,0.688216882724809,0.692820323027551,0.688216882724809,0.674589275394169,0.65247360837373,0.622724387213473,0.586459742075634,0.544991768806601,0.499748026688478,0.452190714137368,0.403739832453488,0.355705814256035,0.309235779002722,0.265275972946228,0.224551263151559,0.187560981045271,0.154589109631224,0.125725885448756,0.100897386463104,0.0798995876107818,0.0624336270200143,0.0481395471212505],[0.00208361428914125,0.00306721717064097,0.00445534288715812,0.00638597401211591,0.00903197168106351,0.0126051319011179,0.0173588771141464,0.023588768587491,0.0316299363433547,0.041850511738228,0.0546402445789954,0.0703937178977472,0.0894879540654759,0.112254733100793,0.138948592247275,0.169712193813295,0.204541459615773,0.243253479402161,0.285460602485018,0.330554217072223,0.377701433395733,0.425857176990707,0.473793082436581,0.520143130558753,0.563464326674264,0.602309053475662,0.635304254302526,0.66123151276833,0.679101561471563,0.688216882724809,0.688216882724809,0.679101561471563,0.66123151276833,0.635304254302526,0.602309053475662,0.563464326674264,0.520143130558753,0.473793082436581,0.425857176990707,0.377701433395733,0.330554217072223,0.285460602485018,0.243253479402161,0.204541459615773,0.169712193813295,0.138948592247275,0.112254733100793,0.0894879540654758,0.0703937178977471,0.0546402445789953,0.0418505117382279],[0.00249453920500861,0.0036477262372236,0.00526336571263113,0.0074940099250892,0.0105286911879137,0.0145963382350979,0.019967461521985,0.0269532538003863,0.0359011989345458,0.0471863202218377,0.0611973583747469,0.0783174697738453,0.0988994843512533,0.123236346117188,0.151528040168199,0.183847024784688,0.220104850230515,0.260023156742052,0.303112500317449,0.348662367221265,0.395745248132001,0.443236738079082,0.489852352747715,0.534200208747332,0.574847061130358,0.610393618182409,0.639553765295602,0.66123151276833,0.674589275394169,0.679101561471563,0.674589275394169,0.66123151276833,0.639553765295601,0.610393618182409,0.574847061130358,0.534200208747332,0.489852352747715,0.443236738079082,0.395745248132001,0.348662367221264,0.303112500317449,0.260023156742052,0.220104850230514,0.183847024784688,0.151528040168199,0.123236346117187,0.0988994843512531,0.0783174697738453,0.0611973583747467,0.0471863202218377,0.0359011989345458],[0.00294694986513851,0.00428064639378394,0.00613557638954216,0.00867782300588868,0.0121108776097161,0.0166782258068148,0.0226638397414695,0.0303897087997554,0.0402095297012097,0.0524977699441988,0.0676335407388234,0.0859790811974574,0.107853162058105,0.133500340016264,0.163057683511186,0.196521274267636,0.233715374046653,0.274267532161665,0.317593000830256,0.362891548359315,0.409159078239893,0.455215389948493,0.499748026688478,0.541370574407708,0.578692177685248,0.610393618182409,0.635304254302526,0.65247360837373,0.66123151276833,0.66123151276833,0.65247360837373,0.635304254302526,0.610393618182409,0.578692177685248,0.541370574407708,0.499748026688478,0.455215389948493,0.409159078239893,0.362891548359315,0.317593000830256,0.274267532161664,0.233715374046653,0.196521274267636,0.163057683511186,0.133500340016264,0.107853162058104,0.0859790811974572,0.0676335407388233,0.0524977699441987,0.0402095297012097,0.0303897087997554],[0.00343529919844191,0.00495685115543939,0.00705759276177738,0.00991554794583818,0.0137463136700149,0.0188046470871136,0.0253836184938987,0.0338104758697079,0.044438402855276,0.0576335016663816,0.0737566153930639,0.0931400267517807,0.11605962002168,0.142703734073939,0.173140607547204,0.207286941616971,0.244880586930282,0.285460602485018,0.328357851644514,0.372698839025217,0.417424639904451,0.461325572510598,0.50309081043154,0.541370574407708,0.574847061130358,0.602309053475662,0.622724387213473,0.635304254302526,0.639553765295601,0.635304254302526,0.622724387213473,0.602309053475662,0.574847061130358,0.541370574407708,0.503090810431539,0.461325572510598,0.417424639904451,0.372698839025216,0.328357851644514,0.285460602485018,0.244880586930282,0.207286941616971,0.173140607547204,0.142703734073939,0.11605962002168,0.0931400267517807,0.0737566153930638,0.0576335016663815,0.0444384028552759,0.0338104758697078,0.0253836184938987],[0.00395153465920295,0.00566385085969027,0.00801064026778585,0.0111797490906169,0.0153959428709982,0.0209213609372375,0.028053236954984,0.0371180741477057,0.0484615495843349,0.0624336270200143,0.0793686953006767,0.0995610169053231,0.123236346117188,0.150521213053114,0.181412000689187,0.215746482184332,0.253180842221555,0.293175290564394,0.33499112025604,0.377701433395733,0.42021676758822,0.461325572510598,0.499748026688478,0.534200208747332,0.563464326674264,0.586459742075634,0.602309053475662,0.610393618182409,0.610393618182409,0.602309053475662,0.586459742075634,0.563464326674264,0.534200208747332,0.499748026688478,0.461325572510598,0.42021676758822,0.377701433395733,0.33499112025604,0.293175290564394,0.253180842221555,0.215746482184332,0.181412000689186,0.150521213053114,0.123236346117188,0.099561016905323,0.0793686953006767,0.0624336270200143,0.0484615495843349,0.0371180741477056,0.028053236954984,0.0209213609372375],[0.00448514440107633,0.00638597401211591,0.00897195880172399,0.0124381789686887,0.0170151483173813,0.0229680478184291,0.0305929836885136,0.0402095297012097,0.0521489488401799,0.0667377454353312,0.0842765813218815,0.105015086918652,0.129123678283553,0.156664100290189,0.187560981045271,0.221577118005127,0.258295434503533,0.297110470673857,0.337231855208577,0.377701433395733,0.417424639904451,0.455215389948493,0.489852352747715,0.520143130558753,0.544991768806601,0.563464326674264,0.574847061130358,0.578692177685248,0.574847061130358,0.563464326674264,0.544991768806601,0.520143130558753,0.489852352747715,0.455215389948493,0.417424639904451,0.377701433395733,0.337231855208577,0.297110470673857,0.258295434503533,0.221577118005127,0.187560981045271,0.156664100290189,0.129123678283553,0.105015086918652,0.0842765813218814,0.0667377454353312,0.0521489488401798,0.0402095297012097,0.0305929836885135,0.0229680478184291,0.0170151483173813],[0.00502338507797734,0.00710480056468868,0.00991554794583818,0.0136549763748181,0.018555582579172,0.0248809891714443,0.032920778533183,0.0429815387213285,0.0553736597424272,0.0703937178977472,0.0883027005936735,0.109300833917683,0.133500340016264,0.160898010876987,0.191349964199119,0.224551263151559,0.260023156742052,0.297110470673857,0.33499112025604,0.372698839025217,0.409159078239893,0.443236738079082,0.473793082436581,0.499748026688478,0.520143130558753,0.534200208747332,0.541370574407708,0.541370574407708,0.534200208747332,0.520143130558752,0.499748026688478,0.473793082436581,0.443236738079082,0.409159078239893,0.372698839025216,0.33499112025604,0.297110470673857,0.260023156742052,0.224551263151559,0.191349964199118,0.160898010876987,0.133500340016264,0.109300833917683,0.0883027005936735,0.0703937178977471,0.0553736597424272,0.0429815387213284,0.0329207785331829,0.0248809891714443,0.018555582579172,0.0136549763748181],[0.00555169909847572,0.00779984627193754,0.0108132333197937,0.0147922593162476,0.0199674615219849,0.0265962623150306,0.0349564857872596,0.0453361181416014,0.0580190086061542,0.0732665400234349,0.0912957306538512,0.112254733100793,0.13619722577862,0.163057683511186,0.19262989231369,0.224551263151559,0.258295434503533,0.293175290564394,0.328357851644514,0.362891548359315,0.395745248132001,0.425857176990707,0.452190714137368,0.473793082436581,0.489852352747715,0.499748026688478,0.50309081043154,0.499748026688478,0.489852352747715,0.47379308243658,0.452190714137368,0.425857176990707,0.395745248132001,0.362891548359315,0.328357851644514,0.293175290564394,0.258295434503533,0.224551263151559,0.19262989231369,0.163057683511186,0.13619722577862,0.112254733100793,0.0912957306538511,0.0732665400234348,0.0580190086061542,0.0453361181416014,0.0349564857872595,0.0265962623150306,0.0199674615219849,0.0147922593162475,0.0108132333197937],[0.00605431167304309,0.00844947259624269,0.0116360033062816,0.015812024445491,0.0212021803857818,0.028053236954984,0.0366264512615908,0.0471863202218377,0.0599855694888049,0.0752465978598418,0.0931400267517807,0.113761485569737,0.137108240625842,0.163057683511186,0.191349964199119,0.221577118005127,0.253180842221555,0.285460602485018,0.317593000830256,0.348662367221265,0.377701433395733,0.403739832453488,0.425857176990707,0.443236738079082,0.455215389948493,0.461325572510598,0.461325572510598,0.455215389948493,0.443236738079082,0.425857176990706,0.403739832453488,0.377701433395733,0.348662367221264,0.317593000830256,0.285460602485018,0.253180842221555,0.221577118005127,0.191349964199118,0.163057683511186,0.137108240625842,0.113761485569737,0.0931400267517806,0.0752465978598418,0.0599855694888049,0.0471863202218377,0.0366264512615908,0.028053236954984,0.0212021803857818,0.015812024445491,0.0116360033062816,0.00844947259624267],[0.00651497924453669,0.00903197168106351,0.0123555335663325,0.0166782258068148,0.0222150656466253,0.0291981112737194,0.0378679089847018,0.0484615495843349,0.0611973583747468,0.0762566042441888,0.0937630346490151,0.113761485569737,0.13619722577862,0.160898010876987,0.187560981045271,0.215746482184332,0.244880586930282,0.274267532161665,0.303112500317449,0.330554217072223,0.355705814256035,0.377701433395733,0.395745248132001,0.409159078239893,0.417424639904451,0.42021676758822,0.417424639904451,0.409159078239893,0.395745248132001,0.377701433395733,0.355705814256035,0.330554217072223,0.303112500317449,0.274267532161664,0.244880586930282,0.215746482184332,0.187560981045271,0.160898010876987,0.13619722577862,0.113761485569737,0.0937630346490151,0.0762566042441887,0.0611973583747468,0.0484615495843349,0.0378679089847018,0.0291981112737194,0.0222150656466252,0.0166782258068148,0.0123555335663324,0.0090319716810635,0.0065149792445367],[0.00691784306183353,0.00952675374976984,0.0129457906819443,0.0173588771141464,0.0229680478184291,0.0299872020239762,0.038632891490204,0.0491120304813809,0.0616067037323443,0.0762566042441888,0.0931400267517807,0.112254733100793,0.133500340016264,0.156664100290189,0.181412000689187,0.207286941616971,0.233715374046653,0.260023156742052,0.285460602485018,0.309235779002722,0.330554217072223,0.348662367221265,0.362891548359315,0.372698839025217,0.377701433395733,0.377701433395733,0.372698839025217,0.362891548359315,0.348662367221265,0.330554217072223,0.309235779002722,0.285460602485018,0.260023156742052,0.233715374046653,0.207286941616971,0.181412000689187,0.156664100290189,0.133500340016264,0.112254733100793,0.0931400267517807,0.0762566042441888,0.0616067037323443,0.0491120304813808,0.038632891490204,0.0299872020239762,0.0229680478184291,0.0173588771141464,0.0129457906819443,0.00952675374976981,0.00691784306183353,0.00495685115543938],[0.00724832705671823,0.00991554794583818,0.0133845897266318,0.0178280077793873,0.02343203316216,0.0303897087997554,0.0388913045198195,0.0491120304813809,0.0611973583747468,0.0752465978598418,0.0912957306538512,0.109300833917683,0.129123678283553,0.150521213053114,0.173140607547204,0.196521274267636,0.220104850230514,0.243253479402161,0.265275972946228,0.285460602485018,0.303112500317449,0.317593000830256,0.328357851644514,0.33499112025604,0.337231855208577,0.33499112025604,0.328357851644514,0.317593000830256,0.303112500317449,0.285460602485018,0.265275972946228,0.243253479402161,0.220104850230514,0.196521274267636,0.173140607547204,0.150521213053114,0.129123678283553,0.109300833917683,0.0912957306538511,0.0752465978598418,0.0611973583747468,0.0491120304813808,0.0388913045198195,0.0303897087997554,0.02343203316216,0.0178280077793873,0.0133845897266318,0.00991554794583817,0.00724832705671822,0.00522839331207607,0.00372141519526626],[0.0074940099250892,0.0101835196339532,0.0136549763748181,0.0180673063282852,0.023588768587491,0.0303897087997554,0.038632891490204,0.0484615495843349,0.0599855694888049,0.0732665400234349,0.0883027005936735,0.105015086918652,0.123236346117188,0.142703734073939,0.163057683511186,0.183847024784688,0.204541459615773,0.224551263151559,0.243253479402161,0.260023156742052,0.274267532161665,0.285460602485018,0.293175290564394,0.297110470673857,0.297110470673857,0.293175290564394,0.285460602485018,0.274267532161664,0.260023156742052,0.24325347940216,0.224551263151559,0.204541459615773,0.183847024784688,0.163057683511186,0.142703734073939,0.123236346117187,0.105015086918652,0.0883027005936734,0.0732665400234348,0.0599855694888048,0.0484615495843349,0.0386328914902039,0.0303897087997554,0.023588768587491,0.0180673063282852,0.0136549763748181,0.0101835196339532,0.0074940099250892,0.0054417680909242,0.00389919722202855,0.00275689218337506],[0.00764539896774982,0.0103202091340496,0.0137463136700149,0.0180673063282852,0.02343203316216,0.0299872020239762,0.0378679089847018,0.0471863202218377,0.0580190086061542,0.0703937178977472,0.0842765813218815,0.0995610169053231,0.11605962002168,0.133500340016264,0.151528040168199,0.169712193813295,0.187560981045271,0.204541459615773,0.220104850230514,0.233715374046653,0.244880586930282,0.253180842221555,0.258295434503533,0.260023156742052,0.258295434503533,0.253180842221555,0.244880586930282,0.233715374046653,0.220104850230514,0.204541459615773,0.187560981045271,0.169712193813295,0.151528040168199,0.133500340016264,0.11605962002168,0.099561016905323,0.0842765813218814,0.0703937178977471,0.0580190086061542,0.0471863202218377,0.0378679089847018,0.0299872020239762,0.02343203316216,0.0180673063282852,0.0137463136700148,0.0103202091340496,0.00764539896774981,0.00558883407150548,0.00403136095448101,0.00286940308774354,0.00201530528765178],[0.00769653857013698,0.0103202091340496,0.0136549763748181,0.0178280077793873,0.0229680478184291,0.0291981112737195,0.0366264512615908,0.0453361181416014,0.0553736597424272,0.0667377454353312,0.0793686953006767,0.0931400267517807,0.107853162058104,0.123236346117188,0.138948592247275,0.154589109631224,0.169712193813295,0.183847024784688,0.196521274267636,0.207286941616971,0.215746482184332,0.221577118005127,0.224551263151559,0.224551263151559,0.221577118005127,0.215746482184332,0.207286941616971,0.196521274267636,0.183847024784688,0.169712193813295,0.154589109631224,0.138948592247275,0.123236346117187,0.107853162058104,0.0931400267517807,0.0793686953006767,0.0667377454353312,0.0553736597424272,0.0453361181416014,0.0366264512615908,0.0291981112737194,0.0229680478184291,0.0178280077793873,0.0136549763748181,0.0103202091340496,0.00769653857013698,0.00566385085969025,0.00411279984789308,0.0029469498651385,0.00208361428914124,0.00145368836219717],[0.00764539896774982,0.0101835196339532,0.0133845897266318,0.0173588771141464,0.0222150656466253,0.028053236954984,0.0349564857872596,0.0429815387213285,0.0521489488401799,0.0624336270200143,0.0737566153930639,0.0859790811974574,0.0988994843512533,0.112254733100793,0.125725885448756,0.138948592247275,0.151528040168199,0.163057683511186,0.173140607547204,0.181412000689187,0.187560981045271,0.191349964199118,0.19262989231369,0.191349964199118,0.187560981045271,0.181412000689187,0.173140607547204,0.163057683511186,0.151528040168199,0.138948592247275,0.125725885448756,0.112254733100793,0.0988994843512532,0.0859790811974573,0.0737566153930638,0.0624336270200143,0.0521489488401799,0.0429815387213285,0.0349564857872595,0.028053236954984,0.0222150656466253,0.0173588771141464,0.0133845897266318,0.0101835196339532,0.00764539896774981,0.00566385085969026,0.00414031011253805,0.00298650564922438,0.00212570608988079,0.00149297487732865,0.00103469223475733],[0.00749400992508921,0.00991554794583818,0.0129457906819443,0.0166782258068148,0.0212021803857818,0.0265962623150306,0.0329207785331829,0.0402095297012098,0.0484615495843349,0.0576335016663816,0.0676335407388234,0.0783174697738453,0.0894879540654758,0.100897386463104,0.112254733100793,0.123236346117188,0.133500340016264,0.142703734073939,0.150521213053114,0.156664100290189,0.160898010876987,0.163057683511186,0.163057683511186,0.160898010876987,0.156664100290189,0.150521213053114,0.142703734073939,0.133500340016264,0.123236346117187,0.112254733100793,0.100897386463104,0.0894879540654758,0.0783174697738453,0.0676335407388233,0.0576335016663815,0.0484615495843349,0.0402095297012097,0.0329207785331829,0.0265962623150306,0.0212021803857818,0.0166782258068148,0.0129457906819443,0.00991554794583817,0.00749400992508922,0.00558883407150548,0.00411279984789308,0.00298650564922438,0.00213992480687477,0.0015130145096929,0.0010555944044147,0.000726708889631284],[0.00724832705671824,0.00952675374976984,0.0123555335663325,0.015812024445491,0.0199674615219849,0.0248809891714443,0.0305929836885136,0.0371180741477057,0.044438402855276,0.0524977699441988,0.0611973583747468,0.0703937178977472,0.0798995876107819,0.0894879540654758,0.0988994843512533,0.107853162058104,0.11605962002168,0.123236346117187,0.129123678283553,0.133500340016264,0.13619722577862,0.137108240625842,0.13619722577862,0.133500340016264,0.129123678283553,0.123236346117187,0.11605962002168,0.107853162058104,0.0988994843512532,0.0894879540654757,0.0798995876107818,0.0703937178977471,0.0611973583747468,0.0524977699441987,0.0444384028552759,0.0371180741477056,0.0305929836885136,0.0248809891714443,0.0199674615219849,0.015812024445491,0.0123555335663324,0.00952675374976981,0.00724832705671822,0.00544176809092421,0.00403136095448102,0.00294694986513851,0.00212570608988079,0.0015130145096929,0.00106265521031268,0.000736463225893188,0.000503638779492394],[0.00691784306183354,0.00903197168106351,0.0116360033062816,0.0147922593162475,0.018555582579172,0.0229680478184291,0.028053236954984,0.0338104758697079,0.0402095297012097,0.0471863202218377,0.0546402445789954,0.0624336270200143,0.0703937178977472,0.0783174697738453,0.0859790811974573,0.0931400267517807,0.0995610169053231,0.105015086918652,0.109300833917683,0.112254733100793,0.113761485569737,0.113761485569737,0.112254733100793,0.109300833917683,0.105015086918652,0.0995610169053231,0.0931400267517807,0.0859790811974573,0.0783174697738452,0.0703937178977471,0.0624336270200143,0.0546402445789953,0.0471863202218377,0.0402095297012097,0.0338104758697078,0.028053236954984,0.0229680478184291,0.018555582579172,0.0147922593162475,0.0116360033062816,0.0090319716810635,0.00691784306183352,0.00522839331207607,0.00389919722202855,0.00286940308774354,0.00208361428914124,0.00149297487732865,0.0010555944044147,0.000736463225893186,0.000507007588241111,0.000344419125477659],[0.0065149792445367,0.00844947259624268,0.0108132333197937,0.0136549763748181,0.0170151483173813,0.0209213609372375,0.0253836184938987,0.0303897087997554,0.0359011989345458,0.0418505117382279,0.0481395471212505,0.0546402445789954,0.0611973583747469,0.0676335407388234,0.0737566153930639,0.0793686953006767,0.0842765813218815,0.0883027005936735,0.0912957306538512,0.0931400267517807,0.0937630346490151,0.0931400267517807,0.0912957306538512,0.0883027005936735,0.0842765813218815,0.0793686953006767,0.0737566153930639,0.0676335407388234,0.0611973583747468,0.0546402445789954,0.0481395471212505,0.0418505117382279,0.0359011989345458,0.0303897087997554,0.0253836184938987,0.0209213609372375,0.0170151483173813,0.0136549763748181,0.0108132333197937,0.00844947259624267,0.0065149792445367,0.00495685115543937,0.00372141519526626,0.00275689218337506,0.00201530528765178,0.00145368836219717,0.00103469223475733,0.000726708889631284,0.000503638779492393,0.000344419125477659,0.000232415326227089],[0.00605431167304308,0.00779984627193754,0.00991554794583818,0.0124381789686887,0.0153959428709982,0.0188046470871136,0.0226638397414694,0.0269532538003863,0.0316299363433546,0.0366264512615908,0.0418505117382279,0.0471863202218377,0.0524977699441987,0.0576335016663815,0.0624336270200143,0.0667377454353312,0.0703937178977471,0.0732665400234348,0.0752465978598417,0.0762566042441888,0.0762566042441887,0.0752465978598417,0.0732665400234348,0.0703937178977471,0.0667377454353312,0.0624336270200143,0.0576335016663815,0.0524977699441987,0.0471863202218376,0.0418505117382279,0.0366264512615907,0.0316299363433546,0.0269532538003863,0.0226638397414694,0.0188046470871135,0.0153959428709982,0.0124381789686887,0.00991554794583815,0.00779984627193753,0.00605431167304307,0.00463716887967558,0.00350469684564326,0.00261371006137221,0.00192341840987105,0.00139668842621767,0.00100077098850099,0.000707586089762225,0.000493666063484277,0.000339857349890199,0.000230871044045739,0.000154757488873036],[0.00555169909847572,0.00710480056468867,0.00897195880172398,0.0111797490906169,0.0137463136700149,0.0166782258068148,0.0199674615219849,0.023588768587491,0.0274977456452354,0.0316299363433547,0.0359011989345458,0.0402095297012097,0.044438402855276,0.0484615495843349,0.0521489488401799,0.0553736597424272,0.0580190086061542,0.0599855694888049,0.0611973583747468,0.0616067037323444,0.0611973583747468,0.0599855694888049,0.0580190086061542,0.0553736597424272,0.0521489488401799,0.0484615495843349,0.044438402855276,0.0402095297012097,0.0359011989345458,0.0316299363433546,0.0274977456452354,0.023588768587491,0.0199674615219849,0.0166782258068148,0.0137463136700148,0.0111797490906169,0.00897195880172398,0.00710480056468866,0.00555169909847571,0.00428064639378393,0.00325688328797803,0.00244514401926112,0.00181140724165957,0.00132414993020807,0.000955141315383807,0.000679841242334636,0.000477481764863565,0.000330914258401014,0.000226299491078578,0.000152707750973788,0.000101682915906592],[0.00502338507797734,0.00638597401211592,0.00801064026778585,0.00991554794583817,0.0121108776097161,0.0145963382350979,0.0173588771141464,0.0203708310016617,0.023588768587491,0.0269532538003863,0.0303897087997554,0.0338104758697079,0.0371180741477057,0.0402095297012098,0.0429815387213285,0.0453361181416014,0.0471863202218377,0.0484615495843349,0.0491120304813809,0.0491120304813809,0.0484615495843349,0.0471863202218377,0.0453361181416014,0.0429815387213285,0.0402095297012098,0.0371180741477057,0.0338104758697079,0.0303897087997554,0.0269532538003863,0.023588768587491,0.0203708310016617,0.0173588771141464,0.0145963382350979,0.0121108776097161,0.00991554794583817,0.00801064026778585,0.00638597401211592,0.00502338507797734,0.00389919722202855,0.00298650564922438,0.00225715241344947,0.00168332447367516,0.00123875150893878,0.000899518215389479,0.000644532965758018,0.00045571121285868,0.000317938924736617,0.000218880511302086,0.000148689360938946,9.96694592696043e-05,6.59255431045717e-05],[0.00448514440107633,0.00566385085969026,0.00705759276177737,0.00867782300588867,0.0105286911879137,0.0126051319011179,0.0148912038269306,0.0173588771141464,0.0199674615219849,0.0226638397414694,0.0253836184938987,0.028053236954984,0.0305929836885136,0.0329207785331829,0.0349564857872596,0.0366264512615908,0.0378679089847018,0.038632891490204,0.0388913045198195,0.038632891490204,0.0378679089847018,0.0366264512615908,0.0349564857872596,0.0329207785331829,0.0305929836885136,0.028053236954984,0.0253836184938987,0.0226638397414694,0.0199674615219849,0.0173588771141464,0.0148912038269306,0.0126051319011179,0.0105286911879137,0.00867782300588866,0.00705759276177736,0.00566385085969026,0.00448514440107633,0.00350469684564326,0.002702302060594,0.00205601715502535,0.00154357943026862,0.0011435117670125,0.000835914195139493,0.000602965091549771,0.000429172657826966,0.000301426429186152,0.000208900759393613,0.000142859167704447,9.64019057318965e-05,6.41907618392877e-05,4.21763359280098e-05],[0.00395153465920296,0.00495685115543938,0.00613557638954216,0.00749400992508921,0.00903197168106351,0.0107413848586375,0.0126051319011179,0.0145963382350979,0.0166782258068148,0.0188046470871136,0.0209213609372375,0.0229680478184291,0.0248809891714443,0.0265962623150306,0.028053236954984,0.0291981112737195,0.0299872020239762,0.0303897087997554,0.0303897087997554,0.0299872020239762,0.0291981112737194,0.028053236954984,0.0265962623150306,0.0248809891714443,0.0229680478184291,0.0209213609372375,0.0188046470871136,0.0166782258068148,0.0145963382350979,0.0126051319011179,0.0107413848586375,0.00903197168106351,0.00749400992508921,0.00613557638954216,0.00495685115543938,0.00395153465920296,0.00310838725689917,0.00241275848236791,0.00184800009527526,0.00139668842621768,0.001041613227331,0.000766518860306113,0.000556606932310108,0.000398826294682618,0.000281986529976784,0.000196735326235722,0.000135439625183767,9.20065070872886e-05,6.16738060663296e-05,4.07936311689087e-05,2.66252310987588e-05],[0.0034352991984419,0.00428064639378393,0.00526336571263112,0.00638597401211591,0.00764539896774981,0.0090319716810635,0.0105286911879137,0.0121108776097161,0.0137463136700149,0.0153959428709982,0.0170151483173813,0.018555582579172,0.0199674615219849,0.0212021803857818,0.0222150656466252,0.0229680478184291,0.02343203316216,0.023588768587491,0.02343203316216,0.0229680478184291,0.0222150656466252,0.0212021803857818,0.0199674615219849,0.0185555825791719,0.0170151483173813,0.0153959428709982,0.0137463136700148,0.0121108776097161,0.0105286911879137,0.0090319716810635,0.00764539896774981,0.0063859740121159,0.00526336571263112,0.00428064639378393,0.00343529919844189,0.00272037759249124,0.00212570608988079,0.00163902905155488,0.00124703744141802,0.000936228250159675,0.00069357494643525,0.00050700758824111,0.00036571681476137,0.000260306375282414,0.0001828243709491,0.000126704715409478,8.6648465233776e-05,5.84707114811223e-05,3.89336651258396e-05,2.55812408546773e-05,1.65854527439267e-05],[0.00294694986513851,0.00364772623722359,0.00445534288715811,0.00536969275307715,0.00638597401211591,0.00749400992508921,0.00867782300588868,0.00991554794583817,0.0111797490906169,0.0124381789686887,0.0136549763748181,0.0147922593162476,0.015812024445491,0.0166782258068148,0.0173588771141464,0.0178280077793873,0.0180673063282852,0.0180673063282852,0.0178280077793873,0.0173588771141464,0.0166782258068148,0.015812024445491,0.0147922593162475,0.0136549763748181,0.0124381789686887,0.0111797490906169,0.00991554794583816,0.00867782300588867,0.0074940099250892,0.00638597401211591,0.00536969275307715,0.00445534288715811,0.00364772623722359,0.00294694986513851,0.00234926855091255,0.00184800009527526,0.0014344344950571,0.00109867402929202,0.00083035996849856,0.000619260466685829,0.00045571121285868,0.000330914258401013,0.000237110427259632,0.000167646786322011,0.000116963193958756,8.0521640181131e-05,5.46997590103583e-05,3.66663449779618e-05,2.42526519562335e-05,1.58292469826671e-05,1.0194611575158e-05],[0.00249453920500861,0.00306721717064097,0.00372141519526626,0.0044553428871581,0.00526336571263112,0.00613557638954215,0.00705759276177736,0.00801064026778584,0.00897195880172396,0.00991554794583816,0.0108132333197937,0.0116360033062816,0.0123555335663324,0.0129457906819443,0.0133845897266318,0.0136549763748181,0.0137463136700148,0.0136549763748181,0.0133845897266318,0.0129457906819443,0.0123555335663324,0.0116360033062816,0.0108132333197937,0.00991554794583815,0.00897195880172395,0.00801064026778583,0.00705759276177735,0.00613557638954214,0.00526336571263111,0.0044553428871581,0.00372141519526625,0.00306721717064096,0.0024945392050086,0.00200191460427378,0.00158529528782163,0.00123875150893878,0.000955141315383805,0.000726708889631281,0.000545585376603709,0.000404179587925524,0.000295457785987859,0.00021312083466597,0.000151693085277635,0.000106540582661962,7.38369514724168e-05,5.04942416858713e-05,3.40737049306877e-05,2.26885253105972e-05,1.49074234016213e-05,9.66514360135466e-06,6.18334448419464e-06],[0.00208361428914125,0.00254493223969906,0.00306721717064097,0.00364772623722359,0.00428064639378393,0.00495685115543938,0.00566385085969026,0.00638597401211591,0.00710480056468866,0.00779984627193754,0.00844947259624266,0.0090319716810635,0.00952675374976983,0.00991554794583816,0.0101835196339532,0.0103202091340496,0.0103202091340496,0.0101835196339532,0.00991554794583816,0.00952675374976982,0.0090319716810635,0.00844947259624266,0.00779984627193753,0.00710480056468866,0.0063859740121159,0.00566385085969025,0.00495685115543937,0.00428064639378393,0.00364772623722359,0.00306721717064096,0.00254493223969906,0.00208361428914124,0.00168332447367516,0.00134192348969622,0.0010555944044147,0.000819361985074015,0.0006275725637469,0.000474309140263594,0.000353727191454193,0.000260306375282414,0.000189021223755468,0.000135439625183767,9.57613638722582e-05,6.68104365259484e-05,4.59946907074348e-05,3.12449981370626e-05,2.09441485896192e-05,1.38533346197713e-05,9.04180935040093e-06,5.8232545341483e-06,3.70071472688658e-06],[0.00171732988374323,0.00208361428914125,0.00249453920500861,0.00294694986513851,0.0034352991984419,0.00395153465920296,0.00448514440107633,0.00502338507797734,0.00555169909847572,0.00605431167304308,0.0065149792445367,0.00691784306183354,0.00724832705671824,0.00749400992508921,0.00764539896774982,0.00769653857013698,0.00764539896774982,0.0074940099250892,0.00724832705671823,0.00691784306183354,0.00651497924453669,0.00605431167304308,0.00555169909847572,0.00502338507797733,0.00448514440107633,0.00395153465920295,0.0034352991984419,0.00294694986513851,0.00249453920500861,0.00208361428914125,0.00171732988374323,0.00139668842621768,0.00112086871693631,0.000887604242175163,0.00069357494643525,0.000534782062322523,0.000406883120273112,0.000305472361182387,0.000226299491078579,0.000165426331763632,0.000119326007210532,8.4932710666214e-05,5.96518982053589e-05,4.13411885215753e-05,2.8271643440878e-05,1.90778117335236e-05,1.27032682611812e-05,8.34664201226666e-06,5.41149811501853e-06,3.46204447661178e-06,2.1855320581026e-06]],"type":"surface","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
<div id="quadratic-discriminant-analysis" class="section level2 hasAnchor" number="12.5">
<h2 class="hasAnchor"><span class="header-section-number">12.5</span> Quadratic Discriminant Analysis<a href="#quadratic-discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>QDA simply abandons the assumption of the common covariance matrix. Hence, <span class="math inline">\(\Sigma_k\)</span>’s are not equal. In this case, the determinant <span class="math inline">\(|\Sigma_k|\)</span> of each covariance matrix will be different. In addition, the MAP decision becomes a quadratic function of the target point <span class="math inline">\(\mathbf{x}\)</span></p>
<p><span class="math display">\[\begin{align}
&amp; \underset{k}{\max} \,\, \log \big( \pi_k f_k(x) \big) \\
=&amp; ~\underset{k}{\max} \,\, -\frac{1}{2} \log |\Sigma_k| - \frac{1}{2} (\mathbf{x}- \boldsymbol \mu_k)^\text{T}\Sigma_k^{-1} (\mathbf{x}- \boldsymbol \mu_k) + \log(\pi_k) \\
=&amp; \mathbf{x}^\text{T}\mathbf{W}_k \mathbf{x}+ \mathbf{w}_k^\text{T}\mathbf{x}+ b_k
\end{align}\]</span></p>
<p>This leads to quadratic decision boundary between class <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span></p>
<p><span class="math display">\[\big\{\mathbf{x}: \mathbf{x}^\text{T}(\mathbf{W}_k - \mathbf{W}_l) \mathbf{x}+ \mathbf{x}^\text{T}(\mathbf{w}_k - \mathbf{w}_l) + (b_k - b_l) = 0\big\}.\]</span></p>
<p>The estimation procedure is also similar:</p>
<ul>
<li>Prior probabilities: <span class="math inline">\(\widehat{\pi}_k = n_k / n = n^{-1} \sum_k \mathbf{1}\{y_i = k\}\)</span>, where <span class="math inline">\(n_k\)</span> is the number of observations in class <span class="math inline">\(k\)</span>.</li>
<li>Centroid: <span class="math inline">\(\widehat{\boldsymbol \mu}_k = n_k^{-1} \sum_{i: \,y_i = k} \mathbf{x}_i\)</span></li>
<li>Sample covariance matrix for each class:
<span class="math display">\[\widehat \Sigma_k = \frac{1}{n_k-1} \sum_{i : \, y_i = k} (\mathbf{x}_i - \widehat{\boldsymbol \mu}_k)(\mathbf{x}_i - \widehat{\boldsymbol \mu}_k)^\text{T}\]</span></li>
</ul>
</div>
<div id="example-the-hand-written-digit-data" class="section level2 hasAnchor" number="12.6">
<h2 class="hasAnchor"><span class="header-section-number">12.6</span> Example: the Hand Written Digit Data<a href="#example-the-hand-written-digit-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We first sample 100 data from both the training and testing sets.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># a plot of some samples </span></span>
<span id="cb137-3"><a href="#cb137-3" aria-hidden="true" tabindex="-1"></a>    findRows <span class="ot">&lt;-</span> <span class="cf">function</span>(zip, n) {</span>
<span id="cb137-4"><a href="#cb137-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Find n (random) rows with zip representing 0,1,2,...,9</span></span>
<span id="cb137-5"><a href="#cb137-5" aria-hidden="true" tabindex="-1"></a>        res <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">length=</span><span class="dv">10</span>, <span class="at">mode=</span><span class="st">&quot;list&quot;</span>)</span>
<span id="cb137-6"><a href="#cb137-6" aria-hidden="true" tabindex="-1"></a>        <span class="fu">names</span>(res) <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">9</span></span>
<span id="cb137-7"><a href="#cb137-7" aria-hidden="true" tabindex="-1"></a>        ind <span class="ot">&lt;-</span> zip[,<span class="dv">1</span>]</span>
<span id="cb137-8"><a href="#cb137-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">9</span>) {</span>
<span id="cb137-9"><a href="#cb137-9" aria-hidden="true" tabindex="-1"></a>        res[[j<span class="sc">+</span><span class="dv">1</span>]] <span class="ot">&lt;-</span> <span class="fu">sample</span>( <span class="fu">which</span>(ind<span class="sc">==</span>j), n ) }</span>
<span id="cb137-10"><a href="#cb137-10" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span>(res) </span>
<span id="cb137-11"><a href="#cb137-11" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb137-12"><a href="#cb137-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb137-13"><a href="#cb137-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb137-14"><a href="#cb137-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb137-15"><a href="#cb137-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># find 100 samples for each digit for both the training and testing data</span></span>
<span id="cb137-16"><a href="#cb137-16" aria-hidden="true" tabindex="-1"></a>    train.id <span class="ot">&lt;-</span> <span class="fu">findRows</span>(zip.train, <span class="dv">100</span>)</span>
<span id="cb137-17"><a href="#cb137-17" aria-hidden="true" tabindex="-1"></a>    train.id <span class="ot">=</span> <span class="fu">unlist</span>(train.id)</span>
<span id="cb137-18"><a href="#cb137-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb137-19"><a href="#cb137-19" aria-hidden="true" tabindex="-1"></a>    test.id <span class="ot">&lt;-</span> <span class="fu">findRows</span>(zip.test, <span class="dv">100</span>)</span>
<span id="cb137-20"><a href="#cb137-20" aria-hidden="true" tabindex="-1"></a>    test.id <span class="ot">=</span> <span class="fu">unlist</span>(test.id)</span>
<span id="cb137-21"><a href="#cb137-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb137-22"><a href="#cb137-22" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> zip.train[train.id, <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb137-23"><a href="#cb137-23" aria-hidden="true" tabindex="-1"></a>    Y <span class="ot">=</span> zip.train[train.id, <span class="dv">1</span>]</span>
<span id="cb137-24"><a href="#cb137-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">dim</span>(X)</span>
<span id="cb137-25"><a href="#cb137-25" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1000  256</span></span>
<span id="cb137-26"><a href="#cb137-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb137-27"><a href="#cb137-27" aria-hidden="true" tabindex="-1"></a>    Xtest <span class="ot">=</span> zip.test[test.id, <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb137-28"><a href="#cb137-28" aria-hidden="true" tabindex="-1"></a>    Ytest <span class="ot">=</span> zip.test[test.id, <span class="dv">1</span>]</span>
<span id="cb137-29"><a href="#cb137-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">dim</span>(Xtest)</span>
<span id="cb137-30"><a href="#cb137-30" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1000  256</span></span></code></pre></div>
<p>We can then fit LDA and QDA and predict.</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fit LDA</span></span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(MASS)</span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a>    dig.lda<span class="ot">=</span><span class="fu">lda</span>(X,Y)</span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a>    Ytest.pred<span class="ot">=</span><span class="fu">predict</span>(dig.lda, Xtest)<span class="sc">$</span>class</span>
<span id="cb138-6"><a href="#cb138-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">table</span>(Ytest, Ytest.pred)</span>
<span id="cb138-7"><a href="#cb138-7" aria-hidden="true" tabindex="-1"></a><span class="do">##      Ytest.pred</span></span>
<span id="cb138-8"><a href="#cb138-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Ytest  0  1  2  3  4  5  6  7  8  9</span></span>
<span id="cb138-9"><a href="#cb138-9" aria-hidden="true" tabindex="-1"></a><span class="do">##     0 92  0  2  2  0  0  1  0  3  0</span></span>
<span id="cb138-10"><a href="#cb138-10" aria-hidden="true" tabindex="-1"></a><span class="do">##     1  0 94  0  0  4  0  2  0  0  0</span></span>
<span id="cb138-11"><a href="#cb138-11" aria-hidden="true" tabindex="-1"></a><span class="do">##     2  2  2 66  7  5  2  4  2 10  0</span></span>
<span id="cb138-12"><a href="#cb138-12" aria-hidden="true" tabindex="-1"></a><span class="do">##     3  2  0  3 75  2  8  0  3  6  1</span></span>
<span id="cb138-13"><a href="#cb138-13" aria-hidden="true" tabindex="-1"></a><span class="do">##     4  0  4  2  1 76  1  3  2  2  9</span></span>
<span id="cb138-14"><a href="#cb138-14" aria-hidden="true" tabindex="-1"></a><span class="do">##     5  2  0  3 10  0 79  0  0  3  3</span></span>
<span id="cb138-15"><a href="#cb138-15" aria-hidden="true" tabindex="-1"></a><span class="do">##     6  0  0  4  1  3  4 86  0  1  1</span></span>
<span id="cb138-16"><a href="#cb138-16" aria-hidden="true" tabindex="-1"></a><span class="do">##     7  0  0  0  2  5  0  0 87  0  6</span></span>
<span id="cb138-17"><a href="#cb138-17" aria-hidden="true" tabindex="-1"></a><span class="do">##     8  2  0  4  5  6  7  1  0 72  3</span></span>
<span id="cb138-18"><a href="#cb138-18" aria-hidden="true" tabindex="-1"></a><span class="do">##     9  0  0  0  1  4  0  0  5  0 90</span></span>
<span id="cb138-19"><a href="#cb138-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mean</span>(Ytest <span class="sc">!=</span> Ytest.pred)</span>
<span id="cb138-20"><a href="#cb138-20" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.183</span></span></code></pre></div>
<p>However, QDA does not work in this case because there are too many parameters</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>    dig.qda <span class="ot">=</span> <span class="fu">qda</span>(X, Y) <span class="co"># error message</span></span></code></pre></div>
<!--chapter:end:04.2-da.Rmd-->
</div>
</div>
<div id="part-machine-learning-algorithms" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART) Machine Learning Algorithms<a href="#part-machine-learning-algorithms" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="support-vector-machines" class="section level1 hasAnchor" number="13">
<h1 class="hasAnchor"><span class="header-section-number">13</span> Support Vector Machines<a href="#support-vector-machines" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Support Vector Machine (SVM) is one of the most popular classification models. The original SVM was proposed by Vladimir Vapnik and Alexey Chervonenkis in 1963. Then two important improvements was developed in the 90’s: the soft margin version <span class="citation">(<a href="#ref-cortes1995support" role="doc-biblioref">Cortes and Vapnik 1995</a>)</span> and the nonlinear SVM using the kernel trick <span class="citation">(<a href="#ref-boser1992training" role="doc-biblioref">Boser, Guyon, and Vapnik 1992</a>)</span>. We will start with the hard margin version, and then introduce all other techniques.</p>
<div id="maximum-margin-classifier" class="section level2 hasAnchor" number="13.1">
<h2 class="hasAnchor"><span class="header-section-number">13.1</span> Maximum-margin Classifier<a href="#maximum-margin-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This is the original SVM proposed in 1963. It shares similarities with the perception algorithm, but in certain sense is a stable version. We observe the training data <span class="math inline">\({\cal D}_n = \{\mathbf{x}_i, y_i\}_{i=1}^n\)</span>, where we code <span class="math inline">\(y_i\)</span> as a binary outcome from <span class="math inline">\(\{-1, 1\}\)</span>. The advantages of using this coding instead of <span class="math inline">\(0/1\)</span> will be seen later. The goal is to find a linear classification rule <span class="math inline">\(f(\mathbf{x}) = \beta_0 + \mathbf{x}^\text{T}\boldsymbol \beta\)</span> such that the classification rule is the sign of <span class="math inline">\(f(\mathbf{x})\)</span>:</p>
<p><span class="math display">\[
\hat{y} =
\begin{cases}
        +1, \quad \text{if} \quad f(\mathbf{x}) &gt; 0\\
        -1, \quad \text{if} \quad f(\mathbf{x}) &lt; 0
\end{cases}
\]</span>
Hence, a correct classification would satisfy <span class="math inline">\(y_i f(\mathbf{x}_i) &gt; 0\)</span>. Let’s look at the following example of data from two classes.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>    n <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a>    p <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate positive and negative examples</span></span>
<span id="cb140-6"><a href="#cb140-6" aria-hidden="true" tabindex="-1"></a>    xneg <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span><span class="dv">1</span>),n,p)</span>
<span id="cb140-7"><a href="#cb140-7" aria-hidden="true" tabindex="-1"></a>    xpos <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p,<span class="at">mean=</span><span class="dv">3</span>,<span class="at">sd=</span><span class="dv">1</span>),n,p)</span>
<span id="cb140-8"><a href="#cb140-8" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">rbind</span>(xpos,xneg)</span>
<span id="cb140-9"><a href="#cb140-9" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">as.factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n),<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>,n))))</span>
<span id="cb140-10"><a href="#cb140-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb140-11"><a href="#cb140-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot </span></span>
<span id="cb140-12"><a href="#cb140-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x,<span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">&gt;</span><span class="dv">0</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb140-13"><a href="#cb140-13" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;X1&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;X2&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb140-14"><a href="#cb140-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Positive&quot;</span>, <span class="st">&quot;Negative&quot;</span>),<span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>),</span>
<span id="cb140-15"><a href="#cb140-15" aria-hidden="true" tabindex="-1"></a>           <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">19</span>), <span class="at">text.col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">cex =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-193-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>There are many linear lines that can perfectly separate the two classes. But which is better? The SVM defines this as the line that maximizes the margin, which can be seen in the following.</p>
<p>We use the <code>e1071</code> package to fit the SVM. There is a cost parameter <span class="math inline">\(C\)</span>, with default value 1. This parameter has a significant impact on non-separable problems. However, for this <strong>separable case</strong>, we should set this to be a very large value, meaning that the cost for having a wrong classification is very large. We also need to specify the <code>linear</code> kernel.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(e1071)</span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: package &#39;e1071&#39; was built under R version 4.2.2</span></span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a>    svm.fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> <span class="fu">data.frame</span>(x, y), <span class="at">type=</span><span class="st">&#39;C-classification&#39;</span>, </span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">kernel=</span><span class="st">&#39;linear&#39;</span>, <span class="at">scale=</span><span class="cn">FALSE</span>, <span class="at">cost =</span> <span class="dv">10000</span>)</span></code></pre></div>
<p>The following code can recover the fitted linear separation margin. Note here that the points on the margins are the ones with <span class="math inline">\(\alpha_i &gt; 0\)</span> (will be introduced later):</p>
<ul>
<li><code>coefs</code> provides the <span class="math inline">\(y_i \alpha_i\)</span> for the support vectors</li>
<li><code>SV</code> are the <span class="math inline">\(x_i\)</span> values correspond to the support vectors</li>
<li><code>rho</code> is negative <span class="math inline">\(\beta_0\)</span></li>
</ul>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a>    b <span class="ot">&lt;-</span> <span class="fu">t</span>(svm.fit<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm.fit<span class="sc">$</span>SV</span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a>    b0 <span class="ot">&lt;-</span> <span class="sc">-</span>svm.fit<span class="sc">$</span>rho</span>
<span id="cb142-3"><a href="#cb142-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb142-4"><a href="#cb142-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># an alternative of b0 as the lecture note</span></span>
<span id="cb142-5"><a href="#cb142-5" aria-hidden="true" tabindex="-1"></a>    b0 <span class="ot">&lt;-</span> <span class="sc">-</span>(<span class="fu">max</span>(x[y <span class="sc">==</span> <span class="sc">-</span><span class="dv">1</span>, ] <span class="sc">%*%</span> <span class="fu">t</span>(b)) <span class="sc">+</span> <span class="fu">min</span>(x[y <span class="sc">==</span> <span class="dv">1</span>, ] <span class="sc">%*%</span> <span class="fu">t</span>(b)))<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb142-6"><a href="#cb142-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb142-7"><a href="#cb142-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot on the data </span></span>
<span id="cb142-8"><a href="#cb142-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x,<span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">&gt;</span><span class="dv">0</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb142-9"><a href="#cb142-9" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;X1&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;X2&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb142-10"><a href="#cb142-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Positive&quot;</span>,<span class="st">&quot;Negative&quot;</span>),<span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>),</span>
<span id="cb142-11"><a href="#cb142-11" aria-hidden="true" tabindex="-1"></a>           <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">19</span>),<span class="at">text.col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb142-12"><a href="#cb142-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> <span class="sc">-</span>b0<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb142-13"><a href="#cb142-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb142-14"><a href="#cb142-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># mark the support vectors</span></span>
<span id="cb142-15"><a href="#cb142-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(x[svm.fit<span class="sc">$</span>index, ], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">cex=</span><span class="dv">3</span>)</span>
<span id="cb142-16"><a href="#cb142-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb142-17"><a href="#cb142-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the two margin lines </span></span>
<span id="cb142-18"><a href="#cb142-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b0<span class="dv">-1</span>)<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb142-19"><a href="#cb142-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b0<span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-195-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>As we can see, the separation line is trying to have the maximum distance from both classes. This is why it is called the <strong>Maximum-margin Classifier</strong>.</p>
</div>
<div id="linearly-separable-svm" class="section level2 hasAnchor" number="13.2">
<h2 class="hasAnchor"><span class="header-section-number">13.2</span> Linearly Separable SVM<a href="#linearly-separable-svm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In linearly SVM, <span class="math inline">\(f(\mathbf{x}) = \beta_0 + \mathbf{x}^\text{T}\boldsymbol \beta\)</span>. When <span class="math inline">\(f(\mathbf{x}) = 0\)</span>, it corresponds to a hyperplane that separates the two classes:</p>
<p><span class="math display">\[\{ \mathbf{x}: \beta_0 + \mathbf{x}^\text{T} \boldsymbol \beta = 0 \}\]</span></p>
<p>Hence, for this separable case, all observations with <span class="math inline">\(y_i = 1\)</span> are on one side <span class="math inline">\(f(\mathbf{x}) &gt; 0\)</span>, and observations with <span class="math inline">\(y_i = -1\)</span> are on the other side.</p>
<center>
<img src="images/SVMdist.png" style="width:40.0%" />
</center>
<p>First, let’s calculate the <strong>distance from any point <span class="math inline">\(\mathbf{x}\)</span> to this hyperplane</strong>. We can first find a point <span class="math inline">\(\mathbf{x}_0\)</span> on the hyperplane, such that <span class="math inline">\(\mathbf{x}_0^\text{T}\boldsymbol \beta= - \beta_0\)</span>. By taking the difference between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{x}_0\)</span>, and project this vector to the direction of <span class="math inline">\(\boldsymbol \beta\)</span>, we have that the distance from <span class="math inline">\(\mathbf{x}\)</span> to the hyperplane is the projection of <span class="math inline">\(\mathbf{x}- \mathbf{x}_0\)</span> onto the normed vector <span class="math inline">\(\frac{\boldsymbol \beta}{\lVert \boldsymbol \beta\lVert}\)</span>:</p>
<p><span class="math display">\[\begin{align}
&amp; \left \langle  \frac{\boldsymbol \beta}{\lVert \boldsymbol \beta\lVert}, \mathbf{x}- \mathbf{x}_0 \right \rangle \\
=&amp; \frac{1}{\lVert \boldsymbol \beta\lVert} (\mathbf{x}- \mathbf{x}_0)^\text{T}\boldsymbol \beta\\
=&amp; \frac{1}{\lVert \boldsymbol \beta\lVert} (\mathbf{x}^\text{T}\boldsymbol \beta+ \beta_0) \\
=&amp; \frac{1}{\lVert \boldsymbol \beta\lVert} f(\mathbf{x}) \\
\end{align}\]</span></p>
<p>Since the goal of SVM is to create the maximum margin, let’s denote this as <span class="math inline">\(M\)</span>. Then we want all observations to be lied on the correct side, with at least an margin <span class="math inline">\(M\)</span>. This means <span class="math inline">\(y_i (\mathbf{x}_i^\text{T}\boldsymbol \beta+ \beta_0) \geq M\)</span>. But the scale of <span class="math inline">\(\boldsymbol \beta\)</span> is also playing a role in calculating the margin. Hence, we will use the normed version. Then, the linearly separable SVM is to solve this constrained optimization problem:</p>
<p><span class="math display">\[\begin{align}
\underset{\boldsymbol \beta, \beta_0}{\text{max}} \quad &amp; M \\
\text{subject to} \quad &amp; \frac{1}{\lVert \boldsymbol \beta\lVert} y_i(\mathbf{x}^\text{T}\boldsymbol \beta+ \beta_0) \geq M, \,\, i = 1, \ldots, n.
\end{align}\]</span></p>
<p>Note that the scale of <span class="math inline">\(\boldsymbol \beta\)</span> can be arbitrary, let’s set it as <span class="math inline">\(\lVert \boldsymbol \beta\rVert = 1/M\)</span>. The maximization becomes minimization, and its equivalent to minimizing <span class="math inline">\(\frac{1}{2} \lVert \boldsymbol \beta\rVert^2\)</span>. Then we have the <strong>primal form</strong> of the SVM optimization problem.</p>
<p><span class="math display">\[\begin{align}
\text{min} \quad &amp; \frac{1}{2} \lVert \boldsymbol \beta\rVert^2 \\
\text{subject to} \quad &amp; y_i(\mathbf{x}^\text{T}\boldsymbol \beta+ \beta_0) \geq 1, \,\, i = 1, \ldots, n.
\end{align}\]</span></p>
<div id="from-primal-to-dual" class="section level3 hasAnchor" number="13.2.1">
<h3 class="hasAnchor"><span class="header-section-number">13.2.1</span> From Primal to Dual<a href="#from-primal-to-dual" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This is a general inequality constrained optimization problem.</p>
<p><span class="math display">\[\begin{align}
\text{min} \quad &amp; g(\boldsymbol \theta) \\
\text{subject to} \quad &amp; h(\boldsymbol \theta) \leq 0, \,\, i = 1, \ldots, n.
\end{align}\]</span></p>
<p>We can consider the corresponding Lagrangian (with all <span class="math inline">\(\alpha_i\)</span>’s positive):</p>
<p><span class="math display">\[{\cal L}(\boldsymbol \theta, \boldsymbol \alpha) = g(\boldsymbol \theta) + \sum_{i = 1}^n \alpha_i h_i(\boldsymbol \theta)\]</span>
Then there can be two ways to optimize this. If we maximize <span class="math inline">\(\alpha_i\)</span>’s first, for any fixed <span class="math inline">\(\boldsymbol \theta\)</span>, then for any <span class="math inline">\(\boldsymbol \theta\)</span> that violates the constraint, i.e., <span class="math inline">\(h_i(\boldsymbol \theta) &gt; 0\)</span> for some <span class="math inline">\(i\)</span>, we can always choose an extremely large <span class="math inline">\(\alpha_i\)</span> so that <span class="math inline">\(\cal{L}(\boldsymbol \theta, \boldsymbol \alpha)\)</span> is infinity. Hence the solution of this <strong>primal form</strong> must satisfy the constraint.</p>
<p><span class="math display">\[\underset{\boldsymbol \theta}{\min} \underset{\boldsymbol \alpha\succeq 0}{\max} {\cal L}(\boldsymbol \theta, \boldsymbol \alpha)\]</span>
On the other hand, if we minimize <span class="math inline">\(\boldsymbol \theta\)</span> first, then maximize for <span class="math inline">\(\boldsymbol \alpha\)</span>, we have the <strong>dual form</strong>:</p>
<p><span class="math display">\[\underset{\boldsymbol \alpha\succeq 0}{\max} \underset{\boldsymbol \theta}{\min} {\cal L}(\boldsymbol \theta, \boldsymbol \alpha)\]</span>
In general, the two are not the same:</p>
<p><span class="math display">\[\underbrace{\underset{\boldsymbol \alpha\succeq 0}{\max} \underset{\boldsymbol \theta}{\min} {\cal L}(\boldsymbol \theta, \boldsymbol \alpha)}_{\text{duel}} \leq \underbrace{\underset{\boldsymbol \theta}{\min} \underset{\boldsymbol \alpha\succeq 0}{\max} {\cal L}(\boldsymbol \theta, \boldsymbol \alpha)}_{\text{primal}}\]</span>
But a sufficient condition is that if both <span class="math inline">\(g\)</span> and <span class="math inline">\(h_i\)</span>’s are convex and also the constraints <span class="math inline">\(h_i\)</span>’s are feasible. We will use this technique to solve the SVM problem.</p>
<p>First, rewrite the problem as</p>
<p><span class="math display">\[\begin{align}
\text{min} \quad &amp; \frac{1}{2} \lVert \boldsymbol \beta\rVert^2 \\
\text{subject to} \quad &amp; - \{ y_i(\mathbf{x}^\text{T}\boldsymbol \beta+ \beta_0) - 1\} \leq 0, i = 1, \ldots, n.
\end{align}\]</span></p>
<p>Then the Lagrangian is</p>
<p><span class="math display">\[{\cal L}(\boldsymbol \beta, \beta_0, \boldsymbol \alpha) = \frac{1}{2} \lVert \boldsymbol \beta\rVert^2 - \sum_{i = 1}^n \alpha_i \big\{ y_i(\mathbf{x}_i^\text{T}\boldsymbol \beta+ \beta_0) - 1 \big\}\]</span>
To solve this using the dual form, we first find the optimizer of <span class="math inline">\(\boldsymbol \beta\)</span> and <span class="math inline">\(\beta_0\)</span>. We take derivatives with respect to them:</p>
<p><span class="math display">\[\begin{align}
    \boldsymbol \beta- \sum_{i = 1}^n \alpha_i y_i \mathbf{x}_i  =&amp;~ 0 \quad (\nabla_\boldsymbol \beta{\cal L}= 0 ) \\
    \sum_{i = 1}^n \alpha_i y_i =&amp;~ 0 \quad (\nabla_{\beta_0} {\cal L}= 0 )
\end{align}\]</span></p>
<p>Take these solution and plug them back into the Lagrangian, we have</p>
<p><span class="math display">\[{\cal L}(\boldsymbol \beta, \beta_0, \boldsymbol \alpha) = \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \mathbf{x}_i^\text{T}\mathbf{x}_j\]</span>
Hence, the dual optimization problem is</p>
<p><span class="math display">\[\begin{align}
\underset{\boldsymbol \alpha}{\max} \quad &amp; \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \mathbf{x}_i^\text{T}\mathbf{x}_j \nonumber \\
\text{subject to} \quad &amp; \alpha_i \geq 0, \,\, i = 1, \ldots, n. \nonumber \\
&amp; \sum_{i = 1}^n \alpha_i y_i = 0
\end{align}\]</span></p>
<p>Compared with the original primal form, this version has a trivial feasible solution with all <span class="math inline">\(\alpha_i\)</span>’s being 0. One can start from this solution to search for the optimizer while maintaining within the contained region. However, the primal form is difficult since there is no apparent way to satisfy the constraint.</p>
<p>After solving the dual form, we have all the <span class="math inline">\(\alpha_i\)</span> values. The ones with <span class="math inline">\(\alpha_i &gt; 0\)</span> are called the support vectors. Based on our previous analysis, <span class="math inline">\(\widehat{\boldsymbol \beta} = \sum_{i = 1}^n \alpha_i y_i x_i\)</span>, and we can also obtain <span class="math inline">\(\beta_0\)</span> by calculating the midpoint of two “closest” support vectors to the separating hyperplane:</p>
<p><span class="math display">\[\widehat{\beta}_0 = - \,\, \frac{\max_{i: y_i = -1} \mathbf{x}_i^\text{T}\widehat{\boldsymbol \beta} + \min_{i: y_i = 1} \mathbf{x}_i^\text{T}\widehat{\boldsymbol \beta} }{2}\]</span>
And the decision is <span class="math inline">\(\text{sign}(\mathbf{x}^\text{T}\widehat{\boldsymbol \beta} + \widehat{\beta}_0)\)</span>. An example has been demonstrated previously with the <code>e1071</code> package.</p>
</div>
</div>
<div id="linearly-non-separable-svm-with-slack-variables" class="section level2 hasAnchor" number="13.3">
<h2 class="hasAnchor"><span class="header-section-number">13.3</span> Linearly Non-separable SVM with Slack Variables<a href="#linearly-non-separable-svm-with-slack-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we cannot have a perfect separation of the two classes, the original SVM cannot find a solution. Hence, a slack was introduce to incorporate such observations:</p>
<p><span class="math display">\[y_i (\mathbf{x}_i^\text{T}\boldsymbol \beta+ \beta_0) \geq (1 - \xi_i)\]</span>
for a positive <span class="math inline">\(\xi\)</span>. Note that when <span class="math inline">\(\xi = 0\)</span>, the observation is lying at the correct side, with enough margin. When <span class="math inline">\(1 &gt; \xi &gt; 0\)</span>, the observation is lying at the correct side, but the margin is not sufficiently large. When <span class="math inline">\(\xi &gt; 1\)</span>, the observation is lying on the wrong side of the separation hyperplane.</p>
<center>
<img src="images/SVMslack.png" style="width:40.0%" />
</center>
<p>This new optimization problem can be formulated as</p>
<p><span class="math display">\[\begin{align}
\text{min} \quad &amp; \frac{1}{2}\lVert \boldsymbol \beta\rVert^2 + C \sum_{i=1}^n \xi_i \\
\text{subject to} \quad &amp; y_i (\mathbf{x}_i^\text{T}\boldsymbol \beta+ \beta_0) \geq (1 - \xi_i), \,\, i = 1, \ldots, n, \\
\text{and} \quad &amp; \xi_i \geq 0, \,\, i = 1, \ldots, n,
\end{align}\]</span></p>
<p>where <span class="math inline">\(C\)</span> is a tuning parameter that controls the emphasis on the slack variable. Large <span class="math inline">\(C\)</span> will be less tolerable on having positive slacks. We can again write the Lagrangian primal <span class="math inline">\({\cal L}(\boldsymbol \beta, \beta_0, \boldsymbol \alpha, \boldsymbol \xi)\)</span> as</p>
<p><span class="math display">\[\frac{1}{2} \lVert \boldsymbol \beta\rVert^2 + C \sum_{i=1}^n \xi_i - \sum_{i = 1}^n \alpha_i \big\{ y_i(x_i^\text{T}\boldsymbol \beta+ \beta_0) - (1 - \xi_i) \big\} - \sum_{i = 1}^n \gamma_i \xi_i,\]</span>
where <span class="math inline">\(\alpha_i\)</span>’s and <span class="math inline">\(\gamma_i\)</span>’s are all positive. We can similarly obtain the solution corresponding to <span class="math inline">\(\boldsymbol \beta\)</span>, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\boldsymbol \xi\)</span>:</p>
<p><span class="math display">\[\begin{align}
\boldsymbol \beta- \sum_{i = 1}^n \alpha_i y_i x_i  =&amp;~ 0 \quad (\nabla_\boldsymbol \beta{\cal L}= 0 ) \\
\sum_{i = 1}^n \alpha_i y_i =&amp;~ 0 \quad (\nabla_{\beta_0} {\cal L}= 0 ) \\
C - \alpha_i - \gamma_i =&amp;~ 0 \quad (\nabla_{\xi_i} {\cal L}= 0 )
\end{align}\]</span></p>
<p>Substituting them back into the Lagrangian, we have the dual form:</p>
<p><span class="math display">\[\begin{align}
\underset{\boldsymbol \alpha}{\max} \quad &amp; \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \color{OrangeRed}{\langle \mathbf{x}_i, \mathbf{x}_j \rangle} \\
\text{subject to} \quad &amp; 0 \leq \alpha_i \leq C, \,\, i = 1, \ldots, n, \\
\text{and} \quad &amp; \sum_{i = 1}^n \alpha_i y_i = 0.
\end{align}\]</span></p>
<p>Here, the inner product <span class="math inline">\(\langle \mathbf{x}_i, \mathbf{x}_j \rangle\)</span> is nothing but <span class="math inline">\(\mathbf{x}_i^\text{T}\mathbf{x}_j\)</span>. The observations with <span class="math inline">\(0 &lt; \alpha_i &lt; C\)</span> are the that lie on the margin. Hence, we can obtain these observations and perform the same calculations as before to obtain <span class="math inline">\(\widehat{\beta}_0\)</span>. The following code generates some data for this situation and fit SVM. We use the default <span class="math inline">\(C = 1\)</span>.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">70</span>)</span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a>    n <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co"># number of data points for each class</span></span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a>    p <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="co"># dimension</span></span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-6"><a href="#cb143-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate the positive and negative examples</span></span>
<span id="cb143-7"><a href="#cb143-7" aria-hidden="true" tabindex="-1"></a>    xneg <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span><span class="dv">1</span>),n,p)</span>
<span id="cb143-8"><a href="#cb143-8" aria-hidden="true" tabindex="-1"></a>    xpos <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p,<span class="at">mean=</span><span class="fl">1.5</span>,<span class="at">sd=</span><span class="dv">1</span>),n,p)</span>
<span id="cb143-9"><a href="#cb143-9" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">rbind</span>(xpos,xneg)</span>
<span id="cb143-10"><a href="#cb143-10" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">as.factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n),<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>,n))))</span>
<span id="cb143-11"><a href="#cb143-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-12"><a href="#cb143-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualize the data</span></span>
<span id="cb143-13"><a href="#cb143-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-14"><a href="#cb143-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x,<span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">&gt;</span><span class="dv">0</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb143-15"><a href="#cb143-15" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;X1&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;X2&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb143-16"><a href="#cb143-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Positive&quot;</span>,<span class="st">&quot;Negative&quot;</span>),<span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>),</span>
<span id="cb143-17"><a href="#cb143-17" aria-hidden="true" tabindex="-1"></a>           <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">19</span>),<span class="at">text.col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb143-18"><a href="#cb143-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-19"><a href="#cb143-19" aria-hidden="true" tabindex="-1"></a>    svm.fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> <span class="fu">data.frame</span>(x, y), <span class="at">type=</span><span class="st">&#39;C-classification&#39;</span>, </span>
<span id="cb143-20"><a href="#cb143-20" aria-hidden="true" tabindex="-1"></a>                   <span class="at">kernel=</span><span class="st">&#39;linear&#39;</span>,<span class="at">scale=</span><span class="cn">FALSE</span>, <span class="at">cost =</span> <span class="dv">1</span>)</span>
<span id="cb143-21"><a href="#cb143-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-22"><a href="#cb143-22" aria-hidden="true" tabindex="-1"></a>    b <span class="ot">&lt;-</span> <span class="fu">t</span>(svm.fit<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm.fit<span class="sc">$</span>SV</span>
<span id="cb143-23"><a href="#cb143-23" aria-hidden="true" tabindex="-1"></a>    b0 <span class="ot">&lt;-</span> <span class="sc">-</span>svm.fit<span class="sc">$</span>rho</span>
<span id="cb143-24"><a href="#cb143-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-25"><a href="#cb143-25" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(x[svm.fit<span class="sc">$</span>index, ], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">cex=</span><span class="dv">3</span>)     </span>
<span id="cb143-26"><a href="#cb143-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> <span class="sc">-</span>b0<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb143-27"><a href="#cb143-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb143-28"><a href="#cb143-28" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b0<span class="dv">-1</span>)<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb143-29"><a href="#cb143-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b0<span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-197-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>If we instead use a smaller <span class="math inline">\(C\)</span>:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualize the data</span></span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x,<span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">&gt;</span><span class="dv">0</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;X1&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;X2&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Positive&quot;</span>,<span class="st">&quot;Negative&quot;</span>),<span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>),</span>
<span id="cb144-5"><a href="#cb144-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">19</span>),<span class="at">text.col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb144-6"><a href="#cb144-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-7"><a href="#cb144-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fit SVM with C = 10</span></span>
<span id="cb144-8"><a href="#cb144-8" aria-hidden="true" tabindex="-1"></a>    svm.fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> <span class="fu">data.frame</span>(x, y), <span class="at">type=</span><span class="st">&#39;C-classification&#39;</span>, </span>
<span id="cb144-9"><a href="#cb144-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">kernel=</span><span class="st">&#39;linear&#39;</span>,<span class="at">scale=</span><span class="cn">FALSE</span>, <span class="at">cost =</span> <span class="fl">0.1</span>)</span>
<span id="cb144-10"><a href="#cb144-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-11"><a href="#cb144-11" aria-hidden="true" tabindex="-1"></a>    b <span class="ot">&lt;-</span> <span class="fu">t</span>(svm.fit<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm.fit<span class="sc">$</span>SV</span>
<span id="cb144-12"><a href="#cb144-12" aria-hidden="true" tabindex="-1"></a>    b0 <span class="ot">&lt;-</span> <span class="sc">-</span>svm.fit<span class="sc">$</span>rho</span>
<span id="cb144-13"><a href="#cb144-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb144-14"><a href="#cb144-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(x[svm.fit<span class="sc">$</span>index, ], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">cex=</span><span class="dv">3</span>)     </span>
<span id="cb144-15"><a href="#cb144-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> <span class="sc">-</span>b0<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb144-16"><a href="#cb144-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb144-17"><a href="#cb144-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b0<span class="dv">-1</span>)<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb144-18"><a href="#cb144-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b0<span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-198-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="example-saheart-data" class="section level2 hasAnchor" number="13.4">
<h2 class="hasAnchor"><span class="header-section-number">13.4</span> Example: <code>SAheart</code> Data<a href="#example-saheart-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If you want to use the <code>1071e</code> package and perform cross-validation, you could consider using the <code>caret</code> package. Make sure that you specify <code>method = "svmLinear2"</code>. The following code is using the <code>SAheart</code> as an example.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data</span>(SAheart)</span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(caret)</span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a>  cost.grid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">cost =</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="dv">2</span>, <span class="at">length =</span> <span class="dv">20</span>))</span>
<span id="cb145-6"><a href="#cb145-6" aria-hidden="true" tabindex="-1"></a>  train_control <span class="ot">=</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number=</span><span class="dv">10</span>, <span class="at">repeats=</span><span class="dv">3</span>)</span>
<span id="cb145-7"><a href="#cb145-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb145-8"><a href="#cb145-8" aria-hidden="true" tabindex="-1"></a>  svm2 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="fu">as.factor</span>(chd) <span class="sc">~</span>., <span class="at">data =</span> SAheart, <span class="at">method =</span> <span class="st">&quot;svmLinear2&quot;</span>, </span>
<span id="cb145-9"><a href="#cb145-9" aria-hidden="true" tabindex="-1"></a>                <span class="at">trControl =</span> train_control,  </span>
<span id="cb145-10"><a href="#cb145-10" aria-hidden="true" tabindex="-1"></a>                <span class="at">tuneGrid =</span> cost.grid)</span>
<span id="cb145-11"><a href="#cb145-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb145-12"><a href="#cb145-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># see the fitted model</span></span>
<span id="cb145-13"><a href="#cb145-13" aria-hidden="true" tabindex="-1"></a>  svm2</span>
<span id="cb145-14"><a href="#cb145-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Support Vector Machines with Linear Kernel </span></span>
<span id="cb145-15"><a href="#cb145-15" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb145-16"><a href="#cb145-16" aria-hidden="true" tabindex="-1"></a><span class="do">## 462 samples</span></span>
<span id="cb145-17"><a href="#cb145-17" aria-hidden="true" tabindex="-1"></a><span class="do">##   9 predictor</span></span>
<span id="cb145-18"><a href="#cb145-18" aria-hidden="true" tabindex="-1"></a><span class="do">##   2 classes: &#39;0&#39;, &#39;1&#39; </span></span>
<span id="cb145-19"><a href="#cb145-19" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb145-20"><a href="#cb145-20" aria-hidden="true" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb145-21"><a href="#cb145-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold, repeated 3 times) </span></span>
<span id="cb145-22"><a href="#cb145-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Summary of sample sizes: 416, 416, 416, 415, 416, 416, ... </span></span>
<span id="cb145-23"><a href="#cb145-23" aria-hidden="true" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb145-24"><a href="#cb145-24" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb145-25"><a href="#cb145-25" aria-hidden="true" tabindex="-1"></a><span class="do">##   cost       Accuracy   Kappa    </span></span>
<span id="cb145-26"><a href="#cb145-26" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.0100000  0.7142923  0.2844994</span></span>
<span id="cb145-27"><a href="#cb145-27" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.1147368  0.7200123  0.3520308</span></span>
<span id="cb145-28"><a href="#cb145-28" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.2194737  0.7164354  0.3454492</span></span>
<span id="cb145-29"><a href="#cb145-29" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.3242105  0.7171600  0.3467866</span></span>
<span id="cb145-30"><a href="#cb145-30" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.4289474  0.7164354  0.3453015</span></span>
<span id="cb145-31"><a href="#cb145-31" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.5336842  0.7164354  0.3450704</span></span>
<span id="cb145-32"><a href="#cb145-32" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.6384211  0.7157108  0.3438517</span></span>
<span id="cb145-33"><a href="#cb145-33" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.7431579  0.7171600  0.3472755</span></span>
<span id="cb145-34"><a href="#cb145-34" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.8478947  0.7157108  0.3437850</span></span>
<span id="cb145-35"><a href="#cb145-35" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.9526316  0.7157108  0.3437850</span></span>
<span id="cb145-36"><a href="#cb145-36" aria-hidden="true" tabindex="-1"></a><span class="do">##   1.0573684  0.7171600  0.3479914</span></span>
<span id="cb145-37"><a href="#cb145-37" aria-hidden="true" tabindex="-1"></a><span class="do">##   1.1621053  0.7164354  0.3459484</span></span>
<span id="cb145-38"><a href="#cb145-38" aria-hidden="true" tabindex="-1"></a><span class="do">##   1.2668421  0.7164354  0.3459484</span></span>
<span id="cb145-39"><a href="#cb145-39" aria-hidden="true" tabindex="-1"></a><span class="do">##   1.3715789  0.7178847  0.3500130</span></span>
<span id="cb145-40"><a href="#cb145-40" aria-hidden="true" tabindex="-1"></a><span class="do">##   1.4763158  0.7171600  0.3479914</span></span>
<span id="cb145-41"><a href="#cb145-41" aria-hidden="true" tabindex="-1"></a><span class="do">##   1.5810526  0.7178847  0.3500130</span></span>
<span id="cb145-42"><a href="#cb145-42" aria-hidden="true" tabindex="-1"></a><span class="do">##   1.6857895  0.7171600  0.3479914</span></span>
<span id="cb145-43"><a href="#cb145-43" aria-hidden="true" tabindex="-1"></a><span class="do">##   1.7905263  0.7171600  0.3479914</span></span>
<span id="cb145-44"><a href="#cb145-44" aria-hidden="true" tabindex="-1"></a><span class="do">##   1.8952632  0.7171600  0.3479914</span></span>
<span id="cb145-45"><a href="#cb145-45" aria-hidden="true" tabindex="-1"></a><span class="do">##   2.0000000  0.7164354  0.3459484</span></span>
<span id="cb145-46"><a href="#cb145-46" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb145-47"><a href="#cb145-47" aria-hidden="true" tabindex="-1"></a><span class="do">## Accuracy was used to select the optimal model using the largest value.</span></span>
<span id="cb145-48"><a href="#cb145-48" aria-hidden="true" tabindex="-1"></a><span class="do">## The final value used for the model was cost = 0.1147368.</span></span></code></pre></div>
<p>Note that when you fit the model, there are a few things you could consider:</p>
<ul>
<li>You can consider centering and scaling the covariates. This can be done during pre-processing. Or you may specify <code>preProcess = c("center", "scale")</code> in the <code>train()</code> function.</li>
<li>You may want to start with a wider range of cost values, then narrow down to a smaller range, since SVM can be quite sensitive to tuning in some cases.</li>
<li>There are many other SVM libraries, such as <code>kernlab</code>. This can be specified by using <code>method = "svmLinear"</code>. However, <code>kernlab</code> uses <code>C</code> as the parameter name for cost. We will show an example later.</li>
</ul>
</div>
<div id="nonlinear-svm-via-kernel-trick" class="section level2 hasAnchor" number="13.5">
<h2 class="hasAnchor"><span class="header-section-number">13.5</span> Nonlinear SVM via Kernel Trick<a href="#nonlinear-svm-via-kernel-trick" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The essential idea of kernel trick can be summarized as using the kernel function of two observations <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{z}\)</span> to replace the inner product between some feature mapping of the two covariate vectors. In other words, if we want to create some nonlinear features of <span class="math inline">\(\mathbf{x}\)</span>, such as <span class="math inline">\(x_1^2\)</span>, <span class="math inline">\(\exp(x_2)\)</span>, <span class="math inline">\(\sqrt{x_3}\)</span>, etc., we may in general write them as</p>
<p><span class="math display">\[\Phi : {\cal X}\rightarrow {\cal F}, \,\,\, \Phi(\mathbf{x}) = (\phi_1(\mathbf{x}), \phi_2(\mathbf{x}), \ldots ),\]</span>
where <span class="math inline">\({\cal F}\)</span> has either finite or infinite dimensions. Then, we can still treat this as a linear SVM by constructing the decision rule as</p>
<p><span class="math display">\[f(x) = \langle \Phi(\mathbf{x}), \boldsymbol \beta\rangle = \Phi(\mathbf{x})^\text{T}\boldsymbol \beta.\]</span>
This is why we used the <span class="math inline">\(\langle \cdot, \cdot\rangle\)</span> operator in the previous example. Now, the kernel trick is essentially skipping the explicit calculation of <span class="math inline">\(\Phi(\mathbf{x})\)</span> by utilizing the property that</p>
<p><span class="math display">\[K(\mathbf{x}, \mathbf{z}) = \langle \Phi(\mathbf{x}), \Phi(\mathbf{z}) \rangle\]</span>
for some kernel function <span class="math inline">\(K(\mathbf{x}, \mathbf{z})\)</span>. Since <span class="math inline">\(\langle \Phi(\mathbf{x}), \Phi(\mathbf{z}) \rangle\)</span> is all we need in the dual form, we can simply replace it by <span class="math inline">\(K(\mathbf{x}, \mathbf{z})\)</span>, which gives the kernel form:</p>
<p><span class="math display">\[\begin{align}
\underset{\boldsymbol \alpha}{\max} \quad &amp; \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \color{OrangeRed}{K(\mathbf{x}_i, \mathbf{x}_j)} \\
\text{subject to} \quad &amp; 0 \leq \alpha_i \leq C, \,\, i = 1, \ldots, n, \\
\text{and} \quad &amp; \sum_{i = 1}^n \alpha_i y_i = 0.
\end{align}\]</span></p>
<p>One most apparent advantage of doing this is to save computational cost. This maybe understood using the following example:</p>
<ul>
<li>Consider kernel function <span class="math inline">\(K(\mathbf{x}, \mathbf{z}) = (\mathbf{x}^\text{T}\mathbf{z})^2\)</span></li>
<li>Consider <span class="math inline">\(\Phi(\mathbf{x})\)</span> being the basis expansion that contains all second order interactions: <span class="math inline">\(x_k x_l\)</span> for <span class="math inline">\(1 \leq k, l \leq p\)</span></li>
</ul>
<p>We can show that the two gives equivalent results, however, the kernel version is much faster. <span class="math inline">\(K(\mathbf{x}, \mathbf{z})\)</span> takes <span class="math inline">\(p+1\)</span> operations, while <span class="math inline">\(\langle \Phi(\mathbf{x}_i), \Phi(\mathbf{x}_j) \rangle\)</span> requires <span class="math inline">\(3p^2\)</span>.</p>
<p><span class="math display">\[\begin{align}
K(\mathbf{x}, \mathbf{z}) &amp;=~ \left(\sum_{k=1}^p x_k z_k\right) \left(\sum_{l=1}^p x_l z_l\right) \\
&amp;=~ \sum_{k=1}^p \sum_{l=1}^p x_k z_k x_l z_l \\
&amp;=~ \sum_{k, l=1}^p (x_k x_l) (z_k z_l) \\
&amp;=~ \langle \Phi(\mathbf{x}),  \Phi(\mathbf{z}) \rangle
\end{align}\]</span></p>
<p>Formally, this property is guaranteed by the <strong>Mercer’s theorem</strong> that states: The kernel matrix <span class="math inline">\(K\)</span> is positive semi-definite if and only if the function <span class="math inline">\(K(x_i ,x_j)\)</span> is equivalent to some inner product <span class="math inline">\(\langle \Phi(\mathbf{x}), \Phi(\mathbf{z}) \rangle\)</span>.</p>
<p>Besides making the calculation of nonlinear functions easier, using the kernel trick also implies that if we use a proper kernel function, then it defines a space of functions <span class="math inline">\({\cal H}\)</span> (reproducing kernel Hilbert space, RKHS) that can be represented in the form of <span class="math inline">\(f(x) = \sum_i \alpha_i K(x, x_i)\)</span> for some <span class="math inline">\(x_i\)</span> in <span class="math inline">\({\cal X}\)</span> (see the Moore–Aronszajn theorem) with a proper definition of inner product. However, this space is of infinite dimension, noticing that <span class="math inline">\(i\)</span> goes from 1 to infinity. However, as long as we search for the solution within <span class="math inline">\({\cal H}\)</span>, and also apply a proper penalty of the estimated function <span class="math inline">\(\widehat{f}(\mathbf{x})\)</span>, then our computational job will reduce to solving the <span class="math inline">\(\alpha_i\)</span>’s that corresponds to the observed <span class="math inline">\(n\)</span> data points, meaning that we only need to solve the solution within a finite space. This is guaranteed by the <strong>Representer theorem</strong>. There are numerous articles on the RKHS. Hence, we will not focus on introducing this technique. However, we will later on use this property in the penalized formulation of SVM.</p>
</div>
<div id="example-mixture.example-data" class="section level2 hasAnchor" number="13.6">
<h2 class="hasAnchor"><span class="header-section-number">13.6</span> Example: <code>mixture.example</code> Data<a href="#example-mixture.example-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use the <code>mixture.example</code> data in the <code>ElemStatLearn</code> package. In addition, we use a different package <code>kernlab</code>. The red dotted line indicates the true decision boundary.</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data</span>(mixture.example)</span>
<span id="cb146-3"><a href="#cb146-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-4"><a href="#cb146-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># redefine data</span></span>
<span id="cb146-5"><a href="#cb146-5" aria-hidden="true" tabindex="-1"></a>    px1 <span class="ot">=</span> mixture.example<span class="sc">$</span>px1</span>
<span id="cb146-6"><a href="#cb146-6" aria-hidden="true" tabindex="-1"></a>    px2 <span class="ot">=</span> mixture.example<span class="sc">$</span>px2</span>
<span id="cb146-7"><a href="#cb146-7" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> mixture.example<span class="sc">$</span>x</span>
<span id="cb146-8"><a href="#cb146-8" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> mixture.example<span class="sc">$</span>y</span>
<span id="cb146-9"><a href="#cb146-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb146-10"><a href="#cb146-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot the data and true decision boundary</span></span>
<span id="cb146-11"><a href="#cb146-11" aria-hidden="true" tabindex="-1"></a>    prob <span class="ot">&lt;-</span> mixture.example<span class="sc">$</span>prob</span>
<span id="cb146-12"><a href="#cb146-12" aria-hidden="true" tabindex="-1"></a>    prob.bayes <span class="ot">&lt;-</span> <span class="fu">matrix</span>(prob, </span>
<span id="cb146-13"><a href="#cb146-13" aria-hidden="true" tabindex="-1"></a>                         <span class="fu">length</span>(px1), </span>
<span id="cb146-14"><a href="#cb146-14" aria-hidden="true" tabindex="-1"></a>                         <span class="fu">length</span>(px2))</span>
<span id="cb146-15"><a href="#cb146-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">contour</span>(px1, px2, prob.bayes, <span class="at">levels=</span><span class="fl">0.5</span>, <span class="at">lty=</span><span class="dv">2</span>, </span>
<span id="cb146-16"><a href="#cb146-16" aria-hidden="true" tabindex="-1"></a>            <span class="at">labels=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;x1&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;x2&quot;</span>,</span>
<span id="cb146-17"><a href="#cb146-17" aria-hidden="true" tabindex="-1"></a>            <span class="at">main=</span><span class="st">&quot;SVM with linear kernal&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb146-18"><a href="#cb146-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(x, <span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb146-19"><a href="#cb146-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-20"><a href="#cb146-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train linear SVM using the kernlab package</span></span>
<span id="cb146-21"><a href="#cb146-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(kernlab)</span>
<span id="cb146-22"><a href="#cb146-22" aria-hidden="true" tabindex="-1"></a>    cost <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb146-23"><a href="#cb146-23" aria-hidden="true" tabindex="-1"></a>    svm.fit <span class="ot">&lt;-</span> <span class="fu">ksvm</span>(x, y, <span class="at">type=</span><span class="st">&quot;C-svc&quot;</span>, <span class="at">kernel=</span><span class="st">&#39;vanilladot&#39;</span>, <span class="at">C=</span>cost)</span>
<span id="cb146-24"><a href="#cb146-24" aria-hidden="true" tabindex="-1"></a><span class="do">##  Setting default kernel parameters</span></span>
<span id="cb146-25"><a href="#cb146-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-26"><a href="#cb146-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot the SVM decision boundary</span></span>
<span id="cb146-27"><a href="#cb146-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract the indices of the support vectors on the margin:</span></span>
<span id="cb146-28"><a href="#cb146-28" aria-hidden="true" tabindex="-1"></a>    sv.alpha<span class="ot">&lt;-</span><span class="fu">alpha</span>(svm.fit)[[<span class="dv">1</span>]][<span class="fu">which</span>(<span class="fu">alpha</span>(svm.fit)[[<span class="dv">1</span>]]<span class="sc">&lt;</span>cost)]</span>
<span id="cb146-29"><a href="#cb146-29" aria-hidden="true" tabindex="-1"></a>    sv.index<span class="ot">&lt;-</span><span class="fu">alphaindex</span>(svm.fit)[[<span class="dv">1</span>]][<span class="fu">which</span>(<span class="fu">alpha</span>(svm.fit)[[<span class="dv">1</span>]]<span class="sc">&lt;</span>cost)]</span>
<span id="cb146-30"><a href="#cb146-30" aria-hidden="true" tabindex="-1"></a>    sv.matrix<span class="ot">&lt;-</span>x[sv.index,]</span>
<span id="cb146-31"><a href="#cb146-31" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(sv.matrix, <span class="at">pch=</span><span class="dv">16</span>, <span class="at">col=</span><span class="fu">ifelse</span>(y[sv.index] <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">cex=</span><span class="fl">1.5</span>)</span>
<span id="cb146-32"><a href="#cb146-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-33"><a href="#cb146-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the hyperplane and the margins:</span></span>
<span id="cb146-34"><a href="#cb146-34" aria-hidden="true" tabindex="-1"></a>    w <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">cbind</span>(<span class="fu">coef</span>(svm.fit)[[<span class="dv">1</span>]])) <span class="sc">%*%</span> <span class="fu">xmatrix</span>(svm.fit)[[<span class="dv">1</span>]]</span>
<span id="cb146-35"><a href="#cb146-35" aria-hidden="true" tabindex="-1"></a>    b <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="fu">b</span>(svm.fit)</span>
<span id="cb146-36"><a href="#cb146-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-37"><a href="#cb146-37" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> <span class="sc">-</span>b<span class="sc">/</span>w[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>w[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb146-38"><a href="#cb146-38" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b<span class="dv">-1</span>)<span class="sc">/</span>w[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>w[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb146-39"><a href="#cb146-39" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b<span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>w[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>w[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-200-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Let’s also try a nonlinear SVM, using the radial kernel.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fit SVM with radial kernel, with cost = 5</span></span>
<span id="cb147-2"><a href="#cb147-2" aria-hidden="true" tabindex="-1"></a>    dat <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">y =</span> <span class="fu">factor</span>(y), x)</span>
<span id="cb147-3"><a href="#cb147-3" aria-hidden="true" tabindex="-1"></a>    fit <span class="ot">=</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> dat, <span class="at">scale =</span> <span class="cn">FALSE</span>, <span class="at">kernel =</span> <span class="st">&quot;radial&quot;</span>, <span class="at">cost =</span> <span class="dv">5</span>)</span>
<span id="cb147-4"><a href="#cb147-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb147-5"><a href="#cb147-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># extract the prediction</span></span>
<span id="cb147-6"><a href="#cb147-6" aria-hidden="true" tabindex="-1"></a>    xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">X1 =</span> px1, <span class="at">X2 =</span> px2)</span>
<span id="cb147-7"><a href="#cb147-7" aria-hidden="true" tabindex="-1"></a>    func <span class="ot">=</span> <span class="fu">predict</span>(fit, xgrid, <span class="at">decision.values =</span> <span class="cn">TRUE</span>)</span>
<span id="cb147-8"><a href="#cb147-8" aria-hidden="true" tabindex="-1"></a>    func <span class="ot">=</span> <span class="fu">attributes</span>(func)<span class="sc">$</span>decision</span>
<span id="cb147-9"><a href="#cb147-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb147-10"><a href="#cb147-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># visualize the decision rule</span></span>
<span id="cb147-11"><a href="#cb147-11" aria-hidden="true" tabindex="-1"></a>    ygrid <span class="ot">=</span> <span class="fu">predict</span>(fit, xgrid)</span>
<span id="cb147-12"><a href="#cb147-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(ygrid <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;bisque&quot;</span>, <span class="st">&quot;cadetblue1&quot;</span>), </span>
<span id="cb147-13"><a href="#cb147-13" aria-hidden="true" tabindex="-1"></a>         <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">cex =</span> <span class="fl">0.2</span>, <span class="at">main=</span><span class="st">&quot;SVM with radial kernal&quot;</span>)</span>
<span id="cb147-14"><a href="#cb147-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(x, <span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb147-15"><a href="#cb147-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb147-16"><a href="#cb147-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># our estimated function value, cut at 0</span></span>
<span id="cb147-17"><a href="#cb147-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">contour</span>(px1, px2, <span class="fu">matrix</span>(func, <span class="dv">69</span>, <span class="dv">99</span>), <span class="at">level =</span> <span class="dv">0</span>, <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb147-18"><a href="#cb147-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb147-19"><a href="#cb147-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the true probability, cut at 0.5</span></span>
<span id="cb147-20"><a href="#cb147-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">contour</span>(px1, px2, <span class="fu">matrix</span>(prob, <span class="dv">69</span>, <span class="dv">99</span>), <span class="at">level =</span> <span class="fl">0.5</span>, <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb147-21"><a href="#cb147-21" aria-hidden="true" tabindex="-1"></a>            <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-201-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>You may also consider some other popular kernels. The following ones are implemented in the <code>e1071</code> package, with additional tuning parameters <span class="math inline">\(\text{coef}_0\)</span> and <span class="math inline">\(\gamma\)</span>.</p>
<ul>
<li>Linear: <span class="math inline">\(K(\mathbf{x}, \mathbf{z}) = \mathbf{x}^\text{T}\mathbf{z}\)</span></li>
<li><span class="math inline">\(d\)</span>th degree polynomial: <span class="math inline">\(K(\mathbf{x}, \mathbf{z}) = (\text{coef}_0 + \gamma \mathbf{x}^\text{T}\mathbf{z})^d\)</span></li>
<li>Radial basis: <span class="math inline">\(K(\mathbf{x}, \mathbf{z}) = \exp(- \gamma \lVert \mathbf{x}- \mathbf{z}\lVert^2)\)</span></li>
<li>Sigmoid: <span class="math inline">\(\tanh(\gamma \mathbf{x}^\text{T}\mathbf{z}+ \text{coef}_0)\)</span></li>
</ul>
<p>Cross-validation can also be doing using the <code>caret</code> package. To specify the kernel, one must correctly specify the <code>method</code> parameter in the <code>train()</code> function. For this example, we use the <code>method = "svmRadial"</code> that uses the <code>kernlab</code> package to fit the model. For this choice, you need to tune just <code>sigma</code> and <code>C</code> (cost). More details are refereed to the <a href="https://topepo.github.io/caret/train-models-by-tag.html#support-vector-machines"><code>caret</code> documentation</a>.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a>  svm.radial <span class="ot">&lt;-</span> <span class="fu">train</span>(y <span class="sc">~</span> ., <span class="at">data =</span> dat, <span class="at">method =</span> <span class="st">&quot;svmRadial&quot;</span>,</span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">preProcess =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</span>
<span id="cb148-3"><a href="#cb148-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">C =</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="dv">1</span>), <span class="at">sigma =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)),</span>
<span id="cb148-4"><a href="#cb148-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>))</span>
<span id="cb148-5"><a href="#cb148-5" aria-hidden="true" tabindex="-1"></a>  svm.radial</span>
<span id="cb148-6"><a href="#cb148-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Support Vector Machines with Radial Basis Function Kernel </span></span>
<span id="cb148-7"><a href="#cb148-7" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb148-8"><a href="#cb148-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 200 samples</span></span>
<span id="cb148-9"><a href="#cb148-9" aria-hidden="true" tabindex="-1"></a><span class="do">##   2 predictor</span></span>
<span id="cb148-10"><a href="#cb148-10" aria-hidden="true" tabindex="-1"></a><span class="do">##   2 classes: &#39;0&#39;, &#39;1&#39; </span></span>
<span id="cb148-11"><a href="#cb148-11" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb148-12"><a href="#cb148-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Pre-processing: centered (2), scaled (2) </span></span>
<span id="cb148-13"><a href="#cb148-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (5 fold) </span></span>
<span id="cb148-14"><a href="#cb148-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Summary of sample sizes: 160, 160, 160, 160, 160 </span></span>
<span id="cb148-15"><a href="#cb148-15" aria-hidden="true" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb148-16"><a href="#cb148-16" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb148-17"><a href="#cb148-17" aria-hidden="true" tabindex="-1"></a><span class="do">##   C     sigma  Accuracy  Kappa</span></span>
<span id="cb148-18"><a href="#cb148-18" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.01  1      0.715     0.43 </span></span>
<span id="cb148-19"><a href="#cb148-19" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.01  2      0.760     0.52 </span></span>
<span id="cb148-20"><a href="#cb148-20" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.01  3      0.770     0.54 </span></span>
<span id="cb148-21"><a href="#cb148-21" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.10  1      0.720     0.44 </span></span>
<span id="cb148-22"><a href="#cb148-22" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.10  2      0.790     0.58 </span></span>
<span id="cb148-23"><a href="#cb148-23" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.10  3      0.800     0.60 </span></span>
<span id="cb148-24"><a href="#cb148-24" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.50  1      0.795     0.59 </span></span>
<span id="cb148-25"><a href="#cb148-25" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.50  2      0.815     0.63 </span></span>
<span id="cb148-26"><a href="#cb148-26" aria-hidden="true" tabindex="-1"></a><span class="do">##   0.50  3      0.830     0.66 </span></span>
<span id="cb148-27"><a href="#cb148-27" aria-hidden="true" tabindex="-1"></a><span class="do">##   1.00  1      0.795     0.59 </span></span>
<span id="cb148-28"><a href="#cb148-28" aria-hidden="true" tabindex="-1"></a><span class="do">##   1.00  2      0.825     0.65 </span></span>
<span id="cb148-29"><a href="#cb148-29" aria-hidden="true" tabindex="-1"></a><span class="do">##   1.00  3      0.835     0.67 </span></span>
<span id="cb148-30"><a href="#cb148-30" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb148-31"><a href="#cb148-31" aria-hidden="true" tabindex="-1"></a><span class="do">## Accuracy was used to select the optimal model using the largest value.</span></span>
<span id="cb148-32"><a href="#cb148-32" aria-hidden="true" tabindex="-1"></a><span class="do">## The final values used for the model were sigma = 3 and C = 1.</span></span></code></pre></div>
</div>
<div id="svm-as-a-penalized-model" class="section level2 hasAnchor" number="13.7">
<h2 class="hasAnchor"><span class="header-section-number">13.7</span> SVM as a Penalized Model<a href="#svm-as-a-penalized-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall that in SVM, we need <span class="math inline">\(y_i f(\mathbf{x}_i)\)</span> to be at least <span class="math inline">\(1 - \xi_i\)</span>, this implies that we would prefer <span class="math inline">\(1 - y_i f(\mathbf{x}_i)\)</span> to be negative or 0. And observation with <span class="math inline">\(1 - y_i f(\mathbf{x}_i)\)</span> should be penalized. Hence, recall that the objective function of dual form in SVM is <span class="math inline">\(\frac{1}{2}\lVert \boldsymbol \beta\rVert^2 + C \sum_{i=1}^n \xi_i\)</span>, we may rewrite this as a new version:</p>
<p><span class="math display">\[\min \,\, \sum_{i=1}^n \big[ 1 - y_i f(\mathbf{x}_i) \big]_{+} \, +\, \lambda \lVert \boldsymbol \beta\rVert^2.\]</span>
Here, we converted <span class="math inline">\(1/(2C)\)</span> to <span class="math inline">\(\lambda\)</span>. And this resembles a familiar form of “Loss <span class="math inline">\(+\)</span> Penalty”, where the slack variables becomes the loss and the norm of <span class="math inline">\(\boldsymbol \beta\)</span> is the penalty. This particular loss function is called the <strong>Hinge loss</strong>, with</p>
<p><span class="math display">\[L(y, f(\mathbf{x})) = [1 - yf(\mathbf{x})]_+ = \max(0, 1 - yf(\mathbf{x}))\]</span>
However, the Hinge loss is not differentiable. There are some other loss functions that can be used as substitute:</p>
<ul>
<li>Logistic loss:
<span class="math display">\[L(y, f(\mathbf{x})) = \log_2( 1 + e^{-y f(\mathbf{x})})\]</span></li>
<li>Modified Huber Loss:
<span class="math display">\[L(y, f(\mathbf{x})) = \begin{cases}
\max(0, 1 - yf(\mathbf{x}))^2 &amp; \text{for} \quad yf(\mathbf{x}) \geq -1 \\
-4 yf(\mathbf{x})  &amp; \text{otherwise}  \\
\end{cases}\]</span></li>
</ul>
<p>Here is a visualization of several different loss functions.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a>  t <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">3</span>, <span class="fl">0.01</span>)</span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># different loss functions</span></span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a>  hinge <span class="ot">=</span> <span class="fu">pmax</span>(<span class="dv">0</span>, <span class="dv">1</span> <span class="sc">-</span> t) </span>
<span id="cb149-5"><a href="#cb149-5" aria-hidden="true" tabindex="-1"></a>  zeroone <span class="ot">=</span> (t <span class="sc">&lt;=</span> <span class="dv">0</span>)</span>
<span id="cb149-6"><a href="#cb149-6" aria-hidden="true" tabindex="-1"></a>  logistic <span class="ot">=</span> <span class="fu">log2</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>t))</span>
<span id="cb149-7"><a href="#cb149-7" aria-hidden="true" tabindex="-1"></a>  modifiedhuber <span class="ot">=</span> <span class="fu">ifelse</span>(t <span class="sc">&gt;=</span> <span class="sc">-</span><span class="dv">1</span>, (<span class="fu">pmax</span>(<span class="dv">0</span>, <span class="dv">1</span> <span class="sc">-</span> t))<span class="sc">^</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">4</span><span class="sc">*</span>t)</span>
<span id="cb149-8"><a href="#cb149-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb149-9"><a href="#cb149-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot</span></span>
<span id="cb149-10"><a href="#cb149-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(t, zeroone, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">4</span>),</span>
<span id="cb149-11"><a href="#cb149-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Loss Functions&quot;</span>)</span>
<span id="cb149-12"><a href="#cb149-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(t, hinge, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, )</span>
<span id="cb149-13"><a href="#cb149-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(t, logistic, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb149-14"><a href="#cb149-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(t, modifiedhuber, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb149-15"><a href="#cb149-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Zero-one&quot;</span>, <span class="st">&quot;Hinge&quot;</span>, <span class="st">&quot;Logistic&quot;</span>, <span class="st">&quot;Modified Huber&quot;</span>),</span>
<span id="cb149-16"><a href="#cb149-16" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>), </span>
<span id="cb149-17"><a href="#cb149-17" aria-hidden="true" tabindex="-1"></a>         <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-203-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>For linear decision rules, with <span class="math inline">\(f(\mathbf{x}) = \beta_0 + \mathbf{x}^\text{T}\boldsymbol \beta\)</span>, this should be trivial to solve. However, we also want to consider nonlinear decision functions. But the above form does not contain a kernel function to use the kernel trick. The <strong>Representer Theorem</strong> <span class="citation">(<a href="#ref-kimeldorf1970correspondence" role="doc-biblioref">Kimeldorf and Wahba 1970</a>)</span> can help us in this case. This theorem was originally developed for in the setting of Chebyshev splines, but later on generalized. The theorem ensures that if we solve the function <span class="math inline">\(f\)</span> with regularization with respect to the norm in the RKHS induced from a kernel function <span class="math inline">\(K\)</span>, then the solution must admits a finite representation of the form (although the space <span class="math inline">\({\cal H}\)</span> we search for the solution is infinite):</p>
<p><span class="math display">\[\widehat{f}(\mathbf{x}) = \sum_{i = 1}^n \beta_i K(\mathbf{x}, \mathbf{x}_i).\]</span>
This suggests that the optimization problem becomes</p>
<p><span class="math display">\[\sum_{i=1}^n L(y_i, \mathbf{K}_i^\text{T}\boldsymbol \beta) + \lambda \boldsymbol \beta^\text{T}\mathbf{K}\boldsymbol \beta,\]</span>
where <span class="math inline">\(\mathbf{K}_{n \times n}\)</span> is the kernel matrix with <span class="math inline">\(\mathbf{K}_{ij} = K(x_i, x_j)\)</span>, and <span class="math inline">\(\mathbf{K}_i\)</span> is the <span class="math inline">\(i\)</span> the column of <span class="math inline">\(\mathbf{K}\)</span>. This is an unconstrained optimization problem that can be solved using gradient decent if <span class="math inline">\(L\)</span> is differentiable. More details will be presented in the next Chapter.</p>
</div>
<div id="kernel-and-feature-maps-another-example" class="section level2 hasAnchor" number="13.8">
<h2 class="hasAnchor"><span class="header-section-number">13.8</span> Kernel and Feature Maps: Another Example<a href="#kernel-and-feature-maps-another-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We give another example about the equivalence of kernel and the inner product of feature maps, which is ensured by the Mercer’s Theorem <span class="citation">(<a href="#ref-mercer1909xvi" role="doc-biblioref">Mercer 1909</a>)</span>. Consider the Gaussian kernel <span class="math inline">\(e^{-\gamma \lVert \mathbf{x}- \mathbf{z}\rVert}\)</span>. We can write, using Tayler expansion,</p>
<p><span class="math display">\[\begin{align}
&amp;e^{\gamma \lVert \mathbf{x}- \mathbf{z}\rVert} \nonumber \\
=&amp; e^{-\gamma \lVert \mathbf{x}\rVert + 2 \gamma \mathbf{x}^\text{T}\mathbf{z}- \gamma \lVert \mathbf{z}\rVert} \nonumber \\
=&amp; e^{-\gamma \lVert \mathbf{x}\rVert - \gamma \lVert \mathbf{z}\rVert} \bigg[ 1 + \frac{2 \gamma \mathbf{x}^\text{T}\mathbf{z}}{1!} + \frac{(2 \gamma \mathbf{x}^\text{T}\mathbf{z})^2}{2!} + \frac{(2 \gamma \mathbf{x}^\text{T}\mathbf{z})^3}{3!} + \cdots \bigg]
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\mathbf{x}^\text{T}\mathbf{z}\)</span> is the inner product of all first order feature maps. We also showed previously <span class="math inline">\((\mathbf{x}^\text{T}\mathbf{z})^2\)</span> is equivalent to the inner product of all second order feature maps (<span class="math inline">\(\Phi_2(\mathbf{x})\)</span>), and <span class="math inline">\((\mathbf{x}^\text{T}\mathbf{z})^3\)</span> would be equivalent to the third order version (<span class="math inline">\(\Phi_3(\mathbf{x})\)</span>), etc.. Hence, the previous equation can be written as the inner product of feature maps in the form of</p>
<p><span class="math display">\[e^{-\gamma \lVert \mathbf{x}\rVert} \bigg[ 1, \sqrt{\frac{2\gamma}{1!}} \mathbf{x}^\text{T}, \sqrt{\frac{(2\gamma)^2}{2!}} \Phi_2^\text{T}(\mathbf{x}), \sqrt{\frac{(2\gamma)^3}{3!}} \Phi_3^\text{T}(\mathbf{x}), \cdots \bigg]\]</span></p>
<p>This shows the Gaussian kernel is corresponding to all polynomials with a scaling factor of <span class="math inline">\(e^{-\gamma \lVert \mathbf{x}\rVert}\)</span></p>
<!--chapter:end:05.1-svm.Rmd-->
</div>
</div>
<div id="reproducing-kernel-hilbert-space" class="section level1 hasAnchor" number="14">
<h1 class="hasAnchor"><span class="header-section-number">14</span> Reproducing Kernel Hilbert Space<a href="#reproducing-kernel-hilbert-space" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In the previous chapter of SVM, we gave an example to show that instead of using the inner product <span class="math inline">\(\langle \Phi(\mathbf{x}), \Phi(\mathbf{z}) \rangle\)</span> between the feature maps of <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{z}\)</span>, we can instead use the kernel trick <span class="math inline">\(K(\mathbf{x}, \mathbf{z})\)</span> to perform the exact same calculation. And also, in the penalized kernel version, we mentioned that the decision rule can be expressed in the finite sample form of <span class="math inline">\(\sum_{i = 1}^n \beta_i K(\cdot, \mathbf{x}_i)\)</span> by the Representer Theorem. All of these are based on a fundamental tool of the Reproducing Kernel Hilbert Space and we will provide some basic knowledge of it. We will also prove the Representer Theorem <span class="citation">(<a href="#ref-kimeldorf1970correspondence" role="doc-biblioref">Kimeldorf and Wahba 1970</a>)</span>, which is very similar to the proof of smoothing spline.</p>
<div id="constructing-the-rkhs" class="section level2 hasAnchor" number="14.1">
<h2 class="hasAnchor"><span class="header-section-number">14.1</span> Constructing the RKHS<a href="#constructing-the-rkhs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall that in the smoothing spline example, we wanted to fit a regression model by solving for a function <span class="math inline">\(f\)</span> in a quite complicated space, the second order Sobolev space. We could not exhaust all the candidates in this space because that would be computationally untraceable. However, the results there shows that the solution has a finite representation. In general, when we solve a regression problem using a <strong>Loss <span class="math inline">\(+\)</span> Penalty</strong> form, we will also enjoy that property if we search the (regression or decision) function <span class="math inline">\(f\)</span> within a RKHS. So let’s first define what this space look like.</p>
<p>We start with the feature space <span class="math inline">\(\cal X\)</span> of <span class="math inline">\(X\)</span>, where <span class="math inline">\(X\)</span> is just the <span class="math inline">\(p\)</span> dimensional feature we often deal with. Let’s say we have a sample <span class="math inline">\(x_1\)</span>, then if we have a kernel function <span class="math inline">\(k(\cdot, \cdot)\)</span>, we can construct a new function called <span class="math inline">\(K(x_1, \cdot)\)</span>. Keep in mind that <span class="math inline">\(K(x_1, \cdot)\)</span> is a function with argument <span class="math inline">\(\cdot\)</span> and parameter <span class="math inline">\(x_1\)</span> in this case. Similarly, we can do another sample, say <span class="math inline">\(x_2\)</span> and generate a function based on that sample, called <span class="math inline">\(K(x_1, \cdot)\)</span>. The following plot shows three of such functions, using red, orange and blue lines, receptively.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-206-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Since we can have many samples from <span class="math inline">\(\cal X\)</span>, we will also have infinite such functions like <span class="math inline">\(K(x, \cdot)\)</span>, and also the linear combinations of them would also be interesting to us. Let’s consider a space <span class="math inline">\({\cal G}\)</span> of all such functions</p>
<p><span class="math display">\[{\cal G} = \left\{\sum_{i}^n \alpha_i K(x_i, \cdot) \mid \alpha \in \mathbb{R}, n \in \mathbb{N}, x_i \in {\cal X} \right\} \]</span>
The black curve in the previous plot is an example of such linear combinations. We can see that the functions within <span class="math inline">\({\cal G}\)</span> start to become more and more flexible as we consider all the linear combinations. And as one final step, we will consider the completion of this space, which leads to the RKHS.</p>
<p><span class="math display">\[\cal H = \bar{\cal G}.\]</span>
Completion here means that <span class="math inline">\(\cal H\)</span> will contain the limits of all Cauchy sequences of such functions in <span class="math inline">\(\cal G\)</span>.</p>
</div>
<div id="properties-of-rkhs" class="section level2 hasAnchor" number="14.2">
<h2 class="hasAnchor"><span class="header-section-number">14.2</span> Properties of RKHS<a href="#properties-of-rkhs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This space <span class="math inline">\(\cal H\)</span> enjoys several important and useful properties. First, by the <a href="https://en.wikipedia.org/wiki/Riesz_representation_theorem">Riesz representation theorem</a>, we know that <span class="math inline">\(\cal H\)</span> is a <strong>Hilbert space with the reproducing property</strong>. For a (real valued) Hilbert space, it must satisfy</p>
<ul>
<li>symmetric: <span class="math inline">\(\langle K_x, K_z \rangle = \langle K_z, K_x \rangle\)</span></li>
<li>linear: <span class="math inline">\(\langle a K_{x_1} + b K_{x_2}, K_z \rangle = a \langle K_{x_1}, K_z \rangle + b \langle K_{x_2}, K_z \rangle\)</span></li>
<li>positive definite: <span class="math inline">\(\langle K_x, K_x \rangle \geq 0\)</span> and <span class="math inline">\(\langle K_x, K_x \rangle = 0\)</span> iff <span class="math inline">\(K_x = 0\)</span></li>
</ul>
<p>Also, the reproducing property means that when we evaluate a function <span class="math inline">\(f \in \cal H\)</span> at a point <span class="math inline">\(x\)</span>, it is the same as calculating the inner product between <span class="math inline">\(f\)</span> and <span class="math inline">\(K_x\)</span>. Formally,</p>
<p><span class="math display">\[f(x) = \langle f, K_x \rangle_{\cal H}\]</span></p>
<p>Now, we could simply take <span class="math inline">\(f = K_z\)</span>, that means, evaluating <span class="math inline">\(K_z(x)\)</span> is</p>
<p><span class="math display">\[K_z(x) = \langle K_z, K_x \rangle_{\cal H}\]</span>
Note that <span class="math inline">\(K_z(x) = K(z, x)\)</span>, this implies that the inner product in <span class="math inline">\(\cal H\)</span> is done by the kernel:</p>
<p><span class="math display">\[\langle K_z, K_x \rangle_{\cal H} = K(z, x)\]</span>
For example, if we have <span class="math inline">\(f(\cdot) = \sum_i \alpha_i K(x_i, \cdot)\)</span>, then evaluating <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span> is</p>
<p><span class="math display">\[\begin{align}
f(x) =&amp; \, \langle f, K(x, \cdot) \rangle_{\cal H} \nonumber \\
=&amp; \, \left\langle \sum_i \alpha_i K(x_i, \cdot), K(x, \cdot) \right\rangle_{\cal H} \nonumber \\
=&amp; \, \sum_i \alpha_i \left\langle K(x_i, \cdot), K(x, \cdot) \right\rangle_{\cal H} \nonumber \\
=&amp; \, \sum_i \alpha_i K(x_i, x)
\end{align}\]</span></p>
<p>The Moore–Aronszajn theorem <span class="citation">(<a href="#ref-aronszajn1950theory" role="doc-biblioref">Aronszajn 1950</a>)</span> ensures that a positive definite kernel <span class="math inline">\(K(\cdot, \cdot)\)</span> on <span class="math inline">\(\cal X\)</span> would uniquely define such a RKHS, where <span class="math inline">\(K(\cdot, \cdot)\)</span> itself is the reproducing kernel. Hence, all we need is the original <span class="math inline">\(\cal X\)</span> and a kernel function. Then the RKHS can be defined as we stated previously, with all the nice properties. Besides these, another results by Mercer interprets kernels as feature maps, which we have already see in the SVM chapter that <span class="math inline">\(K(x, z)= \langle \Phi(x), \Phi(z) \rangle\)</span>. Overall, we set some relationships among these three quantities in their respective spaces:</p>
<ul>
<li>original features <span class="math inline">\(x\)</span></li>
<li>feature maps <span class="math inline">\(\Phi(x)\)</span></li>
<li>functions <span class="math inline">\(K(x, \cdot)\)</span></li>
</ul>
</div>
<div id="the-representer-theorem" class="section level2 hasAnchor" number="14.3">
<h2 class="hasAnchor"><span class="header-section-number">14.3</span> The Representer Theorem<a href="#the-representer-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="math inline">\(\cal H\)</span> is still a very large space of functions. And it is not clear if we want to find <span class="math inline">\(f\)</span> in <span class="math inline">\(\cal H\)</span> for our optimization problem, how do we computationally complete that task. It is unlikely that we can exhaust all such functions. Well, luckily, we don’t need to. This is ensured by the <strong>Representer Theorem</strong>, which states that only a finite sample presentation is needed.</p>
<div class="theorem">
<p><span id="thm:unnamed-chunk-207" class="theorem"><strong>(#thm:unnamed-chunk-207) (Representer Theorem) </strong></span>If we are given a set of data <span class="math inline">\(\{x_i, y_i\}_{i=1}^n\)</span>, and we search for the best solution in <span class="math inline">\({\cal H}\)</span> of the optimization problem
<span class="math display">\[\widehat f = \underset{f \in \cal H}{\arg\min} \,\, {\cal L}(\{y_i, f(x_i)\}_{i=1}^n) + p(\| f \|_{{\cal H}}^2 ),\]</span>
where <span class="math inline">\({\cal L}\)</span> is the loss function, <span class="math inline">\(p\)</span> is a monotone penalty function, and <span class="math inline">\({\cal H}\)</span> is the RKHS with kernel <span class="math inline">\(K\)</span>. Then the solution must take the form
<span class="math display">\[\widehat f = \sum_{i=1}^n w_i K(\cdot, x_i)\]</span></p>
</div>
<p>The proof is quite simple. The logic is the same as the smoothing spline proof.</p>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>We can first use the kernel <span class="math inline">\(K\)</span> associated with <span class="math inline">\(\cal H\)</span> to define a set of functions
<span class="math display">\[K(\cdot, x_i), \, K(\cdot, x_2), \, \cdots, \, K(\cdot, x_n)\]</span></p>
<p>Then, suppose the solution is some function <span class="math inline">\(f \in {\cal H}\)</span>, we could find its projection on the space spaned by these functions. This means that we could write <span class="math inline">\(f\)</span> as</p>
<p><span class="math display">\[f(\cdot) = \sum_{i=1}^n \alpha_i K(\cdot, x_i) + h(\cdot)\]</span></p>
<p>for some <span class="math inline">\(\alpha_i\)</span> and <span class="math inline">\(h(\cdot)\)</span>. Also, since <span class="math inline">\(h(\cdot)\)</span> is in the orthogonal space of all such <span class="math inline">\(K(\cdot, x_i)\)</span>, we have, by the reproducing property,</p>
<p><span class="math display">\[ h(x_i) = \langle K(x_i, \cdot), h(\cdot) \rangle = 0\]</span></p>
<p>for all <span class="math inline">\(i\)</span>. You may recall our proof in the smoothing spline for the same construction of <span class="math inline">\(h\)</span> that has <span class="math inline">\(h(x_i) = 0\)</span> for all <span class="math inline">\(i\)</span>. By the reproducing property, we have, for any observations in the training data,</p>
<p><span class="math display">\[\begin{align}
f(x_j) =&amp; \langle f(\cdot), K(\cdot, x_j) \rangle \nonumber \\
=&amp; \left\langle \sum_{i=1}^n \alpha_i K(x_i, \cdot) + h(\cdot), K(\cdot, x_j) \right\rangle \nonumber \\
=&amp; \sum_{i=1}^n \alpha_i K(x_i, x_j) + \sum_{i=1}^n \alpha_i h(x_j) \nonumber \\
=&amp; \sum_{i=1}^n \alpha_i K(x_i, x_j)
\end{align}\]</span></p>
<p>Which means that, the evaluation of <span class="math inline">\(f(x_j)\)</span> would be the same as just evaluating it on this finite represtantation. Hence the loss function would be the same regardless of whether we have <span class="math inline">\(h\)</span> or not. And also the penalty term of this finite represtantation would be better since</p>
<p><span class="math display">\[\begin{align}
\lVert f \rVert^2 =&amp; \lVert \sum_{i=1}^n \alpha_i K(\cdot, x_i) + h(\cdot) \rVert^2 \nonumber \\
=&amp; \lVert \sum_{i=1}^n \alpha_i K(\cdot, x_i) \rVert^2 + \lVert h(\cdot) \rVert^2 \nonumber \\
\geq&amp; \lVert \sum_{i=1}^n \alpha_i K(\cdot, x_i) \rVert^2
\end{align}\]</span></p>
<p>This completes the proof since this finite represetnation would be the one being prefered than <span class="math inline">\(f\)</span>.</p>
</div>
<!--chapter:end:05.2-rkhs.Rmd-->
</div>
</div>
<div id="kernel-ridge-regression" class="section level1 hasAnchor" number="15">
<h1 class="hasAnchor"><span class="header-section-number">15</span> Kernel Ridge Regression<a href="#kernel-ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>With our understandings of the RKHS and the representer theorem, we can now say that for any regression function models, if we want the solution to be more flexible, we may solve it within a RKHS. For example, consider the following regression problem:</p>
<p><span class="math display">\[\widehat f = \underset{f \in {\cal H}}{\arg\min} \,\, \frac{1}{n} \sum_{i=1}^n \Big(y_i - \widehat f(x_i) \Big)^2 + \lambda \lVert f \rVert_{\cal H}^2\]</span>
Since we know that the solution has to take the form</p>
<p><span class="math display">\[\widehat f = \sum_{i=1}^n \alpha_i K(x_i, \cdot),\]</span>
we can instead solve the problem as a ridge regression type of problem:</p>
<p><span class="math display">\[\widehat f = \underset{f \in {\cal H}}{\arg\min} \,\, \frac{1}{n} \big\lVert \mathbf{y}- \mathbf{K}\boldsymbol \alpha\big\rVert^2 + \lambda \lVert f \rVert_{\cal H}^2,\]</span>
where <span class="math inline">\(\mathbf{K}\)</span> is an <span class="math inline">\(n \times n\)</span> matrix with <span class="math inline">\(K(x_i, x_j)\)</span> at its <span class="math inline">\((i,j)\)</span>th element. With some simple calculation, we also have</p>
<p><span class="math display">\[\begin{align}
\lVert f \rVert_{\cal H}^2 =&amp; \langle f, f \rangle \nonumber \\
=&amp; \langle \sum_{i=1}^n \alpha_i K(x_i, \cdot), \sum_{j=1}^n \alpha_j K(x_j, \cdot) \rangle \nonumber \\
=&amp; \sum_{i, j} \alpha_i \alpha_j \big\langle K(x_i, \cdot), K(x_j, \cdot) \big\rangle \nonumber \\
=&amp; \sum_{i, j} \alpha_i \alpha_j K(x_i, x_j) \nonumber \\
=&amp; \boldsymbol \alpha^\text{T}\mathbf{K}\boldsymbol \alpha
\end{align}\]</span></p>
<p>Hence, the problem becomes</p>
<p><span class="math display">\[\widehat f = \underset{f \in {\cal H}}{\arg\min} \,\, \frac{1}{n} \big\lVert \mathbf{y}- \mathbf{K}\boldsymbol \alpha\big\rVert^2 + \lambda \boldsymbol \alpha^\text{T}\mathbf{K}\boldsymbol \alpha.\]</span>
By taking the derivative with respect to <span class="math inline">\(\boldsymbol \alpha\)</span>, we have (note that <span class="math inline">\(\mathbf{K}\)</span> is symmetric),</p>
<p><span class="math display">\[\begin{align}
-\frac{1}{n} \mathbf{K}^\text{T}(\mathbf{y}- \mathbf{K}\boldsymbol \alpha) + \lambda \mathbf{K}\boldsymbol \alpha\overset{\text{set}}{=} \mathbf{0} \nonumber \\
\mathbf{K}(- \mathbf{y}+ \mathbf{K}\boldsymbol \alpha+ n\lambda \boldsymbol \alpha) = \mathbf{0}.
\end{align}\]</span>
This implies</p>
<p><span class="math display">\[ \boldsymbol \alpha= (\mathbf{K}+ n\lambda \mathbf{I})^{-1} \mathbf{y}.\]</span>
and we obtained the solution.</p>
<div id="example-linear-kernel-and-ridge-regression" class="section level2 hasAnchor" number="15.1">
<h2 class="hasAnchor"><span class="header-section-number">15.1</span> Example: Linear Kernel and Ridge Regression<a href="#example-linear-kernel-and-ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When <span class="math inline">\(K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^\text{T}\mathbf{x}_j\)</span>, we also have <span class="math inline">\(\mathbf{K}= \mathbf{X}\mathbf{X}^\text{T}\)</span>. We should expect this to match the original ridge regression since this is essentially a linear regression. First, plug this into our previous result, we have</p>
<p><span class="math display">\[ \boldsymbol \alpha= (\mathbf{X}\mathbf{X}^\text{T}+ n\lambda \mathbf{I})^{-1} \mathbf{y}.\]</span>
and the fitted value is</p>
<p><span class="math display">\[ \widehat{\mathbf{y}} = \mathbf{K}\boldsymbol \alpha= \mathbf{X}\mathbf{X}^\text{T}(\mathbf{X}\mathbf{X}^\text{T}+ n\lambda \mathbf{I})^{-1} \mathbf{y}\]</span>
Using a matrix identity <span class="math inline">\((\mathbf{P}\mathbf{Q}+ \mathbf{I})^{-1}\mathbf{P}= \mathbf{P}(\mathbf{Q}\mathbf{P}+ \mathbf{I})^{-1}\)</span>, and let <span class="math inline">\(\mathbf{Q}= \mathbf{X}= \mathbf{P}^\text{T}\)</span>, we have</p>
<p><span class="math display">\[ \widehat{\mathbf{y}} = \mathbf{K}\boldsymbol \alpha= \mathbf{X}(\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y}\]</span>
and</p>
<p><span class="math display">\[ \widehat{\mathbf{y}} = \mathbf{X}\underbrace{\big[ \mathbf{X}^\text{T}\boldsymbol \alpha\big]}_{\boldsymbol \beta} = \mathbf{X}\underbrace{\big[ (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y}\big]}_{\boldsymbol \beta}\]</span></p>
<p>which is simply the Ridge regression solution, and also the corresponding linear regression solution <span class="math inline">\(\widehat{\boldsymbol \beta} = \mathbf{X}^\text{T}\widehat{\boldsymbol \alpha}\)</span>. This makes the penalty term <span class="math inline">\(\boldsymbol \alpha^\text{T}\mathbf{K}\boldsymbol \alpha= \boldsymbol \alpha^\text{T}\mathbf{X}\mathbf{X}^\text{T}\boldsymbol \alpha= \boldsymbol \beta^\text{T}\boldsymbol \beta\)</span>, which maps every thing back to the ridge regression form.</p>
</div>
<div id="example-alternative-view" class="section level2 hasAnchor" number="15.2">
<h2 class="hasAnchor"><span class="header-section-number">15.2</span> Example: Alternative View<a href="#example-alternative-view" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This example is motivated from an alternative derivation provided by Prof. Max Welling on his kernel ridge regression lecture note. This understanding matches the SVM primal to dual derivation, but is performed on a linear regression. We can then again switch things to the kernel version (through kernel trick).</p>
<p>Consider a linear regression</p>
<p><span class="math display">\[\underset{\boldsymbol \beta}{\text{minimize}} \,\, \frac{1}{n} \lVert \mathbf{y}- \mathbf{X}\boldsymbol \beta\rVert^2 + \lambda \lVert \boldsymbol \beta\rVert^2\]</span></p>
<p>Introduce a new set of variables</p>
<p><span class="math display">\[z_i = y_i - \mathbf{x}_i^\text{T}\boldsymbol \beta,\]</span>
for <span class="math inline">\(i = 1, \ldots, n\)</span>. Then The original problem becomes</p>
<p><span class="math display">\[\begin{align}
\underset{\boldsymbol \beta, \mathbf{z}}{\text{minimize}} \quad &amp; \frac{1}{2n\lambda} \lVert \mathbf{z}\rVert^2 + \frac{1}{2} \lVert \boldsymbol \beta\rVert^2 \nonumber \\
\text{subj. to} \quad &amp; z_i = y_i - \mathbf{x}_i^\text{T}\boldsymbol \beta, \,\, i = 1, \ldots, n.
\end{align}\]</span></p>
<p>If we use the same strategy from the SVM derivation, we have the Lagrangian</p>
<p><span class="math display">\[{\cal L} = \frac{1}{2n\lambda} \lVert \mathbf{z}\rVert^2 + \frac{1}{2} \lVert \boldsymbol \beta\rVert^2 + \sum_{i=1}^n \alpha_i (y_i - \mathbf{x}_i^\text{T}\boldsymbol \beta- z_i)\]</span>
with <span class="math inline">\(\alpha_i \in \mathbb{R}\)</span>. Switching from primal to dual, by taking derivative w.r.t. <span class="math inline">\(\boldsymbol \beta\)</span> and <span class="math inline">\(\mathbf{z}\)</span>, we have</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \cal L}{\partial z_i} =&amp;\, \frac{1}{n\lambda}z_i - \alpha_i = 0, \quad \text{for} \,\, i = 1, \ldots, n, \nonumber \\
\text{and}\,\, \frac{\partial \cal L}{\partial \boldsymbol \beta} =&amp;\, \boldsymbol \beta- \sum_{i=1}^n \alpha_i \mathbf{x}_i = \mathbf{0}
\end{align}\]</span></p>
<p>Hence, we have, the estimated <span class="math inline">\(\widehat{\boldsymbol \beta}\)</span> is <span class="math inline">\(\sum_{i=1}^n \alpha_i \mathbf{x}_i\)</span> that matches our previous understanding. Also, if we view this as a linear kernel solution, the predicted value of at <span class="math inline">\(\mathbf{x}\)</span> is</p>
<p><span class="math display">\[\begin{align}
f(\mathbf{x}) =&amp; \,\, \mathbf{x}^\text{T}\boldsymbol \beta\nonumber \\
=&amp; \sum_{i=1}^n \alpha_i \mathbf{x}^\text{T}\mathbf{x}_i \nonumber \\
=&amp; \sum_{i=1}^n \alpha_i K(\mathbf{x}, \mathbf{x}_i).
\end{align}\]</span></p>
<p>Now, to complete our dual solution, we plugin these results, and have</p>
<p><span class="math display">\[\begin{align}
\underset{\boldsymbol \alpha}{\max} \underset{\mathbf{z}, \boldsymbol \beta}{\min} {\cal L} =&amp; \frac{n\lambda}{2} \boldsymbol \alpha^\text{T}\boldsymbol \alpha+ \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j x_i^\text{T}x_j + \sum_{j} \alpha_j \big(y_j - x_j^\text{T}\sum_i \alpha_i \mathbf{x}_i - n\lambda \alpha_i \big) \nonumber \\
=&amp; - \frac{n\lambda}{2} \boldsymbol \alpha^\text{T}\boldsymbol \alpha- \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j \langle x_i, x_j \rangle + \sum_{i} \alpha_i y_i \nonumber \\
=&amp; - \frac{n\lambda}{2} \boldsymbol \alpha^\text{T}\boldsymbol \alpha- \frac{1}{2} \boldsymbol \alpha^\text{T}\mathbf{K}\boldsymbol \alpha+ \boldsymbol \alpha^\text{T}\mathbf{y}
\end{align}\]</span></p>
<p>By again taking derivative w.r.t. <span class="math inline">\(\alpha\)</span>, we have</p>
<p><span class="math display">\[ - n\lambda \mathbf{I}\boldsymbol \alpha- \mathbf{K}\boldsymbol \alpha+ \mathbf{y}= \mathbf{0},\]</span>
and the solution is the same as what we had before</p>
<p><span class="math display">\[\boldsymbol \alpha= (\mathbf{K}+ n\lambda \mathbf{I})^{-1} \mathbf{y}\]</span></p>
<!--chapter:end:05.3-kernelridge.Rmd-->
</div>
</div>
<div id="classification-and-regression-trees" class="section level1 hasAnchor" number="16">
<h1 class="hasAnchor"><span class="header-section-number">16</span> Classification and Regression Trees<a href="#classification-and-regression-trees" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>A tree model is very simple to fit and enjoys interpretability. It is also the core component of random forest and boosting. Both trees and random forests can be used for classification and regression problems, although trees are not ideal for regressions problems due to its large bias. There are two main stream of tree models, Classification and Regression Trees (CART, <span class="citation">Breiman et al. (<a href="#ref-breiman1984classification" role="doc-biblioref">1984</a>)</span>) and C4.5 <span class="citation">(<a href="#ref-quinlan1993c4" role="doc-biblioref">Quinlan 1993</a>)</span>, which is an improvement of the ID3 (Iterative Dichotomiser 3) algorithm. The main difference is to use binary or multiple splits and the criteria of the splitting rule. In fact the splitting rule criteria is probably the most essential part of a tree.</p>
<div id="example-classification-tree" class="section level2 hasAnchor" number="16.1">
<h2 class="hasAnchor"><span class="header-section-number">16.1</span> Example: Classification Tree<a href="#example-classification-tree" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s generate a model with nonlinear classification rule.</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a>    n <span class="ot">=</span> <span class="dv">500</span></span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a>    x1 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb150-4"><a href="#cb150-4" aria-hidden="true" tabindex="-1"></a>    x2 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb150-5"><a href="#cb150-5" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">ifelse</span>(x1<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> x2<span class="sc">^</span><span class="dv">2</span> <span class="sc">&lt;</span> <span class="fl">0.6</span>, <span class="fl">0.9</span>, <span class="fl">0.1</span>))</span>
<span id="cb150-6"><a href="#cb150-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb150-7"><a href="#cb150-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">rep</span>(<span class="dv">2</span>,<span class="dv">4</span>))</span>
<span id="cb150-8"><a href="#cb150-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x1, x2, <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb150-9"><a href="#cb150-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">symbols</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="at">circles =</span> <span class="fu">sqrt</span>(<span class="fl">0.6</span>), <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">inches =</span> <span class="cn">FALSE</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-213-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>A classification tree model is recursively splitting the feature space such that eventually each region is dominated by one class. We will use <code>rpart</code> as an example to fit trees, which stands for recursively partitioning.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(rpart)</span>
<span id="cb151-3"><a href="#cb151-3" aria-hidden="true" tabindex="-1"></a>    rpart.fit <span class="ot">=</span> <span class="fu">rpart</span>(<span class="fu">as.factor</span>(y)<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data =</span> <span class="fu">data.frame</span>(x1, x2, y))</span>
<span id="cb151-4"><a href="#cb151-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb151-5"><a href="#cb151-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the tree structure    </span></span>
<span id="cb151-6"><a href="#cb151-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">rep</span>(<span class="fl">0.5</span>, <span class="dv">4</span>))</span>
<span id="cb151-7"><a href="#cb151-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(rpart.fit)</span>
<span id="cb151-8"><a href="#cb151-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">text</span>(rpart.fit)    </span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-214-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if you want to peek into the tree </span></span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># note that we set cp = 0.041, which is a tuning parameter</span></span>
<span id="cb152-4"><a href="#cb152-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we will discuss this later</span></span>
<span id="cb152-5"><a href="#cb152-5" aria-hidden="true" tabindex="-1"></a>    rpart.fit<span class="sc">$</span>cptable</span>
<span id="cb152-6"><a href="#cb152-6" aria-hidden="true" tabindex="-1"></a><span class="do">##           CP nsplit rel error    xerror       xstd</span></span>
<span id="cb152-7"><a href="#cb152-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 0.17040359      0 1.0000000 1.0000000 0.04984280</span></span>
<span id="cb152-8"><a href="#cb152-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 0.14798206      3 0.4843049 0.7264574 0.04692735</span></span>
<span id="cb152-9"><a href="#cb152-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 3 0.01121076      4 0.3363229 0.4484305 0.04010884</span></span>
<span id="cb152-10"><a href="#cb152-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 0.01000000      7 0.3004484 0.4035874 0.03852329</span></span>
<span id="cb152-11"><a href="#cb152-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">prune</span>(rpart.fit, <span class="at">cp =</span> <span class="fl">0.041</span>)</span>
<span id="cb152-12"><a href="#cb152-12" aria-hidden="true" tabindex="-1"></a><span class="do">## n= 500 </span></span>
<span id="cb152-13"><a href="#cb152-13" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb152-14"><a href="#cb152-14" aria-hidden="true" tabindex="-1"></a><span class="do">## node), split, n, loss, yval, (yprob)</span></span>
<span id="cb152-15"><a href="#cb152-15" aria-hidden="true" tabindex="-1"></a><span class="do">##       * denotes terminal node</span></span>
<span id="cb152-16"><a href="#cb152-16" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb152-17"><a href="#cb152-17" aria-hidden="true" tabindex="-1"></a><span class="do">##  1) root 500 223 0 (0.55400000 0.44600000)  </span></span>
<span id="cb152-18"><a href="#cb152-18" aria-hidden="true" tabindex="-1"></a><span class="do">##    2) x2&lt; -0.6444322 90   6 0 (0.93333333 0.06666667) *</span></span>
<span id="cb152-19"><a href="#cb152-19" aria-hidden="true" tabindex="-1"></a><span class="do">##    3) x2&gt;=-0.6444322 410 193 1 (0.47073171 0.52926829)  </span></span>
<span id="cb152-20"><a href="#cb152-20" aria-hidden="true" tabindex="-1"></a><span class="do">##      6) x1&gt;=0.6941279 68   8 0 (0.88235294 0.11764706) *</span></span>
<span id="cb152-21"><a href="#cb152-21" aria-hidden="true" tabindex="-1"></a><span class="do">##      7) x1&lt; 0.6941279 342 133 1 (0.38888889 0.61111111)  </span></span>
<span id="cb152-22"><a href="#cb152-22" aria-hidden="true" tabindex="-1"></a><span class="do">##       14) x2&gt;=0.7484327 53   7 0 (0.86792453 0.13207547) *</span></span>
<span id="cb152-23"><a href="#cb152-23" aria-hidden="true" tabindex="-1"></a><span class="do">##       15) x2&lt; 0.7484327 289  87 1 (0.30103806 0.69896194)  </span></span>
<span id="cb152-24"><a href="#cb152-24" aria-hidden="true" tabindex="-1"></a><span class="do">##         30) x1&lt; -0.6903174 51   9 0 (0.82352941 0.17647059) *</span></span>
<span id="cb152-25"><a href="#cb152-25" aria-hidden="true" tabindex="-1"></a><span class="do">##         31) x1&gt;=-0.6903174 238  45 1 (0.18907563 0.81092437) *</span></span></code></pre></div>
<p>The model proceed with the following steps. Note that steps 5 and 6 may not be really beneficial (consider that we know the true model).</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-215-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Alternatively, there are many other packages that can perform the same analysis. For example, the <code>tree</code> package. However, be careful that this package uses a different splitting rule by default If you want to match the result, use <code>split = "gini"</code>. Note that this plot is very crowded because it will split until pretty much only one class in each terminal node. Hence, you can imaging that there will be a tuning parameter issue. We will discuss this later.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(tree)</span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a>    tree.fit <span class="ot">=</span> <span class="fu">tree</span>(<span class="fu">as.factor</span>(y)<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data =</span> <span class="fu">data.frame</span>(x1, x2, y), <span class="at">split =</span> <span class="st">&quot;gini&quot;</span>)</span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(tree.fit)</span>
<span id="cb153-4"><a href="#cb153-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">text</span>(tree.fit)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-217-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="splitting-a-node" class="section level2 hasAnchor" number="16.2">
<h2 class="hasAnchor"><span class="header-section-number">16.2</span> Splitting a Node<a href="#splitting-a-node" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In a tree model, the splitting mechanism performs in the following way, which is just comparing all possible splits on all variables. For simplicity, we will assume that a binary splitting rule is used, i.e., we split the current node into to two child nodes, and apply the procedure recursively.</p>
<ul>
<li>At the current node, go through each variable to find the best cut-off point that splits the node.</li>
<li>Compare all the best cut-off points across all variable and choose the best one to split the current node and then iterate.</li>
</ul>
<p>So, what error criterion should we use to compare different cut-off points? There are three of them at least:</p>
<ul>
<li>Gini impurity (CART)</li>
<li>Shannon entropy (C4.5)</li>
<li>Mis-classification error</li>
</ul>
<p>Gini impurity is used in CART, while ID3/C4.5 uses the Shannon entropy. These criteria have different effects than the mis-classifications error. They usually prefer more “pure” nodes, meaning that it is more likely to single out a set of pure class terminal node if we use Gini impurity and Shannon entropy. This is because their measures are nonlinear.</p>
<p>Suppose that we have a population (or a set of observations) with <span class="math inline">\(p_k\)</span> proportion of class <span class="math inline">\(k\)</span>, for <span class="math inline">\(k = 1, \ldots, K\)</span>. Then, the Gini impurity is given by</p>
<p><span class="math display">\[ \text{Gini} = \sum_{k = 1}^K p_k (1 - p_k) = 1 - \sum_{k=1}^K p_k^2.\]</span>
The Shannon theory is defined as</p>
<p><span class="math display">\[- \sum_{k=1}^K p_k \log(p_k).\]</span>
And the classification error simply adds up all mis-classified portions if we predict the population into the most prevalent one:</p>
<p><span class="math display">\[ 1 - \underset{k = 1, \ldots, K}{\max} \,\, p_k\]</span>
The following plot shows all three quantities as a function of <span class="math inline">\(p\)</span>, when there are only two classes, i.e., <span class="math inline">\(K = 2\)</span>.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-218-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>For each quantity, smaller value means that the node is more “pure”, hence, there is a higher certainty when we predict a new value. The idea of splitting a node is that, we want the two resulting child node to contain less variation. In other words, we want each child node to be as “pure” as possible. Hence, the idea is to calculate this error criterion both before and after the split and see what cut-off point gives us the best reduction of error. Of course, all of these quantities will be calculated based on the sample version, instead of the truth. For example, if we use the Gini impurity to compare different splits, we use the following quantity for an <strong>internal node</strong> <span class="math inline">\({\cal A}\)</span>:</p>
<p><span class="math display">\[\begin{align}
\text{score}(j, c) = \text{Gini}({\cal A}) - \left( \frac{N_{{\cal A}_L}}{N_{{\cal A}}} \text{Gini}({\cal A}_L) + \frac{N_{{\cal A}_R}}{N_{{\cal A}}} \text{Gini}({\cal A}_R)  \right).
\end{align}\]</span></p>
<p>Here, <span class="math inline">\({\cal A}_L\)</span> (left child node) and <span class="math inline">\({\cal A}_R\)</span> (right child node) denote the two child nodes resulted from a potential split on the <span class="math inline">\(j\)</span>th variable at a cut-off point <span class="math inline">\(c\)</span>, such that</p>
<p><span class="math display">\[{\cal A}_L = \{\mathbf{x}: \mathbf{x}\in {\cal A}, \, x_j \leq c\}\]</span>
and</p>
<p><span class="math display">\[{\cal A}_R = \{\mathbf{x}: \mathbf{x}\in {\cal A}, \, x_j &gt; c\}.\]</span>
Then <span class="math inline">\(N_{\cal A}\)</span>, <span class="math inline">\(N_{{\cal A}_L}\)</span>, <span class="math inline">\(N_{{\cal A}_R}\)</span> are the number of observations in these nodes, respectively. The implication of this is quite intuitive: <span class="math inline">\(\text{Gini}({\cal A})\)</span> calculates the uncertainty of the entire node <span class="math inline">\({\cal A}\)</span>, while the second quantity is a summary of the uncertainty of the two potential child nodes. Hence a larger score indicates a better split, and we may choose the best index <span class="math inline">\(j\)</span> and cut-off point <span class="math inline">\(c\)</span> to proceed,</p>
<p><span class="math display">\[\underset{j \, , \, c}{\mathop{\mathrm{arg\,max}}} \,\, \text{score}(j, c)\]</span></p>
<p>and then work on each child node separately using the same procedure.</p>
</div>
<div id="regression-trees" class="section level2 hasAnchor" number="16.3">
<h2 class="hasAnchor"><span class="header-section-number">16.3</span> Regression Trees<a href="#regression-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The basic procedure for a regression tree is pretty much the same as a classification tree, except that we will use a different way to evaluate how good a potential split is. Note that the variance is a simple quantity to describe the noise within a node, we can use</p>
<p><span class="math display">\[\begin{align}
\text{score}(j, c) = \text{Var}({\cal A}) - \left( \frac{N_{{\cal A}_L}}{N_{{\cal A}}} \text{Var}({\cal A}_L) + \frac{N_{{\cal A}_R}}{N_{{\cal A}}} \text{Var}({\cal A}_R)  \right).
\end{align}\]</span></p>
</div>
<div id="predicting-a-target-point" class="section level2 hasAnchor" number="16.4">
<h2 class="hasAnchor"><span class="header-section-number">16.4</span> Predicting a Target Point<a href="#predicting-a-target-point" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we have a new target point <span class="math inline">\(\mathbf{x}_0\)</span> to predict, the basic strategy is to “drop it down the tree”. This is simply starting from the root node and following the splitting rule to see which terminal node it ends up with. Note that a fitted tree will have a collection of terminal nodes, say, <span class="math inline">\(\{{\cal A}_1, {\cal A}_2, \ldots, {\cal A}_M\}\)</span>, then suppose <span class="math inline">\(\mathbf{x}_0\)</span> falls into terminal node <span class="math inline">\({\cal A}_m\)</span>, we use <span class="math inline">\(\bar{y}_{{\cal A}_m}\)</span>, the average of original training data that falls into this node, as the prediction. The final prediction can be written as</p>
<p><span class="math display">\[\begin{align}
\widehat{f}(\mathbf{x}_0) =&amp; \sum_{m = 1}^M \bar{y}_{{\cal A}_m} \mathbf{1}\{\mathbf{x}_0 \in {\cal A}_m\} \\
=&amp; \sum_{m = 1}^M \frac{\sum_{i=1}^n y_i \mathbf{1}\{\mathbf{x}_i \in {\cal A}_m\}}{\sum_{i=1}^n \mathbf{1}\{\mathbf{x}_i \in {\cal A}_m\}} \mathbf{1}\{\mathbf{x}_0 \in {\cal A}_m\}.
\end{align}\]</span></p>
</div>
<div id="tuning-a-tree-model" class="section level2 hasAnchor" number="16.5">
<h2 class="hasAnchor"><span class="header-section-number">16.5</span> Tuning a Tree Model<a href="#tuning-a-tree-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Tree tuning is essentially about when to stop splitting. Or we could look at this reversely by first fitting a very large tree, then see if we could remove some branches of a tree to make it simpler without sacrificing much accuracy. One approach is called the <strong>cost-complexity pruning</strong>. This is another penalized framework that we use the accuracy as the loss function, and use the tree-size as the penalty part for complexity. Formally, if we have any tree model <span class="math inline">\({\cal T}\)</span>, consider this can be written as</p>
<p><span class="math display">\[\begin{align}
C_\alpha({\cal T}) =&amp;~ \sum_{\text{all terminal nodes $t$ in ${\cal T}$}} N_t \cdot \text{Impurity}(t) + \alpha |{\cal T}| \nonumber \\
=&amp;~ C({\cal T}) + \alpha |{\cal T}|
\end{align}\]</span></p>
<p>Now, we can start with a very large tree, say, fitted until all pure terminal nodes. Call this tree as <span class="math inline">\({\cal T}_\text{max}\)</span>. We can then exhaust all its sub-trees by pruning any branches, and calculate this <span class="math inline">\(C(\cdot)\)</span> function of the sub-tree. Then the tree that gives the smallest value will be our best tree.</p>
<p>But this can be computationally too expensive. Hence, one compromise, instead of trying all possible sub-trees, is to use the <strong>weakest-link cutting</strong>. This means that, we cut the branch (essentially a certain split) that displays the weakest banefit towards the <span class="math inline">\(C(\cdot)\)</span> function. The procedure is the following:</p>
<ul>
<li>Look at an internal node <span class="math inline">\(t\)</span> of <span class="math inline">\({\cal T}_\text{max}\)</span>, and denote the entire branch starting from <span class="math inline">\(t\)</span> as <span class="math inline">\({\cal T}_t\)</span></li>
<li>Compare: remove the entire branch (collapse <span class="math inline">\({\cal T}_t\)</span> into a single terminal node) vs. keep <span class="math inline">\(T_t\)</span>. To do this, calculate
<span class="math display">\[\alpha \leq \frac{C(t) - C({\cal T}_t)}{|T_t| - 1}\]</span>
Note that <span class="math inline">\(|{\cal T}_t| - 1\)</span> is the size difference between the two trees.</li>
<li>Try all internal nodes <span class="math inline">\(t\)</span>, and cut the branch <span class="math inline">\(t\)</span> that has the smallest value on the right hand side. This gives the smallest <span class="math inline">\(\alpha\)</span> value to remove some branches. Then iterate the procedure based on this reduced tree.</li>
</ul>
<p>Note that the <span class="math inline">\(\alpha\)</span> values will get larger as we move more branches. Hence this produces a solution path. Now this is very similar to the Lasso solution path idea, and we could use cross-validation to select the best tuning. By default, the <code>rpart</code> function uses a 10-fold cross-validation. This can be controlled using the <code>rpart.control()</code> function and specify the <code>xval</code> argument. For details, please see the <a href="https://cran.r-project.org/web/packages/rpart/rpart.pdf">documentation</a>. The following plot using <code>plotcp()</code> in the <code>rpart</code> package gives a visualization of the relative cross-validation error. It also produces a horizontal line (the dotted line). It suggests the lowest (plus certain variation) that we could achieve. Hence, we will select the best <code>cp</code> value (<span class="math inline">\(alpha\)</span>) that is above this line. The way that this is constructed is similar to the <code>lambda.1se</code> choice in <code>glmnet</code>.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># and the tuning parameter </span></span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plotcp</span>(rpart.fit)  </span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-219-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">printcp</span>(rpart.fit)</span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb155-3"><a href="#cb155-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Classification tree:</span></span>
<span id="cb155-4"><a href="#cb155-4" aria-hidden="true" tabindex="-1"></a><span class="do">## rpart(formula = as.factor(y) ~ x1 + x2, data = data.frame(x1, </span></span>
<span id="cb155-5"><a href="#cb155-5" aria-hidden="true" tabindex="-1"></a><span class="do">##     x2, y))</span></span>
<span id="cb155-6"><a href="#cb155-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb155-7"><a href="#cb155-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Variables actually used in tree construction:</span></span>
<span id="cb155-8"><a href="#cb155-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] x1 x2</span></span>
<span id="cb155-9"><a href="#cb155-9" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb155-10"><a href="#cb155-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Root node error: 223/500 = 0.446</span></span>
<span id="cb155-11"><a href="#cb155-11" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb155-12"><a href="#cb155-12" aria-hidden="true" tabindex="-1"></a><span class="do">## n= 500 </span></span>
<span id="cb155-13"><a href="#cb155-13" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb155-14"><a href="#cb155-14" aria-hidden="true" tabindex="-1"></a><span class="do">##         CP nsplit rel error  xerror     xstd</span></span>
<span id="cb155-15"><a href="#cb155-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 0.170404      0   1.00000 1.00000 0.049843</span></span>
<span id="cb155-16"><a href="#cb155-16" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 0.147982      3   0.48430 0.72646 0.046927</span></span>
<span id="cb155-17"><a href="#cb155-17" aria-hidden="true" tabindex="-1"></a><span class="do">## 3 0.011211      4   0.33632 0.44843 0.040109</span></span>
<span id="cb155-18"><a href="#cb155-18" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 0.010000      7   0.30045 0.40359 0.038523</span></span></code></pre></div>
<!--chapter:end:05.4-tree.Rmd-->
</div>
</div>
<div id="random-forests" class="section level1 hasAnchor" number="17">
<h1 class="hasAnchor"><span class="header-section-number">17</span> Random Forests<a href="#random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Roughly speaking, random forests <span class="citation">(<a href="#ref-breiman2001random" role="doc-biblioref">Breiman 2001</a>)</span> are parallelly fitted CART models with some randomness. There are several main components:</p>
<ul>
<li>Bootstrapping of data for each tree using the Bagging idea <span class="citation">(<a href="#ref-breiman1996bagging" role="doc-biblioref">Breiman 1996</a>)</span>, and use the averaged result (for regression) or majority voting (for classification) of all trees as the prediction.</li>
<li>At each internal node, we may not consider all variables. Instead, we consider a randomly selected <code>mtry</code> variables to search for the best split. This idea was inspired by <span class="citation">Ho (<a href="#ref-ho1998random" role="doc-biblioref">1998</a>)</span>.</li>
<li>For each tree, we will not perform pruning. Instead, we simply stop when the internal node contains no more than <code>nodesize</code> number of observations.</li>
</ul>
<p>Later on, there were various version of random forests that attempts to improve the performance, from both computational and theoretical prospective. We will introduce them later.</p>
<div id="bagging-predictors" class="section level2 hasAnchor" number="17.1">
<h2 class="hasAnchor"><span class="header-section-number">17.1</span> Bagging Predictors<a href="#bagging-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>CART models may be difficult when dealing with non-axis-aligned decision boundaries. This can be seen from the example below, in a two-dimensional case. The idea of Bagging is that we can fit many CART models, each from a Bootstrap sample, i.e., sample with replacement from the original <span class="math inline">\(n\)</span> observations. The reason that Breiman considered bootstrap samples is because it can approximate the original distribution that generates the data. But the end result is that since each tree may be slightly different from each other, when we stack them, the decision bound can be more “smooth”.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate some data </span></span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb156-3"><a href="#cb156-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb156-4"><a href="#cb156-4" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb156-5"><a href="#cb156-5" aria-hidden="true" tabindex="-1"></a>  x2 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb156-6"><a href="#cb156-6" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">ifelse</span>((x1 <span class="sc">+</span> x2 <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">&amp;</span> (x1 <span class="sc">+</span> x2 <span class="sc">&lt;</span> <span class="fl">0.5</span>) , <span class="fl">0.8</span>, <span class="fl">0.2</span>))</span>
<span id="cb156-7"><a href="#cb156-7" aria-hidden="true" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="at">x2 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>))</span></code></pre></div>
<p>Let’s compare the decision rule of CART and Bagging. For CART, the decision line has to be aligned to axis. For Bagging, we use a total of 200 trees, specified by <code>nbagg</code> in the <code>ipred</code> package.</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit CART</span></span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(rpart)</span>
<span id="cb157-3"><a href="#cb157-3" aria-hidden="true" tabindex="-1"></a>  rpart.fit <span class="ot">=</span> <span class="fu">rpart</span>(<span class="fu">as.factor</span>(y)<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data =</span> <span class="fu">data.frame</span>(x1, x2, y))</span>
<span id="cb157-4"><a href="#cb157-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-5"><a href="#cb157-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># we could fit a different tree using a bootstrap sample</span></span>
<span id="cb157-6"><a href="#cb157-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># rpart.fit = rpart(as.factor(y)~x1+x2, data = data.frame(x1, x2, y)[sample(1:n, n, replace = TRUE), ])</span></span>
<span id="cb157-7"><a href="#cb157-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-8"><a href="#cb157-8" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">predict</span>(rpart.fit, xgrid, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>) <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">201</span>)</span>
<span id="cb157-9"><a href="#cb157-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), pred, <span class="at">levels=</span><span class="fl">0.5</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>,<span class="at">axes=</span><span class="cn">FALSE</span>)</span>
<span id="cb157-10"><a href="#cb157-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(x1, x2, <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb157-11"><a href="#cb157-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">pch=</span><span class="st">&quot;.&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>, <span class="at">col=</span><span class="fu">ifelse</span>(pred, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>))</span>
<span id="cb157-12"><a href="#cb157-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()    </span>
<span id="cb157-13"><a href="#cb157-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">title</span>(<span class="st">&quot;CART&quot;</span>)</span>
<span id="cb157-14"><a href="#cb157-14" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb157-15"><a href="#cb157-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit Bagging</span></span>
<span id="cb157-16"><a href="#cb157-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(ipred)</span>
<span id="cb157-17"><a href="#cb157-17" aria-hidden="true" tabindex="-1"></a>  bag.fit <span class="ot">=</span> <span class="fu">bagging</span>(<span class="fu">as.factor</span>(y)<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data =</span> <span class="fu">data.frame</span>(x1, x2, y), <span class="at">nbagg =</span> <span class="dv">200</span>, <span class="at">ns =</span> <span class="dv">400</span>)</span>
<span id="cb157-18"><a href="#cb157-18" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">predict</span>(<span class="fu">prune</span>(bag.fit), xgrid) <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">201</span>)</span>
<span id="cb157-19"><a href="#cb157-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), pred, <span class="at">levels=</span><span class="fl">0.5</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>,<span class="at">axes=</span><span class="cn">FALSE</span>)</span>
<span id="cb157-20"><a href="#cb157-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(x1, x2, <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb157-21"><a href="#cb157-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">pch=</span><span class="st">&quot;.&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>, <span class="at">col=</span><span class="fu">ifelse</span>(pred, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>))</span>
<span id="cb157-22"><a href="#cb157-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span>
<span id="cb157-23"><a href="#cb157-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">title</span>(<span class="st">&quot;Bagging&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-224-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="random-forests-1" class="section level2 hasAnchor" number="17.2">
<h2 class="hasAnchor"><span class="header-section-number">17.2</span> Random Forests<a href="#random-forests-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random forests are equipped with this Bootstrapping strategy, but also with other things, which are mentioned previously. They are controlled by several key parameters:</p>
<ul>
<li><code>ntree</code>: number of trees</li>
<li><code>sampsize</code>: how many samples to use when fitting each tree</li>
<li><code>mtry</code>: number of randomly sampled variable to consider at each internal node</li>
<li><code>nodesize</code>: stop splitting when the node sample size is no larger than <code>nodesize</code></li>
</ul>
<p>Using the <code>randomForest</code> package, we can fit the model. It is difficult to visualize this when <code>p &gt; 2</code>. But we can look at the testing error.</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate some data with larger p</span></span>
<span id="cb158-2"><a href="#cb158-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb158-3"><a href="#cb158-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb158-4"><a href="#cb158-4" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb158-5"><a href="#cb158-5" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n<span class="sc">*</span>p, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), n, p)</span>
<span id="cb158-6"><a href="#cb158-6" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">=</span> X[, <span class="dv">1</span>]</span>
<span id="cb158-7"><a href="#cb158-7" aria-hidden="true" tabindex="-1"></a>  x2 <span class="ot">=</span> X[, <span class="dv">2</span>]</span>
<span id="cb158-8"><a href="#cb158-8" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">ifelse</span>((x1 <span class="sc">+</span> x2 <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">&amp;</span> (x1 <span class="sc">+</span> x2 <span class="sc">&lt;</span> <span class="fl">0.5</span>), <span class="fl">0.8</span>, <span class="fl">0.2</span>))</span>
<span id="cb158-9"><a href="#cb158-9" aria-hidden="true" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="at">x2 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>))</span>
<span id="cb158-10"><a href="#cb158-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb158-11"><a href="#cb158-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit random forests with a selected tuning</span></span>
<span id="cb158-12"><a href="#cb158-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(randomForest)</span>
<span id="cb158-13"><a href="#cb158-13" aria-hidden="true" tabindex="-1"></a><span class="do">## randomForest 4.7-1.1</span></span>
<span id="cb158-14"><a href="#cb158-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Type rfNews() to see new features/changes/bug fixes.</span></span>
<span id="cb158-15"><a href="#cb158-15" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb158-16"><a href="#cb158-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Attaching package: &#39;randomForest&#39;</span></span>
<span id="cb158-17"><a href="#cb158-17" aria-hidden="true" tabindex="-1"></a><span class="do">## The following object is masked from &#39;package:ggplot2&#39;:</span></span>
<span id="cb158-18"><a href="#cb158-18" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb158-19"><a href="#cb158-19" aria-hidden="true" tabindex="-1"></a><span class="do">##     margin</span></span>
<span id="cb158-20"><a href="#cb158-20" aria-hidden="true" tabindex="-1"></a>  rf.fit <span class="ot">=</span> <span class="fu">randomForest</span>(X, <span class="fu">as.factor</span>(y), <span class="at">ntree =</span> <span class="dv">1000</span>, </span>
<span id="cb158-21"><a href="#cb158-21" aria-hidden="true" tabindex="-1"></a>                        <span class="at">mtry =</span> <span class="dv">7</span>, <span class="at">nodesize =</span> <span class="dv">10</span>, <span class="at">sampsize =</span> <span class="dv">800</span>)</span></code></pre></div>
<p>Instead of generating a set of testing samples labels, let’s directly compare with the “true” decision rule, the Bayes rule.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the testing data </span></span>
<span id="cb159-2"><a href="#cb159-2" aria-hidden="true" tabindex="-1"></a>  Xtest <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n<span class="sc">*</span>p, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), n, p)</span>
<span id="cb159-3"><a href="#cb159-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb159-4"><a href="#cb159-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the Bayes rule</span></span>
<span id="cb159-5"><a href="#cb159-5" aria-hidden="true" tabindex="-1"></a>  BayesRule <span class="ot">=</span> <span class="fu">ifelse</span>((Xtest[, <span class="dv">1</span>] <span class="sc">+</span> Xtest[, <span class="dv">2</span>] <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">&amp;</span> </span>
<span id="cb159-6"><a href="#cb159-6" aria-hidden="true" tabindex="-1"></a>                     (Xtest[, <span class="dv">1</span>] <span class="sc">+</span> Xtest[, <span class="dv">2</span>] <span class="sc">&lt;</span> <span class="fl">0.5</span>), <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb159-7"><a href="#cb159-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb159-8"><a href="#cb159-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>( (<span class="fu">predict</span>(rf.fit, Xtest) <span class="sc">==</span> <span class="st">&quot;1&quot;</span>) <span class="sc">==</span> BayesRule )</span>
<span id="cb159-9"><a href="#cb159-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.785</span></span></code></pre></div>
</div>
<div id="effect-of-mtry" class="section level2 hasAnchor" number="17.3">
<h2 class="hasAnchor"><span class="header-section-number">17.3</span> Effect of <code>mtry</code><a href="#effect-of-mtry" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the two dimensional setting, we probably won’t see much difference by using random forests, since the only effective change is <code>mtry = 1</code>, which is not really different than <code>mtry = 2</code> (the CART choice). You can try this by yourself.
However, the difference would be significant in higher dimensional settings, in our case <span class="math inline">\(p=10\)</span>. This is again an issue of bias-variance trade-off. The intuition is that, when we use a small <code>mtry</code>, and when <span class="math inline">\(p\)</span> is large, we may by chance randomly select some irrelevant variables that has nothing to do with the outcome. Then this particular split would be wasted. Missing the true variable may cause larger bias. On the other hand, when we use a large <code>mtry</code>, we will be greedy for signals since we compare many different variables and pick the best one. But this is also as the risk of over-fitting. Hence, tuning is necessary.</p>
<p>Just as an example, let’s try a small <code>mtry</code>:</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a>  rf.fit <span class="ot">=</span> <span class="fu">randomForest</span>(X, <span class="fu">as.factor</span>(y), <span class="at">ntree =</span> <span class="dv">1000</span>, </span>
<span id="cb160-2"><a href="#cb160-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">mtry =</span> <span class="dv">1</span>, <span class="at">nodesize =</span> <span class="dv">10</span>, <span class="at">sampsize =</span> <span class="dv">800</span>)</span>
<span id="cb160-3"><a href="#cb160-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-4"><a href="#cb160-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>( (<span class="fu">predict</span>(rf.fit, Xtest) <span class="sc">==</span> <span class="st">&quot;1&quot;</span>) <span class="sc">==</span> BayesRule )</span>
<span id="cb160-5"><a href="#cb160-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.634</span></span></code></pre></div>
</div>
<div id="effect-of-nodesize" class="section level2 hasAnchor" number="17.4">
<h2 class="hasAnchor"><span class="header-section-number">17.4</span> Effect of <code>nodesize</code><a href="#effect-of-nodesize" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we use a small <code>nodesize</code>, we are at the risk of over-fitting. This is similar to the 1NN example. When we use large <code>nodesize</code>, there could be under-fitting.</p>
</div>
<div id="variable-importance" class="section level2 hasAnchor" number="17.5">
<h2 class="hasAnchor"><span class="header-section-number">17.5</span> Variable Importance<a href="#variable-importance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random forests model provides a way to evaluate the importance of each variable. This can be done by specifying the <code>importance</code> argument. We usually use the <code>MeanDecreaseAccuracy</code> or <code>MeanDecreaseGini</code> column as the summary of the importance of each variable.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a>  rf.fit <span class="ot">=</span> <span class="fu">randomForest</span>(X, <span class="fu">as.factor</span>(y), <span class="at">ntree =</span> <span class="dv">1000</span>, </span>
<span id="cb161-2"><a href="#cb161-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">mtry =</span> <span class="dv">7</span>, <span class="at">nodesize =</span> <span class="dv">10</span>, <span class="at">sampsize =</span> <span class="dv">800</span>,</span>
<span id="cb161-3"><a href="#cb161-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">importance=</span><span class="cn">TRUE</span>)</span>
<span id="cb161-4"><a href="#cb161-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-5"><a href="#cb161-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">importance</span>(rf.fit)</span>
<span id="cb161-6"><a href="#cb161-6" aria-hidden="true" tabindex="-1"></a><span class="do">##             0         1 MeanDecreaseAccuracy MeanDecreaseGini</span></span>
<span id="cb161-7"><a href="#cb161-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 1  39.1053057 39.823786           45.4232897         47.79065</span></span>
<span id="cb161-8"><a href="#cb161-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 2  38.1820764 40.119387           45.1964485         54.89580</span></span>
<span id="cb161-9"><a href="#cb161-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 3   3.2719270  1.461298            3.2274895         28.44828</span></span>
<span id="cb161-10"><a href="#cb161-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 4  -0.2777943 -6.287430           -4.8470758         22.09006</span></span>
<span id="cb161-11"><a href="#cb161-11" aria-hidden="true" tabindex="-1"></a><span class="do">## 5   2.0937973  1.654224            2.5256400         28.57575</span></span>
<span id="cb161-12"><a href="#cb161-12" aria-hidden="true" tabindex="-1"></a><span class="do">## 6   2.2354984 -2.435663           -0.1297796         25.29836</span></span>
<span id="cb161-13"><a href="#cb161-13" aria-hidden="true" tabindex="-1"></a><span class="do">## 7   0.2083020  2.724449            2.0679184         24.28751</span></span>
<span id="cb161-14"><a href="#cb161-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 8   0.2018946  3.350897            2.3962745         25.51630</span></span>
<span id="cb161-15"><a href="#cb161-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 9  -1.6159803  2.150674            0.3912234         23.41498</span></span>
<span id="cb161-16"><a href="#cb161-16" aria-hidden="true" tabindex="-1"></a><span class="do">## 10  2.6081961  4.417256            4.8004480         27.80399</span></span></code></pre></div>
</div>
<div id="kernel-view-of-random-forets" class="section level2 hasAnchor" number="17.6">
<h2 class="hasAnchor"><span class="header-section-number">17.6</span> Kernel view of Random Forets<a href="#kernel-view-of-random-forets" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>I wrote a small function that will extract the kernel weights from a random forests for predicting a testing point <span class="math inline">\(x\)</span>. This is essentially the counts for how many times a training data falls into the same terminal node as <span class="math inline">\(x\)</span>. Since the prediction on <span class="math inline">\(x\)</span> are essentially the average of them in a weighted fashion, this is basically a kernel averaging approach. However, the kernel weights are adaptive to the true structure.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate the 2 dimensional case</span></span>
<span id="cb162-2"><a href="#cb162-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb162-3"><a href="#cb162-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb162-4"><a href="#cb162-4" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb162-5"><a href="#cb162-5" aria-hidden="true" tabindex="-1"></a>  x2 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb162-6"><a href="#cb162-6" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">ifelse</span>((x1 <span class="sc">+</span> x2 <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">&amp;</span> (x1 <span class="sc">+</span> x2 <span class="sc">&lt;</span> <span class="fl">0.5</span>) , <span class="fl">0.8</span>, <span class="fl">0.2</span>))</span>
<span id="cb162-7"><a href="#cb162-7" aria-hidden="true" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="at">x2 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>))</span>
<span id="cb162-8"><a href="#cb162-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb162-9"><a href="#cb162-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit a random forest model</span></span>
<span id="cb162-10"><a href="#cb162-10" aria-hidden="true" tabindex="-1"></a>  rf.fit <span class="ot">=</span> <span class="fu">randomForest</span>(<span class="fu">cbind</span>(x1, x2), <span class="fu">as.factor</span>(y), <span class="at">ntree =</span> <span class="dv">300</span>, </span>
<span id="cb162-11"><a href="#cb162-11" aria-hidden="true" tabindex="-1"></a>                        <span class="at">mtry =</span> <span class="dv">1</span>, <span class="at">nodesize =</span> <span class="dv">20</span>, <span class="at">keep.inbag =</span> <span class="cn">TRUE</span>)</span>
<span id="cb162-12"><a href="#cb162-12" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">predict</span>(rf.fit, xgrid) <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">201</span>)</span>
<span id="cb162-13"><a href="#cb162-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb162-14"><a href="#cb162-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="at">mar=</span><span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">2</span>, <span class="fl">0.5</span>))</span>
<span id="cb162-15"><a href="#cb162-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-16"><a href="#cb162-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># check the kernel weight at different points</span></span>
<span id="cb162-17"><a href="#cb162-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plotRFKernel</span>(rf.fit, <span class="fu">data.frame</span>(<span class="fu">cbind</span>(x1, x2)), <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">0.4</span>))</span>
<span id="cb162-18"><a href="#cb162-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plotRFKernel</span>(rf.fit, <span class="fu">data.frame</span>(<span class="fu">cbind</span>(x1, x2)), <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.6</span>))</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-231-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>As contrast, here is the regular Gaussian kernel weights (after some tuning). This effect will play an important role when <span class="math inline">\(p\)</span> is large.</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Gaussian kernel weights</span></span>
<span id="cb163-2"><a href="#cb163-2" aria-hidden="true" tabindex="-1"></a>  onex <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">0.4</span>)</span>
<span id="cb163-3"><a href="#cb163-3" aria-hidden="true" tabindex="-1"></a>  h <span class="ot">=</span> <span class="fl">0.2</span></span>
<span id="cb163-4"><a href="#cb163-4" aria-hidden="true" tabindex="-1"></a>  wt <span class="ot">=</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">rowSums</span>(<span class="fu">sweep</span>(<span class="fu">cbind</span>(x1, x2), <span class="dv">2</span>, onex, <span class="at">FUN =</span> <span class="st">&quot;-&quot;</span>)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>h<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb163-5"><a href="#cb163-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), pred, </span>
<span id="cb163-6"><a href="#cb163-6" aria-hidden="true" tabindex="-1"></a>          <span class="at">levels=</span><span class="fl">0.5</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>,<span class="at">axes=</span><span class="cn">FALSE</span>)</span>
<span id="cb163-7"><a href="#cb163-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(x1, x2, <span class="at">cex =</span> <span class="dv">4</span><span class="sc">*</span>wt<span class="sc">^</span>(<span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>), <span class="at">pch =</span> <span class="dv">1</span>, <span class="at">cex.axis=</span><span class="fl">1.25</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb163-8"><a href="#cb163-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(x1, x2, <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb163-9"><a href="#cb163-9" aria-hidden="true" tabindex="-1"></a>         <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.75</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb163-10"><a href="#cb163-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">pch=</span><span class="st">&quot;.&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>, </span>
<span id="cb163-11"><a href="#cb163-11" aria-hidden="true" tabindex="-1"></a>         <span class="at">col=</span><span class="fu">ifelse</span>(pred, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>))</span>
<span id="cb163-12"><a href="#cb163-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(onex[<span class="dv">1</span>], onex[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">4</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span><span class="dv">4</span>, <span class="at">lwd =</span> <span class="dv">6</span>)</span>
<span id="cb163-13"><a href="#cb163-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-233-1.png" width="45%" style="display: block; margin: auto;" /></p>
<!--chapter:end:05.5-rf.Rmd-->
</div>
</div>
<div id="boosting" class="section level1 hasAnchor" number="18">
<h1 class="hasAnchor"><span class="header-section-number">18</span> Boosting<a href="#boosting" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Boosting is another ensemble model, created in the form of</p>
<p><span class="math display">\[F_T(x) = \sum_{t = 1}^T \alpha_t f_t(x)\]</span></p>
<p>However, it is different from random forest, in which each <span class="math inline">\(f_t(x)\)</span> is learned parallelly. These <span class="math inline">\(f_t(x)\)</span>’s are called weak learners and are constructed <strong>sequentially</strong>, with coefficients <span class="math inline">\(\alpha_t\)</span>’s to represent their weights. The most classical model, AdaBoost was proposed by <span class="citation">Freund and Schapire (<a href="#ref-freund1997decision" role="doc-biblioref">1997</a>)</span> for classification problems, and a more statically view of this model called gradient boosting machines <span class="citation">(<a href="#ref-friedman2001greedy" role="doc-biblioref">J. H. Friedman 2001</a>)</span> can handle any loss function we commonly use. We will first introduce AdaBoost and then discuss gradient boosting.</p>
<div id="adaboost" class="section level2 hasAnchor" number="18.1">
<h2 class="hasAnchor"><span class="header-section-number">18.1</span> AdaBoost<a href="#adaboost" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Following our common notation, we observe a set of data <span class="math inline">\(\{\mathbf{x}_i, y_i\}_{i=1}^n\)</span>. Similar to SVM, we code <span class="math inline">\(y_i\)</span>s as <span class="math inline">\(-1\)</span> or <span class="math inline">\(1\)</span>. The AdaBoost works by creating <span class="math inline">\(F_T(x)\)</span> sequentially and use <span class="math inline">\(\text{sign}(F_T(x))\)</span> as the classification rule. The algorithm is given in the following:</p>
<ul>
<li>Initiate weights <span class="math inline">\(w_i^{(1)} = 1/n\)</span>, for <span class="math inline">\(i = 1, \ldots, n\)</span></li>
<li>For <span class="math inline">\(t = 1, \ldots, T\)</span>, do
<ul>
<li>Fit a classifier <span class="math inline">\(f_t(x)\)</span> to the training data with subject weights <span class="math inline">\(w_i^{(t)}\)</span>’s.</li>
<li>Compute the weighed error rate
<span class="math display">\[\epsilon_t = \sum_{i=1}^n w_i^{(t)} \mathbf{1}\{y_i \neq f_t(x_i) \}\]</span></li>
<li>Compute
<span class="math display">\[\alpha_t = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t}\]</span></li>
<li>Update subject weights
<span class="math display">\[w_i^{(t + 1)} = \frac{1}{Z_t} w_i^{(t)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\}\]</span>
where <span class="math inline">\(Z_t\)</span> is a normalizing constant make <span class="math inline">\(w_i^{(t + 1)}\)</span>’s sum up to 1:
<span class="math display">\[Z_t = \sum_{i=1}^n w_i^{(t)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\}\]</span></li>
</ul></li>
<li>Output the final model
<span class="math display">\[F_T(x) = \sum_{t = 1}^T \alpha_t f_t(x)\]</span>
and the decision rule is <span class="math inline">\(\text{sign}(F_T(x))\)</span>.</li>
</ul>
<p>An important mechanism in AdaBoost is the weight update step. We can notice that the weight is increased if <span class="math inline">\(\exp\big\{ - \alpha_t y_i f_t(x_i) \big\}\)</span> is larger than 1. This is simply when <span class="math inline">\(y_i f_t(x_i)\)</span> is negative, i.e., subject <span class="math inline">\(i\)</span> got mis-classified by <span class="math inline">\(f_t\)</span> at this iteration. Hence, during the next iteration <span class="math inline">\(t+1\)</span>, the model <span class="math inline">\(f_{(t+1)}\)</span> will more likely to address this subject. Here, <span class="math inline">\(f_t\)</span> can be any classification model, for example, we could use a tree model. The following figures demonstrate this idea of updating weights and aggregate the learners.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="fl">0.1</span>)</span>
<span id="cb164-2"><a href="#cb164-2" aria-hidden="true" tabindex="-1"></a>  x2 <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>,</span>
<span id="cb164-3"><a href="#cb164-3" aria-hidden="true" tabindex="-1"></a>         <span class="fl">0.8</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.2</span>)</span>
<span id="cb164-4"><a href="#cb164-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb164-5"><a href="#cb164-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the data</span></span>
<span id="cb164-6"><a href="#cb164-6" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, </span>
<span id="cb164-7"><a href="#cb164-7" aria-hidden="true" tabindex="-1"></a>        <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb164-8"><a href="#cb164-8" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">cbind</span>(<span class="st">&quot;x1&quot;</span> <span class="ot">=</span> x1, <span class="st">&quot;x2&quot;</span> <span class="ot">=</span> x2)</span>
<span id="cb164-9"><a href="#cb164-9" aria-hidden="true" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="st">&quot;x1&quot;</span> <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">1.1</span>, <span class="fl">0.01</span>), <span class="st">&quot;x2&quot;</span> <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">0.9</span>, <span class="fl">0.01</span>))</span>
<span id="cb164-10"><a href="#cb164-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb164-11"><a href="#cb164-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot data</span></span>
<span id="cb164-12"><a href="#cb164-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb164-13"><a href="#cb164-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb164-14"><a href="#cb164-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb164-15"><a href="#cb164-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb164-16"><a href="#cb164-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit gbm with 3 trees</span></span>
<span id="cb164-17"><a href="#cb164-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(gbm)</span>
<span id="cb164-18"><a href="#cb164-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Loaded gbm 2.1.8.1</span></span>
<span id="cb164-19"><a href="#cb164-19" aria-hidden="true" tabindex="-1"></a>  gbm.fit <span class="ot">=</span> <span class="fu">gbm</span>(y <span class="sc">~</span>., <span class="fu">data.frame</span>(x1, x2, <span class="at">y=</span> <span class="fu">as.numeric</span>(y <span class="sc">==</span> <span class="dv">1</span>)), </span>
<span id="cb164-20"><a href="#cb164-20" aria-hidden="true" tabindex="-1"></a>                <span class="at">distribution=</span><span class="st">&quot;adaboost&quot;</span>, <span class="at">interaction.depth =</span> <span class="dv">1</span>, </span>
<span id="cb164-21"><a href="#cb164-21" aria-hidden="true" tabindex="-1"></a>                <span class="at">n.minobsinnode =</span> <span class="dv">1</span>, <span class="at">n.trees =</span> <span class="dv">3</span>, </span>
<span id="cb164-22"><a href="#cb164-22" aria-hidden="true" tabindex="-1"></a>                <span class="at">shrinkage =</span> <span class="dv">1</span>, <span class="at">bag.fraction =</span> <span class="dv">1</span>)</span>
<span id="cb164-23"><a href="#cb164-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb164-24"><a href="#cb164-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># you may peek into each tree</span></span>
<span id="cb164-25"><a href="#cb164-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pretty.gbm.tree</span>(gbm.fit, <span class="at">i.tree =</span> <span class="dv">1</span>)</span>
<span id="cb164-26"><a href="#cb164-26" aria-hidden="true" tabindex="-1"></a><span class="do">##   SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight Prediction</span></span>
<span id="cb164-27"><a href="#cb164-27" aria-hidden="true" tabindex="-1"></a><span class="do">## 0        0          0.25        1         2           3            2.5     10       0.00</span></span>
<span id="cb164-28"><a href="#cb164-28" aria-hidden="true" tabindex="-1"></a><span class="do">## 1       -1          1.00       -1        -1          -1            0.0      2       1.00</span></span>
<span id="cb164-29"><a href="#cb164-29" aria-hidden="true" tabindex="-1"></a><span class="do">## 2       -1         -0.25       -1        -1          -1            0.0      8      -0.25</span></span>
<span id="cb164-30"><a href="#cb164-30" aria-hidden="true" tabindex="-1"></a><span class="do">## 3       -1          0.00       -1        -1          -1            0.0     10       0.00</span></span>
<span id="cb164-31"><a href="#cb164-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb164-32"><a href="#cb164-32" aria-hidden="true" tabindex="-1"></a>  <span class="co"># we can view the predicted decision rule</span></span>
<span id="cb164-33"><a href="#cb164-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb164-34"><a href="#cb164-34" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb164-35"><a href="#cb164-35" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb164-36"><a href="#cb164-36" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">predict</span>(gbm.fit, xgrid)</span>
<span id="cb164-37"><a href="#cb164-37" aria-hidden="true" tabindex="-1"></a><span class="do">## Using 3 trees...</span></span>
<span id="cb164-38"><a href="#cb164-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb164-39"><a href="#cb164-39" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-237-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Here is a rundown of the algorithm. Let’s initialize all weights as <span class="math inline">\(1/n\)</span>. We only used trees with a single split as weak learners. The first tree is splitting at <span class="math inline">\(X_1 = 0.25\)</span>. After the first split, we need to adjust the weights.</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a>  w1 <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb165-2"><a href="#cb165-2" aria-hidden="true" tabindex="-1"></a>  f1 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x[, <span class="dv">1</span>] <span class="sc">&lt;</span> <span class="fl">0.25</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb165-3"><a href="#cb165-3" aria-hidden="true" tabindex="-1"></a>  e1 <span class="ot">=</span> <span class="fu">sum</span>(w1<span class="sc">*</span>(<span class="fu">f1</span>(X) <span class="sc">!=</span> y))</span>
<span id="cb165-4"><a href="#cb165-4" aria-hidden="true" tabindex="-1"></a>  a1 <span class="ot">=</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>e1)<span class="sc">/</span>e1)</span>
<span id="cb165-5"><a href="#cb165-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb165-6"><a href="#cb165-6" aria-hidden="true" tabindex="-1"></a>  w2 <span class="ot">=</span> w1<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span> a1<span class="sc">*</span>y<span class="sc">*</span><span class="fu">f1</span>(X))</span>
<span id="cb165-7"><a href="#cb165-7" aria-hidden="true" tabindex="-1"></a>  w2 <span class="ot">=</span> w2<span class="sc">/</span><span class="fu">sum</span>(w2)</span>
<span id="cb165-8"><a href="#cb165-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb165-9"><a href="#cb165-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the first tree</span></span>
<span id="cb165-10"><a href="#cb165-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb165-11"><a href="#cb165-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb165-12"><a href="#cb165-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb165-13"><a href="#cb165-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb165-14"><a href="#cb165-14" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">f1</span>(xgrid)</span>
<span id="cb165-15"><a href="#cb165-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb165-16"><a href="#cb165-16" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb165-17"><a href="#cb165-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb165-18"><a href="#cb165-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># weights after the first tree</span></span>
<span id="cb165-19"><a href="#cb165-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb165-20"><a href="#cb165-20" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb165-21"><a href="#cb165-21" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">30</span><span class="sc">*</span>w2)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-239-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>We can notice that the observations got correctly classified will decrease their weights while those mis-classified will increase the weights.</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a>  f2 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x[, <span class="dv">2</span>] <span class="sc">&gt;</span> <span class="fl">0.65</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb166-2"><a href="#cb166-2" aria-hidden="true" tabindex="-1"></a>  e2 <span class="ot">=</span> <span class="fu">sum</span>(w2<span class="sc">*</span>(<span class="fu">f2</span>(X) <span class="sc">!=</span> y))</span>
<span id="cb166-3"><a href="#cb166-3" aria-hidden="true" tabindex="-1"></a>  a2 <span class="ot">=</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>e2)<span class="sc">/</span>e2)</span>
<span id="cb166-4"><a href="#cb166-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb166-5"><a href="#cb166-5" aria-hidden="true" tabindex="-1"></a>  w3 <span class="ot">=</span> w2<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span> a2<span class="sc">*</span>y<span class="sc">*</span><span class="fu">f2</span>(X))</span>
<span id="cb166-6"><a href="#cb166-6" aria-hidden="true" tabindex="-1"></a>  w3 <span class="ot">=</span> w3<span class="sc">/</span><span class="fu">sum</span>(w3)</span>
<span id="cb166-7"><a href="#cb166-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb166-8"><a href="#cb166-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the second tree</span></span>
<span id="cb166-9"><a href="#cb166-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb166-10"><a href="#cb166-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb166-11"><a href="#cb166-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">30</span><span class="sc">*</span>w2)</span>
<span id="cb166-12"><a href="#cb166-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb166-13"><a href="#cb166-13" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">f2</span>(xgrid)</span>
<span id="cb166-14"><a href="#cb166-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb166-15"><a href="#cb166-15" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb166-16"><a href="#cb166-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb166-17"><a href="#cb166-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># weights after the second tree</span></span>
<span id="cb166-18"><a href="#cb166-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb166-19"><a href="#cb166-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb166-20"><a href="#cb166-20" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">30</span><span class="sc">*</span>w3)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-240-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>And then we have the third step. Combining all three steps and their decision function, we have the final classifier</p>
<p><span class="math display">\[\begin{align}
F_3(x) =&amp; \sum_{t=1}^3 \alpha_t f_t(x) \nonumber \\
=&amp; 0.4236 \cdot f_1(x) + 0.6496 \cdot f_2(x) + 0.9229 \cdot f_3(x)
\end{align}\]</span></p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="#cb167-1" aria-hidden="true" tabindex="-1"></a>  f3 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x[, <span class="dv">1</span>] <span class="sc">&lt;</span> <span class="fl">0.85</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb167-2"><a href="#cb167-2" aria-hidden="true" tabindex="-1"></a>  e3 <span class="ot">=</span> <span class="fu">sum</span>(w3<span class="sc">*</span>(<span class="fu">f3</span>(X) <span class="sc">!=</span> y))</span>
<span id="cb167-3"><a href="#cb167-3" aria-hidden="true" tabindex="-1"></a>  a3 <span class="ot">=</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>e3)<span class="sc">/</span>e3)</span>
<span id="cb167-4"><a href="#cb167-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb167-5"><a href="#cb167-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the third tree</span></span>
<span id="cb167-6"><a href="#cb167-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb167-7"><a href="#cb167-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb167-8"><a href="#cb167-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">30</span><span class="sc">*</span>w3)</span>
<span id="cb167-9"><a href="#cb167-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb167-10"><a href="#cb167-10" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">f3</span>(xgrid)</span>
<span id="cb167-11"><a href="#cb167-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb167-12"><a href="#cb167-12" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb167-13"><a href="#cb167-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb167-14"><a href="#cb167-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the final decision rule </span></span>
<span id="cb167-15"><a href="#cb167-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb167-16"><a href="#cb167-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb167-17"><a href="#cb167-17" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb167-18"><a href="#cb167-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb167-19"><a href="#cb167-19" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">=</span> a1<span class="sc">*</span><span class="fu">f1</span>(xgrid) <span class="sc">+</span> a2<span class="sc">*</span><span class="fu">f2</span>(xgrid) <span class="sc">+</span> a3<span class="sc">*</span><span class="fu">f3</span>(xgrid)</span>
<span id="cb167-20"><a href="#cb167-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb167-21"><a href="#cb167-21" aria-hidden="true" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb167-22"><a href="#cb167-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.25</span>) <span class="co"># f1</span></span>
<span id="cb167-23"><a href="#cb167-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">h =</span> <span class="fl">0.65</span>) <span class="co"># f2</span></span>
<span id="cb167-24"><a href="#cb167-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.85</span>) <span class="co"># f3</span></span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-241-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="training-error-of-adaboost" class="section level2 hasAnchor" number="18.2">
<h2 class="hasAnchor"><span class="header-section-number">18.2</span> Training Error of AdaBoost<a href="#training-error-of-adaboost" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There is an interesting property about the boosting algorithm that if we can always find a classifier that performs better than random guessing at each iteration <span class="math inline">\(t\)</span>, then the training error will eventually converge to zero. This works by analyzing the weight after the last iteration <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[\begin{align}
w_i^{(T+1)} =&amp; \frac{1}{Z_T} w_i^{(T)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\} \nonumber \\
=&amp; \frac{1}{Z_1\cdots Z_T} w_i^{(1)} \prod_{t = 1}^T \exp\big\{ - \alpha_t y_i f_t(x_i) \big\} \nonumber \\
=&amp; \frac{1}{Z_1\cdots Z_T} \frac{1}{n} \exp\Big\{ - y_i \sum_{t = 1}^T \alpha_t f_t(x_i) \Big\}
\end{align}\]</span></p>
<p>Since <span class="math inline">\(\sum_{t = 1}^T \alpha_t f_t(x_i)\)</span> is just the model at the <span class="math inline">\(T\)</span>-th iteration, we can write it as <span class="math inline">\(F_T(x_i)\)</span>. Noticing that they sum up to 1, we have</p>
<p><span class="math display">\[1 = \sum_{i = 1}^n w_i^{(T+1)} = \frac{1}{Z_1\cdots Z_T} \frac{1}{n} \sum_{i = 1}^n \exp\big\{ - y_i F_T(x_i) \big\}\]</span>
and
<span class="math display">\[Z_1\cdots Z_T = \frac{1}{n} \sum_{i = 1}^n \exp\big\{ - y_i F_T(x_i) \big\}\]</span>
On the right-hand-side, this is the exponential loss after we fit the model. In fact, this quantity would bound above the 0/1 loss, since the exponential loss is <span class="math inline">\(\exp[ - y f(x) ]\)</span>,</p>
<ul>
<li>For correctly classified subjects, <span class="math inline">\(y f(x) &gt; 0\)</span>, and <span class="math inline">\(\exp[ - y f(x) ] &gt; 0\)</span></li>
<li>For incorrectly classified subjects, <span class="math inline">\(y f(x) &lt; 0\)</span> the exponential loss is larger than 1</li>
</ul>
<p>This means that</p>
<p><span class="math display">\[Z_1\cdots Z_T &gt; \frac{1}{n} \sum_{i = 1}^n \mathbf{1} \big\{ y_i \neq \text{sign}(F_T(x_i)) \big\}\]</span>
Hence, if we want the final model to have low training error, we should bound above the <span class="math inline">\(Z_t\)</span>’s. Recall that <span class="math inline">\(Z_t\)</span> is used to normalize the weights, we have</p>
<p><span class="math display">\[Z_t = \sum_i^{n} w_i^{(t)} \exp[ - \alpha_t y_i f_t(x_i) ].\]</span>
We have two cases at this iteration, <span class="math inline">\(y_i f(x_i) = 1\)</span> for correct subjects, and <span class="math inline">\(y_i f(x_i) = -1\)</span> for the incorrect ones, hence,
By our definition, <span class="math inline">\(\epsilon_t = \sum_i w_i^{(t)} \mathbf{1} \big\{ y_i \neq f_t(x_i) \big\}\)</span> is the proportion of weights for mis-classified samples.
<span class="math display">\[\begin{align}
Z_t =&amp; \,\,\sum_{i=1}^n w_i^{(t)} \exp[ - \alpha_t y_i f_t(x_i)] \nonumber\\
=&amp;\,\,\sum_{y_i = f_t(x_i)} w_i^{(t)} \exp[ - \alpha_t ] +  \sum_{y_i \neq f_t(x_i)} w_i^{(t)} \exp[ \alpha_t ] \nonumber\\
=&amp; \,\, \exp[ - \alpha_t ] \sum_{y_i = f_t(x_i)} w_i^{(t)} + \exp[ \alpha_t ] \sum_{y_i \neq f_t(x_i)} w_i^{(t)}
\end{align}\]</span></p>
<p>So we have</p>
<p><span class="math display">\[ Z_t = (1 - \epsilon_t) \exp[ - \alpha_t ] + \epsilon_t \exp[ \alpha_t ].\]</span></p>
<p>If we want to minimize the product of all <span class="math inline">\(Z_t\)</span>’s, we can consider minimizing each of them. Let’s consider this as a function of <span class="math inline">\(\alpha_t\)</span>, then by taking a derivative with respect to <span class="math inline">\(\alpha_t\)</span>, we have</p>
<p><span class="math display">\[ - (1 - \epsilon_t) \exp[ - \alpha_t ] + \epsilon_t \exp[ \alpha_t ] = 0\]</span>
and</p>
<p><span class="math display">\[\alpha_t = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t}.\]</span>
Plugging this back into <span class="math inline">\(Z_t\)</span>, we have</p>
<p><span class="math display">\[Z_t = 2 \sqrt{\epsilon_t(1-\epsilon_t)}\]</span>
Since <span class="math inline">\(\epsilon_t(1-\epsilon_t)\)</span> can only attain maximum of <span class="math inline">\(1/4\)</span>, <span class="math inline">\(Z_t\)</span> must be smaller than 1. This makes the product <span class="math inline">\(Z_1 \cdots Z_T\)</span> converging to 0. If we look at this more closely, by defining <span class="math inline">\(\gamma_t = \frac{1}{2} - \epsilon_t\)</span> as the improvement from a random model (with error <span class="math inline">\(1/2\)</span>), then</p>
<p><span class="math display">\[\begin{align}
Z_t =&amp; 2 \sqrt{\epsilon_t(1-\epsilon_t)} \nonumber \\
=&amp; \sqrt{1 - 4 \gamma_t^2} \nonumber \\
\leq&amp; \exp\big[ - 2 \gamma_t^2 \big]
\end{align}\]</span></p>
<p>The last equation is because by Taylor expansion, <span class="math inline">\(\exp\big[ - 4 \gamma_t^2 \big] \geq 1 - 4 \gamma_t^2\)</span>. Then, we can finally put all <span class="math inline">\(Z_t\)</span>’s together:</p>
<p><span class="math display">\[\begin{align}
\text{Training Error} =&amp; \sum_{i = 1}^n \mathbf{1} \big\{ y_i \neq \text{sign}(F_T(x_i)) \big\} \nonumber \\
=&amp; \sum_{i = 1}^n \exp \big[ - y_i \neq F_T(x_i) \big] \nonumber \\
=&amp; Z_1 \cdots Z_T \nonumber \\
\leq&amp; \exp \big[ - 2 \sum_{t=1}^T \gamma_t^2 \big],
\end{align}\]</span></p>
<p>which converges to 0 as long as <span class="math inline">\(\sum_{t=1}^T \gamma_t^2\)</span> accumulates up to infinite. But of course, in practice, it would increasing difficult find <span class="math inline">\(f_t(x)\)</span> that reduces the training error greatly.</p>
</div>
<div id="gradient-boosting" class="section level2 hasAnchor" number="18.3">
<h2 class="hasAnchor"><span class="header-section-number">18.3</span> Gradient Boosting<a href="#gradient-boosting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s take an alternative view of this problem, we use an additive structure to fit models</p>
<p><span class="math display">\[F_T(x) = \sum_{t = 1}^T \alpha_t f(x; \boldsymbol \theta_t)\]</span></p>
<p>by minimizing a loss function</p>
<p><span class="math display">\[\underset{\{\alpha_t, \boldsymbol \theta_t\}_{t=1}^T}{\min} \sum_{i=1}^n L\big(y_i, F_T(x_i)\big)\]</span>
In this framework, we may choose a loss function <span class="math inline">\(L\)</span> that is suitable for the problem, and also choose the base learner <span class="math inline">\(f(x; \boldsymbol \theta)\)</span> with parameter <span class="math inline">\(\boldsymbol \theta\)</span>. Examples of this include linear function, spline, tree, etc.. While it maybe difficult to minimize over all parameters <span class="math inline">\(\{\alpha_t, \boldsymbol \theta_t\}_{t=1}^T\)</span>, we may consider doing this in a stage-wise fashion. The algorithm could work in the following way:</p>
<ul>
<li>Set <span class="math inline">\(F_0(x) = 0\)</span></li>
<li>For <span class="math inline">\(t = 1, \ldots, T\)</span>
<ul>
<li>Choose <span class="math inline">\((\alpha_t, \boldsymbol \theta_t)\)</span> to minimize the loss
<span class="math display">\[\underset{\alpha, \boldsymbol \theta}{\min} \,\, \sum_{i=1}^n L\big(y_i, F_{t-1}(x_i) + \alpha f(x_i; \boldsymbol \theta)\big)\]</span></li>
<li>Update <span class="math inline">\(F_t(x) = F_{t-1}(x) + \alpha_t f(x; \boldsymbol \theta_t)\)</span></li>
</ul></li>
<li>Output <span class="math inline">\(F_T(x)\)</span> as the final model</li>
</ul>
<p>The previous AdaBoost example is using exponential loss function. Also, it doesn’t pick an optimal <span class="math inline">\(f(x; \boldsymbol \theta)\)</span> at each step. We just need a model that is better than random. The step size <span class="math inline">\(\alpha_t\)</span> is optimized at each <span class="math inline">\(t\)</span> given the fitted <span class="math inline">\(f(x; \boldsymbol \theta_t)\)</span>.</p>
<p>Another example is the forward stage-wise linear regression. In this case, we fit a single variable linear model at each step <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[f(x, j) = \text{sign}\big(\text{Cor}(X_j, \mathbf{r})\big) X_j\]</span>
* <span class="math inline">\(\mathbf{r}\)</span> is the residual, as <span class="math inline">\(r_i = y_i - F_{t-1}(x_i)\)</span>
* <span class="math inline">\(j\)</span> is the index that has the largest absolute correlation with <span class="math inline">\(\mathbf{r}\)</span></p>
<p>Then we give a very small step size <span class="math inline">\(\alpha_t\)</span>, say, <span class="math inline">\(\alpha_t = 10^{-5}\)</span>, and with sign equal to the correlation between <span class="math inline">\(X_j\)</span>. In this case, <span class="math inline">\(F_t(x)\)</span> is almost equivalent to the Lasso solution path, as <span class="math inline">\(t\)</span> increases.</p>
<p>We may notice that <span class="math inline">\(r_i\)</span> is in fact the negative gradient of the squared-error loss, as a function of the fitted function:</p>
<p><span class="math display">\[r_{it} = - \left[ \frac{\partial \, \big(y_i - F(x_i)\big)^2 }{\partial \, F(x_i)} \right]_{F(x_i) = F_{t-1}(x_i)}\]</span>
and we are essentially fitting a weak leaner <span class="math inline">\(f_t(x)\)</span> to the residuals and update the fitted model <span class="math inline">\(F_t(x)\)</span>. The following example shows the result of using a tree leaner as <span class="math inline">\(f_t(x)\)</span>:</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(gbm)</span>
<span id="cb168-2"><a href="#cb168-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-3"><a href="#cb168-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># a simple regression problem</span></span>
<span id="cb168-4"><a href="#cb168-4" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb168-5"><a href="#cb168-5" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.001</span>)</span>
<span id="cb168-6"><a href="#cb168-6" aria-hidden="true" tabindex="-1"></a>  fx <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(<span class="dv">3</span><span class="sc">*</span>pi<span class="sc">*</span>x)</span>
<span id="cb168-7"><a href="#cb168-7" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">fx</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x))</span>
<span id="cb168-8"><a href="#cb168-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-9"><a href="#cb168-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">ylab =</span> <span class="st">&quot;y&quot;</span>, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb168-10"><a href="#cb168-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the true regression line</span></span>
<span id="cb168-11"><a href="#cb168-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(x, <span class="fu">fx</span>(x), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-243-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We can see that the fitted model progressively approaximates the true function.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fit regression boosting</span></span>
<span id="cb169-2"><a href="#cb169-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># I use a very large shrinkage value for demonstrating the functions</span></span>
<span id="cb169-3"><a href="#cb169-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># in practice you should use 0.1 or even smaller values for stability</span></span>
<span id="cb169-4"><a href="#cb169-4" aria-hidden="true" tabindex="-1"></a>  gbm.fit <span class="ot">=</span> <span class="fu">gbm</span>(y<span class="sc">~</span>x, <span class="at">data =</span> <span class="fu">data.frame</span>(x, y), <span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span>,</span>
<span id="cb169-5"><a href="#cb169-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">n.trees=</span><span class="dv">300</span>, <span class="at">shrinkage=</span><span class="fl">0.5</span>, <span class="at">bag.fraction=</span><span class="fl">0.8</span>)</span>
<span id="cb169-6"><a href="#cb169-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb169-7"><a href="#cb169-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># somehow, cross-validation for 1 dimensional problem creates error</span></span>
<span id="cb169-8"><a href="#cb169-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># gbm(y ~ ., data = data.frame(x, y), cv.folds = 3) # this produces an error  </span></span>
<span id="cb169-9"><a href="#cb169-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb169-10"><a href="#cb169-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the fitted regression function at several iterations</span></span>
<span id="cb169-11"><a href="#cb169-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb169-12"><a href="#cb169-12" aria-hidden="true" tabindex="-1"></a>  size<span class="ot">=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">50</span>,<span class="dv">100</span>,<span class="dv">300</span>)</span>
<span id="cb169-13"><a href="#cb169-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb169-14"><a href="#cb169-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)</span>
<span id="cb169-15"><a href="#cb169-15" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb169-16"><a href="#cb169-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb169-17"><a href="#cb169-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">ylab =</span> <span class="st">&quot;y&quot;</span>, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb169-18"><a href="#cb169-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(x, <span class="fu">fx</span>(x), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>)</span>
<span id="cb169-19"><a href="#cb169-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb169-20"><a href="#cb169-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this returns the fitted function, but not class</span></span>
<span id="cb169-21"><a href="#cb169-21" aria-hidden="true" tabindex="-1"></a>    Fx <span class="ot">=</span> <span class="fu">predict</span>(gbm.fit, <span class="at">n.trees=</span>size[i])</span>
<span id="cb169-22"><a href="#cb169-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(x, Fx, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb169-23"><a href="#cb169-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="fu">paste</span>(<span class="st">&quot;# of Iterations = &quot;</span>, size[i]))</span>
<span id="cb169-24"><a href="#cb169-24" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-244-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>This idea can be generalized to any loss function <span class="math inline">\(L\)</span>. This is the <strong>gradient boosting</strong> model:</p>
<ul>
<li>At each iteration <span class="math inline">\(t\)</span>, calculate ``pseudo-residuals’’, i.e., the negative gradient for each observation
<span class="math display">\[g_{it} = - \left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F(x_i) = F_{t-1}(x_i)}\]</span></li>
<li>Fit <span class="math inline">\(f_t(x, \boldsymbol \theta_t)\)</span> to pseudo-residual <span class="math inline">\(g_{it}\)</span>’s</li>
<li>Search for the best
<span class="math display">\[\alpha_t = \underset{\alpha}{\arg\min} \sum_{i=1}^n L\big(y_i, F_{t-1}(x_i) + \alpha f(x_i; \boldsymbol \theta_t)\big)\]</span></li>
<li>Update <span class="math inline">\(F_t(x) = F_{t-1}(x) + \alpha_t f(x; \boldsymbol \theta_t)\)</span></li>
</ul>
<p>Hence, the only change when modeling different outcomes is to choose the loss function <span class="math inline">\(L\)</span>, and derive the pseudo-residuals</p>
<ul>
<li>For regression, the loss is <span class="math inline">\(\frac{1}{2} (y - f(x))^2\)</span>, and the pseudo-residual is <span class="math inline">\(y_i - f(x_i)\)</span></li>
<li>For quantile regression to model median, the loss is <span class="math inline">\(|y - f(x)|\)</span>, and the pseudo-residual is sign<span class="math inline">\((y_i - f(x_i))\)</span> \</li>
<li>For classification, we can use the deviance <span class="math inline">\(y\log(p) + (1-y)\log(1-p)\)</span>, and express <span class="math inline">\(p\)</span> as the log-odds of a scale predictor, i.e., <span class="math inline">\(f = \log(p/(1-p))\)</span>. Then the pseudo-residual is <span class="math inline">\(y_i - p(x_i)\)</span></li>
</ul>
<!--chapter:end:05.6-boosting.Rmd-->
</div>
</div>
<div id="part-unsupervised-learning" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART) Unsupervised Learning<a href="#part-unsupervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="k-means" class="section level1 hasAnchor" number="19">
<h1 class="hasAnchor"><span class="header-section-number">19</span> K-Means<a href="#k-means" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="basic-concepts" class="section level2 hasAnchor" number="19.1">
<h2 class="hasAnchor"><span class="header-section-number">19.1</span> Basic Concepts<a href="#basic-concepts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <span class="math inline">\(k\)</span>-means clustering algorithm attempts to solve the following optimization problem:</p>
<p><span class="math display">\[ \underset{C, \, \{m_k\}_{k=1}^K}\min \sum_{k=1}^K \sum_{C(i) = k} \lVert x_i - m_k \rVert^2, \]</span>
where <span class="math inline">\(C(\cdot): \{1, \ldots, n\} \rightarrow \{1, \ldots, K\}\)</span> is a cluster assignment function, and <span class="math inline">\(m_k\)</span>’s are the cluster means. To solve this problem, <span class="math inline">\(k\)</span>-means uses an iterative approach that updates <span class="math inline">\(C(\cdot)\)</span> and <span class="math inline">\(m_k\)</span>’s alternatively. Suppose we have a set of six observations.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-247-1.png" width="35%" style="display: block; margin: auto;" /></p>
<p>We first randomly assign them into two clusters (initiate a random <span class="math inline">\(C\)</span> function). Based on this cluster assignment, we can calculate the corresponding cluster mean <span class="math inline">\(m_k\)</span>’s.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-249-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Then we will assign each observation to the closest cluster mean. In this example, only the blue point on the top will be moved to a new cluster. Then the cluster means can then be recalculated.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-250-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>When there is nothing to move anymore, the algorithm stops. Keep in mind that we started with a random cluster assignment, and this objective function is not convex. Hence we may obtain different results if started with different values. The solution is to try different starting points and use the best final results. This can be tuned using the <code>nstart</code> parameter in the <code>kmeans()</code> function.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># some random data</span></span>
<span id="cb170-2"><a href="#cb170-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb170-3"><a href="#cb170-3" aria-hidden="true" tabindex="-1"></a>    mat <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">1000</span>), <span class="dv">50</span>, <span class="dv">20</span>)</span>
<span id="cb170-4"><a href="#cb170-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb170-5"><a href="#cb170-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if we use only one starting point</span></span>
<span id="cb170-6"><a href="#cb170-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">kmeans</span>(mat, <span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">1</span>)<span class="sc">$</span>tot.withinss</span>
<span id="cb170-7"><a href="#cb170-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 885.8913</span></span>
<span id="cb170-8"><a href="#cb170-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb170-9"><a href="#cb170-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if we use multiple starting point and pick the best one</span></span>
<span id="cb170-10"><a href="#cb170-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">kmeans</span>(mat, <span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">100</span>)<span class="sc">$</span>tot.withinss</span>
<span id="cb170-11"><a href="#cb170-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 883.8241</span></span></code></pre></div>
</div>
<div id="example-1-iris-data" class="section level2 hasAnchor" number="19.2">
<h2 class="hasAnchor"><span class="header-section-number">19.2</span> Example 1: <code>iris</code> data<a href="#example-1-iris-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use the classical <code>iris</code> data as an example. This dataset contains three different classes, but the goal here is to learn the clusters without knowing the class labels.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot the original data using two variables</span></span>
<span id="cb171-2"><a href="#cb171-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">head</span>(iris)</span>
<span id="cb171-3"><a href="#cb171-3" aria-hidden="true" tabindex="-1"></a><span class="do">##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species</span></span>
<span id="cb171-4"><a href="#cb171-4" aria-hidden="true" tabindex="-1"></a><span class="do">## 1          5.1         3.5          1.4         0.2  setosa</span></span>
<span id="cb171-5"><a href="#cb171-5" aria-hidden="true" tabindex="-1"></a><span class="do">## 2          4.9         3.0          1.4         0.2  setosa</span></span>
<span id="cb171-6"><a href="#cb171-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 3          4.7         3.2          1.3         0.2  setosa</span></span>
<span id="cb171-7"><a href="#cb171-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 4          4.6         3.1          1.5         0.2  setosa</span></span>
<span id="cb171-8"><a href="#cb171-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 5          5.0         3.6          1.4         0.2  setosa</span></span>
<span id="cb171-9"><a href="#cb171-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 6          5.4         3.9          1.7         0.4  setosa</span></span>
<span id="cb171-10"><a href="#cb171-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(ggplot2)</span>
<span id="cb171-11"><a href="#cb171-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(iris, <span class="fu">aes</span>(Petal.Length, Petal.Width, <span class="at">color =</span> Species)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-252-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>The last two variables in the <code>iris</code> data carry more information on separating the three classes. Hence we will only use the <code>Petal.Length</code> and <code>Petal.Width</code>.</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(colorspace)</span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">2</span>), <span class="at">xpd =</span> <span class="cn">TRUE</span>)</span>
<span id="cb172-3"><a href="#cb172-3" aria-hidden="true" tabindex="-1"></a>    MASS<span class="sc">::</span><span class="fu">parcoord</span>(iris[, <span class="sc">-</span><span class="dv">5</span>], <span class="at">col =</span> <span class="fu">rainbow_hcl</span>(<span class="dv">3</span>)[iris<span class="sc">$</span>Species], </span>
<span id="cb172-4"><a href="#cb172-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">var.label =</span> <span class="cn">TRUE</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb172-5"><a href="#cb172-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="at">x =</span> <span class="fl">1.2</span>, <span class="at">y =</span> <span class="fl">1.3</span>, <span class="at">cex =</span> <span class="dv">1</span>,</span>
<span id="cb172-6"><a href="#cb172-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">as.character</span>(<span class="fu">levels</span>(iris<span class="sc">$</span>Species)),</span>
<span id="cb172-7"><a href="#cb172-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">fill =</span> <span class="fu">rainbow_hcl</span>(<span class="dv">3</span>), <span class="at">horiz =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-254-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Let’s perform the <span class="math inline">\(k\)</span>-means clustering</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb173-2"><a href="#cb173-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-3"><a href="#cb173-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># k mean clustering</span></span>
<span id="cb173-4"><a href="#cb173-4" aria-hidden="true" tabindex="-1"></a>  iris.kmean <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(iris[, <span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">20</span>)</span>
<span id="cb173-5"><a href="#cb173-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb173-6"><a href="#cb173-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the center of each class</span></span>
<span id="cb173-7"><a href="#cb173-7" aria-hidden="true" tabindex="-1"></a>  iris.kmean<span class="sc">$</span>centers</span>
<span id="cb173-8"><a href="#cb173-8" aria-hidden="true" tabindex="-1"></a><span class="do">##   Petal.Length Petal.Width</span></span>
<span id="cb173-9"><a href="#cb173-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 1     1.462000    0.246000</span></span>
<span id="cb173-10"><a href="#cb173-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 2     5.595833    2.037500</span></span>
<span id="cb173-11"><a href="#cb173-11" aria-hidden="true" tabindex="-1"></a><span class="do">## 3     4.269231    1.342308</span></span>
<span id="cb173-12"><a href="#cb173-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb173-13"><a href="#cb173-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the within cluster variation </span></span>
<span id="cb173-14"><a href="#cb173-14" aria-hidden="true" tabindex="-1"></a>  iris.kmean<span class="sc">$</span>withinss</span>
<span id="cb173-15"><a href="#cb173-15" aria-hidden="true" tabindex="-1"></a><span class="do">## [1]  2.02200 16.29167 13.05769</span></span>
<span id="cb173-16"><a href="#cb173-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb173-17"><a href="#cb173-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the between cluster variation </span></span>
<span id="cb173-18"><a href="#cb173-18" aria-hidden="true" tabindex="-1"></a>  iris.kmean<span class="sc">$</span>betweenss</span>
<span id="cb173-19"><a href="#cb173-19" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 519.524</span></span>
<span id="cb173-20"><a href="#cb173-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb173-21"><a href="#cb173-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the fitted clusters vs. the truth</span></span>
<span id="cb173-22"><a href="#cb173-22" aria-hidden="true" tabindex="-1"></a>  iris.kmean<span class="sc">$</span>cluster <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(iris.kmean<span class="sc">$</span>cluster)</span>
<span id="cb173-23"><a href="#cb173-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb173-24"><a href="#cb173-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(iris, <span class="fu">aes</span>(Petal.Length, Petal.Width, <span class="at">color =</span> Species)) <span class="sc">+</span> <span class="co"># true cluster</span></span>
<span id="cb173-25"><a href="#cb173-25" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.3</span>, <span class="at">size =</span> <span class="fl">3.5</span>) <span class="sc">+</span> </span>
<span id="cb173-26"><a href="#cb173-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&#39;red&#39;</span>, <span class="st">&#39;green&#39;</span>, <span class="st">&#39;blue&#39;</span>)) <span class="sc">+</span></span>
<span id="cb173-27"><a href="#cb173-27" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>)[iris.kmean<span class="sc">$</span>cluster]) <span class="co"># fitted cluster </span></span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-255-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="example-2-clustering-of-image-pixels" class="section level2 hasAnchor" number="19.3">
<h2 class="hasAnchor"><span class="header-section-number">19.3</span> Example 2: clustering of image pixels<a href="#example-2-clustering-of-image-pixels" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s first load and plot an image of Leo.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(jpeg)</span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a>    img<span class="ot">&lt;-</span><span class="fu">readJPEG</span>(<span class="st">&quot;data/leo.jpg&quot;</span>)</span>
<span id="cb174-3"><a href="#cb174-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb174-4"><a href="#cb174-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate a blank image</span></span>
<span id="cb174-5"><a href="#cb174-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">rep</span>(<span class="fl">0.2</span>, <span class="dv">4</span>))</span>
<span id="cb174-6"><a href="#cb174-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">400</span>), <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">500</span>), <span class="at">xaxt =</span> <span class="st">&#39;n&#39;</span>, <span class="at">yaxt =</span> <span class="st">&#39;n&#39;</span>, </span>
<span id="cb174-7"><a href="#cb174-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">bty =</span> <span class="st">&#39;n&#39;</span>, <span class="at">pch =</span> <span class="st">&#39;&#39;</span>, <span class="at">ylab =</span> <span class="st">&#39;&#39;</span>, <span class="at">xlab =</span> <span class="st">&#39;&#39;</span>)</span>
<span id="cb174-8"><a href="#cb174-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-9"><a href="#cb174-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rasterImage</span>(img, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">400</span>, <span class="dv">500</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-256-1.png" width="35%" style="display: block; margin: auto;" /></p>
<p>For a <code>jpg</code> file, each pixel is stored as a vector with 3 elements — representing red, green and blue intensities. However, by the way, that this objective <code>img</code> being constructed, it is stored as a 3d array. The first two dimensions are the height and width of the figure. We need to vectorize them and treat each pixel as an observation.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="#cb175-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">dim</span>(img)</span>
<span id="cb175-2"><a href="#cb175-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 500 400   3</span></span>
<span id="cb175-3"><a href="#cb175-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb175-4"><a href="#cb175-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this apply function applies vecterization to each layer (r/g/b) of the image. </span></span>
<span id="cb175-5"><a href="#cb175-5" aria-hidden="true" tabindex="-1"></a>    img_expand <span class="ot">=</span> <span class="fu">apply</span>(img, <span class="dv">3</span>, c)</span>
<span id="cb175-6"><a href="#cb175-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb175-7"><a href="#cb175-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and now we have the desired data matrix</span></span>
<span id="cb175-8"><a href="#cb175-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">dim</span>(img_expand)</span>
<span id="cb175-9"><a href="#cb175-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 200000      3</span></span></code></pre></div>
<p>Before performing the <span class="math inline">\(k\)</span>-mean clustering, let’s have a quick peek at the data in a 3d view. Since there are too many observations, we randomly sample a few.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(scatterplot3d)</span>
<span id="cb176-2"><a href="#cb176-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb176-3"><a href="#cb176-3" aria-hidden="true" tabindex="-1"></a>    sub_pixels <span class="ot">=</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(img_expand), <span class="dv">1000</span>)</span>
<span id="cb176-4"><a href="#cb176-4" aria-hidden="true" tabindex="-1"></a>    sub_img_expand <span class="ot">=</span> img_expand[sub_pixels, ]</span>
<span id="cb176-5"><a href="#cb176-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb176-6"><a href="#cb176-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scatterplot3d</span>(sub_img_expand, <span class="at">pch =</span> <span class="dv">19</span>, </span>
<span id="cb176-7"><a href="#cb176-7" aria-hidden="true" tabindex="-1"></a>                  <span class="at">xlab =</span> <span class="st">&quot;red&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;green&quot;</span>, <span class="at">zlab =</span> <span class="st">&quot;blue&quot;</span>, </span>
<span id="cb176-8"><a href="#cb176-8" aria-hidden="true" tabindex="-1"></a>                  <span class="at">color =</span> <span class="fu">rgb</span>(sub_img_expand[,<span class="dv">1</span>], sub_img_expand[,<span class="dv">2</span>],</span>
<span id="cb176-9"><a href="#cb176-9" aria-hidden="true" tabindex="-1"></a>                              sub_img_expand[,<span class="dv">3</span>]))</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-258-1.png" width="55%" style="display: block; margin: auto;" /></p>
<p>The next step is to perform the <span class="math inline">\(k\)</span>-mean and obtain the cluster label. For example, let’s try 5 clusters.</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="#cb177-1" aria-hidden="true" tabindex="-1"></a>  kmeanfit <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(img_expand, <span class="dv">5</span>)</span>
<span id="cb177-2"><a href="#cb177-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-3"><a href="#cb177-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># to produce the new graph, we simply replicate the cluster mean </span></span>
<span id="cb177-4"><a href="#cb177-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># for all observations in the same cluster</span></span>
<span id="cb177-5"><a href="#cb177-5" aria-hidden="true" tabindex="-1"></a>  new_img_expand <span class="ot">=</span> kmeanfit<span class="sc">$</span>centers[kmeanfit<span class="sc">$</span>cluster, ]</span>
<span id="cb177-6"><a href="#cb177-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb177-7"><a href="#cb177-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># now we need to convert this back to the array that can be plotted as an image. </span></span>
<span id="cb177-8"><a href="#cb177-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># this is a lazy way to do it, but get the job done</span></span>
<span id="cb177-9"><a href="#cb177-9" aria-hidden="true" tabindex="-1"></a>  new_img <span class="ot">=</span> img</span>
<span id="cb177-10"><a href="#cb177-10" aria-hidden="true" tabindex="-1"></a>  new_img[, , <span class="dv">1</span>] <span class="ot">=</span> <span class="fu">matrix</span>(new_img_expand[,<span class="dv">1</span>], <span class="dv">500</span>, <span class="dv">400</span>)</span>
<span id="cb177-11"><a href="#cb177-11" aria-hidden="true" tabindex="-1"></a>  new_img[, , <span class="dv">2</span>] <span class="ot">=</span> <span class="fu">matrix</span>(new_img_expand[,<span class="dv">2</span>], <span class="dv">500</span>, <span class="dv">400</span>)</span>
<span id="cb177-12"><a href="#cb177-12" aria-hidden="true" tabindex="-1"></a>  new_img[, , <span class="dv">3</span>] <span class="ot">=</span> <span class="fu">matrix</span>(new_img_expand[,<span class="dv">3</span>], <span class="dv">500</span>, <span class="dv">400</span>)</span>
<span id="cb177-13"><a href="#cb177-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-14"><a href="#cb177-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the new image</span></span>
<span id="cb177-15"><a href="#cb177-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">400</span>), <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">500</span>), <span class="at">xaxt =</span> <span class="st">&#39;n&#39;</span>, <span class="at">yaxt =</span> <span class="st">&#39;n&#39;</span>, <span class="at">bty =</span> <span class="st">&#39;n&#39;</span>, </span>
<span id="cb177-16"><a href="#cb177-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="st">&#39;&#39;</span>, <span class="at">ylab =</span> <span class="st">&#39;&#39;</span>, <span class="at">xlab =</span> <span class="st">&#39;&#39;</span>)</span>
<span id="cb177-17"><a href="#cb177-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-18"><a href="#cb177-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rasterImage</span>(new_img, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">400</span>, <span class="dv">500</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-259-1.png" width="35%" style="display: block; margin: auto;" /></p>
<p>With this technique, we can easily reproduce results with different <span class="math inline">\(k\)</span> values. Apparently, as <span class="math inline">\(k\)</span> increases, we get better resolution. <span class="math inline">\(k = 30\)</span> seems to recover the original image fairly well.</p>
<pre><code>## Warning: did not converge in 10 iterations</code></pre>
<p><img src="SMLR_files/figure-html/unnamed-chunk-261-1.png" width="100%" style="display: block; margin: auto;" /></p>
<!--chapter:end:06.1-kmeans.Rmd-->
</div>
</div>
<div id="hierarchical-clustering" class="section level1 hasAnchor" number="20">
<h1 class="hasAnchor"><span class="header-section-number">20</span> Hierarchical Clustering<a href="#hierarchical-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="basic-concepts-1" class="section level2 hasAnchor" number="20.1">
<h2 class="hasAnchor"><span class="header-section-number">20.1</span> Basic Concepts<a href="#basic-concepts-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we have a set of six observations:</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-265-1.png" width="30%" style="display: block; margin: auto;" /></p>
<p>The goal is to progressively group them together until there is only one group. During this process, we will always choose the closest two groups (some may be individuals) to merge.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-267-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>If we evaluate the distance between two observations, that would be very easy. For example, the Euclidean distance and Hamming distance can be used. But what about the distance between two groups? Suppose we have two groups of observations <span class="math inline">\(G\)</span> and <span class="math inline">\(H\)</span>, then several distance metric can be considered:</p>
<ul>
<li><strong>Complete linkage</strong>: the furthest pair
<span class="math display">\[d(G, H) = \underset{i \in G, \, j \in G}{\max} d(x_i, x_j)\]</span></li>
<li><strong>Single linkage</strong>: the closest pair
<span class="math display">\[d(G, H) = \underset{i \in G, \, j \in G}{\min} d(x_i, x_j)\]</span></li>
<li><strong>Average linkage</strong>: average distance
<span class="math display">\[d(G, H) = \frac{1}{n_G n_H} \sum_{i \in G} \sum_{i \in H} d(x_i, x_j)\]</span></li>
</ul>
<p>The <code>R</code> function <code>hclust()</code> uses the complete linkage as default. To perform a hierarchical clustering, we need to know all the pair-wise distances, i.e., <span class="math inline">\(d(x_i, x_j)\)</span>. Let’s consider the Euclidean distance.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the Euclidean distance can be computed using dist()</span></span>
<span id="cb179-2"><a href="#cb179-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.matrix</span>(<span class="fu">dist</span>(x))</span>
<span id="cb179-3"><a href="#cb179-3" aria-hidden="true" tabindex="-1"></a><span class="do">##          1         2         3         4         5         6</span></span>
<span id="cb179-4"><a href="#cb179-4" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 0.000000 1.2294164 1.7864196 1.1971565 1.4246185 1.5698349</span></span>
<span id="cb179-5"><a href="#cb179-5" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 1.229416 0.0000000 2.3996575 0.8727261 1.9243764 2.2708670</span></span>
<span id="cb179-6"><a href="#cb179-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 3 1.786420 2.3996575 0.0000000 2.8586738 0.4782442 0.2448835</span></span>
<span id="cb179-7"><a href="#cb179-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 1.197156 0.8727261 2.8586738 0.0000000 2.4219048 2.6741260</span></span>
<span id="cb179-8"><a href="#cb179-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 5 1.424618 1.9243764 0.4782442 2.4219048 0.0000000 0.4204479</span></span>
<span id="cb179-9"><a href="#cb179-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 6 1.569835 2.2708670 0.2448835 2.6741260 0.4204479 0.0000000</span></span></code></pre></div>
<p>We use this distance matrix in the hierarchical clustering algorithm <code>hclust()</code>. The <code>plot()</code> function will display the merging process. This should be exactly the same as we demonstrated previously.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-270-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>The height of each split represents how separated the two subsets are (the distance when they are merged). Selecting the number of clusters is still a tricky problem. Usually, we pick a cutoff where the height of the next split is short. Hence, the above example fits well with two clusters.</p>
</div>
<div id="example-1-iris-data-1" class="section level2 hasAnchor" number="20.2">
<h2 class="hasAnchor"><span class="header-section-number">20.2</span> Example 1: <code>iris</code> data<a href="#example-1-iris-data-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <code>iris</code> data contains three clusters and four variables. We use all variables in the distance calculation and use the default complete linkage.</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a>  iris_hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(iris[, <span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>]))</span>
<span id="cb180-2"><a href="#cb180-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(iris_hc)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-271-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>This does not seem to perform very well, considering that we know the true number of classes is three. This shows that, in practice, the detected clusters can heavily depend on the variables you use. Let’s try some other linkage functions.</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a>  iris_hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(iris[, <span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>]), <span class="at">method =</span> <span class="st">&quot;average&quot;</span>)</span>
<span id="cb181-2"><a href="#cb181-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(iris_hc, <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-272-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>This looks better, at least more consistent with the truth. Now we can also consider using other package to plot this result. For example, the <code>ape</code> package provides some interesting choices.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="#cb182-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(ape)</span>
<span id="cb182-2"><a href="#cb182-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="fu">as.phylo</span>(iris_hc), <span class="at">type =</span> <span class="st">&quot;unrooted&quot;</span>, <span class="at">cex =</span> <span class="fl">0.6</span>, <span class="at">no.margin =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-273-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We can also add the true class colors to the plot. This plot is motivated by the <code>dendextend</code> package vignettes. Of course in a realistic situation, we wouldn’t know what the true class is.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-274-1.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="example-2-rna-expression-data" class="section level2 hasAnchor" number="20.3">
<h2 class="hasAnchor"><span class="header-section-number">20.3</span> Example 2: RNA Expression Data<a href="#example-2-rna-expression-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use a tissue gene expression dataset from the <code>tissuesGeneExpression</code> library, available from bioconductor. I prepared the data to include only 100 genes. You can download the data from the course website. In this first step, we simply plot the data using a heatmap. By default, a heatmap uses red to denote higher values, and yellow for lower values. Note that we first plot the data without organizing the columns or rows. The data is also standardized based on columns (genes).</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">load</span>(<span class="st">&quot;data/tissue.Rda&quot;</span>)</span>
<span id="cb183-2"><a href="#cb183-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">dim</span>(expression)</span>
<span id="cb183-3"><a href="#cb183-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 189 100</span></span>
<span id="cb183-4"><a href="#cb183-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">table</span>(tissue)</span>
<span id="cb183-5"><a href="#cb183-5" aria-hidden="true" tabindex="-1"></a><span class="do">## tissue</span></span>
<span id="cb183-6"><a href="#cb183-6" aria-hidden="true" tabindex="-1"></a><span class="do">##  cerebellum       colon endometrium hippocampus      kidney       liver    placenta </span></span>
<span id="cb183-7"><a href="#cb183-7" aria-hidden="true" tabindex="-1"></a><span class="do">##          38          34          15          31          39          26           6</span></span>
<span id="cb183-8"><a href="#cb183-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">head</span>(expression[, <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>])</span>
<span id="cb183-9"><a href="#cb183-9" aria-hidden="true" tabindex="-1"></a><span class="do">##                 211298_s_at 203540_at 211357_s_at</span></span>
<span id="cb183-10"><a href="#cb183-10" aria-hidden="true" tabindex="-1"></a><span class="do">## GSM11805.CEL.gz    7.710426  5.856596   12.618471</span></span>
<span id="cb183-11"><a href="#cb183-11" aria-hidden="true" tabindex="-1"></a><span class="do">## GSM11814.CEL.gz    4.741010  5.813841    5.116707</span></span>
<span id="cb183-12"><a href="#cb183-12" aria-hidden="true" tabindex="-1"></a><span class="do">## GSM11823.CEL.gz   11.730652  5.986338   13.206078</span></span>
<span id="cb183-13"><a href="#cb183-13" aria-hidden="true" tabindex="-1"></a><span class="do">## GSM11830.CEL.gz    5.061337  6.316815    9.780614</span></span>
<span id="cb183-14"><a href="#cb183-14" aria-hidden="true" tabindex="-1"></a><span class="do">## GSM12067.CEL.gz    4.955245  6.561705    8.589003</span></span>
<span id="cb183-15"><a href="#cb183-15" aria-hidden="true" tabindex="-1"></a><span class="do">## GSM12075.CEL.gz   10.469501  5.880740   13.050554</span></span>
<span id="cb183-16"><a href="#cb183-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">heatmap</span>(<span class="fu">scale</span>(expression), <span class="at">Rowv =</span> <span class="cn">NA</span>, <span class="at">Colv =</span> <span class="cn">NA</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-275-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Hierarchical clustering may help us discover interesting patterns. If we reorganize the columns and rows based on the clusters, then it may reveal underlying subclasses of issues, or subgroups of genes.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="#cb184-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">heatmap</span>(<span class="fu">scale</span>(expression))</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-276-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Note that there are many other <code>R</code> packages that produce more interesting plots. For example, you can try the <a href="https://cran.r-project.org/web/packages/heatmaply/vignettes/heatmaply.html">heatmaply</a> package.</p>
<!--chapter:end:06.2-hclust.Rmd-->
</div>
</div>
<div id="principle-component-analysis" class="section level1 hasAnchor" number="21">
<h1 class="hasAnchor"><span class="header-section-number">21</span> Principle Component Analysis<a href="#principle-component-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="basic-concepts-2" class="section level2 hasAnchor" number="21.1">
<h2 class="hasAnchor"><span class="header-section-number">21.1</span> Basic Concepts<a href="#basic-concepts-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Principle Component Analysis (PCA) is arguably the most commonly used approach for dimension reduction and visualization. The idea is to capture major signals of variation in a dataset. A nice demonstration of the search of direction is provided at this <a href="https://www.r-bloggers.com/principal-component-analysis-in-r/">r-bloggers</a> site:</p>
<center>
<img src="images/PCA2.gif" />
</center>
<p>Let’s look at a two-dimensional case, we are trying to find a line (direction) on this plain, such that if all points are projected onto this line, their coordinates have the largest variance, compared with any other line. The following code is used to generate a set of observations.</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate some random data from a 2-dimensional normal distribution. </span></span>
<span id="cb185-2"><a href="#cb185-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(MASS)</span>
<span id="cb185-3"><a href="#cb185-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb185-4"><a href="#cb185-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb185-5"><a href="#cb185-5" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb185-6"><a href="#cb185-6" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.5</span>, <span class="sc">-</span><span class="fl">0.65</span>, <span class="sc">-</span><span class="fl">0.65</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb185-7"><a href="#cb185-7" aria-hidden="true" tabindex="-1"></a>  x_org <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), Sigma)</span>
<span id="cb185-8"><a href="#cb185-8" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> <span class="fu">scale</span>(x_org, <span class="at">scale =</span> <span class="cn">FALSE</span>, <span class="at">center =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="#cb186-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>), <span class="at">ylim=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>), </span>
<span id="cb186-2"><a href="#cb186-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.75</span>)</span>
<span id="cb186-3"><a href="#cb186-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb186-4"><a href="#cb186-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-280-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Let’ start with finding a direction to project all the observations onto. And we want this projection to have the largest variation. Of course the direction that goes along the spread of the data would be the best choice for the purpose of large variance.</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>), <span class="at">ylim=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.75</span>)</span>
<span id="cb187-2"><a href="#cb187-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb187-3"><a href="#cb187-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb187-4"><a href="#cb187-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-5"><a href="#cb187-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This line is obtained from performing PCA</span></span>
<span id="cb187-6"><a href="#cb187-6" aria-hidden="true" tabindex="-1"></a>  pc1 <span class="ot">=</span> <span class="fu">princomp</span>(x)<span class="sc">$</span>loadings[,<span class="dv">1</span>]</span>
<span id="cb187-7"><a href="#cb187-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> pc1[<span class="dv">2</span>]<span class="sc">/</span>pc1[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-281-1.png" width="40%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb188-2"><a href="#cb188-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The direction </span></span>
<span id="cb188-3"><a href="#cb188-3" aria-hidden="true" tabindex="-1"></a>  pc1</span>
<span id="cb188-4"><a href="#cb188-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1]  0.5659608 -0.8244322</span></span></code></pre></div>
<p>Once we have the first direction, we can also remove the projection from the original covariates, and search for a direction <span class="math inline">\(\mathbf{v}_2\)</span> that is orthogonal to <span class="math inline">\(\mathbf{v}_1\)</span>, with <span class="math inline">\(\mathbf{v}_1^\text{T}\mathbf{v}_2 = 0\)</span>, such that it contains large variation of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>))</span>
<span id="cb189-2"><a href="#cb189-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), <span class="at">ylim=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb189-3"><a href="#cb189-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb189-4"><a href="#cb189-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb189-5"><a href="#cb189-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-6"><a href="#cb189-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># largest PC </span></span>
<span id="cb189-7"><a href="#cb189-7" aria-hidden="true" tabindex="-1"></a>  pc1 <span class="ot">=</span> <span class="fu">princomp</span>(x)<span class="sc">$</span>loadings[,<span class="dv">1</span>]</span>
<span id="cb189-8"><a href="#cb189-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> pc1[<span class="dv">2</span>]<span class="sc">/</span>pc1[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb189-9"><a href="#cb189-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-10"><a href="#cb189-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># second largest PC</span></span>
<span id="cb189-11"><a href="#cb189-11" aria-hidden="true" tabindex="-1"></a>  pc2 <span class="ot">=</span> <span class="fu">princomp</span>(x)<span class="sc">$</span>loadings[,<span class="dv">2</span>]</span>
<span id="cb189-12"><a href="#cb189-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> pc2[<span class="dv">2</span>]<span class="sc">/</span>pc2[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-282-1.png" width="40%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="#cb190-1" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb190-2"><a href="#cb190-2" aria-hidden="true" tabindex="-1"></a>  pc2</span>
<span id="cb190-3"><a href="#cb190-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8244322 0.5659608</span></span>
<span id="cb190-4"><a href="#cb190-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">t</span>(pc1) <span class="sc">%*%</span> pc2</span>
<span id="cb190-5"><a href="#cb190-5" aria-hidden="true" tabindex="-1"></a><span class="do">##      [,1]</span></span>
<span id="cb190-6"><a href="#cb190-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]    0</span></span></code></pre></div>
<p>We can also see how much variation these two directions accounts for in the original data. The following shows the corresponding standard deviation.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">princomp</span>(x)<span class="sc">$</span>sdev</span>
<span id="cb191-2"><a href="#cb191-2" aria-hidden="true" tabindex="-1"></a><span class="do">##    Comp.1    Comp.2 </span></span>
<span id="cb191-3"><a href="#cb191-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 1.0748243 0.2206133</span></span></code></pre></div>
<p>Formally, we can generalized this to <span class="math inline">\(\mathbf{X}\)</span> with any dimensions. And the key tool is to perform the singular value decomposition (SVD):</p>
<p><span class="math display">\[\mathbf{X}= \mathbf{U}\mathbf{D}\mathbf{V}^\text{T}\]</span>
The <span class="math inline">\(\mathbf{V}\)</span> matrix here corresponds to the directions we found. Hence, <span class="math inline">\(\mathbf{v}_1\)</span> is its first column, <span class="math inline">\(\mathbf{v}_1\)</span> is its second column, etc.. <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix ordered from the largest to the smallest values, correspond to the standard deviation of the spreads. And <span class="math inline">\(\mathbf{U}\)</span> represents the coordinates once we project <span class="math inline">\(\mathbf{X}\)</span> onto those directions. An alternative way to understand this is by matrix approximation, if we want to find a rank-1 matrix that best approximate <span class="math inline">\(\mathbf{X}\)</span> with the Frobenius norm, we optimize</p>
<p><span class="math display">\[\text{minimize} \quad \lVert \mathbf{X}- \mathbf{u}_1 d_1 \mathbf{v}_1^\text{T}\rVert_2^2\]</span></p>
<p>This can be generalized into any dimensional problem. Another alternative formulation is to use eigen-decomposition of <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span>, which can be written as</p>
<p><span class="math display">\[\mathbf{X}^\text{T}\mathbf{X}= \mathbf{V}\mathbf{D}\mathbf{U}^\text{T}\mathbf{U}\mathbf{D}\mathbf{V}^\text{T}= \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}\]</span></p>
<p>But one thing we usually need to take care of is the centering issue. This is why we used <code>scale()</code> function at the beginning. However, we only center, but not scale the data. If we do not center, then the first principle component (PC) could be a direction that points to the center of the data. Note that when <span class="math inline">\(\mathbf{X}\)</span> is already centered, <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span> is the covariance matrix. Hence PCA is also performing eigen-decomposition to the covariance matrix.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="#cb192-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x_org, <span class="at">main =</span> <span class="st">&quot;Before Centering&quot;</span>, </span>
<span id="cb192-2"><a href="#cb192-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>), <span class="at">ylim=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb192-3"><a href="#cb192-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb192-4"><a href="#cb192-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb192-5"><a href="#cb192-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb192-6"><a href="#cb192-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">0.3</span>))</span>
<span id="cb192-7"><a href="#cb192-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x, <span class="at">main =</span> <span class="st">&quot;After Centering&quot;</span>, </span>
<span id="cb192-8"><a href="#cb192-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>), <span class="at">ylim=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb192-9"><a href="#cb192-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb192-10"><a href="#cb192-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)    </span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-285-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Finally, for any dimensional data <span class="math inline">\(\mathbf{X}\)</span>, we usually visualize them in the first two directions, or three. Note that the coordinates on the PC’s can be obtained using either the <code>scores</code> (<span class="math inline">\(\mathbf{U}\)</span>) in the fitted object of <code>princomp</code>, or simply multiply the original data matrix by the loading matrix <span class="math inline">\(\mathbf{V}\)</span>.</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a>    pcafit <span class="ot">&lt;-</span> <span class="fu">princomp</span>(x)</span>
<span id="cb193-2"><a href="#cb193-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-3"><a href="#cb193-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the new coordinates on PC&#39;s</span></span>
<span id="cb193-4"><a href="#cb193-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">head</span>(pcafit<span class="sc">$</span>scores)</span>
<span id="cb193-5"><a href="#cb193-5" aria-hidden="true" tabindex="-1"></a><span class="do">##           Comp.1      Comp.2</span></span>
<span id="cb193-6"><a href="#cb193-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]  0.88434533  0.13503607</span></span>
<span id="cb193-7"><a href="#cb193-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,] -0.08990294 -0.01851954</span></span>
<span id="cb193-8"><a href="#cb193-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]  1.13589963  0.20234582</span></span>
<span id="cb193-9"><a href="#cb193-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,] -1.78763374 -0.04571218</span></span>
<span id="cb193-10"><a href="#cb193-10" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,] -0.26536435  0.14271170</span></span>
<span id="cb193-11"><a href="#cb193-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,]  1.11779894 -0.41759594</span></span>
<span id="cb193-12"><a href="#cb193-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb193-13"><a href="#cb193-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># direct calculation based on projection </span></span>
<span id="cb193-14"><a href="#cb193-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">head</span>(x <span class="sc">%*%</span> pcafit<span class="sc">$</span>loadings)</span>
<span id="cb193-15"><a href="#cb193-15" aria-hidden="true" tabindex="-1"></a><span class="do">##           Comp.1      Comp.2</span></span>
<span id="cb193-16"><a href="#cb193-16" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]  0.88434533  0.13503607</span></span>
<span id="cb193-17"><a href="#cb193-17" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,] -0.08990294 -0.01851954</span></span>
<span id="cb193-18"><a href="#cb193-18" aria-hidden="true" tabindex="-1"></a><span class="do">## [3,]  1.13589963  0.20234582</span></span>
<span id="cb193-19"><a href="#cb193-19" aria-hidden="true" tabindex="-1"></a><span class="do">## [4,] -1.78763374 -0.04571218</span></span>
<span id="cb193-20"><a href="#cb193-20" aria-hidden="true" tabindex="-1"></a><span class="do">## [5,] -0.26536435  0.14271170</span></span>
<span id="cb193-21"><a href="#cb193-21" aria-hidden="true" tabindex="-1"></a><span class="do">## [6,]  1.11779894 -0.41759594</span></span>
<span id="cb193-22"><a href="#cb193-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-23"><a href="#cb193-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># visualize the data on the PCs</span></span>
<span id="cb193-24"><a href="#cb193-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Note that the both axies are scaled </span></span>
<span id="cb193-25"><a href="#cb193-25" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>, <span class="fl">4.2</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>))</span>
<span id="cb193-26"><a href="#cb193-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(pcafit<span class="sc">$</span>scores[,<span class="dv">1</span>], pcafit<span class="sc">$</span>scores[,<span class="dv">2</span>], <span class="at">xlab =</span> <span class="st">&quot;First PC&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Second PC&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb193-27"><a href="#cb193-27" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb193-28"><a href="#cb193-28" aria-hidden="true" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-287-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>There are many different functions in <code>R</code> that performs PCA. <code>princomp</code> and <code>prcomp</code> are the most popular ones.</p>
<div id="note-scaling" class="section level3 hasAnchor" number="21.1.1">
<h3 class="hasAnchor"><span class="header-section-number">21.1.1</span> Note: Scaling<a href="#note-scaling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You should always center the variables when performing PCA, however, whether to use scaling (force each variable to have a standard deviation of 1) depends on the particular application. When you have variables that are extremely disproportionate, e.g., age vs. RNA expression, scaling should be used. This is to prevent some variables from dominating the PC loadings due to their large scales. When all the variables are of the similar type, e.g., color intensities of pixels in a figure, it is better to use the original scale. This is because the variables with larger variations may carry more signal. Scaling may lose that information.</p>
</div>
</div>
<div id="example-1-iris-data-2" class="section level2 hasAnchor" number="21.2">
<h2 class="hasAnchor"><span class="header-section-number">21.2</span> Example 1: <code>iris</code> Data<a href="#example-1-iris-data-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use the <code>iris</code> data again. All four variables are considered in this analysis. We plot the first and second PC directions.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="#cb194-1" aria-hidden="true" tabindex="-1"></a>    iris_pc <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(iris[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb194-2"><a href="#cb194-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(ggplot2)</span>
<span id="cb194-3"><a href="#cb194-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(iris_pc<span class="sc">$</span>x), <span class="fu">aes</span>(<span class="at">x=</span>PC1, <span class="at">y=</span>PC2)) <span class="sc">+</span> </span>
<span id="cb194-4"><a href="#cb194-4" aria-hidden="true" tabindex="-1"></a>        <span class="fu">geom_point</span>(<span class="at">color=</span><span class="fu">c</span>(<span class="st">&quot;chartreuse4&quot;</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>)[iris<span class="sc">$</span>Species], <span class="at">size =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-288-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>One may be interested in plotting all pair-wise direction to see if lower PC’s provide useful information.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pairs</span>(iris_pc<span class="sc">$</span>x, <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;chartreuse4&quot;</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>)[iris<span class="sc">$</span>Species], <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-289-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>However, usually, the lower PC’s are less informative. This can also be speculated from the eigenvalue plot, which shows how influential each PC is.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="#cb196-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(iris_pc, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">main =</span> <span class="st">&quot;Iris PCA Variance&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-290-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Feature contributions to the PC can be accessed through the magnitude of the loadings. This table shows that <code>Petal.Length</code> is the most influential variable on the first PC, with loading <span class="math inline">\(\approx 0.8567\)</span>.</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a>    iris_pc<span class="sc">$</span>rotation</span>
<span id="cb197-2"><a href="#cb197-2" aria-hidden="true" tabindex="-1"></a><span class="do">##                      PC1         PC2         PC3        PC4</span></span>
<span id="cb197-3"><a href="#cb197-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Sepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872</span></span>
<span id="cb197-4"><a href="#cb197-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Sepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231</span></span>
<span id="cb197-5"><a href="#cb197-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Petal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390</span></span>
<span id="cb197-6"><a href="#cb197-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Petal.Width   0.35828920  0.07548102 -0.54583143  0.7536574</span></span></code></pre></div>
<p>We can further visualize this on a plot. This can be helpful when the number of variables is large.</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="#cb198-1" aria-hidden="true" tabindex="-1"></a>    features <span class="ot">=</span> <span class="fu">row.names</span>(iris_pc<span class="sc">$</span>rotation)</span>
<span id="cb198-2"><a href="#cb198-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(iris_pc<span class="sc">$</span>rotation), <span class="fu">aes</span>(<span class="at">x=</span>PC1, <span class="at">y=</span>PC2, <span class="at">label=</span>features,<span class="at">color=</span>features)) <span class="sc">+</span> </span>
<span id="cb198-3"><a href="#cb198-3" aria-hidden="true" tabindex="-1"></a>        <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">geom_text</span>(<span class="at">size=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-292-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="example-2-handwritten-digits" class="section level2 hasAnchor" number="21.3">
<h2 class="hasAnchor"><span class="header-section-number">21.3</span> Example 2: Handwritten Digits<a href="#example-2-handwritten-digits" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The handwritten zip code digits data contains 7291 training data and 2007 testing data. Each image is a <span class="math inline">\(16 \times 16\)</span>-pixel gray-scale image. Hence they are converted to a vector of 256 variables.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb199-2"><a href="#cb199-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Handwritten Digit Recognition Data</span></span>
<span id="cb199-3"><a href="#cb199-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the first column is the true digit</span></span>
<span id="cb199-4"><a href="#cb199-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">dim</span>(zip.train)</span>
<span id="cb199-5"><a href="#cb199-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 7291  257</span></span></code></pre></div>
<p>Here is a sample of some images:</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-294-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Let’s do a simpler task, using just three letters: 1, 4 and 8.</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="#cb200-1" aria-hidden="true" tabindex="-1"></a>    zip.sub <span class="ot">=</span> zip.train[zip.train[,<span class="dv">1</span>] <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">8</span>), <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb200-2"><a href="#cb200-2" aria-hidden="true" tabindex="-1"></a>    zip.sub.truth <span class="ot">=</span> <span class="fu">as.factor</span>(zip.train[zip.train[,<span class="dv">1</span>] <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">8</span>), <span class="dv">1</span>])</span>
<span id="cb200-3"><a href="#cb200-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">dim</span>(zip.sub)</span>
<span id="cb200-4"><a href="#cb200-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 2199  256</span></span>
<span id="cb200-5"><a href="#cb200-5" aria-hidden="true" tabindex="-1"></a>    zip_pc <span class="ot">=</span> <span class="fu">prcomp</span>(zip.sub)</span>
<span id="cb200-6"><a href="#cb200-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(zip_pc, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">main =</span> <span class="st">&quot;Digits 1, 4, and 8: PCA Variance&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-295-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>The eigenvalue results suggest that the first two principal components are much more influential than the rest. A pair-wise PC plot of the first four PC’s may further confirm that speculation.</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="#cb201-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pairs</span>(zip_pc<span class="sc">$</span>x[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;chartreuse4&quot;</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>)[zip.sub.truth], <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-296-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Let’s look at the first two PCs more closely. Even without knowing the true class (no colors) we can still vaguely see 3 clusters.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="#cb202-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(ggplot2)</span>
<span id="cb202-2"><a href="#cb202-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(zip_pc<span class="sc">$</span>x), <span class="fu">aes</span>(<span class="at">x=</span>PC1, <span class="at">y=</span>PC2)) <span class="sc">+</span> </span>
<span id="cb202-3"><a href="#cb202-3" aria-hidden="true" tabindex="-1"></a>        <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-297-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Finally, let’s briefly look at the results of PCA for all 10 different digits. Of course, more PC’s are needed for this task. You can also plot other PC’s to get more information.</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="#cb203-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(colorspace)</span>
<span id="cb203-2"><a href="#cb203-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb203-3"><a href="#cb203-3" aria-hidden="true" tabindex="-1"></a>    zip_pc <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(zip.train)</span>
<span id="cb203-4"><a href="#cb203-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb203-5"><a href="#cb203-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(zip_pc, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">main =</span> <span class="st">&quot;All Digits: PCA Variance&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-298-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="#cb204-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-2"><a href="#cb204-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(<span class="fu">prcomp</span>(zip.train)<span class="sc">$</span>x), <span class="fu">aes</span>(<span class="at">x=</span>PC1, <span class="at">y=</span>PC2)) <span class="sc">+</span> </span>
<span id="cb204-3"><a href="#cb204-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="fu">rainbow_hcl</span>(<span class="dv">10</span>)[zip.train[,<span class="dv">1</span>]<span class="sc">+</span><span class="dv">1</span>], <span class="at">size =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-298-2.png" width="45%" style="display: block; margin: auto;" /></p>
<!--chapter:end:06.3-pca.Rmd-->
</div>
</div>
<div id="self-organizing-map" class="section level1 hasAnchor" number="22">
<h1 class="hasAnchor"><span class="header-section-number">22</span> Self-Organizing Map<a href="#self-organizing-map" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="basic-concepts-3" class="section level2 hasAnchor" number="22.1">
<h2 class="hasAnchor"><span class="header-section-number">22.1</span> Basic Concepts<a href="#basic-concepts-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>I found the best demonstration of the Self-Organizing Map algorithm is the following graph that displays it over iterations. It is available at <a href="https://annalyzin.wordpress.com/2017/11/02/self-organizing-map/">this website</a>:</p>
<center>
<img src="images/SOM2.gif" width="400" />
</center>
<p>Let’s understand this by pairing it with the algorithm. There are several different algorithms available, but one of the most popular ones is proposed by <span class="citation">Kohonen (<a href="#ref-kohonen1990self" role="doc-biblioref">1990</a>)</span>. Here, we present a SOM with a 2-dimensional output. The following are the inputs:</p>
<ul>
<li><span class="math inline">\(\{x_i\}_{i=1}^n\)</span> is a set of <span class="math inline">\(n\)</span> observations, with dimension <span class="math inline">\(p\)</span> (the yellow and green dots in the figure).</li>
<li><span class="math inline">\(w_{ij}\)</span>, <span class="math inline">\(i = 1, \ldots p\)</span>, <span class="math inline">\(j = 1, \ldots q\)</span> are a grid of centers (the connected black dots). They are similar to the centers in a k-mean algorithm. However, they also preserve some geometric relationships among <span class="math inline">\(w_{ij}\)</span>’s, meaning that <span class="math inline">\(w_{ij}\)</span>’s are closer if their indices <span class="math inline">\({i, j}\)</span> are closer (connected in the figure).</li>
<li><span class="math inline">\(\alpha\)</span> this is a learning rate between <span class="math inline">\([0, 1]\)</span>. This controls how fast the <span class="math inline">\(w_{ij}\)</span>’s are updated.</li>
<li><span class="math inline">\(r\)</span> is also a tuning parameter. This controls how many <span class="math inline">\(w_{ij}\)</span>’s will be updated at each iteration</li>
</ul>
<p>Now, we look at the algorithm. This is different from <span class="math inline">\(k\)</span>-means because we do not use all the observations immediately. The algorithm works by stream-in the observations one-by-one. Whenever a new observation <span class="math inline">\(x_k\)</span>, <span class="math inline">\(k = 1, \ldots, n\)</span> comes in, we will update the centers <span class="math inline">\(w_{ij}\)</span>’s by the following:</p>
<ul>
<li>For all <span class="math inline">\(w_{ij}\)</span>, calculate the distance between each <span class="math inline">\(w_{ij}\)</span> and <span class="math inline">\(x_k\)</span>. Let <span class="math inline">\(d_{ij} = \lVert x_k - w_{ij} \rVert\)</span>. By default, we use Euclidean distance.</li>
<li>Select the closest <span class="math inline">\(w_{ij}\)</span>, denoted as <span class="math inline">\(w_{\ast}\)</span></li>
<li>Update each <span class="math inline">\(w_{ij}\)</span> based on the fomular <span class="math inline">\(w_{ij} = w_{ij} + \alpha \, h(w_\ast, w_{ij}, r) \, \lVert x_k - w_{ij} \rVert\)</span></li>
</ul>
<p>After each iteration (updating with one more observation), we will decrease the value of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(r\)</span>. In the <code>kohonen</code> package, the <span class="math inline">\(\alpha\)</span> starts at 0.05, and gradually decreases to 0.01, while <span class="math inline">\(r\)</span> is chosen to be 2/3 of all cluster means at the first iteration.</p>
<p>Using the <code>kohonen</code> package, we perform a SOM on the Handwritten Digit Recognition Data. The heatmap shows how each <span class="math inline">\(w_{ij}\)</span> is away from it’s neighboring <span class="math inline">\(w_{ij}\)</span>’s. The extreme bright one means that the center is quite isolated by itself.</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(kohonen)</span>
<span id="cb205-2"><a href="#cb205-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: package &#39;kohonen&#39; was built under R version 4.2.2</span></span>
<span id="cb205-3"><a href="#cb205-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb205-4"><a href="#cb205-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Attaching package: &#39;kohonen&#39;</span></span>
<span id="cb205-5"><a href="#cb205-5" aria-hidden="true" tabindex="-1"></a><span class="do">## The following object is masked from &#39;package:class&#39;:</span></span>
<span id="cb205-6"><a href="#cb205-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb205-7"><a href="#cb205-7" aria-hidden="true" tabindex="-1"></a><span class="do">##     somgrid</span></span>
<span id="cb205-8"><a href="#cb205-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-9"><a href="#cb205-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Handwritten Digit Recognition Data</span></span>
<span id="cb205-10"><a href="#cb205-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb205-11"><a href="#cb205-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-12"><a href="#cb205-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the first column is the true digit</span></span>
<span id="cb205-13"><a href="#cb205-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dim</span>(zip.train)</span>
<span id="cb205-14"><a href="#cb205-14" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 7291  257</span></span>
<span id="cb205-15"><a href="#cb205-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb205-16"><a href="#cb205-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># for speed concern, I only use a few variables (pixels)</span></span>
<span id="cb205-17"><a href="#cb205-17" aria-hidden="true" tabindex="-1"></a>  zip.SOM <span class="ot">&lt;-</span> <span class="fu">som</span>(zip.train[, <span class="fu">seq</span>(<span class="dv">2</span>, <span class="dv">257</span>, <span class="at">length.out =</span> <span class="dv">10</span>)], </span>
<span id="cb205-18"><a href="#cb205-18" aria-hidden="true" tabindex="-1"></a>                 <span class="at">grid =</span> <span class="fu">somgrid</span>(<span class="dv">10</span>, <span class="dv">10</span>, <span class="st">&quot;rectangular&quot;</span>))</span>
<span id="cb205-19"><a href="#cb205-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(zip.SOM, <span class="at">type =</span> <span class="st">&quot;dist.neighbours&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-301-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="#cb206-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(zip.SOM, <span class="at">type =</span> <span class="st">&quot;mapping&quot;</span>, <span class="at">pchs =</span> <span class="dv">20</span>, </span>
<span id="cb206-2"><a href="#cb206-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Mapping Type SOM&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-301-2.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="#cb207-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot(zip.SOM, main = &quot;Default SOM Plot&quot;)</span></span>
<span id="cb207-2"><a href="#cb207-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb207-3"><a href="#cb207-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># you can try using all the pixels</span></span>
<span id="cb207-4"><a href="#cb207-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># zip.SOM &lt;- som(zip.train[, 2:257], </span></span>
<span id="cb207-5"><a href="#cb207-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">#            grid = somgrid(10, 10, &quot;rectangular&quot;))</span></span>
<span id="cb207-6"><a href="#cb207-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot(zip.SOM, type = &quot;dist.neighbours&quot;)</span></span></code></pre></div>
<p>We could also look at the class labels (digits) coming out of the SOM. Particularly the plot on the right-hand side shows the proportion of subjects with each label for the subjects in each cluster (using a pie chart).</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb208-2"><a href="#cb208-2" aria-hidden="true" tabindex="-1"></a>    zip.SOM2 <span class="ot">&lt;-</span> <span class="fu">xyf</span>(zip.train[, <span class="fu">seq</span>(<span class="dv">2</span>, <span class="dv">257</span>, <span class="at">length.out =</span> <span class="dv">10</span>)], </span>
<span id="cb208-3"><a href="#cb208-3" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">classvec2classmat</span>(zip.train[, <span class="dv">1</span>]),</span>
<span id="cb208-4"><a href="#cb208-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">grid =</span> <span class="fu">somgrid</span>(<span class="dv">10</span>, <span class="dv">10</span>, <span class="st">&quot;hexagonal&quot;</span>), <span class="at">rlen =</span> <span class="dv">300</span>)</span>
<span id="cb208-5"><a href="#cb208-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb208-6"><a href="#cb208-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(zip.SOM2, <span class="at">type =</span> <span class="st">&quot;codes&quot;</span>, <span class="at">main =</span> <span class="fu">c</span>(<span class="st">&quot;Codes X&quot;</span>, <span class="st">&quot;Codes Y&quot;</span>))</span>
<span id="cb208-7"><a href="#cb208-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning in par(opar): argument 1 does not name a graphical parameter</span></span>
<span id="cb208-8"><a href="#cb208-8" aria-hidden="true" tabindex="-1"></a>    zip.SOM2.hc <span class="ot">&lt;-</span> <span class="fu">cutree</span>(<span class="fu">hclust</span>(<span class="fu">dist</span>(zip.SOM2<span class="sc">$</span>codes[[<span class="dv">2</span>]])), <span class="dv">10</span>)</span>
<span id="cb208-9"><a href="#cb208-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">add.cluster.boundaries</span>(zip.SOM2, zip.SOM2.hc)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-302-1.png" width="45%" style="display: block; margin: auto;" /></p>
<!--chapter:end:06.4-som.Rmd-->
</div>
</div>
<div id="spectral-clustering" class="section level1 hasAnchor" number="23">
<h1 class="hasAnchor"><span class="header-section-number">23</span> Spectral Clustering<a href="#spectral-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="basic-concepts-4" class="section level2 hasAnchor" number="23.1">
<h2 class="hasAnchor"><span class="header-section-number">23.1</span> Basic Concepts<a href="#basic-concepts-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Spectral clustering essentially consists of two steps. First, we construct the graph Laplacian <span class="math inline">\(\mathbf{L}\)</span> (or normalized version), then we perform eigen-decomposition of the matrix. Lets show an example, replicated from <span class="citation">Von Luxburg (<a href="#ref-von2007tutorial" role="doc-biblioref">2007</a>)</span>.</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="#cb209-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb209-2"><a href="#cb209-2" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">50</span></span>
<span id="cb209-3"><a href="#cb209-3" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="fl">0.2</span>), <span class="fu">rnorm</span>(n, <span class="dv">2</span>, <span class="fl">0.2</span>), <span class="fu">rnorm</span>(n, <span class="dv">4</span>, <span class="fl">0.2</span>), <span class="fu">rnorm</span>(n, <span class="dv">6</span>, <span class="fl">0.2</span>))</span>
<span id="cb209-4"><a href="#cb209-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">hist</span>(x, <span class="at">breaks =</span> <span class="dv">100</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-305-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We use the adjacency matrix defined as <span class="math display">\[w_{ij} = \exp\bigg\{\frac{- \lVert x_i - x_j\rVert^2 }{2 \sigma^2 } \bigg\},\]</span> and calculate the Laplacian <span class="math display">\[\mathbf{L} = \mathbf{D} - \mathbf{W}.\]</span> We can then use the eigen decomposition to recover the underlying features.</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># construct the adjacency matrix</span></span>
<span id="cb210-2"><a href="#cb210-2" aria-hidden="true" tabindex="-1"></a>  W <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">exp</span>(<span class="sc">-</span><span class="fu">dist</span>(<span class="fu">as.matrix</span>(x))<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="dv">4</span>)</span>
<span id="cb210-3"><a href="#cb210-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">heatmap</span>(W, <span class="at">Rowv =</span> <span class="cn">NA</span>, <span class="at">Colv=</span><span class="cn">NA</span>, <span class="at">symm =</span> <span class="cn">TRUE</span>, <span class="at">revC =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-306-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="#cb211-1" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb211-2"><a href="#cb211-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute the degree of each vertex</span></span>
<span id="cb211-3"><a href="#cb211-3" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">=</span> <span class="fu">colSums</span>(W)</span>
<span id="cb211-4"><a href="#cb211-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb211-5"><a href="#cb211-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the laplacian matrix</span></span>
<span id="cb211-6"><a href="#cb211-6" aria-hidden="true" tabindex="-1"></a>  L <span class="ot">=</span> <span class="fu">diag</span>(d) <span class="sc">-</span> W</span>
<span id="cb211-7"><a href="#cb211-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb211-8"><a href="#cb211-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># eigen-decomposition</span></span>
<span id="cb211-9"><a href="#cb211-9" aria-hidden="true" tabindex="-1"></a>  f <span class="ot">=</span> <span class="fu">eigen</span>(L, <span class="at">symmetric =</span> <span class="cn">TRUE</span>)</span>
<span id="cb211-10"><a href="#cb211-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb211-11"><a href="#cb211-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the eigen-values </span></span>
<span id="cb211-12"><a href="#cb211-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># we need the samll ones, but notice that the smallest one should be exactly zero</span></span>
<span id="cb211-13"><a href="#cb211-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="fu">rev</span>(f<span class="sc">$</span>values)[<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, </span>
<span id="cb211-14"><a href="#cb211-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">&quot;red&quot;</span>, <span class="dv">4</span>), <span class="fu">rep</span>(<span class="st">&quot;blue&quot;</span>, <span class="dv">196</span>)))</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-306-2.png" width="45%" style="display: block; margin: auto;" /></p>
<p>These are the feature embedding we obtained. But the eigen-value associated with the smallest one will not be used.</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="#cb212-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the last four eigen-vectors</span></span>
<span id="cb212-2"><a href="#cb212-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(f<span class="sc">$</span>vectors[, <span class="dv">200</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.15</span>, <span class="fl">0.15</span>))</span>
<span id="cb212-3"><a href="#cb212-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(f<span class="sc">$</span>vectors[, <span class="dv">199</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.15</span>, <span class="fl">0.15</span>))</span>
<span id="cb212-4"><a href="#cb212-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(f<span class="sc">$</span>vectors[, <span class="dv">198</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.15</span>, <span class="fl">0.15</span>))</span>
<span id="cb212-5"><a href="#cb212-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(f<span class="sc">$</span>vectors[, <span class="dv">197</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.15</span>, <span class="fl">0.15</span>))</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-308-1.png" width="95%" style="display: block; margin: auto;" /></p>
<p>On the other hand, if we use an adjacency matrix that contains several non-connected blocks, then we would observe four zero eigen-values. For example, using the KNN adjacency index, we may obtain four separated blocks.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="#cb213-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(FNN)</span>
<span id="cb213-2"><a href="#cb213-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb213-3"><a href="#cb213-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Attaching package: &#39;FNN&#39;</span></span>
<span id="cb213-4"><a href="#cb213-4" aria-hidden="true" tabindex="-1"></a><span class="do">## The following objects are masked from &#39;package:class&#39;:</span></span>
<span id="cb213-5"><a href="#cb213-5" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb213-6"><a href="#cb213-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     knn, knn.cv</span></span>
<span id="cb213-7"><a href="#cb213-7" aria-hidden="true" tabindex="-1"></a>  nn <span class="ot">=</span> <span class="fu">get.knn</span>(x, <span class="at">k=</span><span class="dv">10</span>)</span>
<span id="cb213-8"><a href="#cb213-8" aria-hidden="true" tabindex="-1"></a>  W <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="dv">200</span>, <span class="dv">200</span>)</span>
<span id="cb213-9"><a href="#cb213-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>)</span>
<span id="cb213-10"><a href="#cb213-10" aria-hidden="true" tabindex="-1"></a>    W[i, nn<span class="sc">$</span>nn.index[i, ]] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb213-11"><a href="#cb213-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb213-12"><a href="#cb213-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># W is not necessary symmetric</span></span>
<span id="cb213-13"><a href="#cb213-13" aria-hidden="true" tabindex="-1"></a>  W <span class="ot">=</span> <span class="fu">pmax</span>(W, <span class="fu">t</span>(W))</span>
<span id="cb213-14"><a href="#cb213-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb213-15"><a href="#cb213-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">heatmap</span>(W, <span class="at">Rowv =</span> <span class="cn">NA</span>, <span class="at">Colv=</span><span class="cn">NA</span>, <span class="at">symm =</span> <span class="cn">TRUE</span>, <span class="at">revC =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-309-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We use a normalized graph Laplacian, defined as</p>
<p><span class="math display">\[\mathbf{L}_\text{sym} = \mathbf{I} - \mathbf{D^{-1/2} \mathbf{W} D^{-1/2}}\]</span></p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="#cb214-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compute the degree of each vertex</span></span>
<span id="cb214-2"><a href="#cb214-2" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">=</span> <span class="fu">colSums</span>(W)</span>
<span id="cb214-3"><a href="#cb214-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb214-4"><a href="#cb214-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the laplacian matrix</span></span>
<span id="cb214-5"><a href="#cb214-5" aria-hidden="true" tabindex="-1"></a>  L <span class="ot">=</span> <span class="fu">diag</span>(<span class="dv">200</span>) <span class="sc">-</span> <span class="fu">diag</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(d)) <span class="sc">%*%</span> W <span class="sc">%*%</span> <span class="fu">diag</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(d))</span>
<span id="cb214-6"><a href="#cb214-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb214-7"><a href="#cb214-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># eigen-decomposition</span></span>
<span id="cb214-8"><a href="#cb214-8" aria-hidden="true" tabindex="-1"></a>  f <span class="ot">=</span> <span class="fu">eigen</span>(L, <span class="at">symmetric =</span> <span class="cn">TRUE</span>)</span>
<span id="cb214-9"><a href="#cb214-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb214-10"><a href="#cb214-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the eigen-values </span></span>
<span id="cb214-11"><a href="#cb214-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># we need the smallest ones</span></span>
<span id="cb214-12"><a href="#cb214-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="fu">rev</span>(f<span class="sc">$</span>values)[<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, </span>
<span id="cb214-13"><a href="#cb214-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">&quot;red&quot;</span>, <span class="dv">4</span>), <span class="fu">rep</span>(<span class="st">&quot;blue&quot;</span>, <span class="dv">196</span>)))</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-311-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="#cb215-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the last four eigen-vectors</span></span>
<span id="cb215-2"><a href="#cb215-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(f<span class="sc">$</span>vectors[, <span class="dv">200</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.15</span>, <span class="fl">0.15</span>))</span>
<span id="cb215-3"><a href="#cb215-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(f<span class="sc">$</span>vectors[, <span class="dv">199</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.15</span>, <span class="fl">0.15</span>))</span>
<span id="cb215-4"><a href="#cb215-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(f<span class="sc">$</span>vectors[, <span class="dv">198</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.15</span>, <span class="fl">0.15</span>))</span>
<span id="cb215-5"><a href="#cb215-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(f<span class="sc">$</span>vectors[, <span class="dv">197</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.15</span>, <span class="fl">0.15</span>))</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-313-1.png" width="95%" style="display: block; margin: auto;" /></p>
<p>We can then perform <span class="math inline">\(k\)</span>-means clustering using the top eigen-vectors (except the smallest one) as the features.</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="#cb216-1" aria-hidden="true" tabindex="-1"></a>  cl <span class="ot">=</span> <span class="fu">kmeans</span>(f<span class="sc">$</span>vectors[, <span class="dv">197</span><span class="sc">:</span><span class="dv">199</span>], <span class="at">centers =</span> <span class="dv">4</span>, <span class="at">nstart =</span> <span class="dv">100</span>)</span>
<span id="cb216-2"><a href="#cb216-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, cl<span class="sc">$</span>cluster, <span class="at">ylab =</span> <span class="st">&quot;Cluster&quot;</span>, <span class="at">col =</span> cl<span class="sc">$</span>cluster, <span class="at">pch =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-315-1.png" width="95%" style="display: block; margin: auto;" /></p>
<!--chapter:end:06.5-spectral.Rmd-->
</div>
</div>
<div id="part-reference" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART) Reference<a href="#part-reference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="reference" class="section level1 hasAnchor" number="24">
<h1 class="hasAnchor"><span class="header-section-number">24</span> Reference<a href="#reference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!--chapter:end:99-reference.Rmd-->
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-aronszajn1950theory" class="csl-entry">
Aronszajn, Nachman. 1950. <span>“Theory of Reproducing Kernels.”</span> <em>Transactions of the American Mathematical Society</em> 68 (3): 337–404.
</div>
<div id="ref-boser1992training" class="csl-entry">
Boser, Bernhard E, Isabelle M Guyon, and Vladimir N Vapnik. 1992. <span>“A Training Algorithm for Optimal Margin Classifiers.”</span> In <em>Proceedings of the Fifth Annual Workshop on Computational Learning Theory</em>, 144–52.
</div>
<div id="ref-boyd2004convex" class="csl-entry">
Boyd, Stephen, and Lieven Vandenberghe. 2004. <em>Convex Optimization</em>. Cambridge university press.
</div>
<div id="ref-breiman1996bagging" class="csl-entry">
Breiman, Leo. 1996. <span>“Bagging Predictors.”</span> <em>Machine Learning</em> 24 (2): 123–40.
</div>
<div id="ref-breiman2001random" class="csl-entry">
———. 2001. <span>“Random Forests.”</span> <em>Machine Learning</em> 45 (1): 5–32.
</div>
<div id="ref-breiman1984classification" class="csl-entry">
Breiman, Leo, Jerome H Friedman, Richard A Olshen, and Charles J Stone. 1984. <em>Classification and Regression Trees</em>. Monterey, CA: Wadsworth &amp; Brooks/Cole Advanced Books &amp; Software.
</div>
<div id="ref-cayton2005algorithms" class="csl-entry">
Cayton, Lawrence. 2005. <span>“Algorithms for Manifold Learning.”</span> <em>Univ. Of California at San Diego Tech. Rep</em> 12 (1-17): 1.
</div>
<div id="ref-cortes1995support" class="csl-entry">
Cortes, Corinna, and Vladimir Vapnik. 1995. <span>“Support-Vector Networks.”</span> <em>Machine Learning</em> 20 (3): 273–97.
</div>
<div id="ref-efron2004least" class="csl-entry">
Efron, Bradley, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. 2004. <span>“Least Angle Regression.”</span> <em>The Annals of Statistics</em> 32 (2): 407–99.
</div>
<div id="ref-freund1997decision" class="csl-entry">
Freund, Yoav, and Robert E Schapire. 1997. <span>“A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.”</span> <em>Journal of Computer and System Sciences</em> 55 (1): 119–39.
</div>
<div id="ref-friedman2001greedy" class="csl-entry">
Friedman, Jerome H. 2001. <span>“Greedy Function Approximation: A Gradient Boosting Machine.”</span> <em>Annals of Statistics</em>, 1189–1232.
</div>
<div id="ref-friedman2010regularization" class="csl-entry">
Friedman, Jerome, Trevor Hastie, and Rob Tibshirani. 2010. <span>“Regularization Paths for Generalized Linear Models via Coordinate Descent.”</span> <em>Journal of Statistical Software</em> 33 (1): 1.
</div>
<div id="ref-golub1979generalized" class="csl-entry">
Golub, Gene H, Michael Heath, and Grace Wahba. 1979. <span>“Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter.”</span> <em>Technometrics</em> 21 (2): 215–23.
</div>
<div id="ref-hastie2001elements" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. Springer series in statistics New York.
</div>
<div id="ref-ho1998random" class="csl-entry">
Ho, Tin Kam. 1998. <span>“The Random Subspace Method for Constructing Decision Forests.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 20 (8): 832–44.
</div>
<div id="ref-hoerl1970ridge" class="csl-entry">
Hoerl, Arthur E, and Robert W Kennard. 1970. <span>“Ridge Regression: Biased Estimation for Nonorthogonal Problems.”</span> <em>Technometrics</em> 12 (1): 55–67.
</div>
<div id="ref-james2013introduction" class="csl-entry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.
</div>
<div id="ref-kimeldorf1970correspondence" class="csl-entry">
Kimeldorf, George S, and Grace Wahba. 1970. <span>“A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines.”</span> <em>The Annals of Mathematical Statistics</em> 41 (2): 495–502.
</div>
<div id="ref-kohonen1990self" class="csl-entry">
Kohonen, Teuvo. 1990. <span>“The Self-Organizing Map.”</span> <em>Proceedings of the IEEE</em> 78 (9): 1464–80.
</div>
<div id="ref-li1991sliced" class="csl-entry">
Li, Ker-Chau. 1991. <span>“Sliced Inverse Regression for Dimension Reduction.”</span> <em>Journal of the American Statistical Association</em> 86 (414): 316–27.
</div>
<div id="ref-mcinnes2018umap" class="csl-entry">
McInnes, Leland, John Healy, Nathaniel Saul, and Lukas Großberger. 2018. <span>“UMAP: Uniform Manifold Approximation and Projection.”</span> <em>Journal of Open Source Software</em> 3 (29).
</div>
<div id="ref-mercer1909xvi" class="csl-entry">
Mercer, James. 1909. <span>“Xvi. Functions of Positive and Negative Type, and Their Connection the Theory of Integral Equations.”</span> <em>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character</em> 209 (441-458): 415–46.
</div>
<div id="ref-nocedal2006numerical" class="csl-entry">
Nocedal, Jorge, and Stephen Wright. 2006. <em>Numerical Optimization</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-quinlan1993c4" class="csl-entry">
Quinlan, J Ross. 1993. <em>C4. 5: Programs for Machine Learning</em>. Elsevier.
</div>
<div id="ref-tibshirani1996regression" class="csl-entry">
Tibshirani, Robert. 1996. <span>“Regression Shrinkage and Selection via the Lasso.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 58 (1): 267–88.
</div>
<div id="ref-von2007tutorial" class="csl-entry">
Von Luxburg, Ulrike. 2007. <span>“A Tutorial on Spectral Clustering.”</span> <em>Statistics and Computing</em> 17 (4): 395–416.
</div>
<div id="ref-yeh2018building" class="csl-entry">
Yeh, I-Cheng, and Tzu-Kuang Hsu. 2018. <span>“Building Real Estate Valuation Models with Comparative Approach Through Case-Based Reasoning.”</span> <em>Applied Soft Computing</em> 65: 260–71.
</div>
<div id="ref-zou2005regularization" class="csl-entry">
Zou, Hui, and Trevor Hastie. 2005. <span>“Regularization and Variable Selection via the Elastic Net.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2): 301–20.
</div>
</div>
</div>
<!--bookdown:body:end-->
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
